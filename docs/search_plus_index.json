{"./":{"url":"./","title":"简介","keywords":"","body":"Curiouser's Devops Roadmap This gitbook records the technical roadmap of Devops Curiouser. Link GitBook Access URL: https://gitbook.curiouser.top GitHub: https://github.com/RationalMonster What I had done at Openshift or Kubernetes kubernetes Common CI/CD Flow 1. Gitlab Webhook + Jenkins SharedLibraries/Kubernetes + SonarScanner Maven Plugin Gitlab CI/CD Workflow Logging Logging与Metrics Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-04 23:27:48 "},"origin/openshift-allinone安装.html":{"url":"origin/openshift-allinone安装.html","title":"Allinone","keywords":"","body":"搭建Allinone全组件Openshift 3.11 一、Overviews Prerequisite IP地址：192.168.1.86 CentOS：7.5.1804 硬盘划分 系统盘60G / 数据盘100G /var/lib/docker ; 100G /data/nfs 开启Selinuxsed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/selinux/config && \\ setenforce 0 Context Docker：版本 1.13，Overlay2(执行Ansible准备脚本时会进行安装) Openshift：版本 3.11 Kubernetes：版本 v1.11.0 二、使用Ansible安装部署 设置主机名并在本地Host文件中添加IP地址域名映射关系ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 配置中科大Openshift的YUM 源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ bash -c ' cat > /etc/yum.repos.d/all.repo 安装基础软件yum install -y git vim net-tools lrzsz unzip bind-utils yum-utils bridge-utils python-passlib wget java-1.8.0-openjdk-headless httpd-tools lvm2 安装Ansible 2.6.5 yum install -y https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm 获取openshift ansible部署脚本代码，禁用ansible脚本中的指定repo git clone https://github.com/openshift/openshift-ansible.git -b release-3.11 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2 (可选)将附件中定制化的OKD登陆页面文件放置/etc/origin/master/custom路径下（自定义的登陆首页）# 路径需要新建 mkdir -p /etc/origin/master/custom 配置Ansible部署Openshift的主机清单/etc/ansible/hosts [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] openshift_ip=192.168.1.86 openshift_public_ip=192.168.1.86 ansible_default_ipv4.address=192.168.1.86 ansible_ssh_user=root openshift_deployment_type=origin deployment_type=origin openshift_release=3.11 openshift_image_tag=v3.11.0 ansible_ssh_pass=**Root用户SSH密码** ######################### Components Cert and CA Expire Days ################# openshift_hosted_registry_cert_expire_days=36500 openshift_ca_cert_expire_days=36500 openshift_node_cert_expire_days=36500 openshift_master_cert_expire_days=36500 etcd_ca_default_days=36500 ####################### Multitenant Network ####################### os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant ####################### OKD ####################### openshift_clock_enabled=true openshift_enable_unsupported_configurations=True openshift_node_groups=[{'name': 'allinone', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage ####################### OKD master config ####################### openshift_master_api_port=8443 openshift_master_cluster_public_hostname=allinone.okd311.curiouser.com openshift_master_cluster_hostname=allinone.okd311.curiouser.com openshift_master_default_subdomain=apps.okd311.curiouser.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin':'$apr1$eG8zNL.C$fvACBzDJ7.N7KdJORT12E0'} openshift_master_oauth_template=custom/login.html openshift_master_session_name=ssn openshift_master_session_max_seconds=3600 ####################### Docker ####################### container_runtime_docker_storage_setup_device=/dev/sdb container_runtime_docker_storage_type=overlay2 openshift_examples_modify_imagestreams=true openshift_docker_options=\"--selinux-enabled -l warn --ipv6=false --insecure-registry=0.0.0.0/0 --log-opt max-size=10M --log-opt max-file=3 --registry-mirror=https://zlsoueh7.mirror.aliyuncs.com\" ####################### Web Console ####################### openshift_web_console_extension_script_urls=[\"https://xhua-static.sh1a.qingstor.com/allinone/allinone-webconsole.js\"] openshift_web_console_extension_stylesheet_urls=[\"https://hermes-uat.curiouser.com/curiouser/M00/00/3A/rBACF1vz8NyALOS3AAApT8C9PDY549.css\"] ####################### Registry ####################### openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/data/nfs openshift_hosted_registry_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=10Gi ####################### metrics ####################### openshift_metrics_install_metrics=true openshift_metrics_image_version=v3.11.0 openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/data/nfs openshift_metrics_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=10Gi ####################### logging ####################### openshift_logging_install_logging=true openshift_logging_image_version=v3.11.0 openshift_logging_es_ops_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_es_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_elasticsearch_pvc_size=5Gi openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/data/nfs openshift_logging_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=10Gi ################### Prometheus Cluster Monitoring ################### openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_prometheus_storage_enabled=true openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi ##################### Disable Components ############# openshift_enable_service_catalog=false ansible_service_broker_install=false [masters] allinone.okd311.curiouser.com [etcd] allinone.okd311.curiouser.com [nfs] allinone.okd311.curiouser.com [nodes] allinone.okd311.curiouser.com openshift_node_group_name='allinone' 创建NFS挂载目录 pvcreate /dev/sdc && \\ vgcreate -s 4m data /dev/sdc && \\ lvcreate --size 45G -n nfs data && \\ mkfs.xfs /dev/data/nfs && \\ echo \"/dev/data/nfs /data/nfs xfs defaults 0 0\" >> /etc/fstab && \\ mkdir /data/nfs -p && \\ mount -a && \\ df -mh （可选）预先拉取安装过程中可能使用的镜像docker pull docker.io/openshift/origin-node:v3.11.0 && \\ docker pull docker.io/openshift/origin-control-plane:v3.11.0 && \\ docker pull docker.io/openshift/origin-haproxy-router:v3.11.0 && \\ docker pull docker.io/openshift/origin-deployer:v3.11.0 && \\ docker pull docker.io/openshift/origin-pod:v3.11.0 && \\ docker pull docker.io/openshift/origin-docker-registry:v3.11.0 && \\ docker pull docker.io/openshift/origin-console:v3.11.0 && \\ docker pull docker.io/openshift/origin-service-catalog:v3.11.0 && \\ docker pull docker.io/openshift/origin-web-console:v3.11.0 && \\ docker pull docker.io/cockpit/kubernetes:latest && \\ docker pull docker.io/openshift/oauth-proxy:v1.1.0 && \\ docker pull docker.io/openshift/origin-docker-builder:v3.11.0 && \\ docker pull docker.io/openshift/prometheus-alertmanager:v0.15.2 && \\ docker pull docker.io/openshift/prometheus-node-exporter:v0.16.0 && \\ docker pull docker.io/openshift/prometheus:v2.3.2 && \\ docker pull docker.io/grafana/grafana:5.2.1 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker tag quay-mirror.qiniu.com/coreos/etcd:v3.2.22 quay.io/coreos/etcd:v3.2.22 && \\ docker rmi quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 quay.io/coreos/kube-state-metrics:v1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker tag quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 quay.io/coreos/configmap-reload:v0.0.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker pull quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker tag quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-config-reloader:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 执行OKD Ansible Playbook先执行安装检查的Playbook ansible-playbook /root/openshift-ansible/playbooks/prerequisites.yml 再执行安装Playbook ansible-playbook /root/openshift-ansible/playbooks/deploy_cluster.yml 授予admin用户以管理员权限 oc adm policy add-cluster-role-to-user cluster-admin admin 三、配置Openshift的后端存储 使用Ceph RBD作为后端存储 搭建单节点的Ceph，详见（Ceph RBD单节点安装） 创建Storageclass 使用NFS作为后端存储：详见 参考文章链接 当主机有多网卡时指定组件监听的网卡IP地址：https://github.com/ViaQ/Main/blob/master/README-install.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-ocp43-install.html":{"url":"origin/openshift-ocp43-install.html","title":"OCP 4.3 集群","keywords":"","body":"OCP 4.3集群安装 一、简介 Openshift从4.0时代从OKD(Openshift Kubernetes Distribution)更名为OCP（openshift container platform) 二、安装操作 1、安装架构 VM Name 硬件配置 OS IP地址 服务 ocp43-tools 4C8G100G系统盘 CentOS 7.7 192.168.1.80 PXE、DHCP 、DNS、Web Server、TFTP、NFSv4、Load Balance ocp43-bootstrap 4C8G100G系统盘 暂不安装OS后续会使用辅助节点上的PXE安装上Readhat CoreOS 192.168.1.55 临时K8S集群，用来安装OCP、安装完后可以弃用 ocp43-master1 8C16G100G系统盘 暂不安装OS后续会使用PXE安装上Readhat CoreOS 192.168.1.81 OCP集群Master节点 ocp43-worker1 8C16G100G系统盘 暂不安装OS后续会使用PXE安装上Readhat CoreOS 192.168.1.91 OCP集群Worker节点 2、需预先提供的事项 2.1、在ESXI中创建以上配置的虚拟机 可使用govc命令快速创建，参考链接 ocp43-bootstrap、ocp43-master1、ocp43-worker1虚拟机创建后先关机 ocp43-bootstrap、ocp43-master1、ocp43-worker1虚拟机的网卡Mac地址，获取方式可使用govc命令：govc device.info -vm ocp4-worker1 |grep \"MAC Address:\" 2.2、辅助节点安装PXE/DHCP/DNS/LB 可使用红帽大神开源的Ansible部署脚本，github地址：https://github.com/RedHatOfficial/ocp4-helpernode 2.3、拉取镜像的pull secret 安装过程中需要从红帽镜像仓库拉取镜像时使用的pull secret。 可在https://cloud.redhat.com/openshift/install/metal/user-provisioned中获取（需要注册登录RedHat账号） 注意：pull secret的有效期只有一天，尽快在一天时间内安装好集群 后续安装过程中可通过echo | openssl s_client -connect api-int.ocp43.curiouser.com:6443 | openssl x509 -noout -text查看有效期 3、辅助节点准备 3.1、基础环境准备 开启SELinux 固定IP地址 生成SSH密钥 设置代理 设置主机名 开启ssh-agent 安装ansible、git等基础软件 sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export NO_PROXY=\"mirrors.ustc.edu.cn,api.ocp43.curiouser.com\"' >> ~/.zshrc source ~/.zshrc hostnamectl set-hostname --static tools.ocp43.curiouser.com echo \"PREFIX=24\\nIPADDR=192.168.1.80\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 yum -y install ansible git ssh-keygen -t rsa -b 4096 -N \"\" -f ~/.ssh/id_rsa eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_rsa 由于主机名修改需要重启才能生效，所以该节点需要重启（虽然也可以不重启修改主机名、但是修改的地方较多、以防万一留下隐患、保险起见重启吧） 3.2、下载Ansible脚本 git clone https://github.com/RedHatOfficial/ocp4-helpernode 3.3、创建配置文件 bash -c 'cat > ~/ocp4-helpernode/vars.yaml 3.4、预下载安装OCP/RedHat CoreOS的二进制文件 辅助节点上的PXE服务需要给集群其他节点提供安装OCP和基础OS的二进制文件。在Ansilbe脚本执行时会去联网下载、我们可以收到先下载下来放到指定的文件夹下，加速Ansible执行速度。这些文件的下载地址可以在Ansible脚本文件中查看。 cat ~/ocp4-helpernode/vars/main.yml --- ocp_bios: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.3/4.3.8/rhcos-4.3.8-x86_64-metal.x86_64.raw.gz\" ocp_initramfs: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.3/4.3.8/rhcos-4.3.8-x86_64-installer-initramfs.x86_64.img\" ocp_install_kernel: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.3/4.3.8/rhcos-4.3.8-x86_64-installer-kernel-x86_64\" ocp_client: \"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.8/openshift-client-linux-4.3.8.tar.gz\" ocp_installer: \"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.8/openshift-install-linux-4.3.8.tar.gz\" 手动下载完后、需要将它们重名并放到指定路径下 cp openshift-client-linux-4.3.8.tar.gz /usr/local/src/openshift-client-linux.tar.gz cp openshift-install-linux-4.3.8.tar.gz /usr/local/src/openshift-install-linux.tar.gz mkdir -p /var/www/html/install/ /var/lib/tftpboot/rhcos/ /var/lib/tftpboot/rhcos/ cp rhcos-4.3.8-x86_64-metal.raw.gz /var/www/html/install/bios.raw.gz cp rhcos-4.3.8-x86_64-installer-initramfs.img /var/lib/tftpboot/rhcos/initramfs.img cp rhcos-4.3.8-x86_64-installer-kernel /var/lib/tftpboot/rhcos/kernel # 因为后续会使用到openshift安装工具openshift-instsall，所以在辅助节点也解压一份命令到可执行路径下 tar -zxvf ~/openshift-install-linux-4.3.8.tar.gz -C /usr/local/bin/ rm -rf /usr/local/bin/README.md 3.5、执行Ansible脚本 cd ~/ocp4-helpernode ansible-playbook -e @vars.yaml tasks/main.yml 3.6、检查辅助节点上安装的服务状况 /usr/local/bin/helpernodecheck {dns-masters|dns-workers|dns-etcd|dns-other|install-info|haproxy|services|nfs-info} 4、创建安装OCP配置文件 4.1、创建配置文件的定义信息文件 mkdir ~/ocp43 bash -c 'cat > ~/ocp43/install-config.yaml 4.2、使用openshift-install工具命令生成配置文件 ./openshift-install create manifests --dir=ocp43 4.3、使用openshift-install工具命令生成PXE安装CoreOS的ignition文件 ./openshift-install create ignition-configs --dir=ocp43 # 上述命令会在～/ocp43路径下生成以下文件 auth bootstrap.ign master.ign metadata.json worker.ign 将ign文件拷贝到/var/www/html/ignition目录下，并修改权限 cd ocp43 cp bootstrap.ign master.ign worker.ign /var/www/html/ignition chmod 775 /var/www/html/ignition/* cd .. 5、将集群节点虚拟机开机 要是辅助节点上所有的服务准备就绪了、此时将ocp43-bootstrap、ocp43-master1、ocp43-worker1节点虚拟机开机。它们就会发现局域网内辅助节点上的DHCP、TFTP、PXE服务，自动从网卡启动，然后在PXE服务的引导下拉取OS镜像文件安装操作系统。 6、执行等待安装完成命令 openshift-install --dir=ocp43 wait-for bootstrap-complete --log-level=info 7、注意事项！等待、等待、等待 在安装过程中，由于Bootstrap、Master、Worker节点默认时区是UTC时区，而pull secret的有效期是一天，申请完后立即安装的话。安装过程中安装组件式时报证书无效，需要等到8小时，让系统时间等到pull secret有效期。 可通过echo | openssl s_client -connect api-int.ocp43.curiouser.com:6443 | openssl x509 -noout -text查看pull secret相关证书的有效期 疑问待后续解决的：集群节点为什么会出现时区设置不一致的？为什么手动设置时区后，系统层面生效但POD没有及时生效？是不是需要改动tool节点上某个服务的配置？ 8、验证 tar -zxvf ~/openshift-client-linux-4.3.8.tar.gz -C /usr/local/bin/ export KUBECONFIG=~/ocp43/auth/kubeconfig oc get node # ./openshift-install --dir=ocp43 wait-for install-complete INFO Waiting up to 30m0s for the cluster at https://api.ocp43.curiouser.com:6443 to initialize... INFO Waiting up to 10m0s for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/ocp43/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp43.curiouser.com INFO Login to the console with user: kubeadmin, password: ***** 9、集群外访问 相关自带服务Web界面 本地hosts中配置访问的服务域名与tools节点IP地址的映射 192.168.1.80 api.ocp43.curiouser.com 192.168.1.80 oauth-openshift.apps.ocp43.curiouser.com # OCP的WebConsole 192.168.1.80 console-openshift-console.apps.ocp43.curiouser.com # OCP的用户认证服务地址 192.168.1.80 oauth-openshift.apps.ocp43.curiouser.com # OCP的HTTP WEB服务地址 192.168.1.80 downloads-openshift-console.apps.ocp43.curiouser.com # OCP的Prometheus 192.168.1.80 alertmanager-main-openshift-monitoring.apps.ocp43.curiouser.com 192.168.1.80 grafana-openshift-monitoring.apps.ocp43.curiouser.com 192.168.1.80 prometheus-k8s-openshift-monitoring.apps.ocp43.curiouser.com 192.168.1.80 thanos-querier-openshift-monitoring.apps.ocp43.curiouser.com oc/kubectl命令远程访问操作集群 kubeconfig配置文件在tools节点ocp安装配置目录下的auth路径下，拷贝出来配置在本地管理员电脑的kubectlconfig即可 oc/kubectl客户端可在OCP自带的HTTP服务中下载（https://downloads-openshift-console.apps.ocp43.curiouser.com) 三、节点扩容 参考 https://www.jianshu.com/p/72a981aec92a （非常感谢少坡同学予以的指导！） https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes的持久化存储.html":{"url":"origin/openshift-Kubernetes的持久化存储.html","title":"数据持久化","keywords":"","body":"一、Kubernetes持久化存储简介 通常情况下，我们可以认为容器或者Pod的生命周期时短暂的，当容器被销毁时，容器内部的数据也同时被清除。 对于容器，数据持久化存储的重要性不言而喻。Docker有存储卷的概念，用来将磁盘上的或另一个容器中的目录挂载到容器的某一个路径下。即使容器挂掉了，挂载Volume中的数据依旧存在。然而没有对其生命周期进行管理。而Kubernetes提供了多种不同类型资源的Volume存储卷，供POD挂载到容器的不同路径下,常见的有： emptyDir：pod被调度到某个宿主机上的时候才创建，而同一个pod内的容器都能读写EmptyDir中的同一个文件。删除容器并不会对它造成影响，只有删除整个Pod时，它才会被删除，它的生命周期与所挂载的Pod一致 hostPath：将宿主机的文件系统的文件或目录挂接到Pod中 secret：将Kubernetes中secret对象资源挂载到POD中 configMap：将Kubernetes中config对象资源挂载到POD中 persistentVolumeClaim：将PersistentVolume挂接到Pod中作为存储卷。使用此类型的存储卷，用户不需要关注存储卷的详细信息。 nfs glusterfs cephfs vspherevolume iscsi .... 对于以上大部分的volume类型，对使用用户是极其不友好的。理解他们体系中的概念配置是一件复杂的事情，有时我们其实并不关心他们的各种存储实现，只希望能够简单安全可靠地存储数据。所以K8S对存储的供应和使用做了抽象，以API形式提供给管理员和用户使用。因此引入了两个新的API资源：Persistent Volume（持久卷PV）和Persistent Volume Claim（持久卷申请PVC）。 PVC负责定义使用多大的存储空间，什么样的读写方式等常见要求即可，而PV负责抽象各种存储系统的技术细节（例如存储系统IP地址端口，客户端证书密钥等），满足PVC的存储需求，继而作为Kubernetes集群的存储对象资源。 apiVersion: \"v1\" kind: \"PersistentVolumeClaim\" metadata: name: \"ceph-pvc-test\" namespace: \"default\" spec: accessModes: - \"ReadWriteMany\" resources: requests: storage: \"2Gi\" volumeName: \"pv-nfs-test\" # 指定PV PersistentVolumesClaim的属性 Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 Volume Modes：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Resources：指定使用多大的存储空间 Selector：PVC可以指定标签选择器进行更深度的过滤PV，只有匹配了选择器标签的PV才能绑定给PVC。选择器包含两个字段： matchLabels（匹配标签） - PV必须有一个包含该值得标签 matchExpressions（匹配表达式） - 一个请求列表，包含指定的键、值的列表、关联键和值的操作符。合法的操作符包含In，NotIn，Exists，和DoesNotExist。 　　所有来自matchLabels和matchExpressions的请求，都是逻辑与关系的，它们必须全部满足才能匹配上。 Class apiVersion: v1 kind: PersistentVolume metadata: name: ceph-pv-test spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - 192.168.122.133:6789 pool: rbd image: ceph-image user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Retain claimRef: name: \"pvc-test\" namespace: \"default\" PersistentVolumes的属性 Capacity：指定存储容量大小 Volume Mode：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Class: 一个PV可以有一种class，通过设置storageClassName属性来选择指定的StorageClass。有指定class的PV只能绑定给请求该class的PVC。没有设置storageClassName属性的PV只能绑定给未请求class的PVC(过去，使用volume.beta.kubernetes.io/storage-class注解，而不是storageClassName属性。该注解现在依然可以工作，但在Kubernetes的未来版本中已经被完全弃用了) Reclaim Policy Mount Options Node Affinity Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 PersistentVolumes的周期状态 Available: 空闲的，未绑定给PVC Bound: 绑定上了某个PVC Released: PVC已经删除了，但是PV还没有被回收 Failed: PV在自动回收中失败了 PV支持的存储系统: GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CSI FC (Fibre Channel) Flexvolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) Portworx Volumes ScaleIO Volumes StorageOS PV和PVC 之间的关联遵循如下的生命周期： Provisioning-供应: PV的创建阶段，有以下两种创建方式 静态手工：集群管理员通过手工的方式创建pv 动态自动：通过PersistentVolume Controller动态调度，Kubernetes将能够按照用户的需要，根据PVC的资源请求，寻找StorageClasse定义的符合要求的底层存储自动创建其需要的存储卷。 Binding-绑定: PV分配绑定到PVC Using-使用： POD挂载使用PVC类型的Volume Reclaiming-回收：PV释放后的回收利用策略 Retain保留: 保留现场，人工回收 Delete删除: 自动删除，动态删除后端存储。需要IaaS层的支持，目前只有Ceph RBD和OpenStack Cinder支持 Recycle复用：通过rm -rf删除卷上的所有数据。目前只有NFS和HostPath支持（逐渐在抛弃该方式，建议使用） 二、使用StorageClass提供动态存储供应 通常情况下，Kubernetes集群管理员需要手工创建所需的PV存储资源。从Kubernetes 1.2以后可以使用Storageclass实现动态自动地根据用户需求创建某种存储系统类型的PV。同时，可以定义多个 StorageClass ，给集群提供不同存储系统类型的PV资源。 1. 定义创建StorageClass 每一个存储类都必须包含以下参数 provisioner: 决定由哪个Provisioner来创建PV parameters: Provisioner需要的参数,可选项：Delete(Default),Retain reclaimPolicy: PV的回收策略 可选参数： Mount Options Volume Binding Mode Allowed Topologies Note: StorageClass一旦被创建，将不能被更新 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd # 指定Provisioner provisioner: kubernetes.io/rbd parameters: monitors: 10.20.30.40:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" Kubernetes支持的Provisioner Provisioner 是否内置插件 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local StorageClas可以支持第三方的Provisioner，只要该插件符合Kubernetes的规范 内置的Provisioner名称带有“kubernetes.io”前缀 Github仓库：https://github.com/kubernetes-incubator/external-storage 有官方支持的第三方Provisioner 2. 指定StorageClass动态创建PV 在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储； 在Kubernetes v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce # 指定所使用的存储类，此存储类将会自动创建符合要求的PV storageClassName: ceph-rbd resources: requests: storage: 30Gi 3. 指定默认的StorageClass 创建StorageClass时可添加添加storageclass.kubernetes.io/is-default-class注解来指定为默认的存储类。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" # 将此storageclass设置为默认 name: nfs-client-storageclass provisioner: fuseim.pri/ifs parameters: archiveOnDelete: \"true\" 一个集群中，最多只能有一个默认的存储类 如果没有默认的存储类，在PersistentVolumeClaim中也没有显示指定storageClassName，将无法创建PersistentVolume。 参考链接 https://kubernetes.io/docs/concepts/storage/volumes/ https://kubernetes.io/docs/concepts/storage/storage-classes/ https://www.kubernetes.org.cn/4078.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-nfs-client.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-client.html","title":"NFS Client provisioner","keywords":"","body":"一、NFS Client Provisioner https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client https://www.kubernetes.org.cn/3894.html Provisioner的定义原理: openshift-Kubernetes的持久化存储 二、安装部署 1. 创建NFS服务端 yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ mkdir -p /data/nfs/appstorage-nfs-client-provisioner && \\ echo \"/data/nfs/appstorage-nfs-client-provisioner *(rw,no_root_squash,sync)\" >> /etc/exports && \\ exportfs -a && \\ showmount -e $HOSTNAME 2. 创建RBAC kind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 3. 修改Deployment并以此部署POD 先拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest quay.io/external_storage/nfs-client-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: $HOSTNAME # NFS Server的地址 - name: NFS_PATH value: /data/nfs/appstorage-nfs-client-provisioner # NFS Server要挂载的路径 volumes: - name: nfs-client-root nfs: server: $HOSTNAME #指定NFS Server的地址 path: /data/nfs/appstorage-nfs-client-provisioner #指定NFS Server要挂载的路径 三、使用 1. 创建StorageClass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. =======================================================补充内容========================================================= #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\" 删除PVC时不会保留数据，\"true\"将保留PVC的数据，形成以\"archived-\"开头的文件夹 2. 创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc #当默认storageclass就是nfs-client-storageclass，可不要该注解 annotations: volume.beta.kubernetes.io/storage-class: \"nfs-client-storageclass\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 1. 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX nfs-client-storageclass 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX Delete Bound default/test nfs-client-storageclass 10m 2. 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 3. 查看NFS目录 /data/nfs/k8s-app-nfs-storage/ └── [drwxrwxrwx 32] default-test-pvc-e8a15786-5a09-11e9-ad53-000c296286d8 ├── [-rw-r--r-- 947] 1.log └── [-rw-r--r-- 1.0K] 2.log Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-nfs-server.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-server.html","title":"NFS Server Provisioner","keywords":"","body":"一、NFS Server Provisioner Github项目地址：https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/nfs Provisioner的定义原理：openshift-Kubernetes的持久化存储 NFS Provisioner的部署文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/deployment.md NFS Provisioner的使用文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/usage.md 二、在Kubernetes上部署 1、（可选）预拉取镜像 docker pull quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest quay.io/kubernetes_incubator/nfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest 2、创建PodSecurityPolicy apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: nfs-provisioner spec: fsGroup: rule: RunAsAny allowedCapabilities: - DAC_READ_SEARCH - SYS_RESOURCE runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - secret - hostPath 3、创建RBAC kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-provisioner apiGroup: rbac.authorization.k8s.io 4、使用deployment创建POD（推荐） apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: # 定义提供者的名称，存储类通过此名称指定提供者 - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 5、（可选）使用StatefulSet创建POD apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: StatefulSet apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner serviceName: \"nfs-provisioner\" replicas: 1 template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner terminationGracePeriodSeconds: 10 containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 三、使用 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" ​ =======================================================补充内容========================================================= ​ #其他参数： gid: # \"none\" or a supplemental group like \"1001\". NFS shares will be created with permissions such that pods running with the supplemental group can read & write to the share, but non-root pods without the supplemental group cannot. Pods running as root can read & write to shares regardless of the setting here, unless the rootSquash parameter is set true. If set to \"none\", anybody root or non-root can write to the share. Default (if omitted) \"none\". rootSquash: # \"true\" or \"false\". Whether to squash root users by adding the NFS Ganesha root_id_squash or kernel root_squash option to each export. Default \"false\". mountOptions: # a comma separated list of mount options for every PV of this class to be mounted with. The list is inserted directly into every PV's mount options annotation/field without any validation. Default blank \"\". ​ #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: #注解 annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" 创建PVC时指定storageclass kind: PersistentVolumeClaim apiVersion: v1 metadata: name: nfs annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX example-nfs 5m3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX Delete Bound default/test example-nfs 5m9s 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 查看/srv目录 /srv ├── [-rw-r--r-- 4.4K] ganesha.log ├── [-rw------- 36] nfs-provisioner.identity ├── [drwxrwsrwx 32] pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 │ ├── [-rw-r--r-- 5.1K] 1.log │ └── [-rw-r--r-- 5.7K] 2.log └── [-rw------- 1.1K] vfs.conf 注意：删除掉PVC，PV也会自动删除，底层的NFS目录也会跟着删除 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-glusterfs.html":{"url":"origin/openshift-Kubernetes-provisioner-glusterfs.html","title":"Glusterfs Provisioner","keywords":"","body":"一、OKD集群中添加容器化的GlusteFS Prerequisite OKD集群（3.11）至少有三个节点 OKD官方操作指南：https://docs.okd.io/3.11/install_config/persistent_storage/persistent_storage_glusterfs.html#install-config-persistent-storage-persistent-storage-glusterfs GlusterFS官方操作指南：https://docs.gluster.org/en/latest/Administrator%20Guide/overview/ heketi-cli官方操作指南：https://github.com/heketi/heketi 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false ​ [glusterfs] allinone311.okd.curiouser.com glusterfs_devices='[ \"/dev/vdf\" ]' node1.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' node2.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' #至少是三个节点 glusterfs节点上安装软件 yum install glusterfs-fuse && \\ yum update glusterfs-fuse 配置glusterfs节点上的Selinux setsebool -P virt_sandbox_use_fusefs on && \\ setsebool -P virt_use_fusefs on 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 二、向OKD集群中添加集群外的GlusteFS 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false openshift_storage_glusterfs_is_native=false openshift_storage_glusterfs_heketi_is_native=true openshift_storage_glusterfs_heketi_executor=ssh openshift_storage_glusterfs_heketi_ssh_port=22 openshift_storage_glusterfs_heketi_ssh_user=root openshift_storage_glusterfs_heketi_ssh_sudo=false openshift_storage_glusterfs_heketi_ssh_keyfile=\"/root/.ssh/id_rsa\" ​ [glusterfs] gluster1.example.com glusterfs_ip=192.168.10.11 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster2.example.com glusterfs_ip=192.168.10.12 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster3.example.com glusterfs_ip=192.168.10.13 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 三、卸载 ansible-playbook -e \"openshift_storage_glusterfs_wipe=true\" /root/openshift-ansible/playbooks/openshift-glusterfs/uninstall.yml 四、OKD中通过storage动态使用glusterfs作为PVC的后端存储 1. 创建storage class 创建storage class(ansible playbook执行过程中会自动创建storageclass) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: 'http://heketi-storage.app-storage.svc:8080' restuser: admin secretName: heketi-storage-admin-secret secretNamespace: app-storage reclaimPolicy: Delete volumeBindingMode: Immediate 如果使用的集群外的Glusterfs集群，需要手动创建storage class。 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: \"http://10.42.0.0:8080\" restauthenabled: \"false\" 2. 创建PVC时使用Glusterfs的storage class apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gluster1 spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi storageClassName: glusterfs-storage 五、主机上mount挂载使用容器化的GlusterFS 挂载命令格式： mount -t glusterfs GlusterFS容器化pod所在的节点IP地址:/volume_name /mnt/glusterfs 示例： $ mount -t glusterfs 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 /mnt/glusterfs && \\ df -mh 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 10G 136M 9.9G 2% /mnt/glusterfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-cephfs.html":{"url":"origin/openshift-Kubernetes-provisioner-cephfs.html","title":"Ceph FileSystem Provisioner","keywords":"","body":"相关链接 官方文档： https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs Provisioner的定义原理：Kubernetes的存储--> StorageClass provisioner 姊妹篇： Preflight 1. Openshift创建cephfs命名空间 oc new-project cephfs --display-name=\"Ceph FileSystem Provisioner\" 2. 拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest quay.io/external_storage/cephfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest 一、安装部署 1. 获取Ceph Filesystem Client.admin用户的密钥环 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" 2. 创建Secrets oc create secret generic cephfs-secret-admin --from-literal=key='AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ==' --namespace=cephfs 3. 创建RBAC --- apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 4. 使用Deployment创建Ceph-FileSystem-provisioner的POD apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 二、使用 1. 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cephfs provisioner: ceph.com/cephfs parameters: monitors: allinone.okd311.curiouser.com:6789 adminId: admin adminSecretName: cephfs-secret-admin adminSecretNamespace: \"cephfs\" claimRoot: /pvc-volumes 2、创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-test spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-cephrbd.html":{"url":"origin/openshift-Kubernetes-provisioner-cephrbd.html","title":"Ceph RBD Provisioner","keywords":"","body":"一、获取ceph client admin用户的密钥环keyring 查看Ceph集群Admin节点的集群配置文件夹my-cluster下的ceph.client.admin.keyring文件来获取key值 $> cat ceph.client.admin.keyring [client.admin] key = AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA== caps mds = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" $ ceph auth list #获取所有客户端用户 $ ceph auth get client.admin #获取客户端指定用户 二、使用admin的keyring在openshift上创建secret CLI $> oc create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQAil11anEPOORAArxzRkH9iS1IOGKQfK87+Ag==' --namespace=default YAML kind: Secret apiVersion: v1 metadata: name: ceph-secret namespace: default selfLink: /api/v1/namespaces/default/secrets/ceph-secret data: key: QVFDcFNlMWJ0Y3VxSFJBQWlST25zY1VDMWpnTWRwZkRJMFd0THc9PQ== type: kubernetes.io/rbd 三、创建storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd-sc provisioner: kubernetes.io/rbd parameters: monitors: 192.168.0.26:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret #说明:adminId默认值为admin,pool默认值为rbd, userId默认值与adminId一样.所以这三个值可以不填写。 四、可以在console界面创建，也可以通过PVC的YAML配置文件中指定使用Ceph kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd-sc 结果如下图： Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-资源对象常见操作.html":{"url":"origin/openshift-资源对象常见操作.html","title":"常见资源对象操作","keywords":"","body":"常用资源对象操作 1、登录 oc project oc login -u 用户名 集群master的URL oc whoami #查看当前登录的用户，加-t参数可查看当前用户的token 2、切换Project oc project 3、查看集群节点 oc get node/no oc get node/no node1.test.openshift.com 4、查看集群节点的详细信息 oc describe node node1.test.openshift.com 5、查看某个节点上的所有Pods oc adm manage-node node1.test.openshift.com --list-pods 6、使节点禁止参与调度 oc adm manage-node router1.test.openshift.com --schedulable=false 7、疏散某个节点上的所有POD oc adm drain router1.test.openshift.com --ignore-daemonsets 8、清除旧的Build和Deployments历史版（所有namespace） 统计要清除的资源个数 #oc adm prune deployments --keep-younger-than=24h --keep-complete=5 --keep-failed=5|wc -l 确认清除动作 # oc adm prune [deployments|builds|images] --confirm --keep-younger-than=24h --keep-complete=5 --keep-failed=5 参数详解 --confirm 确认操作 --keep-younger-than=1h0m0s Specify the minimum age of a Build for it to be considered a candidate for pruning. --keep-complete=5 Per BuildConfig, specify the number of builds whose status is complete that will be preserved. --keep-failed=1 Per BuildConfig, specify the number of builds whose status is failed, error, or cancelled that will be preserved. --orphans=false If true, prune all builds whose associated BuildConfig no longer exists and whose status is complete, failed, error, or cancelled. 示例： 清理images（在admin用户下执行） # oc adm prune images --keep-younger-than=400m --keep-tag-revisions=10 --registry-url=docker-registry.default.svc:5000 --certificate-authority=/etc/origin/master/registry.crt --confirm 9、删除所有Namespace中非Running的pods for i in `oc get po --all-namespaces|grep -v \"Running\"|grep -v \"NAMESPACE\"|awk '{print $1}'|sort -u` ; do echo \"===================Namespace $i===================\"; oc -n $i delete po `oc get po -n $i |grep -v \"Running\"|grep -v \"NAME\"|awk 'BEGIN{ORS=\" \"}{print $1}'`; done 10、强制删除POD oc delete po gitlab-ce-16-ntzst --force --grace-period=0 11、资源的查看 #查看当前项目的所有资源 oc get all #查看当前项目的所有资源，外加输出label信息 oc get all --show-labels # 查看指定资源 oc get pod/po oc get service/svc oc get persistentvolumes/pv 12、通过label选择器删除namespace下所有的资源 #如果namespace下所有的资源都打上了“name=test”标签 oc delete all -l name=test 13、项目的管理 #创建项目 oc new-project --display-name=显示的项目名 --description=项目描述 project_name #删除项目 oc delete project 项目名 #查看当前处于哪个项目下 oc project #查看所有项目 oc projects 14、模板的管理 #创建模板(模板文件格式为YAML/JSON.也可以在Openshift的web页面上直接导入) oc create -f #查看模板 oc get templates #编辑模板 oc edit template #删除模板 oc delete template 附录 buildconfigs (aka 'bc') #构建配置 builds #构建版本 certificatesigningrequests (aka 'csr') clusters (valid only for federation apiservers) clusterrolebindings clusterroles componentstatuses (aka 'cs') configmaps (aka 'cm') daemonsets (aka 'ds') deployments (aka 'deploy') deploymentconfigs (aka 'dc') endpoints (aka 'ep') events (aka 'ev') horizontalpodautoscalers (aka 'hpa') imagestreamimages (aka 'isimage') imagestreams (aka 'is') imagestreamtags (aka 'istag') ingresses (aka 'ing') groups jobs limitranges (aka 'limits') namespaces (aka 'ns') networkpolicies nodes (aka 'no') persistentvolumeclaims (aka 'pvc') persistentvolumes (aka 'pv') poddisruptionbudgets (aka 'pdb') podpreset pods (aka 'po') podsecuritypolicies (aka 'psp') podtemplates policies projects replicasets (aka 'rs') replicationcontrollers (aka 'rc') resourcequotas (aka 'quota') rolebindings roles routes secrets serviceaccounts (aka 'sa') services (aka 'svc') statefulsets users storageclasses thirdpartyresources Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html":{"url":"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html","title":"将Secret和ConfigMap以文件的形式挂载到容器","keywords":"","body":"将Secret和ConfigMap以文件的形式挂载到容器 一、Context ConfigMap或者Secret在默认挂载到容器是以Volumes的形式，如果挂载路径下原有的其他文件，则会覆盖掉。 如果将挂载路径直接写成文件的绝对路径，这会在挂载路径下创建以文件名为名字的文件夹，文件会在这个文件夹下 containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret 二、操作 挂载Secret或Config类型的volume时，添加一个subPath字段即可，可将其以文件的形式挂载，而不是以目录的形式。 Secret 1. 创建secret apiVersion: v1 kind: Secret metadata: name:test-secret type: Opaque data: test.txt: >- ************************ 2. 容器中挂载secret containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume readOnly: true subPath: test.txt # .... volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret secret中的test.txt文件将会单个文件的形式挂载到/etc/test/目录下 Configmap 1. 创建ConfigMap oc create configmap crack-jar --from-file=atlassian-extras-3.2.jar --from-literal=text=atlassian-extras-3.2.jar 2. 容器中挂载ConfigMap # .... volumeMounts: - mountPath: /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/atlassian-extras-3.2.jar name: crack-jar readOnly: true subPath: atlassian-extras-3.2.jar # .... volumes: - configMap: defaultMode: 420 name: crack-jar name: crack-jar Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-集群节点管理.html":{"url":"origin/openshift-集群节点管理.html","title":"节点管理","keywords":"","body":"一、集群添加Node节点 Ansible脚本有新增节点的Playbook脚本，准备好新增节点的基础环境，在集群的ansible管理节点上执行该Playbook就行。 　Context OKD版本 OS版本 Docker版本 Ansible版本 3.11 CentOS 7.5.1804 1.13.1 2.6.5 1. 新增节点Prerequisite 新增node节点IP地址及主机名：192.168.1.23 node6.okd.curiouser.com 开启seLinux sed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/sysconfig/selinux && \\ setenforce 1 安装docker，jdk及基础软件 yum install -y docker vim lrzsz wget unzip net-tools telnet bind-utils && \\ systemctl enable docker && \\ systemctl start docker && \\ systemctl status docker && \\ yum localinstall -y jdk-8u191-linux-x64.rpm && \\ docker info && \\ java -version 配置DNS，发现集群其他节点的IP地址与域名的映射关系.(注意DNSMasq服务端的iptables是否放行DNS的53 UDP端口) 由于集群内有DNSMasq服务端，配置/etc/resolv.conf echo \"nameserver 192.168.1.22\" >> /etc/resolv.conf && \\ ping allinone311.okd.curiouser.com Note: #DNSMasq服务端放行DNS的53 UDP端口 iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 配置Openshift的YUM源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ scp allinone311.okd.curiouser.com:/etc/yum.repos.d/all.repo /etc/yum.repos.d/ && \\ yum clean all && \\ yum makecache 2. ansible管理节点 打通ansible管理节点到新增node节点的SSH免密通道 ssh-copy-id -i root@node6.okd.curiouser.com && \\ ssh root@node1.okd.curiouser.com ansible管理节点的ansible主机清单文件inventory中添加新增节点相关信息 [OSEv3:children] ... new_nodes [new_nodes] node1.okd.curiouser.com openshift_node_group_name=\"node-config-all-in-one\" ansible管理节点执行新增节点的Ansible Playbookansible-playbook /root/openshift-ansible/playbooks/openshift-node/scaleup.yml 注意1： 当执行脚本时tower主机会把它的dnsmasq配置/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下。由于tower主机的/etc/dnsmasq.d/origin-upstream-dns.conf设置的上游DNS服务器为外网的。不希望新增节点的上游DNS服务器走外网，而是走tower主机，形成集群只有Tower主机一个节点的dns对外，其他主机作为Tower主机dns服务的客户端。所以当tower主机/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下的时候，及时修改上游dns服务器为tower主机。然后重启dnsmasq。有两个明显的坑: ① 无法重启dnsmasq，报以下错误： DBus error: Connection \":1.50\" is not allowed to own the service \"uk.org.thekelleys.dnsmasq\" due to security policies in the configuration file 解决方案：重启dbus，再重启dnsmasq systemctl restart dbus && \\ systemctl restart dnsmasq ②tower主机的iptables服务开启，dns的53端口没有放开，导致新增节点的dns无法连接上游dns服务器（即Tower主机的dns服务） 解决方案：tower主机放行dns服务的UDP 53端口。（可在新增节点尝试nslookup解析域名试一下） iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 注意2： 如果出现收集allinone节点facts超时的报错，出现一下错误提示 The full traceback is: Traceback (most recent call last): File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/basic.py\", line 2853, in run_command cmd = subprocess.Popen(args, **kwargs) File \"/usr/lib64/python2.7/subprocess.py\", line 711, in __init__ errread, errwrite) File \"/usr/lib64/python2.7/subprocess.py\", line 1308, in _execute_child data = _eintr_retry_call(os.read, errpipe_read, 1048576) File \"/usr/lib64/python2.7/subprocess.py\", line 478, in _eintr_retry_call return func(*args) File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/facts/timeout.py\", line 37, in _handle_timeout raise TimeoutError(msg) TimeoutError: Timer expired after 10 seconds TimeoutError: Timer expired after 10 seconds 请在/etc/ansible/ansible.cfg 设置\"gather_subset = !all\"或者\"gather_timeout=300\"。原因可能是已经运行allinone节点上的facts（特别是docker images layer的挂载信息）过多，造成收集facts超时，默认收集facts超时时间是10。 相关连接：https://github.com/ansible/ansible/issues/43884 二、删除节点 疏散要删除节点上的POD oc adm drain [--pod-selector=] --force=true --grace-period=-1 --timeout=5s --delete-local-data=true 删除Node oc delete node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-使用Cockpit监控集群节点的系统状态.html":{"url":"origin/openshift-使用Cockpit监控集群节点的系统状态.html","title":"节点状态监控","keywords":"","body":"一、Cockpit简介 Cockpit 是一个自由开源的服务器管理软件，它使得我们可以通过它好看的 web 前端界面轻松地管理我们的 GNU/Linux 服务器。Cockpit 使得 linux 系统管理员、系统维护员和开发者能轻松地管理他们的服务器并执行一些简单的任务，例如管理存储、检测日志、启动或停止服务以及一些其它任务。它的报告界面添加了一些很好的功能使得可以轻松地在终端和 web 界面之间切换。另外，它不仅使得管理一台服务器变得简单，更重要的是只需要一个单击就可以在一个地方同时管理多个通过网络连接的服务器。它非常轻量级，web 界面也非常简单易用。在这篇博文中，我们会学习如何安装 Cockpit 并用它管理我们的运行着 Fedora、CentOS、Arch Linux 以及 RHEL 发行版操作系统的服务器。下面是 Cockpit 在我们的 GNU/Linux 服务器中一些非常棒的功能： 它包含 systemd 服务管理器。 有一个用于故障排除和日志分析的 Journal 日志查看器。 包括 LVM 在内的存储配置比以前任何时候都要简单。 用 Cockpit 可以进行基本的网络配置。 可以轻松地添加和删除用户以及管理多台服务器。 二、Cockpit安装 所有集群节点安装cockpit并启动服务 yum install -y cockpit cockpit-docker cockpit-kubernetes ;\\ systemctl enable cockpit ;\\ systemctl start cockpit ;\\ netstat -lanp |grep 9090 iptables放行端口 vi /etc/sysconfig/iptables #-A INPUT -p tcp -m state --state NEW -m tcp --dport 9090 -j ACCEPT systemctl restart iptables Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-集群组件TLS证书管理.html":{"url":"origin/openshift-集群组件TLS证书管理.html","title":"集群组件TLS证书管理","keywords":"","body":"Openshift组件:Master、Node、Etcd、Router、Registry之间的TLS证书管理 一、安装时指定证书的有效期 默认情况下，etcd证书、openshift证书的有效期为5年，kubelet证书、私有镜像仓库registry证书、Route证书的有效期为2年。在集群安装时可以通过设置ansible/hosts中的参数来指定证书的有效期 [OSEv3:vars] openshift_hosted_registry_cert_expire_days=730 openshift_ca_cert_expire_days=1825 openshift_node_cert_expire_days=730 openshift_master_cert_expire_days=730 etcd_ca_default_days=1825 二、使用openshift的ansible playbook查看当前集群所有证书的有效期 在/etc/ansible/hosts中添加变量 [OSEv3:vars] ... openshift_is_atomic=false ansible_distribution=centos openshift_certificate_expiry_config_base=/etc/origin openshift_certificate_expiry_warning_days=30 openshift_certificate_expiry_show_all=no # 可选项 # openshift_certificate_expiry_generate_html_report=no # openshift_certificate_expiry_html_report_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.html # openshift_certificate_expiry_save_json_results=no # openshift_certificate_expiry_json_results_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.json ... 检查 $ ansible-playbook playbooks/openshift-checks/certificate_expiry/easy-mode.yaml #执行完成后可在roles/openshift_certificate_expiry/defaults/main.yml中的openshift_certificate_expiry_html_report_path变量指定路径下看到证书检查报告文件。分别是HTML格式和JSON格式的文件。 # （默认证书检查报告文件路径是：当前用户家目录下~/cert-expiry-report.时间戳.html和cert-expiry-report.时间戳.JSON）查看所有证书的过期时间 它将会展示出所有Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书的过期时间 三、更新证书 更新证书方法可以只针对Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书中的一种进行更新，也可以全部进行更新。 确保ansible/hosts中的参数有如下信息 openshift_master_cluster_hostname=master.example.com openshift_master_cluster_public_hostname=master.example.com 重新生成证书进行更新 ①全部一次性更新 ansible-playbook playbooks/redeploy-certificates.yml ②只更新master CA证书 ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml ③只更新etcd CA证书 ansible-playbook playbooks/openshift-etcd/redeploy-ca.yml ④只更新master Certificates证书 ansible-playbook playbooks/openshift-master/redeploy-certificates.yml ⑤只更新etcd Certificates证书 ansible-playbook playbooks/openshift-etcd/redeploy-certificates.yml ⑥只更新node Certificates证书 ansible-playbook playbooks/openshift-node/redeploy-certificates.yml ⑦只更新私有镜像仓库Rgistry Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-registry-certificates.yml ⑧只更新Router Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-router-certificates.yml 四、安装时使用自定义Master CA证书（以Master的CA证书为例） 将证书的路径写在inventory的配置参数中 ... [OSEv3.vars] ... openshift_master_ca_certificate={'certfile': '', 'keyfile': ''} ... 执行正常部署 ansible-playbook playbooks/deploy_cluster.yml 五、已运行的集群，更新自定义证书 同步骤四，将证书的路径写在inventory的配置参数中，运行更新Master CA证书的playbook ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml 六、更新完成后可能遇到的问题 The installer detected the wrong host names and the issue was identified too late The certificates are expired and you need to update them You have a new CA and want to create certificates using it instead allinone的集群下更新所有证书时，在重启docker那一步中，容易卡住 参考连接 https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html#advanced-install-custom-certificates https://docs.openshift.com/container-platform/3.11/install_config/redeploying_certificates.html#install-config-cert-expiry https://www.jianshu.com/p/ffc4d6369d4e Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-WebConsole定制化.html":{"url":"origin/openshift-WebConsole定制化.html","title":"定制WebConsole界面","keywords":"","body":"一、定制WebConsole中左上角的logo 制作图标 使用Windows 10自带的Paint 3D。制作高度40pixel，宽度为logo字体宽的透明画布（建议logo字体宽度为100-300pixel之间）。保存为PNG格式。 将PNG图片转成SVG格式 http://www.bejson.com/convert/image_to_svg/ 将SVG文件进行Base64加密 https://www.css-js.com/tools/base64.html 将下面CSS文件上传到一个HTTPS的静态资源服务器上 #header-logo { background-image: url('data:image/svg+xml;base64,base64加过密的SVG图片源码'); width: 230px; height: 40px; } # 参考 #header-logo{ background-image:url('data:image/svg+xml;base64,77u/PD******'); width: 230px; height: 40px; } 或者 #header-logo{ background-image: url(\"logo图片的访问UTRL（必须是HTTPS）\"); width: 300px; height: 40px; } 修改WebConsole的配置文件 待Webconsole的容器重启过后（等待约5分钟），再次刷新页面可见修改过后的效果。可使用F12调出浏览器开发者模式，查看页面渲染的元素。 二、汉化项目左侧导航栏 创建js (function() { window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[0].label=\"概览\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[1].label=\"应用\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[2].label=\"构建\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[3].label=\"资源\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[4].label=\"存储\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[5].label=\"监控\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[6].label=\"商店\" window.OPENSHIFT_CONSTANTS.APP_LAUNCHER_NAVIGATION = [ { title: \"Sharing Videos\", iconClass: \"fa fa-video-camera\", href: \"https://yun.baidu.com/s/1xIwYILHQebEHZOcW4yvsAw\", tooltip: \"一键部署Openshift相关视频\" }]; }()); 上传到https服务器上 修改WebConsole的配置文件 三、定制登陆页面 导出login模板文件 oc adm create-login-template > login.html 修改该HTML文件，然后放到master节点上的/etc/origin/master/login-template/路径下（示例可见附件） 修改Master节点的/etc/origin/master/master-config.yaml文件 oauthConfig: ... templates: login: login-template/login.html #login-template/login.html是相对于/etc/origin/master/master-config.yaml文件路径的相对位置 重启master节点上的OKD的api进程 # 使用okd3.11新命令：master-restart master-restart api Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html":{"url":"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html","title":"集群管理遇到的问题","keywords":"","body":"问题一 描述 正在运行的Openshift Allinone 3.11集群创建POD突然报出 network: failed to allocate for range 0: no IP addresses available in range set 现象 整个Allinone集群所有的POD不超多100个，但是/var/lib/cni/networks/openshift-sdn/的IP地址文件却有252个。造成当前节点的容器网络无法再为POD分配IP地址 解决方案 将对应节点标记为不可调用 oc adm manage-node node1.test.openshift.com --schedulable=false 驱散对应节点上的POD oc adm drain node1.test.openshift.com --ignore-daemonsets 停止docker和origin-node服务 systemctl stop docker origin-node.service 删除/var/lib/cni/networks/openshift-sdn/路径下所有文件 rm -rf /var/lib/cni/networks/openshift-sdn/* 重启docker和origin-node服务 systemctl start docker origin-node.service 将节点标记为可调度 oc adm manage-node node1.test.openshift.com --schedulable=true 相关链接 https://access.redhat.com/solutions/3328541 https://github.com/debianmaster/openshift-examples/issues/59 https://github.com/cloudnativelabs/kube-router/issues/383 https://github.com/jsenon/api-cni-cleanup/blob/master/k8s/deployment.yml#L42 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-openshift的用户认证.html":{"url":"origin/openshift-openshift的用户认证.html","title":"用户认证","keywords":"","body":"一、用户认证 Openshift通过OAuth进行用户的认证。在Openshift的master节点上运行着一个内置的OAuth服务对用户的请求进行认证检查。一旦OAuth服务器通过登录信息确认了用户的信息，OAuth服务器就返回用户的访问Token。通过这个Token，用户可以在有效的时间内对系统进行访问。 #登录命令 $ oc login -u 用户名 ​ #查看以哪个用户登录的 $ oc whoami $ oc whoami -t 查看当前用户当前Session的Token ​ #system:admin是集群默认的管理员，该用户是一个特殊用户，它不能通过用户名密码登录，它也没有Token。 作为身份验证的登录信息，如用户名密码，并非保存在Openshift集群中，而是保存在用户信息管理系统中，这些用户信息管理系统在Openshift中被称为Identity Provider。但Openshift并不提供用户信息管理系统，而是提供了不同的适配器连接不同的用户信息管理系统。通过配置，Openshift可以连接到以下用户信息管理系统： LADP（Lightweight Directory Access Protocol） 微软的活动目录（Active Directory） AllowALL DenyAll HTPasswd Github #查看当前Openshift集群支持的用户信息管理系统 cat /etc/origin/master/master-config.yaml|grep provider -A 3 provider: apiVersion: v1 file: /etc/origin/master/htpasswd kind: HTPasswdPasswordIdentityProvider #Htpasswd是Apache提供的一个基于文本文件管理用户名密码的用户信息管理工具 Openshift的用户管理，在后台创建用户时，会同时创建一个User对象和Identity对象（该对象保存了用户来源哪一个Identity Provider及用户信息）。 #查看集群中所有用户 $oc get user NAME UID FULL NAME IDENTITIES admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:admin dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 htpasswd_auth:dev #查看用户的Identity对象 $oc get identity NAME IDP NAME IDP USER NAME USER NAME USER UID htpasswd_auth:admin htpasswd_auth admin admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:dev htpasswd_auth dev dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 Openshift的用户组管理。用户组的信息来源有两个：一个是Identity Provider，二是通过用户在Openshift中定义的。 #通过oadm groups命令在Openshift中对组及组成员进行管理 $> oadm groups #添加用户到用户组 $> oadm groups add-users group_name user_name #查看用户组 $> oc get group #创建用户组 $> oadm groups new group_name #删除组 $> oc delete group group_name 二、用户权限管理 用户角色权限管理 授予及撤销用户某种角色 oc policy add-role-to-user view test oc policy remove-role-from-user view test 查看项目的角色绑定关系 oc get rolebinding -n 项目名 授予某用户对某项目的某角色 oc policy add-role-to-user view test -n test 查看角色绑定的规则 oc describe clusterrole registry-viewer 用户管理 新增用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd test test\" 查看已创建的用户 oc get user 或 cat /etc/origin/master/htpasswd 删除用户 oc delete user test ansible masters -m shell -a \"htpasswd -D /etc/origin/master/htpasswd ha\" 用户组管理 创建用户组、添加用户到用户组 oc adm groups new test oc adm groups add-users test 用户1 用户2 用户3 查看创建的用户组及组内的成员用户 oc get group 删除用户组 oc delete group test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-openshift用户权限管理实例.html":{"url":"origin/openshift-openshift用户权限管理实例.html","title":"用户权限管理实例","keywords":"","body":"Openshift用户权限管理实例 由于公司的日常项目开发测试环境都迁移到openshift上了。有众多开发测试人员需要登陆到openshift上进行操作，如果直接给admin权限，肯定是不行的。而openshift是支持多租户的权限管理。所以，就在创建普通用户的基础上赋予各种不同的权限限制来自控制对openshift上project的操作。 一、Prerequisite 开发人员对CI环境有操作权限，对SIT、UAT环境只有查看权限 测试人员对SIT环境有操作权限，对CI环境只有查看权限 所有人员有自己的登录账户，均可见openshift上所有的业务项目，不可见系统项目 二、实现过程 创建登录用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev1 dev1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev2 dev2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev3 dev3\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester1 tester1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester2 tester2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester3 tester3\" 创建用户组 oc adm groups new developer oc adm groups new tester 将用户添加到用户组中 oc adm groups add-users developer dev1 dev2 dev3 oc adm groups add-users tester tester1 tester2 tester3 针对项目，给用户组赋予系统角色 oc adm policy add-role-to-group edit developer -n aci oc adm policy add-role-to-group edit developer -n bci oc adm policy add-role-to-group edit developer -n cci oc adm policy add-role-to-group view developer -n asit oc adm policy add-role-to-group view developer -n auat oc adm policy add-role-to-group view developer -n bsit oc adm policy add-role-to-group view developer -n buat oc adm policy add-role-to-group view developer -n csit oc adm policy add-role-to-group view developer -n cuat ​ oc adm policy add-role-to-group edit tester -n asit oc adm policy add-role-to-group edit tester -n auat oc adm policy add-role-to-group edit tester -n bsit oc adm policy add-role-to-group edit tester -n buat oc adm policy add-role-to-group edit tester -n csit oc adm policy add-role-to-group edit tester -n cuat oc adm policy add-role-to-group view tester -n aci oc adm policy add-role-to-group view tester -n bci oc adm policy add-role-to-group view tester -n cci 实际操作过程中，在以某以开发人员登录过程openshift过程中，依旧会看到openshift 其他一些项目的namespace。例如base namespace，该namespace项目是在registry镜像注册仓库中创建镜像项目时自动创建的openshift namespace（在registry镜像注册仓库中创建base镜像项目是为了存放一些自定义的s2i镜像）。为了使其他openshift namespace使用其中的s2i镜像，特别在registry镜像注册仓库中是镜像项目的访问策略设置为共享的。种种以上，导致openshift上的base namespace是能被所有的已认证的用户查看到。 在openshift中查看base项目的membership 可以发现，凡是在registry镜像注册仓库中设置问访问策略设置为共享的，都会在openshift 项目中添加一个系统用户system:authenticated 。这个系统用户上绑定的是这个角色registry-viewer。在openshift后台查看该角色的详细信息 # oc describe clusterrole registry-viewer Name: registry-viewer Created: About an hour ago Labels: Annotations: authorization.openshift.io/system-only=true openshift.io/reconcile-protect=false Verbs Non-Resource URLs Resource Names API Groups Resources [get list watch] [] [] [ image.openshift.io] [imagestreamimages imagestreammappings imagestreams imagestreamtags] [get] [] [] [ image.openshift.io] [imagestreams/layers] [get] [] [] [] [namespaces] [get] [] [] [project.openshift.io ] [projects] 发现该角色有对namespace资源拥有get动作。仔细想想，该system:authenticated用户只是让openshift其他项目的系统用户能够拉取其下的镜像流。而在Kubernetes中使用命名空间的概念来分隔资源。在同一个命名空间中，某一个对象的名称在其分类中必须唯一，但是分布在不同命名空间中的对象则可以同名。OpenShift中继承了Kubernetes命名空间的概念，而且在其之上定义了Project对象的概念。每一个Project会和一个Namespace相关联，甚至可以简单地认为，Project就是Namespace。所以，该用户对project资源有获取权限，那就把对namespace的权限给去掉试试。 先导出角色registry-viewer的bindding配置文件 oc export clusterrole registry-viewer > registry-viewer.yml 然后修改配置文件，注释掉get namespace的动作 apiVersion: v1 kind: ClusterRole metadata: annotations: authorization.openshift.io/system-only: \"true\" openshift.io/reconcile-protect: \"false\" creationTimestamp: null name: registry-viewer rules: - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreamimages - imagestreammappings - imagestreams - imagestreamtags verbs: - get - list - watch - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreams/layers verbs: - get #- apiGroups: # - \"\" # attributeRestrictions: null # resources: # - namespaces # verbs: # - get - apiGroups: - project.openshift.io - \"\" attributeRestrictions: null resources: - projects verbs: - get 再将集群中角色删掉（此时特别注意:从集群中删掉registry-viewer角色后会导致已有镜像注册仓库中镜像的访问策略从共有变成私有,base项目的membership中会删掉system:authenticated该用户） oc delete clusterrole registry-viewer 接着再从配置文件中创建角色 oc create -f registry-viewer.yml 最后再次修改镜像注册仓库中镜像的访问策略从私有变成共有。再次查看base项目中membership. 最有再以测试人员账户登录查看。不再显示base项目。测试其他项目去拉取base项目中的镜像，看去掉registry-viewer角色中role是否有影响。 实际使用过程中，测试人员需要以openshift上的用户名密码登录openshift上的jenkins，还要对jenkins做操作，比如在jenkins上做构建操作，查看构建日志等。需要对测试人员分组tester赋予对jenkins的编辑权限。初步思路是直接给tester分组服务系统角色clusterrole edit（oc adm policy add-cluster-role-to-group edit tester）。但是再以测试人员登录时还是能看到jenkins的项目，甚至能操作openshift上jenkins pod的重新部署。这是不可接受的。 那就换个思路。自己创建一个集群角色clusterrole，在角色上绑定若干规则，再将这个集群角色赋予测试组，相应的测试组成员能登录jenkins，并对jenkins做操作。 具体过程如下： 先查看集群角色edit的配置，看edit都对那些资源都有什么动作 oc describe clusterrole edit ​ Name: edit Created: 7 months ago Labels: Annotations: openshift.io/description=A user that can create and edit most objects in a project, but can not update the project's membership. Verbs Non-Resource URLs Resource Names API Groups Resources [create delete deletecollection get list patch update watch] [] [] [] [pods pods/attach pods/exec pods/portforward pods/proxy] [create delete deletecollection get list patch update watch] [] [] [] [configmaps endpoints persistentvolumeclaims replicationcontrollers replicationcontrollers/scale secrets serviceaccounts services services/proxy] [get list watch] [] [] [] [bindings events limitranges namespaces namespaces/status pods/log pods/status replicationcontrollers/status resourcequotas resourcequotas/status] [impersonate] [] [] [] [serviceaccounts] [create delete deletecollection get list patch update watch] [] [] [autoscaling] [horizontalpodautoscalers] [create delete deletecollection get list patch update watch] [] [] [batch] [cronjobs jobs scheduledjobs] [create delete deletecollection get list patch update watch] [] [] [extensions] [deployments deployments/rollback deployments/scale horizontalpodautoscalers jobs replicasets replicasets/scale replicationcontrollers/scale] [get list watch] [] [] [extensions] [daemonsets] [create delete deletecollection get list patch update watch] [] [] [apps] [deployments deployments/scale deployments/status statefulsets] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildconfigs buildconfigs/webhooks builds] [get list watch] [] [] [build.openshift.io ] [builds/log] [create] [] [] [build.openshift.io ] [buildconfigs/instantiate buildconfigs/instantiatebinary builds/clone] [update] [] [] [build.openshift.io ] [builds/details] [edit view] [] [] [build.openshift.io] [jenkins] [create delete deletecollection get list patch update watch] [] [] [apps.openshift.io ] [deploymentconfigs deploymentconfigs/scale generatedeploymentconfigs] [create] [] [] [apps.openshift.io ] [deploymentconfigrollbacks deploymentconfigs/instantiate deploymentconfigs/rollback] [get list watch] [] [] [apps.openshift.io ] [deploymentconfigs/log deploymentconfigs/status] [create delete deletecollection get list patch update watch] [] [] [image.openshift.io ] [imagestreamimages imagestreammappings imagestreams imagestreams/secrets imagestreamtags] [get list watch] [] [] [image.openshift.io ] [imagestreams/status] [get update] [] [] [image.openshift.io ] [imagestreams/layers] [create] [] [] [image.openshift.io ] [imagestreamimports] [get] [] [] [project.openshift.io ] [projects] [get list watch] [] [] [quota.openshift.io ] [appliedclusterresourcequotas] [create delete deletecollection get list patch update watch] [] [] [route.openshift.io ] [routes] [create] [] [] [route.openshift.io ] [routes/custom-host] [get list watch] [] [] [route.openshift.io ] [routes/status] [create delete deletecollection get list patch update watch] [] [] [template.openshift.io ] [processedtemplates templateconfigs templateinstances templates] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildlogs] [get list watch] [] [] [] [resourcequotausages] 导出集群角色edit的配置文件到本地文件，在其上做修改 oc export clusterrole edit > jenkins-clusterrole.yml 编辑 jenkins-clusterrole.yml（只保留相重要的，其他的都删掉） apiVersion: v1 kind: ClusterRole metadata: annotations: openshift.io/description: A user that can view jenkins project, and edit jenkins job. #添加clusterrole角色的说明简介 creationTimestamp: null name: jenkins #修改clusterrole名字为jenkins rules: - apiGroups: - \"\" attributeRestrictions: null resources: #clusterrole edit中有好多对其他资源的操作。例persistentvolumeclaims、replicationcontrollers、replicationcontrollersscale。对这些资源没有什么用处，就可以删掉啦。 - configmaps - endpoints - secrets - serviceaccounts - services - services/proxy verbs: - get - list - apiGroups: - \"\" attributeRestrictions: null resources: - serviceaccounts verbs: - impersonate - apiGroups: - build.openshift.io attributeRestrictions: null resources: - jenkins verbs: - edit - view 导入jenkins clusterrole oc create -f jenkins-clusterrole.yml 将jenkins clusterrole赋予测试组 oc adm policy add-cluster-role-to-group jenkins tester 以测试组成员登录openshift，jenkins项目不可见了。再以测试组成员登录jenkins，发现登录 出现以下界面 点击\"Allow selected permissions\"，发现也能正常登录jenkins。然后进行一次构建触发。发现一切正常。Bazinga！Everything is ok ! Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-开启router的haproxy-statisc.html":{"url":"origin/openshift-开启router的haproxy-statisc.html","title":"openshift开启router的haproxy-statisc","keywords":"","body":" 设置router POD 所在节点的iptables对1936端口的放行 iptables -I OS_FIREWALL_ALLOW -p tcp -m tcp --dport 1936 -j ACCEPT 获取访问router haproxy statics 页面的用户名密码。 删除掉router dc中的环境变量”ROUTER_METRICS_TYPE“ 这个环境变量默认值为“haproxy”。不删除的话，访问的时候会报一下错误 Forbidden: User \"system:anonymous\" cannot get routers/metrics.route.openshift.io at the cluster scope 将健康检查readiness的HTTP GET URL由“/healthz/ready”改为\"/healthz\"。（不然router POD无法通过健康检查） 验证监听端口80，443，1936 ss -ntl|grep 80 ss -ntl|grep 443 ss -ntl|grep 1936 访问router haproxy statistics 页面。 访问方式是：http://:@router所在节点IP地址:1936 例如：http://admin:MJbJFvODhP@allinone.curiouser.com:1936 相关链接 https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#using-wildcard-routes https://bugzilla.redhat.com/show_bug.cgi?id=1579054 https://github.com/openshift/origin/issues/17025 https://blog.chmouel.com/2016/09/27/how-to-view-openshift-haproxy-stats/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-多租户网络.html":{"url":"origin/openshift-多租户网络.html","title":"openshift的多租户网络","keywords":"","body":"一、Openshift容器网络简介 Openshift容器网络默认是基于Open vSwitch（OVS）实现的。 Openshift提供两种网络方案： ovs-subnet(子网模式)：为集群节点上的容器提供一个扁平化的二层虚拟网路，所有在这个二层网路中容器可直接通信。 ovs-multitenet(多租户模式)：基于项目的网络隔离，即不同项目间的容器之间不能直接通信。启动多租户网络隔离后，每个项目创建后都会被分配一个虚拟网络ID（Virtual Network ID ,VNID）.OVS网桥会为该项目的所有数据流量标记上VNID，在默认情况下，只有数据包上的VNID与目标容器所在项目的VNID匹配上后，数据包才允许被转发到目标容器中。当有些项目的容器应用是通过公共服务的，后期可通过配置将多个项目见的网络连通，或者将项目设置为全局可访问。 二、启动多租户网络 需要将集群中所有的master节点配置文件/etc/origin/master/master-config.yaml和node节点配置文件/etc/origin/node/node-config.yaml中的networkPluginName的属性值从redhat/openshift-ovs-subnet修改为redhat/openshift-ovs-multitenant，然后重启Openshift集群Master节点的origin-master-controllers.service服务和Node节点的origin-node.service服务 三、测试，查看网络隔离 在一个项目中的一个pod的终端中ping/telnet/curl/nslook另一个项目中的pod的ip地址或者对应svc的FQDN（..svc.cluster.local） 查看namespace的Netid是否一致 $ oc get netnamespaces NAME NETID EGRESS IPS default 0 [] kube-public 5899696 [] kube-service-catalog 0 [] demo 13843039 [] dubbo 11344186 [] jenkins 13843039 [] 当NETID相同时，表示这个两个project的网络是相通的 当NETID为0时，表示这个Project的网络全局可访问 四、连通隔离的网络 # project 1,2,3中所有的pod，service可以通过容器IP相互访问（通过service的FQDN不能相互访问） oc adm pod-network join-projects --to= #将某个project中所有的pod和service设置为全局可访问 oc adm pod-network make-projects-global 参考链接 https://docs.okd.io/3.11/admin_guide/managing_networking.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-25 16:31:03 "},"origin/openshift-kubernetes的审计日志功能.html":{"url":"origin/openshift-kubernetes的审计日志功能.html","title":"Kubernetes的审计日志功能","keywords":"","body":"一、Overviews kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。 审计的目的 Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题： 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？ 产生的阶段 kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有： RequestReceived ：apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted ：在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete ：当响应 body 发送完并且不再发送数据。 Panic：内部服务器出错，请求未完成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成 审计记录日志级别 当前支持的日志记录级别有： None: 符合这条规则的日志将不会记录。 Metadata: 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request: 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse: 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。 日志记录策略 在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段 不要记录所有的资源，不要记录一个资源的所有子资源 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 审计日志格式 json{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1beta1\", \"metadata\": { \"creationTimestamp\": \"2019-07-23T09:02:19Z\" }, \"level\": \"Request\", \"timestamp\": \"2019-07-23T09:02:19Z\", \"auditID\": \"eb481add-fdac-48a3-a302-1c33d73bfdbf\", \"stage\": \"RequestReceived\", \"requestURI\": \"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\", \"verb\": \"update\", \"user\": { \"username\": \"system:openshift-master\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.1.96\" ], \"objectRef\": { \"resource\": \"configmaps\", \"namespace\": \"kube-system\", \"name\": \"openshift-master-controllers\", \"apiVersion\": \"v1\" }, \"requestReceivedTimestamp\": \"2019-07-23T09:02:19.148057Z\", \"stageTimestamp\": \"2019-07-23T09:02:19.148057Z\" } legacy 2019-07-23T23:50:06.223368641+08:00 AUDIT: id=\"3574e2e0-06b1-44d8-bc6c-5983c402d55e\" stage=\"ResponseComplete\" ip=\"192.168.1.96\" method=\"update\" user=\"system:openshift-master\" groups=\"\\\"system:masters\\\",\\\"system:authenticated\\\"\" as=\"\" asgroups=\"\" namespace=\"kube-system\" uri=\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\" response=\"200\" 支持的审计日志存储后端 审计后端可以将审计事件导出到外部存储。 Kube-apiserver 提供两个后端： Log 后端: 将事件写入文件，落到磁盘。如果有多个api-server，审计日志文件会分散，无法集中分析。此时可以使用logstash或fluend进行日志采集汇聚到elsticsearch中 Webhook 后端: 将事件发送到外部存储系统的API接口中。例如可以将审计日志发生到logstash监听的http接口中进行处理并吐到elsticsearch中进行汇聚查看 注意 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 二、openshift开启自定义策略的审计功能 创建审计日志的存储路径 mkdir /etc/origin/master/audit # 注意：审计日志文件的存储路径必须是kube-system命名空间下apiservser pod挂载目录下的子路径。 # ocp 3.11版本的apiserver是以pod的形式运行在kube-system命名空间下的，它所需要的配置文件等Volume资源都是以hostpath的形式挂载上去的，例如ocp节点上的/etc/origin/master目录 编辑/etc/origin/master/master-config.yaml ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 ****省略******** 创建自定义的审计策略配置文件 kind: Policy omitStages: - \"ResponseStarted\" rules: - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"configmaps\",\"secrets\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the metadata level. - level: None verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the request level. - level: None resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # Log login failures from the web console or CLI. Review the logs and refine your policies. - level: Metadata nonResourceURLs: - /login* - /oauth* - level: Metadata userGroups: [\"system:authenticated:oauth\"] verbs: [\"create\", \"delete\"] resources: - group: \"project.openshift.io\" resources: [\"projectrequests\", \"projects\"] omitStages: - RequestReceived 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 三、使用openshift集群的Fluentd收集审计日志到集群内的elasticsearch 配置OCP集群中的Fluentd挂载审计日志的存储目录(OCP集群中日志系统的fluentd是以DaemonSet形式收集节点上容器的日志到elasticsearch的，它是将节点的/var/lib/docker目录以hostpath形式挂载到容器中的) oc set volume ds/logging-fluentd --add --mount-path=/etc/origin/master/audit --name=audit --type=hostPath --path=/etc/origin/master/audit -n openshift-logging 配置OCP集群中的Fluentd监控审计日志目录下的日志 oc edit cm/logging-fluentd -n openshift-logging *****省略******* ## sources *****省略******* @include configs.d/user/input-audit.conf *****省略******* input-audit.conf: | @type tail @id audit-ocp path /etc/origin/master/audit/audit-ocp.log pos_file /etc/origin/master/audit/audit.pos tag audit.requests format json @type copy @type elasticsearch log_level debug host \"#{ENV['OPS_HOST']}\" port \"#{ENV['OPS_PORT']}\" scheme https ssl_version TLSv1_2 index_name .audit user fluentd password changeme client_key \"#{ENV['OPS_CLIENT_KEY']}\" client_cert \"#{ENV['OPS_CLIENT_CERT']}\" ca_file \"#{ENV['OPS_CA']}\" type_name com.redhat.ocp.audit reload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'false'}\" reload_after \"#{ENV['ES_RELOAD_AFTER'] || '100'}\" sniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::ElasticsearchSimpleSniffer'}\" reload_on_failure false flush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '5s'}\" max_retry_wait \"#{ENV['ES_RETRY_WAIT'] || '300'}\" disable_retry_limit true buffer_type file buffer_path '/var/lib/fluentd/buffer-output-es-auditlog' buffer_queue_limit \"#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }\" buffer_chunk_limit \"#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }\" buffer_queue_full_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}\" request_timeout 2147483648 *****省略******* 重启Fluentd oc delete po -l component=fluentd -n openshift-logging 在ocp集群系统的Kibana上添加\".audit*\"的Index Pattern,并在\"Discover\"查看、筛选审计日志 四、将审计日志通过WebHook 发送到OCP外部的Logstash或者Fluentd 接收后端 可使用Logstash或者Fluentd作为后端来接收Api-Server通过web hook方式发送的审计日志。Logstash和Fluentd可以是ocp集群外二进制方式安装运行的，也可以是原生Docker运行的，甚至可以是另外一个集群中容器化的。一个原则就是不要放到审计日志产生集群的内部。防止apiserver启动起来了，有了一些操作，logstash还没有启动起来，丢失审计日志。再者审计日志后端最好选择适合自己的，审计日志落一份，重复记录也没多大意义。 方式一：使用OCP集群外二进制方式安装的Logstash来接收ApiServer通过web hook方式发送过来的审计日志并过滤、存储到本地文件中 安装logstash bash -c 'cat > /etc/yum.repos.d/elasticsearch.repo 设置logstash，/etc/logstash/logstash.yml # ------------ Pipeline Configuration Settings -------------- # Where to fetch the pipeline configuration for the main pipeline path.config: /etc/logstash/conf.d/ *************************省略****************************** # ------------ Data path ------------------ # Which directory should be used by logstash and its plugins for any persistent needs. Defaults to LOGSTASH_HOME/data path.data: /data/logs/logstash/data/ *************************省略****************************** # ------------ Debugging Settings ------------- # Options for log.level: fatal/error/warn/info (default)/debug/trace log.level: info path.logs: /data/logs/logstash/logs 创建监听HTTP 8081端口的pipeline cat /etc/logstash/conf.d/accept-audit-log.conf input{ http{ host => \"0.0.0.0\" port => 8081 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=>[items] # We only need event subelement, remove others. remove_field=>[headers, metadata, apiVersion, kind, \"@version\", host] } mutate{ rename => {items=>event} } } output{ file{ # Audit events from different users will be saved into different files. path=>\"/data/logs/logstash/ocp-audit-logs/ocp-audit-%{[event][user][username]}/audit-%{+YYYY-MM-dd}.log\" } } EOF 启动logstash mkdir -p /data/logs/logstash/{data,logs,ocp-audit-logs} chown -R logstash:logstash /data/logs/logstash system start logstash # 或者 /usr/share/logstash/bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/ 测试logstash的联通性。一是看logstash pipeline监听的HTTP端口是否开启。二是尝试发送一个带有模拟数据的POST请求，看其是否会pipeline指定的数据目录生成日志文件 ss -ntl |grep 8081 curl -X POST \\ http://192.168.1.96:8081 \\ -H 'Accept: */*' \\ -H 'Cache-Control: no-cache' \\ -H 'Connection: keep-alive' \\ -H 'Content-Type: application/json' \\ -H 'accept-encoding: gzip, deflate' \\ -d '{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1beta1\",\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:27:54Z\"},\"level\":\"Request\",\"timestamp\":\"2019-07-23T14:27:54Z\",\"auditID\":\"29bf32ba-4bea-4b4f-a1fb-cd091b2188ff\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"verb\":\"update\",\"user\":{\"username\":\"system:openshift-master\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"resource\":\"configmaps\",\"namespace\":\"kube-system\",\"name\":\"openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"apiVersion\":\"v1\",\"resourceVersion\":\"8285989\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ConfigMap\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-master-controllers\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"resourceVersion\":\"8285989\",\"creationTimestamp\":\"2019-03-09T11:31:08Z\",\"annotations\":{\"control-plane.alpha.kubernetes.io/leader\":\"{\\\"holderIdentity\\\":\\\"allinone.okd311.curiouser.com\\\",\\\"leaseDurationSeconds\\\":15,\\\"acquireTime\\\":\\\"2019-03-09T11:31:01Z\\\",\\\"renewTime\\\":\\\"2019-07-23T14:27:54Z\\\",\\\"leaderTransitions\\\":0}\"}}},\"requestReceivedTimestamp\":\"2019-07-23T14:27:54.894767Z\",\"stageTimestamp\":\"2019-07-23T14:27:54.899643Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}}' 创建audit的webhook配置文件/etc/origin/master/audit-policy.yaml cat /etc/origin/master/audit-policy.yaml apiVersion: v1 clusters: - cluster: server: http://192.168.1.96:8081 name: logstash contexts: - context: cluster: logstash user: \"\" name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF 编辑/etc/origin/master/master-config.yaml，添加webhook相关的参数 ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 #==========以下配置项为添加的webhook参数=========================================================================== webHookKubeConfig: /etc/origin/master/audit-webhook-config.yaml # 指定WebHook的配置文件（同样路径要指定在ApiServer POD已挂载的路径下） webHookMode: batch # 可选参数\"batch\"和\"blocking\" ****省略******** 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 验证，用除\"system:admin\"用户外的其他用户创建project，然后再删除project，最后查看logstash配置的审计日志存储目录下是否生成对应的文件 oc login -u admin -p oc new-project test oc delete project test $ tree -L 2 /data/logs/logstash/ocp-audit-logs/ /data/logs/logstash/ocp-audit-logs/ ├── ocp-audit-admin │ └── audit-2019-07-23.log └── ocp-audit-system:openshift-master └── audit-2019-07-23.log $ cat /data/logs/logstash/ocp-audit-logs/ocp-audit-admin/audit-2019-07-23.log 产生以下内容。显示一次创建成功，另一次创建失败，原因是project已经存在（特意测试），一次删除project等日志。 {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:03Z\"},\"stageTimestamp\":\"2019-07-23T14:45:03.019249Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:02Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:02.964347Z\",\"auditID\":\"eec24884-b70a-4b27-80c1-431111d2f4f5\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:10Z\"},\"stageTimestamp\":\"2019-07-23T14:45:10.842999Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:10Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"reason\":\"AlreadyExists\",\"code\":409},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:10.836487Z\",\"auditID\":\"a7b64f47-94eb-4723-b101-24e112cd0735\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"self-provisioners\\\" of ClusterRole \\\"self-provisioner\\\" to Group \\\"system:authenticated:oauth\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:19Z\"},\"stageTimestamp\":\"2019-07-23T14:45:19.813911Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:19Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projects\",\"namespace\":\"test\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Success\",\"code\":200},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:19.805940Z\",\"auditID\":\"9d3260ca-3bff-49da-9fe9-346043a29991\",\"verb\":\"delete\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projects/test\"}} 五、审计策略配置详解 apiVersion: audit.k8s.io/v1 kind: Policy omitStages: - \"RequestReceived\" # 审计阶段 rules: # rule按顺序匹配 - level: Request # 审计级别 verbs: - create - delete resources: - group: \"\" resources: - pods 参考链接 https://austindewey.com/2018/10/17/integrating-advanced-audit-with-aggregated-logging-in-openshift-3-11/#test-it-outhttps://www.outcoldsolutions.com/docs/monitoring-openshift/v4/audit/https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-config-advanced-audithttps://docs.openshift.com/container-platform/3.11/security/monitoring.htmlhttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6chttps://www.jianshu.com/p/8117bc2fb966https://cloud.google.com/kubernetes-engine/docs/concepts/audit-policy?hl=zh-cnhttps://github.com/rbo/openshift-examples/tree/master/efk-auditloghttps://github.com/openshift/origin-aggregated-logging/issues/1226 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-elasticsearch容器化部署.html":{"url":"origin/openshift-elasticsearch容器化部署.html","title":"Elasticsearch容器化部署","keywords":"","body":"一、拉取镜像 docker pull docker.io/elasticsearch/elasticsearch:6.6.1 #或者 docker pull docker.elastic.co/elasticsearch/elasticsearch:6.6.1 二、Docker部署 修改系统 echo \"vm.max_map_count=262144\" >> /etc/sysctl.conf sysctl -w vm.max_map_count=262144 Docker单节点部署 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.io/elasticsearch/elasticsearch:6.6.1 Docker compose集群部署 version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch2 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnet volumes: esdata1: driver: local esdata2: driver: local networks: esnet: 三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: elasticsearch name: elasticsearch spec: replicas: 1 selector: app: elasticsearch deploymentconfig: elasticsearch strategy: type: Recreate template: metadata: labels: app: elasticsearch deploymentconfig: elasticsearch spec: containers: - env: - name: discovery.type value: single-node - name: cluster.name value: curiouser - name: bootstrap.memory_lock value: 'true' - name: path.repo value: /usr/share/elasticsearch/snapshots-repository - name: TZ value: Asia/Shanghai - name: ES_JAVA_OPTS value: '-Xms1g -Xmx2g' - name: xpack.monitoring.collection.enabled value: 'true' - name: xpack.security.enabled value: 'true' - name: ELASTIC_USERNAME value: \"elastic\" - name: \"ELASTIC_PASSWORD\" value: \"elastic\" image: 'docker.elastic.co/elasticsearch/elasticsearch:7.1.1' imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 name: elasticsearch ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 80 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 resources: limits: cpu: '2' memory: 3Gi requests: cpu: '1' memory: 2Gi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-data - mountPath: /usr/share/elasticsearch/snapshots-repository name: elasticsearch-snapshots-repository dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: elasticsearch name: elasticsearch spec: ports: - name: 9200-tcp port: 9200 protocol: TCP targetPort: 9200 - name: 9300-tcp port: 9300 protocol: TCP targetPort: 9300 selector: deploymentconfig: elasticsearch sessionAffinity: None type: ClusterIP 数据目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi snapshot repository存储目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-snapshots-repository spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 四. Kubernetes部署 Deployment kind: Deployment apiVersion: apps/v1 metadata: labels: elastic-app: elasticsearch role: master name: elasticsearch-master namespace: elk spec: replicas: 1 revisionHistoryLimit: 10 strategy: type: Recreate selector: matchLabels: elastic-app: elasticsearch role: master template: metadata: labels: elastic-app: elasticsearch role: master spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository && chown -R 1000.0 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository'] volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data - name: elasticsearch-snapshots-repository mountPath: /usr/share/elasticsearch/snapshots-repository containers: - name: elasticsearch-master-data image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: \"cluster.name\" value: \"Curiouser\" - name: \"bootstrap.memory_lock\" value: \"false\" - name: discovery.type value: single-node - name: \"node.master\" value: \"true\" - name: \"node.data\" value: \"true\" - name: \"node.ingest\" value: \"false\" - name: xpack.monitoring.collection.enabled value: \"true\" - name: \"xpack.monitoring.elasticsearch.collection.enabled\" value: \"true\" - name: \"xpack.security.enabled\" value: \"true\" - name: \"path.repo\" value: \"/usr/share/elasticsearch/snapshots-repository\" - name: \"ES_JAVA_OPTS\" value: \"-Xms2048m -Xmx2048m\" - name: TZ value: Asia/Shanghai - name: \"xpack.monitoring.exporters.my_local.type\" value: \"local\" - name: \"xpack.monitoring.exporters.my_local.use_ingest\" value: \"false\" resources: requests: memory: \"2Gi\" cpu: \"2\" limits: memory: \"4096Mi\" cpu: \"3\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 volumeMounts: - name: elasticsearch-data mountPath: \"/usr/share/elasticsearch/data\" - name: elasticsearch-snapshots-repository mountPath: \"/usr/share/elasticsearch/snapshots-repository\" restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository PersistentVolumeClaim --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-data namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-snapshots-repository namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi Service kind: Service apiVersion: v1 metadata: labels: elastic-app: elasticsearch-service name: elasticsearch namespace: elk spec: ports: - port: 9200 targetPort: 9200 protocol: TCP selector: elastic-app: elasticsearch type: ClusterIP Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kibana容器化部署.html":{"url":"origin/openshift-Kibana容器化部署.html","title":"Kibana容器化部署","keywords":"","body":"三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: kibana name: kibana spec: replicas: 1 selector: app: kibana deploymentconfig: kibana strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: kibana deploymentconfig: kibana spec: containers: - env: - name: ELASTICSEARCH_USERNAME value: kibana - name: ELASTICSEARCH_PASSWORD value: uLAWAfW1b7UHZdHEigCW - name: TZ value: Asia/Shanghai image: docker.elastic.co/kibana/kibana:7.1.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 name: kibana ports: - containerPort: 5601 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 resources: limits: cpu: \"1\" memory: 1500Mi requests: cpu: 500m memory: 800Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: kibana name: kibana spec: ports: - name: 5601-tcp port: 5601 protocol: TCP targetPort: 5601 selector: deploymentconfig: kibana sessionAffinity: None type: ClusterIP Route apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: apache-kibana name: kibana spec: port: targetPort: 5601-tcp to: kind: Service name: kibana weight: 100 wildcardPolicy: None 四. Kubernetes上部署 Deployment apiVersion: apps/v1beta2 kind: Deployment metadata: labels: app: kibana name: kibana namespace: elk spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: kibana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: kibana spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com containers: - image: kibana/kibana:7.2.0 imagePullPolicy: IfNotPresent name: kibana envFrom: - secretRef: name: kibana-config-env env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOSTS value: '[\"http://elasticsearch.elk.svc:9200\"]' ports: - containerPort: 5601 name: web protocol: TCP resources: requests: memory: \"1Gi\" cpu: \"0.5\" limits: memory: \"2Gi\" cpu: \"1\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: {} privileged: false procMount: Default readOnlyRootFilesystem: false runAsNonRoot: false stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsConfig: {} dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Secret apiVersion: v1 kind: Secret metadata: labels: app: kibana name: kibana-config-env namespace: elk stringData: ELASTICSEARCH_USERNAME: kibana ELASTICSEARCH_PASSWORD: kibana Service apiVersion: v1 kind: Service metadata: name: kibana namespace: elk labels: app: kibana spec: ports: - port: 5601 name: web selector: app: kibana Ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: elk spec: rules: - host: kibana.apps.k8s.curiouser.com http: paths: - path: / backend: serviceName: kibana servicePort: 5601 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 14:58:39 "},"origin/kubernetes-集群角色及插件.html":{"url":"origin/kubernetes-集群角色及插件.html","title":"Kubernetes的集群角色及插件","keywords":"","body":"一、Master节点上的组件 kube-apiserver：对外暴露了Kubernetes API。它是的 Kubernetes 前端控制层。它被设计为水平扩展，即通过部署更多实例来缩放。 kube-controller-manager：运行控制器，它们是处理集群中常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成独立的可执行文件，并在单个进程中运行。这些控制器包括: 节点控制器: 当节点移除时，负责注意和响应。 副本控制器: 负责维护系统中每个副本控制器对象正确数量的 Pod。 端点控制器: 填充端点(Endpoints) 对象(即连接 Services & Pods)。 服务帐户和令牌控制器: 为新的命名空间创建默认帐户和 API 访问令牌 kube-scheduler：监视没有分配节点的新创建的 Pod，选择一个节点供他们运行。 etcd：用于 Kubernetes 的后端存储。存储所有集群数据。 cloud-controller-manager：用于与底层云提供商交互的控制器。云控制器管理器可执行组件是 Kubernetes v1.6 版本中引入的 Alpha 功能。仅运行云提供商特定的控制器循环。您必须在 - kube-controller-manager 中禁用这些控制器循环，您可以通过在启动 kube-controller-manager 时将 --cloud-provider 标志设置为external来禁用控制器循环。允许云供应商代码和 Kubernetes 核心彼此独立发展，在以前的版本中，Kubernetes 核心代码依赖于云提供商特定的功能代码。在未来的版本中，云供应商的特定代码应由云供应商自己维护，并与运行 Kubernetes 的云控制器管理器相关联。以下控制器具有云提供商依赖关系: 节点控制器: 用于检查云提供商以确定节点是否在云中停止响应后被删除 路由控制器: 用于在底层云基础架构中设置路由 服务控制器: 用于创建，更新和删除云提供商负载平衡器 数据卷控制器: 用于创建，附加和装载卷，并与云提供商进行交互以协调卷 二、Node节点上的组件 kubelet：是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能: 挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过 rkt)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建镜像Pod（Mirror Pod） 将 Pod 的状态报告回系统的其余部分。 将节点的状态报告回系统的其余部分。 kube-proxy：通过维护主机上的网络规则并执行连接转发，实现了Kubernetes服务抽象 Container Runtime：运行容器的底层平台。Kubernetes支持的容器平台：Docker、containerd、cri-o、rktlet 三、插件 网络插件 ACI: provides integrated container networking and network security with Cisco ACI. Calico is a secure L3 networking and network policy provider. Canal unites Flannel and Calico, providing networking and network policy. Cilium is a L3 network and network policy plugin that can enforce HTTP/API/L7 - policies transparently. Both routing and overlay/encapsulation mode are - supported. CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,such as Calico, Canal, Flannel, Romana, or Weave. Contiv provides configurable networking (native L3 using BGP, overlay using - vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy - framework. Contiv project is fully open sourced. The installer provides both - kubeadm and non-kubeadm based installation options. Contrail, based on Tungsten Fabric, is a open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads. Flannel is an overlay network provider that can be used with Kubernetes. Knitter is a network solution supporting multiple networking in Kubernetes. Multus is a Multi plugin for multiple network support in Kubernetes to support - all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, - DPDK, OVS-DPDK and VPP based workloads in Kubernetes. NSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and - container orchestrators such as Kubernetes, as well as integration between - NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service - (PKS) and OpenShift. Nuage is an SDN platform that provides policy-based networking between - Kubernetes Pods and non-Kubernetes environments with visibility and security - monitoring. Romana is a Layer 3 networking solution for pod networks that also supports the - NetworkPolicy API. Kubeadm add-on installation details available here. Weave Net provides networking and network policy, will carry on working on both - sides of a network partition, and does not require an external database 服务发现插件 CoreDNS is a flexible, extensible DNS server which can be installed as the in-cluster DNS for pods 可视化及控制插件 Dashboard is a dashboard web interface for Kubernetes. Weave Scope is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a Weave Cloud account or host the UI yourself 参考链接 https://kubernetes.io/docs/concepts/cluster-administration/addons/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-容器的访问方式.html":{"url":"origin/kubernetes-容器的访问方式.html","title":"kubernetes容器的访问方式","keywords":"","body":"Kubernetes容器的访问方式 一、简介 当在kubernetes中使用docker镜像启动成一个容器，形成一个POD时，kubernetes的CNI组件(例如Calico、OVS)会随机动态给分配一个IP地址。通过访问这个POD IP地址加应用服务监听暴露出来的端口即可访问POD中的服务。可是POD生命周期是动态化的，IP地址重启后会改变。为屏蔽POD IP地址的动态变化和对多POD实例的负载均衡，引入了Service这个资源对象。 二、Service Service的类型大致可分成4种： ClusterIP： 默认方式。根据是否生成ClusterIP又可分为普通Service和Headless Service两类： 普通Service：通过为Kubernetes的Service分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问。为最常见的方式。 Headless Service：该服务不会分配Cluster IP，也不通过kube-proxy做反向代理和负载均衡。而是通过DNS提供稳定的网络ID来访问，DNS会将headless service的后端直接解析为podIP列表。主要供StatefulSet使用。 NodePort：通过将service的port映射到集群内每个节点的相同一个端口，实现通过nodeIP:nodePort从集群外访问服务。 LoadBalancer：和nodePort类似，不过除了使用一个Cluster IP和nodePort之外，还会向所使用的公有云申请一个负载均衡器(负载均衡器后端映射到各节点的nodePort)，实现从集群外通过LB访问服务。 ExternalName：是 Service 的特例。此模式主要面向运行在集群外部的服务，通过它可以将外部服务映射进k8s集群，且具备k8s内服务的一些特征（如具备namespace等属性），来为集群内部提供服务。此模式要求kube-dns的版本为1.7或以上。这种模式和前三种模式（除headless service）最大的不同是重定向依赖的是dns层次，而不是通过kube-proxy。 Service中主要涉及三种Port： * port 这里的port表示service暴露在clusterIP上的端口，clusterIP:Port 是提供给集群内部访问kubernetes服务的入口。 targetPort containerPort，targetPort是pod上的端口，从port和nodePort上到来的数据最终经过kube-proxy流入到后端pod的targetPort上进入容器。 nodePort nodeIP:nodePort 是提供给从集群外部访问kubernetes服务的入口。 总的来说，port和nodePort都是service的端口，前者暴露给从集群内访问服务，后者暴露给从集群外访问服务。从这两个端口到来的数据都需要经过反向代理kube-proxy流入后端具体pod的targetPort，从而进入到pod上的容器内。 1.3 IP 使用Service服务还会涉及到几种IP： ClusterIP Pod IP 地址是实际存在于某个网卡(可以是虚拟设备)上的，但clusterIP就不一样了，没有网络设备承载这个地址。它是一个虚拟地址，由kube-proxy使用iptables规则重新定向到其本地端口，再均衡到后端Pod。当kube-proxy发现一个新的service后，它会在本地节点打开一个任意端口，创建相应的iptables规则，重定向服务的clusterIP和port到这个新建的端口，开始接受到达这个服务的连接。 Pod IP Pod的IP，每个Pod启动时，会自动创建一个镜像为gcr.io/google_containers/pause的容器，Pod内部其他容器的网络模式使用container模式，并指定为pause容器的ID，即：network_mode: \"container:pause容器ID\"，使得Pod内所有容器共享pause容器的网络，与外部的通信经由此容器代理，pause容器的IP也可以称为Pod IP。 节点IP Node-IP，service对象在Cluster IP range池中分配到的IP只能在内部访问，如果服务作为一个应用程序内部的层次，还是很合适的。如果这个service作为前端服务，准备为集群外的客户提供业务，我们就需要给这个服务提供公共IP了。指定service的spec.type=NodePort，这个类型的service，系统会给它在集群的各个代理节点上分配一个节点级别的端口，能访问到代理节点的客户端都能访问这个端口，从而访问到服务。 三、Ingress 参考 https://blog.csdn.net/liukuan73/article/details/82585732 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes的容器网络.html":{"url":"origin/kubernetes的容器网络.html","title":"kubernetes的容器网络CNI","keywords":"","body":"Kubernetes容器网络CNI 一、简介 二、Calico 三、OpenvSwitch 四、Flannel Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-kube-proxy-iptables-ipvs.html":{"url":"origin/kubernetes-kube-proxy-iptables-ipvs.html","title":"kube-proxy的实现方式之iptables与ipvs模式","keywords":"","body":"Kube-proxy的iptables与ipvs实现方式 一、什么是IPVS？ IPVS (IP Virtual Server，IP虚拟服务器)是基于Netfilter的、作为linux内核的一部分实现传输层负载均衡的技术，通常称为第4层LAN交换。 IPVS集成在LVS(Linux Virtual Server)中，它在主机中运行，并在真实服务器集群前充当负载均衡器。IPVS可以将对TCP/UDP服务的请求转发给后端的真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。因此IPVS天然支持Kubernetes Service。 二、为什么选择IPVS？ 随着kubernetes使用量的增长，其资源的可扩展性变得越来越重要。特别是对于使用kubernetes运行大型工作负载的开发人员或者公司来说，service的可扩展性至关重要。 kube-proxy是为service构建路由规则的模块，之前依赖iptables来实现主要service类型的支持，比如(ClusterIP和NodePort)。但是iptables很难支持上万级的service，因为iptables纯粹是为防火墙而设计的，并且底层数据结构是内核规则的列表。 kubernetes早在1.6版本就已经有能力支持5000多节点，这样基于iptables的kube-proxy就成为集群扩容到5000节点的瓶颈。举例来说，如果在一个5000节点的集群，我们创建2000个service，并且每个service有10个pod，那么我们就会在每个节点上有至少20000条iptables规则，这会导致内核非常繁忙。 ipvs (IP Virtual Server) 实现了传输层负载均衡，也就是我们常说的4层LAN交换，作为Linux 内核的一部分。ipvs运行在主机上，在真实服务器集群前充当负载均衡器。ipvs可以将基于TCP和UDP的服务请求转发到真实服务器上，并使真实服务器的服务在单个IP 地址上显示为虚拟服务。 基于IPVS的集群内负载均衡就可以完美的解决这个问题。IPVS是专门为负载均衡设计的，并且底层使用哈希表这种非常高效的数据结构，几乎可以允许无限扩容。 三、IPVS与IPTABLES的区别 IPVS模式在Kubernetes v1.8中引入，并在v1.9中进入了beta。 1.11中实现了GA(General Availability)。IPTABLES模式在v1.1中添加，并成为自v1.2以来的默认操作模式。 IPVS和IPTABLES都基于netfilter。 IPVS模式和IPTABLES模式之间的差异如下： IPVS为大型集群提供了更好的可扩展性和性能。 IPVS支持比iptables更复杂的负载平衡算法（最小负载，最少连接，位置，加权等）。 IPVS支持服务器健康检查和连接重试等。 四、IPVS要求 k8s版本 >= v1.11 使用ipvs需要安装相应的工具来处理”yum install ipset ipvsadm -y“ 确保 ipvs已经加载内核模块， ip_vs、ip_vs_rr、ip_vs_wrr、ip_vs_sh、nf_conntrack_ipv4(如果这些内核模块不加载，当kube-proxy启动后，会退回到iptables模式) 五、IPVS原理分析 参考 https://blog.csdn.net/fanren224/article/details/86548398 https://www.jianshu.com/p/89f126b241db https://segmentfault.com/a/1190000016333317 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/k8s-cni-traefik-common-operation.html":{"url":"origin/k8s-cni-traefik-common-operation.html","title":"常用操作","keywords":"","body":"Traefik常用操作 一、设置简单认证 1、使用htpasswd工具快速生成用户名密码文件 htpasswd -bc basic-auth-secret username password 2、创建包含用户名密码的secret kubectl create secret generic test-basic-auth --from-file=basic-auth-secret --namespace=test 3、ingress配置中添加注解 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: basic ingress.kubernetes.io/auth-secret: test-basic-auth name: test namespace: test .....省略....... 参考： https://doc.traefik.io/traefik/v1.7/configuration/backends/kubernetes/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-02 13:47:56 "},"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html":{"url":"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html","title":"Kubeadm安装单机版Kubernetes","keywords":"","body":"使用Kubeadm安装单机版Kubernetes kubeadm是Kubernetes官方提供的用于快速安装 Kubernetes 集群的工具，通过将集群的各个组件进行容器化安装管理，通过kubeadm的方式安装集群比二进制的方式安装要方便 Prerequisite Hostname IP 地址 硬件 Kubernetes版本 Docker版本 allinone.k8s114.curiouser.com 172.16.1.12 最低2C2G v1.14.0 18.09.4 关闭防火墙 关闭Selinux 关闭Swap 加载br_netfilter 添加配置内核参数 hosts文件添加主机名与IP的映射关 #关闭防火墙 \\ #关闭Swap \\ #关闭Selinux \\ #加载br_netfilter \\ #添加配置内核参数 \\ #加载配置 \\ systemctl disable firewalld && systemctl stop firewalld \\ swapoff -a && sed -i 's/.\\\\*swap.\\\\*/#&/' /etc/fstab \\ setenforce 0 \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config && \\ modprobe br_netfilter && \\ bash -c 'cat > /etc/sysctl.d/k8s.conf > /etc/hosts 一、安装 安装docker kubeadm kubelet kubectl yum -y install yum-utils && \\ yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo && \\ bash -c 'cat > /etc/yum.repos.d/kubernetes.repo (可选)添加dockers日志相关配置 --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5 (可选)预拉取镜像 docker pull mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker pull mirrorgooglecontainers/pause:3.1 && \\ docker pull mirrorgooglecontainers/etcd:3.3.10 && \\ docker pull coredns/coredns:1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 && \\ docker pull calico/cni:v3.3.6 && \\ docker pull calico/node:v3.3.6 && \\ docker tag mirrorgooglecontainers/kube-proxy:v1.14.0 k8s.gcr.io/kube-proxy:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-scheduler:v1.14.0 k8s.gcr.io/kube-scheduler:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-apiserver:v1.14.0 k8s.gcr.io/kube-apiserver:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.0 k8s.gcr.io/kube-controller-manager:v1.14.0 && \\ docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 && \\ docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10 && \\ docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 && \\ docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker rmi mirrorgooglecontainers/pause:3.1 && \\ docker rmi mirrorgooglecontainers/etcd:3.3.10 && \\ docker rmi coredns/coredns:1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 初始化apiserver kubeadm init --apiserver-advertise-address=0.0.0.0 --kubernetes-version=v1.14.0 --pod-network-cidr=192.168.0.0/16 配置常规用户或root用户如何使用kubectl访问集群 mkdir -p $HOME/.kube && \\ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && \\ chown $(id -u):$(id -g) $HOME/.kube/config 设置Master节点可被调度 kubectl taint nodes --all node-role.kubernetes.io/master- #该参数node-role.kubernetes.io/master会污染所有节点，包括master节点,这意味着调度器可以调度POD到所有节点。 (可选)设置Kubectl命令别名及命令补全 yum install -y bash-completion && \\ echo \"alias k='kubectl'\" >> /etc/bashrc && \\ source 二、安装容器网络插件 Calico kubeadm初始化apiserver时添加\"--pod-network-cidr=192.168.0.0/16\" 网络工作在amd64，arm64，ppc64le kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml && \\ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml Flannel Prerequisite 设置 /proc/sys/net/bridge/bridge-nf-call-iptables为1，将桥接的IPv4流量传递到iptables的链 sysctl net.bridge.bridge-nf-call-iptables=1 kubeadm初始化apiserver时添加\"--pod-network-cidr=10.244.0.0/16\" flannel网络工作在amd64, arm, arm64, ppc64le,s390x 安装 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 三、验证 查看所有namespace下的POD kubectl get pods --all-namespaces kubectl get pod -n kube-system 查看集群Node节点 kubectl get node systemctl status kubelet.service 查看版本 kubectl version 显示集群信息 kubectl cluster-info 四、添加Node节点 kubeadm join 172.16.1.12:6443 --token i7xcb9.sz5t4sa8xx3ntc2h --discovery-token-ca-cert-hash sha256:487275a22ea4af5a1ea30ee4b0f21f8c27104d17f6a259bf4990f1569a3301cd 查看Master的Token kubeadm token list Master创建Token kubeadm token create Master节点创建\"--discovery-token-ca-cert-hash\"值 openssl x509 -pubkey -in /etc/kubernet 五、安装UI管理界面 DashBoard 项目GitHub：https://github.com/kubernetes/dashboard.git # ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role & Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"create\"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics from heapster. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 #===修改原rolebind类型RoleBinding为ClusterRoleBinding kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io #修改原role类型Role为ClusterRole kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 ports: #====修改原容器端口8443为9090 - containerPort: 9090 protocol: TCP args: #==== #- --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTP path: / #修改原健康检查端口8443为9090 port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # #使用Nodeport的方式访问Dashboard kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-external namespace: kube-system spec: ports: - port: 9090 targetPort: 9090 nodePort: 30090 type: NodePort selector: k8s-app: kubernetes-dashboard 拉取Template中使用的Image docker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 && \\ docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 && \\ docker rmi mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 Weave Scope 官方文档： https://www.weave.works/docs/scope/latest/installing/#k8s 安装部署 kubectl apply -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" （可选）修改svc使用NodePort访问 $ kubectl edit service/weave-scope-app -n weave apiVersion: v1 kind: Service #.........省略........ spec: externalTrafficPolicy: Cluster ports: - name: app #===== nodePort: 30040 port: 80 protocol: TCP targetPort: 4040 selector: app: weave-scope name: weave-scope-app weave-cloud-component: scope weave-scope-component: app sessionAffinity: None #====== type: NodePort Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-Kubeasz二进制安装集群.html":{"url":"origin/kubernetes-Kubeasz二进制安装集群.html","title":"Kubeasz二进制安装Kubernetes集群","keywords":"","body":"使用Kubeasz离线安装二进制Kubernetes集群 一、集群规划 k8s版本：1.18 CNI：Caclio kubeproxy模式：IPVS 证书有效期：100年 IngressContoller：Traefik 2.0 Registry：Habor CSI：NFS Provisioner、Ceph RBD Provisioner、Ceph Filesystem Provisioner、LocalVolume Provisioner 主机FQDN Domain：k8s118.curiouser.com 主机名 硬件配置 IP地址 服务 tools 8C16G100G /data100G 192.168.1.60 NFS Server、Ceph、Harbor、Nginx、Chrony Node1 8C16G100G /data100G 192.168.1.61 AnsibleK8S Master(etcd、apiserver、controllermanager、scheduler、kubelet、kueb-proxy、docker) Node2 8C16G100G /data100G 192.168.1.62 K8S Worker(kubelet、kube-proxy、docker) Node3 8C16G100G /data100G 192.168.1.63 K8S Worker(kubelet、kube-proxy、docker) 二、Kubeasz ansible脚本简介 Kubeasz Github地址：https://github.com/easzlab/kubeasz kubeasz 2.0.1 开始支持完全离线安装，目前已测试 Ubuntu1604|1804 CentOS7 Debian9|10 系统。 kubeasz 项目代码 --> /etc/ansible kubernetes 集群组件二进制 --> /etc/ansible/bin 其他集群组件二进制（etcd/CNI等） --> /etc/ansible/bin 操作系统基础依赖软件包（haproxy/ipvsadm/ipset/socat等） --> /etc/ansible/down/packages 集群基本插件镜像（coredns/dashboard/metrics-server等） --> /etc/ansible/down # 分步安装 ansible-playbook 01.prepare.yml ansible-playbook 02.etcd.yml ansible-playbook 03.docker.yml ansible-playbook 04.kube-master.yml ansible-playbook 05.kube-node.yml ansible-playbook 06.network.yml ansible-playbook 07.cluster-addon.yml # 一步安装 ansible-playbook 90.setup.yml kubeasz创建集群主要在以下两个地方进行配置： ansible hosts 文件（模板在examples目录）：集群主要节点定义和主要参数配置、全局变量 roles/xxx/defaults/main.yml文件：其他参数配置或者部分组件附加参数 配置 lb 节点负载均衡算法：修改roles/lb/defaults/main.yml 变量 BALANCE_ALG: \"roundrobin\" 配置 docker 国内镜像加速站点：修改 roles/docker/defaults/main.yml相关变量 配置 apiserver 支持公网域名：修改roles/kube-master/defaults/main.yml 相关变量 配置 flannel 使用镜像版本：修改roles/flannel/defaults/main.yml相关变量 配置选择不同 addon 组件：修改roles/cluster-addon/defaults/main.yml 作为 kubeasz 项目的推荐命令行脚本，easzctl 十分轻量、简单；（后续会不断完善补充） 命令集 1：集群层面操作 切换/创建集群 context 删除当前集群 显示所有集群 创建集群 创建单机集群（类似 minikube） 命令集 2：集群内部操作 增加工作节点 增加主节点 增加 etcd 节点 删除 etcd 节点 删除任意节点 升级集群 命令集3：额外操作 开启/关闭基础认证 集群 context 由 ansible hosts 配置、roles 配置等组成，用以区分不同的 k8s 集群，从而实现多集群的创建和管理；当然 easzctl 命令行不是必须的，你仍旧可以使用之前熟悉的方式安装/管理集群。 典型 easzctl 创建管理的集群拓扑如下 +----------------+ +-----------------+ |easzctl 1.1.1.1 | |cluster-aio: | +--+---+---+-----+ | | | | | |master 4.4.4.4 | | | +-------------------->+etcd 4.4.4.4 | | | |node 4.4.4.4 | | +--------------+ +-----------------+ | | v v +--+------------+ +---+----------------------------+ | cluster-1: | | cluster-2: | | | | | | master 2.2.2.1| | master 3.3.3.1/3.3.3.2 | | etcd 2.2.2.2| | etcd 3.3.3.1/3.3.3.2/3.3.3.3 | | node 2.2.2.3| | node 3.3.3.4/3.3.3.5/3.3.3.6 | +---------------+ +--------------------------------+ 使用 easzctl 举例 随时运行 easzctl help 获取命令行提示信息 1.创建 context：准备集群名称（例如：test-cluster1），运行 easzctl checkout test-cluster1 如果 context: test-cluster1 不存在，那么会根据 default 配置创建它；如果存在则切换当前 context 为 test-cluster1 2.准备 context 以后，根据你的需要配置 ansible hosts 文件和其他配置，然后运行 easzctl setup 3.安装成功后，运行 easzctl list 显示当前所有集群信息 4.重复步骤 1/2 可以创建多个集群 5.切换到某个集群 easzctl checkout xxxx，然后执行增加/删除节点操作 三、安装k8s 1.18集群 0、参考 https://github.com/easzlab/kubeasz/blob/master/docs/setup/00-planning_and_overall_intro.md https://github.com/easzlab/kubeasz/blob/master/docs/setup/config_guide.md 1、各节点基础OS配置 rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.61\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node1.k8s118.curiouser.com reboot now rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.62\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node2.k8s118.curiouser.com reboot now rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.63\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node3.k8s118.curiouser.com reboot now 2、Node1节点安装ansible pip install pip --upgrade -i https://mirrors.aliyun.com/pypi/simple/ pip install ansible==2.6.18 netaddr==0.7.19 -i https://mirrors.aliyun.com/pypi/simple/ 3、Node1节点配置节点FQDN与IP的映射并打通SSH免密钥登录 echo \"192.168.1.60 tools.k8s118.curiouser.com tools\" >> /etc/hosts echo \"192.168.1.61 node1.k8s118.curiouser.com node1\" >> /etc/hosts echo \"192.168.1.62 node2.k8s118.curiouser.com node2\" >> /etc/hosts echo \"192.168.1.63 node3.k8s118.curiouser.com node3\" >> /etc/hosts ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id tools ssh-copy-id node1 ssh-copy-id node2 ssh-copy-id node3 4、Node1节点下载kubeasz中的安装准备工具脚本easzup export release=2.2.0 curl -C- -fLO --retry 3 https://github.com/easzlab/kubeasz/releases/download/$release/easzup chmod +x ./easzup ./easzup -D 执行成功后，所有文件均已整理好放入目录/etc/ansible，只要把该目录整体复制到任何离线的机器上，即可开始安装集群 离线文件不包括： 管理端 ansible 安装，但可以使用 kubeasz 容器运行 ansible 脚本 其他更多 kubernetes 插件镜像 5、配置k8s集群参数的主机清单 模版配置：https://github.com/easzlab/kubeasz/blob/master/example/hosts.multi-node cp /etc/ansible/example/hosts.multi-node /etc/ansible/hosts [etcd] 192.168.1.61 NODE_NAME=etcd1 [kube-master] 192.168.1.61 [kube-node] 192.168.1.62 192.168.1.63 # [optional] loadbalance for accessing k8s from outside [ex-lb] 192.168.1.60 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.6250 EX_APISERVER_PORT=8443 # [optional] ntp server for the cluster [chrony] 192.168.1.60 [all:vars] # Cluster container-runtime supported: docker, containerd CONTAINER_RUNTIME=\"docker\" # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\"calico\" # Service proxy mode of kube-proxy: 'iptables' or 'ipvs' PROXY_MODE=\"ipvs\" # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.68.0.0/16\" # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\"172.20.0.0/16\" # NodePort Range NODE_PORT_RANGE=\"20000-40000\" # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"cluster.local.\" # -------- Additional Variables (don't change the default value right now) --- # Binaries Directory bin_dir=\"/opt/kube/bin\" # CA and other components cert/key Directory ca_dir=\"/etc/kubernetes/ssl\" # Deploy Directory (kubeasz workspace) base_dir=\"/etc/ansible\" 6、修改Ansible中K8S服务配置 ⓪设置离线安装 sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/chrony/defaults/main.yml sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/ex-lb/defaults/main.yml sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/kube-node/defaults/main.yml sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/prepare/defaults/main.yml ①修改集群证书相关配置 参考：https://github.com/easzlab/kubeasz/blob/master/docs/setup/01-CA_and_prerequisite.md sed -i 's/HangZhou/Shanghai/g' /etc/ansible/roles/deploy/templates/* vi /etc/ansible/roles/deploy/defaults/main.yml # CA 证书相关参数 CA_EXPIRY: \"876000h\" CERT_EXPIRY: \"876000h\" # apiserver 默认第一个master节点 KUBE_APISERVER: \"https://{{ groups['kube-master'][0] }}:6443\" CLUSTER_NAME: \"Curiouser\" CREATE_READONLY_KUBECONFIG: false ②修改Docker配置 vi /etc/ansible/roles/docker/defaults/main.yml # docker日志相关 LOG_DRIVER: \"json-file\" LOG_LEVEL: \"warn\" LOG_MAX_SIZE: \"50m\" LOG_MAX_FILE: 10 # docker容器存储目录 STORAGE_DIR: \"/data/docker\" # 开启Restful API ENABLE_REMOTE_API: false # 启用 docker 仓库镜像 ENABLE_MIRROR_REGISTRY: true # 设置 docker 仓库镜像 REG_MIRRORS: '[\"https://dockerhub.azk8s.cn\", \"https://docker.mirrors.ustc.edu.cn\"]' # 信任的HTTP仓库 INSECURE_REG: '[\"127.0.0.1/8\",\"192.168.1.60\"]' ③其他插件配置 vi /etc/ansible/roles/cluster-addon/defaults/main.yml # dns 自动安装，'dns_backend'可选\"coredns\"和“kubedns” dns_install: \"yes\" dns_backend: \"coredns\" # 设置 dns svc ip (这里选用 SERVICE_CIDR 中第2个IP) CLUSTER_DNS_SVC_IP: \"{{ SERVICE_CIDR | ipaddr('net') | ipaddr(2) | ipaddr('address') }}\" kubednsVer: \"1.14.13\" corednsVer: \"1.6.6\" kubedns_offline: \"kubedns_{{ kubednsVer }}.tar\" coredns_offline: \"coredns_{{ corednsVer }}.tar\" dns_offline: \"{%- if dns_backend == 'coredns' -%} \\ {{ coredns_offline }} \\ {%- else -%} \\ {{ kubedns_offline }} \\ {%- endif -%}\" # metric server 自动安装 metricsserver_install: \"yes\" metricsVer: \"v0.3.6\" metricsserver_offline: \"metrics-server_{{ metricsVer }}.tar\" # dashboard 自动安装 # dashboard v2.x.x 不依赖于heapster dashboard_install: \"no\" dashboardVer: \"v2.0.0-rc3\" dashboard_offline: \"dashboard_{{ dashboardVer }}.tar\" dashboardMetricsScraperVer: \"v1.0.3\" metricsscraper_offline: \"metrics-scraper_{{ dashboardMetricsScraperVer }}.tar\" # ingress 自动安装，可选 \"traefik\" 和 \"nginx-ingress\" ingress_install: \"no\" ingress_backend: \"traefik\" traefikVer: \"v1.7.20\" nginxingVer: \"0.21.0\" traefik_offline: \"traefik_{{ traefikVer }}.tar\" nginx_ingress_offline: \"nginx_ingress_{{ nginxingVer }}.tar\" # metallb 自动安装 metallb_install: \"no\" metallbVer: \"v0.7.3\" # 模式选择: 二层 \"layer2\" 或者三层 \"bgp\" metallb_protocol: \"layer2\" metallb_offline: \"metallb_{{ metallbVer }}.tar\" metallb_vip_pool: \"192.168.1.240/29\" # efk 自动安装 #efk_install: \"no\" # prometheus 自动安装 #prometheus_install: \"no\" 7、执行ansible playbook ansible-playbook /etc/ansible/90.setup.yml 8、验证 kubectl get node -owide kubectl get pod --all-namespaces /opt/kube/bin/calicoctl get node 三、Tools安装基础服务 1、安装NFS Server yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ echo \"/data/nfs/k8s-storage 192.168.1.0/24(rw,no_root_squash,sync)\" >> /etc/exports && \\ mkdir -p /data/nfs/k8s-storage && \\ exportfs -a && \\ showmount -e $HOSTNAME 2、安装Harbor 最新kubeasz安装脚本中不支持安装最新版本的harbor,所以要使用docker compose在tools节点安装 ①安装docker、docker-compose yum install -y yum-utils && \\ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo && \\ yum list docker-ce --showduplicates | sort -r && \\ yum install docker-ce-18.06.3.ce docker-compose && \\ bash -c 'cat > /etc/docker/daemon.json ②部署Harbor export harbor_ver=v2.0.0 && \\ wget https://github.com/goharbor/harbor/releases/download/$harbor_ver/harbor-online-installer-$harbor_ver.tgz && \\ tar -zxvf harbor-online-installer-* && \\ rm -rf harbor-online-installer-* && \\ mkdir -p /data/harbor/{data,logs} && \\ mv harbor /data/harbor/ && \\ cd /data/harbor/harbor && \\ bash -c 'cat > /data/harbor/harbor/harbor.yml ③验证 访问http://192.168.1.60 四、K8S集群配置 1、在K8S集群中添加harbor用户认证的Secret ①Harbor中创建用户并授权 在Harbor中创建用户k8s，在指定的仓库中授予访客的权限，仅限于可以拉取镜像 ②在K8S指定命名空间下创建harbor账号的Secret for i in {default,kube-system} ; do kubectl create secret docker-registry harbor-secret --docker-server=192.168.1.60 --docker-username=k8s --docker-password=**** --docker-email=***@163.com -n $i ;done ③指定默认default ServiceAccount的镜像拉取Secret 不用每个Deployment中都要添加imagepullsecret kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"harbor-secret\"}]}' -n kube-system 2、Node1节点安装Helm 3 ①下载安装helm3 export helm_ver=v3.2.1 wget https://get.helm.sh/helm-$helm_ver-linux-amd64.tar.gz tar -zxvf helm-* linux-amd64/helm mv linux-amd64/helm /usr/local/bin chmod +x /usr/local/bin/helm rm -rf linux-amd64 helm-* ②添加远程charts仓库 helm repo add googleapis-incubator https://kubernetes-charts-incubator.storage.googleapis.com helm repo add googleapis-stable https://kubernetes-charts.storage.googleapis.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo add traefik https://containous.github.io/traefik-helm-chart helm repo add harbor https://helm.goharbor.io helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm repo add elastic https://helm.elastic.co helm repo add kong https://charts.konghq.com helm repo add pingcap https://charts.pingcap.org/ helm repo list helm repo update 3、配置CSI Ceph Filesystem Provisoner ①tools节点安装单节点Ceph Filesystem，并获取admin用户的密钥环 参考： ceph-filesystem单节点安装 ceph-filesystem-provisioner ceph auth get client.admin ②创建Ceph admin用户的Secret kubectl create secret generic ceph-admin --type=\"kubernetes.io/rbd\" --from-literal=key='******' --namespace=default ③部署带有RBAC的Ceph Filesystem Provisoner 对象资源 --- apiVersion: v1 kind: ServiceAccount metadata: name: ceph-fs-provisioner namespace: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ceph-fs-provisioner namespace: default rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ceph-fs-provisioner namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ceph-fs-provisioner subjects: - kind: ServiceAccount name: ceph-fs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-fs-provisioner namespace: default rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-fs-provisioner subjects: - kind: ServiceAccount name: ceph-fs-provisioner namespace: default roleRef: kind: ClusterRole name: ceph-fs-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: ceph-fs-provisioner namespace: default spec: selector: matchLabels: app: ceph-fs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: ceph-fs-provisioner spec: imagePullSecrets: - name: harbor-secret containers: - name: ceph-fs-provisioner image: \"192.168.1.60/tools/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: default command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: ceph-fs-provisioner ④创建storageclass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-fs provisioner: ceph.com/cephfs parameters: monitors: 192.168.1.60:6789 adminId: admin adminSecretName: ceph-admin adminSecretNamespace: \"default\" claimRoot: /ceph-fs-pvc-volumes ⑤验证测试 创建pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-fs-pvc-test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-fs 创建临时POD挂载PVC，写入测试数据 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: ceph-fs-pvc-test 将Ceph Filesystem的pool挂载到Node1节点上查看其中是否产生文件 mkdir /mnt/mycephfs mount -t ceph tools.k8s118.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=***** tree /mnt/mycephfs/pvc-volumes/kubernetes /mnt/mycephfs/pvc-volumes/kubernetes └── kubernetes-dynamic-pvc-33ee21eb-984f-11ea-be0a-52a5fa47eee8 └── 1.log └── 2.log Ceph RBD Provisioner ①tools节点安装单节点Ceph，并获取admin用户的密钥环 参考：ceph-rbd单节点安装 ②部署带有RBAC的Ceph RBD Provisioner对象资源 --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-rbd-provisioner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ceph-rbd-provisioner rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ceph-rbd-provisioner roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ceph-rbd-provisioner subjects: - kind: ServiceAccount name: ceph-rbd-provisioner namespace: default --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-rbd-provisioner subjects: - kind: ServiceAccount name: ceph-rbd-provisioner namespace: default roleRef: kind: ClusterRole name: ceph-rbd-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: v1 kind: ServiceAccount metadata: name: ceph-rbd-provisioner --- apiVersion: apps/v1 kind: Deployment metadata: name: ceph-rbd-provisioner spec: replicas: 1 selector: matchLabels: app: ceph-rbd-provisioner strategy: type: Recreate template: metadata: labels: app: ceph-rbd-provisioner spec: imagePullSecrets: - name: harbor-secret containers: - name: ceph-rbd-provisioner image: \"192.168.1.60/tools/ceph-rbd-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph-rbd-provisioner - name: PROVISIONER_SECRET_NAMESPACE value: default serviceAccount: ceph-rbd-provisioner ③创建storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd provisioner: ceph-rbd-provisioner parameters: monitors: 192.168.1.60:6789 adminId: admin adminSecretName: ceph-admin adminSecretNamespace: default pool: rbd #ceph创建是默认rbd池 userId: admin userSecretName: ceph-admin userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" # fsType：Kubernetes 支持的 fsType。默认：\"ext4\"。 # imageFormat：Ceph RBD 镜像格式，“1” 或者 “2”。默认值是 “1”。 # imageFeatures：这个参数是可选的，只能在你将 imageFormat 设置为 “2” 才使用。 目前支持的功能只是 # # layering。默认是 “\"，没有功能打开。 ④所有K8s节点安装ceph-common包 kubernetes 的所有节点（尤其是 master 节点）上需要安装 ceph-common客户端,不然稍后测试时，pvc/pv创建都正常,但是pod挂载失败，报failed to create rbd image: executable file not found in $PATH, command output [ceph] name=Ceph packages for $basearch baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 priority=1 type=rpm-md gpgkey=http://mirrors.163.com/ceph/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 priority=1 type=rpm-md gpgkey=http://mirrors.163.com/ceph/keys/release.asc yum install -y ceph-common ⑤验证测试 创建pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-rbd-pvc-test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd 创建临时POD挂载PVC，写入测试数据 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: ceph-rbd-pvc-test ⑥排错方式 persistent-volume-controller服务受kube-controller-manager控制，可以通过查看kube-controller-manager的日志排错 journalctl -xe -u kube-controller-manager.service ⑦参考 https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/rbd https://blog.51cto.com/wangzhijian/2159701 NFS Client Provisoner ①拉取镜像并推送到harbor中 docker login -u admin -p ***** 192.168.1.60 docekr pull quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11 docekr tag quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11 192.168.1.60/tools/nfs-client-provisioner:v3.1.0-k8s1.11 docekr push 192.168.1.60/tools/nfs-client-provisioner:v3.1.0-k8s1.11 ②使用helm部署NFS Client Provisoner helm install nfs-client-provisioner --set nfs.server=192.168.1.60 --set nfs.path=/data/nfs/k8s-storage googleapis-stable/nfs-client-provisioner --namespace default --set image.repository=192.168.1.60/tools/nfs-client-provisioner --set image.tag=v3.1.0-k8s1.11 --set storageClass.defaultClass=true bash -c 'cat > /tmp/a ③测试验证 创建一个PVC vi /tmp/test.pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi kubectl apply -f /tmp/test.pvc -n default 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 验证NFS Server节点tools上NFS目录下是否产生文件 ls /data/nfs/k8s-storage/default-test-pvc-59663468-2352-4be5-8b08-432045ce8a18/ 1.log 2.log 删除对应测试pod、pvc kubectl -n default delete pod/counter pvc/test --force --grace-period 0 五、K8S Worker节点管理 参考：https://github.com/easzlab/kubeasz/blob/master/docs/op/op-node.md 1、添加节点 新增kube-node节点大致流程为：/etc/ansible/tools/02.addnode.yml [可选]新节点安装 chrony 时间同步 新节点预处理 prepare 新节点安装 docker 服务 新节点安装 kube-node 服务 新节点安装网络插件相关 ①创建新节点VM并配置OS基础配置 rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.64\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node4.k8s118.curiouser.com reboot now ②Node1节点配置新增节点FQDN与IP的映射并打通SSH免密钥登录 echo \"192.168.1.64 node4.k8s118.curiouser.com node4\" >> /etc/hosts ssh-copy-id node4 ③Node1节点运行easzctl add-node命令 easzctl add-node 192.168.1.64 ④验证 kubectl get node -owide ⑤（可选）添加非标准 ssh 22端口的节点 目前 easzctl 暂不支持自动添加非标准 ssh 端口的节点，可以手动操作如下： 假设待添加节点192.168.2.1，ssh 端口 10022；配置免密登录ssh-copy-id -p 10022 192.168.2.1，按提示输入密码 在 /etc/ansible/hosts文件 [kube-node] 组下添加一行： 192.168.2.1 ansible_ssh_port=10022 最后执行 ansible-playbook /etc/ansible/tools/02.addnode.yml -e NODE_TO_ADD=192.168.2.1 2、删除节点 删除 node 节点流程：/etc/ansible/tools/12.delnode.yml 检测是否可以删除 迁移节点上的 pod 删除 node 相关服务及文件 从集群删除 node ①Node1节点运行easzctl del-node命令 easzctl del-node 192.168.1.64 六、K8S Master节点管理 参考：https://github.com/easzlab/kubeasz/blob/master/docs/op/op-master.md 1、添加Master节点 新增kube-master节点大致流程为：/etc/ansible/tools/03.addmaster.yml [可选]新节点安装 chrony 时间同步 新节点预处理 prepare 新节点安装 docker 服务 新节点安装 kube-master 服务 新节点安装 kube-node 服务 新节点安装网络插件相关 禁止业务 pod调度到新master节点 更新 node 节点 haproxy 负载均衡并重启 ①创建新节点VM并配置OS基础配置 rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.64\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node4.k8s118.curiouser.com reboot now ②Node1节点配置新增节点FQDN与IP的映射并打通SSH免密钥登录 echo \"192.168.1.64 node4.k8s118.curiouser.com node4\" >> /etc/hosts ssh-copy-id node4 ③Node1节点运行easzctl add-master命令 easzctl add-master 192.168.1.64 ④验证 # 在新节点master 服务状态 systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler # 查看新master的服务日志 journalctl -u kube-apiserver -f # 查看集群节点，可以看到新 master节点 Ready, 并且禁止了POD 调度功能 kubectl get node 2、删除 Master 节点 删除kube-master节点大致流程为：/etc/ansible/tools/13.delmaster.yml 检测是否可以删除 迁移节点 pod 删除 master 相关服务及文件 删除 node 相关服务及文件 从集群删除 node 节点 从 ansible hosts 移除节点 在 ansible 控制端更新 kubeconfig 更新 node 节点 haproxy 配置 ①Node1节点运行easzctl del-master命令 easzctl del-master 192.168.1.64 七、集群备份与恢复 参考：https://github.com/easzlab/kubeasz/blob/master/docs/op/cluster_restore.md 在高可用k8s集群中 etcd集群保存了整个集群的状态，因此这里的备份与恢复重点就是： 从运行的etcd集群备份数据到磁盘文件 从etcd备份文件恢复数据，从而使集群恢复到备份时状态 备份与恢复操作说明 1.首先搭建一个测试集群，部署几个测试deployment，验证集群各项正常后，进行一次备份： $ ansible-playbook /etc/ansible/23.backup.yml 执行完毕可以在备份目录下检查备份情况，示例如下： /etc/ansible/.cluster/backup/ ├── hosts ├── hosts-201907030954 ├── snapshot-201907030954.db ├── snapshot-201907031048.db └── snapshot.db 2.模拟误删除操作（略） 3.恢复集群及验证 可以在 roles/cluster-restore/defaults/main.yml 文件中配置需要恢复的 etcd备份版本（从上述备份目录中选取），默认使用最近一次备份；执行恢复后，需要一定时间等待 pod/svc 等资源恢复重建。 $ ansible-playbook /etc/ansible/24.restore.yml 如果集群主要组件（master/etcd/node）等出现不可恢复问题，可以尝试使用如下步骤 清理 --> 创建 --> 恢复 ansible-playbook /etc/ansible/99.clean.yml ansible-playbook /etc/ansible/90.setup.yml ansible-playbook /etc/ansible/24.restore.yml 参考 https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md 八、升级 快速k8s版本升级 快速升级是指只升级k8s版本，比较常见如Bug修复 重要特性发布时使用。 首先去官网release下载待升级的k8s版本，例如https://dl.k8s.io/v1.11.5/kubernetes-server-linux-amd64.tar.gz 解压下载的tar.gz文件，找到如下 kube* 开头的二进制，复制替换ansible控制端目录 /etc/ansible/bin 对应文件 kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler 在ansible控制端执行ansible-playbook -t upgrade_k8s 22.upgrade.yml即可完成k8s 升级，不会中断业务应用 如果使用 easzctl 命令行，可按如下执行： 首先确认待升级的集群（如果有多集群的话） easzctl checkout 执行升级 easzctl upgrade 其他升级说明 其他升级是指升级k8s组件包括：etcd版本 docker版本，一般不需要用到，以下仅作说明。 1.下载所有组件相关新的二进制解压并替换 /etc/ansible/bin/ 目录下文件 2.升级 etcd: ansible-playbook -t upgrade_etcd 02.etcd.yml，注意：etcd 版本只能升级不能降低！ 3.升级 docker （建议使用k8s官方支持的docker稳定版本） 如果可以接受短暂业务中断，执行 ansible-playbook -t upgrade_docker 03.docker.yml 如果要求零中断升级，执行 ``` ansible-playbook -t download_docker 03.docker.yml ``` ，然后手动执行如下 - 待升级节点，先应用`kubectl cordon`和`kubectl drain`命令迁移业务pod - 待升级节点执行 `systemctl restart docker` - 恢复节点可调度 `kubectl uncordon` Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-01 18:19:38 "},"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html":{"url":"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html","title":"kubernetes集群性能监控","keywords":"","body":"Kubernetes或Openshift的Prometheus监控体系 一、Overview 在早期，也就是 1.10 以前的 K8s 版本。大家都会使用类似像 Heapster 这样的组件来去进行监控的采集。那为什么 Kubernetes 会将 Heapster 放弃掉而转换到 metrics-service 呢？其实这个主要的一个动力来源是由于 Heapster 在做监控数据接口的标准化。为什么要做监控数据接口标准化呢？ 第一点在于客户的需求是千变万化的，比如说今天用 Heapster 进行了基础数据的一个资源采集，那明天的时候，我想在应用里面暴露在线人数的一个数据接口，放到自己的接口系统里进行数据的一个展现，以及类似像 HPA 的一个数据消费。那这个场景在 Heapster 下能不能做呢？答案是不可以的，所以这就是 Heapster 自身扩展性的弊端； 第二点是 Heapster 里面为了保证数据的离线能力，提供了很多的 sink，而这个 sink 包含了类似像 influxdb、sls、钉钉等等一系列 sink。这个 sink 主要做的是把数据采集下来，并且把这个数据离线走，然后很多客户会用 influxdb 做这个数据离线，在 influxdb 上去接入类似像 grafana 监控数据的一个可视化的软件，来实践监控数据的可视化。 但是后来社区发现，这些 sink 很多时候都是没有人来维护的。这也导致整个 Heapster 的项目有很多的 bug，这个 bug 一直存留在社区里面，是没有人修复的，这个也是会给社区的项目的活跃度包括项目的稳定性带来了很多的挑战。 基于这两点原因，K8s 把 Heapster 抛弃掉了，然后做了一个精简版的监控采集组件，叫做 metrics-server。 上图是 Heapster 内部的一个架构。大家可以发现它分为几个部分，第一个部分是 core 部分，然后上层是有一个通过标准的 http 或者 https 暴露的这个 API。然后中间是 source 的部分，source 部分相当于是采集数据暴露的不同的接口，然后 processor 的部分是进行数据转换以及数据聚合的部分。最后是 sink 部分，sink 部分是负责数据离线的，这个是早期的 Heapster 的一个应用的架构。那到后期的时候呢，K8s 做了这个监控接口的一个标准化，逐渐就把 Heapster 进行了裁剪，转化成了 metrics-server。 目前版本的 metrics-server 大致的一个结构就变成了上图这样，是非常简单的：有一个 core 层、中间的 source 层，以及简单的 API 层，额外增加了 API Registration 这层。这层的作用就是它可以把相应的数据接口注册到 K8s 的 API server 之上，以后客户不再需要通过这个 API 层去访问 metrics-server，而是可以通过这个 API 注册层，通过 API server 访问 API 注册层，再到 metrics-server。这样的话，真正的数据消费方可能感知到的并不是一个 metrics-server，而是说感知到的是实现了这样一个 API 的具体的实现，而这个实现是 metrics-server。这个就是 metrics-server 改动最大的一个地方。 Kubernetes 的监控接口标准 在 K8s 里面针对于监控，有三种不同的接口标准。它将监控的数据消费能力进行了标准化和解耦，实现了一个与社区的融合，社区里面主要分为三类。 第一类 Resource Metrice 对应的接口是 metrics.k8s.io，主要的实现就是 metrics-server，它提供的是资源的监控，比较常见的是节点级别、pod 级别、namespace 级别、class 级别。这类的监控指标都可以通过 metrics.k8s.io 这个接口获取到。 第二类 Custom Metrics 对应的 API 是 custom.metrics.k8s.io，主要的实现是 Prometheus。它提供的是资源监控和自定义监控，资源监控和上面的资源监控其实是有覆盖关系的，而这个自定义监控指的是：比如应用上面想暴露一个类似像在线人数，或者说调用后面的这个数据库的 MySQL 的慢查询。这些其实都是可以在应用层做自己的定义的，然后并通过标准的 Prometheus 的 client，暴露出相应的 metrics，然后再被 Prometheus 进行采集。 而这类的接口一旦采集上来也是可以通过类似像 custom.metrics.k8s.io 这样一个接口的标准来进行数据消费的，也就是说现在如果以这种方式接入的 Prometheus，那你就可以通过 custom.metrics.k8s.io 这个接口来进行 HPA，进行数据消费。 第三类 External Metrics External Metrics 其实是比较特殊的一类，因为我们知道 K8s 现在已经成为了云原生接口的一个实现标准。很多时候在云上打交道的是云服务，比如说在一个应用里面用到了前面的是消息队列，后面的是 RBS 数据库。那有时在进行数据消费的时候，同时需要去消费一些云产品的监控指标，类似像消息队列中消息的数目，或者是接入层 SLB 的 connection 数目，SLB 上层的 200 个请求数目等等，这些监控指标。 那怎么去消费呢？也是在 K8s 里面实现了一个标准，就是 external.metrics.k8s.io。主要的实现厂商就是各个云厂商的 provider，通过这个 provider 可以通过云资源的监控指标。在阿里云上面也实现了阿里巴巴 cloud metrics adapter 用来提供这个标准的 external.metrics.k8s.io 的一个实现。 二、Kube-State-Metrics 监听 Kubernetes API server 并自动生成相关对象的metrics信息(并不修改相关对象的配置)，在80端口(默认)暴露出HTTP的endpoint /metric GIthub：https://github.com/kubernetes/kube-state-metrics kube-state-metrics VS metrics-server 三、Metrics-Server 四、OpenShfit cluster-monitoring-operator cluster-monitoring-operator ：负责在 OpenShfit 环境中部署基于 Prometheus 的监控系统 GIthub：https://github.com/openshift/cluster-monitoring-operator 部署基于 Prometheus 监控系统中的组件 Prometheus Operator Prometheus Alertmanager cluster for cluster and application level alerting kube-state-metrics node_exporter 五、Prometheus Operator prometheus operator：使用operator部署、配置、管理Prometheus和Alertmanager GIthub： https://github.com/coreos/prometheus-operator 相关博客：https://blog.csdn.net/ygqygq2/article/details/83655552 功能： Create/Destroy: 在Kubernetes namespace中更容易启动一个Prometheus实例，一个特定的应用程序或团队更容易使用Operator。 Simple Configuration: 配置Prometheus的基础东西，比如在Kubernetes的本地资源versions, persistence, retention policies, 和replicas。 Target Services via Labels: 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。 架构 node-exporter：以Daemonset的形式部署在Openshift集群的各个节点上，采集OS级别的metrics信息 监控的Target Prometheus itself Prometheus-Operator cluster-monitoring-operator Alertmanager cluster instances Kubernetes apiserver kubelets (the kubelet embeds cAdvisor for per container metrics) kube-controllers kube-state-metrics node-exporter etcd (if etcd monitoring is enabled) Note: Prometheus Pod 中，除了 Prometheus 容器外，还有一个 prometheus-config-reloader 容器。它负责导入在需要的时候让Prometheus 重新加载配置文件。 配置文件被以 Secret 形式创建并挂载给 prometheus-config-reloader Pod。一旦配置有变化，它会调用 Prometheus 的接口，使其重新加载配置文件。 相关链接 https://docs.okd.io/3.11/install_config/prometheus_cluster_monitoring.html#prometheus-cluster-monitoring http://www.cnblogs.com/sammyliu/p/10155442.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-集群组件.html":{"url":"origin/kubernetes-集群组件.html","title":"kubernetes集群组件","keywords":"","body":"Kubernetes的集群组件 一、集群架构 二、etcd 是高可用的 key/value 存储系统，用于持久化存储集群中的所有资源对象，比如：Node，Pod，Serivce，RC,namespace 等。API server 提供了操作 etcd 的封装接口 API，以 Rest 的方式提供，这些 API 基本上都是集群中资源对象的增删改查及监听资源变化的接口，比如创建 Pod、RC，监听 Pod 的变化等接口。API server 是连接其他所有服务组件的中间枢纽。 三、kube-apiserver 提供了资源对象的唯一操作入口，其他组件都必须通过它提供的 API 来操作资源数据，通过对相关的资源数据\"全量查询\" + \"变化监听\"，这些组件可以很\"实时\"的完成相关的业务功能。比如提交一个新的 Pod 到 API server 中，Controller Manger 可以立即就发现并开始作用。它还有一套完备的安全机制，包括认证、授权及准入控制等相关模块。 四、kube-controllermanager 集群内部的管理控制中心，主要完成了集群的故障检测和恢复的自动化工作。比如对 RC 定义的 Pod 进行维护；根据 service 和 Pod 的关系，完成服务的 Endpoints 对象的创建和更新；还有 Node 的发现、管理和状态监控，死亡容器所占资源及本地缓存的镜像文件的清理等工作 保障集群中各种资源处于期望状态，当监控到某个资源状态不正常时，管理控制器会触发对应的调度操作，主要由以下几个部分组成: 节点控制器(Node Controller) 副本控制器(Replication Controller) 端点控制器(Endpoints Controller) 命名空间控制器(Namespace Controller) 身份认证控制器(Serviceaccounts Controller) 五、kube-scheduler 集群的调度器，负责 Pod 在集群节点中的调度分配，也负责 Volume（CVI）和网络（CNI）的管理，按照预定的调度策略将 Pod 调度到相应的机器上； 调度器，接收来自于管理控制器(kube-controller-manager)触发的调度操作请求，然后根据请求规格、调度约束、整体资源情况等因素进行调度计算，最后将任务写到etcd，目标节点的kubelet 组件监听到由其负责的资源创建工作，然后执行具体调度任务 六、kube-proxy 实现 Service 的代理及软件模式的负载均衡器。 七、kubelet 负责本地节点上 Pod 的创建、修改、监控、删除等生命周期管理，同时会上报本 Node 的状态信息到 API server。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-kubectl多集群上下文配置.html":{"url":"origin/kubernetes-kubectl多集群上下文配置.html","title":"kubectl多集群上下文配置","keywords":"","body":"kubectl命令行配置多Kubernetes集群 一. 下载kubectl kubectl github下载地址：https://github.com/kubernetes/kubectl/releases 二. 创建配置文件夹 Linux mkdir ~/.kube Windows CMD mkdir %USERPROFILE%\\.kube # %USERPROFILE% 当前用户目录 三. 创建编辑kubectl配置文件 apiVersion: v1 # 集群信息 clusters: - cluster: certificate-authority-data: **CA证书*** server: https://开发k8s环境APIServer的IP地址:6443 name: k8s-dev - cluster: certificate-authority-data: **CA证书*** server: https://测试k8s环境APIServer的IP地址:8443 name: k8s-test - cluster: certificate-authority-data: **CA证书*** server: https://UAT k8s环境APIServer的IP地址:8443 name: k8s-uat - cluster: certificate-authority-data: **CA证书*** server: https://生产k8s环境APIServer的IP地址:8443 name: k8s-pro # 集群上下文环境 contexts: - context: cluster: k8s-dev user: k8s-dev-admin name: k8s-dev - context: cluster: k8s-test user: k8s-test-admin name: k8s-test - context: cluster: k8s-uat user: k8s-uat-admin name: k8s-uat - context: cluster: k8s-pro user: k8s-pro-readonly name: k8s-pro # 当前使用的上下文环境 current-context: k8s-dev kind: Config preferences: {} #集群用户信息及证书信息 users: - name: k8s-dev user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-test user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-uat user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-pro user: client-certificate-data: **用户证书** client-key-data： **用户私钥** 四. 切换Kubernetes集群上下文 #切换至开发k8s环境上下文 kubectl config use-context k8s-dev #切换至开发k8s环境上下文 kubectl config use-context k8s-test #切换至开发k8s环境上下文 kubectl config use-context k8s-uat #切换至开发k8s环境上下文 kubectl config use-context k8s-pro 五. kubectl命令的别名和快速切换集群上下文的别名 设置别名快速使用kubectl命令 Windows doskey k=kubectl $* # $*表示这个命令还可能有其他参数 Linux alias k='kubectl' 设置别名快速切换Kubectl集群上下文 Windows doskey k2d=kubectl config use-context k8s-dev doskey k2t=kubectl config use-context k8s-test doskey k2u=kubectl config use-context k8s-uat doskey k2p=kubectl config use-context k8s-pro Linux alias k2d='kubectl config use-context k8s-dev' alias k2t='kubectl config use-context k8s-test' alias k2u='kubectl config use-context k8s-uat' alias k2p='kubectl config use-context k8s-pro' Windows和Linux下设置别名永久生效 Windows ①创建bat脚本cmdalias.cmd @doskey k=kubectl $* @doskey k2d=kubectl config use-context k8s-dev @doskey k2t=kubectl config use-context k8s-test @doskey k2u=kubectl config use-context k8s-uat @doskey k2p=kubectl config use-context k8s-pro # @表示执行这条命令时不显示这条命令本身 ②修改注册表 方式1：手动在注册HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor下添加一项AutoRun，把值设为bat脚本的路径 方式2：创建编写一个注册表修改文件，名为：add-regkey.reg，双击行这个文件,导入注册表添加的值 Windows Registry Editor Version 5.00 [HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor] \"AutoRun\"=\"%USERPROFILE%\\\\.kube\\\\cmdalias.cmd\" Linux echo \"alias k='kubectl'\" >> /etc/profile && \\ echo \"alias k2d='kubectl config use-context k8s-dev'\" >> /etc/profile && \\ echo \"alias k2t='kubectl config use-context k8s-test'\" >> /etc/profile && \\ echo \"alias k2u='kubectl config use-context k8s-uat'\" >> /etc/profile && \\ echo \"alias k2p='kubectl config use-context k8s-pro'\" >> /etc/profile && \\ source /etc/profile 六. Kubectl Config命令详解 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used a list of paths (normal path delimitting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Usage: kubectl config SUBCOMMAND [options] Available Commands: current-context Displays the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts rename-context Renames a context from the kubeconfig file. set Sets an individual value in a kubeconfig file set-cluster Sets a cluster entry in kubeconfig set-context Sets a context entry in kubeconfig set-credentials Sets a user entry in kubeconfig unset Unsets an individual value in a kubeconfig file use-context Sets the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file 参考连接 https://blog.csdn.net/u013360850/article/details/83315188 https://www.awaimai.com/2445.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-NetworkPolicy.html":{"url":"origin/kubernetes-NetworkPolicy.html","title":"Network Policy容器流量管理","keywords":"","body":"Kubernetes Network Policy容器网络流量限制 一、Overview Kubernetes使用命名空间namesapce做多租户隔离，但是如果不配置网络策略，namespace的隔离也仅仅是作用于在kubernetes编排调度时的隔离，实际上不同namespace下的pod还是可以相互联通的。此时就需要使用Kubernetes提供的networkPolicy,用于隔离不同租户应用的网络流量来减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。Kubernetes中的Network Policy只定义了规范，并没有提供实现，而是把实现留给了网络插件 Kubernetes v1.7+版本GA，API版本为http://networking.k8s.io/v1 二、使用Calico做Network policy Network Policy的实现仰赖CNI插件的支持，目前已经支持的cni插件包括： Calico Kube-router Romana Weave Net trireme Calico的NetworkPolicy功能支持以下特性： 支持多种endpoint: pods/containers, VMs 支持限制入站和出站的流量访问 规则策略支持: 动作: allow, deny, log, pass 源和目标的匹配标准: 端口: numbered, ports in a range, and Kubernetes named ports 协议: TCP, UDP, ICMP, SCTP, UDPlite, ICMPv6, protocol numbers (1-255) HTTP attributes (if using Istio service mesh) ICMP attributes IP version (IPv4, IPv6) IP or CIDR Endpoint selectors (using label expression to select pods, VMs, host - interfaces, and/or network sets) Namespace selectors Service account selectors Optional packet handling controls: disable connection tracking, apply before DNAT, apply to forwarded traffic and/or locally terminated traffic Preflight k8s集群版本大于v1.3.0 calico-cni网络插件的二进制文件 kubelet添加配置Flag 需要配置kubelet 让pod启动时使用calico网络插件，kubelet可以配置使用calico在启动时配置参数： --network-plugin=cni --cni-conf-dir=/etc/cni/net.d # CNI插件的配置文件目录,该目录下的配置文件内容需要符合CNI规范 --cni-bin-dir=/opt/cni/bin # CNI插件的可执行文件目录，默认为/opt/cni/bin API Server添加配置Flag --allow-privileged=true # calico-node的POD需要以特权模式运行在各node上 三、策略规则的声明配置 Network Policy策略规则是用来定义命名空间有哪些POD，能被谁访问，能访问谁的。相应地，在Network Policy声明文件中的字段有： spec.podSelector: 定义该命名空间有哪些POD遵循本networkpolicy约束 spec.ingress.from: 定义受本networkpolicy约束的POD的入站规则 spec.egress.to：定义受本networkpolicy约束的POD的出站规则 spec.ingress.from的选择器有： podSelector namespaceSelector ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ...下文省略... namespaceSelector和podSelector（注意YAML语法的区别） ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ...下文省略... ipBlock 四、示例说明 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: # POD选择器，选择遵循本networkpolicy约束的POD podSelector: matchLabels: role: db # 流量访问策略类型 policyTypes: - Ingress - Egress # 流量访问入站规则 ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 # POD流量访问出站规则 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 上述例子，网络流量访问策略规则可解释为： \"default\"命名空间下标签为\"role=db\"的POD的入站规则： 允许被\"default\"命名空间下，所有带标签\"role=frontend\"的POD访问TCP 6379端口 允许被标签为\"project=myproject\"的命名空间下所有的POD访问TCP 6379端口 允许被IP地址为172.17.0.0–172.17.0.255或172.17.2.0–172.17.255.255的POD访问TCP 6379端口 \"default\"命名空间下标签为\"role=db\"的POD的出站规则： 允许访问10.0.0.0/24网段的POD的5978端口 五、默认策略 默认情况下，如果namespace下没有network policy,则该namespace下所有POD的入站规则和出站规则都是开放的。network policy只影响命名空间下被Pod Selector选择的POD，其他依旧是默认规则。 namespace下的所有pod，入站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Ingress namespace下的所有pod，入站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} ingress: - {} policyTypes: - Ingress namespace下的所有pod，出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Egress namespace下的所有pod，出站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} egress: - {} policyTypes: - Egress 同namespace的pod，入站和出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress 六、场景测试 限制服务只能被带有特定label的应用访问 $ kubectl create ns test1 $ kubectl -n test1 run nginx --image=nginx $ kubectl -n test1 expose deployment nginx --port=80 kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: test1 name: access-nginx spec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: \"true\" # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ (无法访问) # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --labels=\"access=true\" --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ Connecting to nginx.test1.svc:80 (10.68.86.216:80) saving to 'index.html' index.html 100% | *********************************************************************************************************************| 612 0:00:00 ETA 'index.html' saved 限制带\"run=busybox\"标签的Pod只能访问www.baidu.com kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: busybox-egress-baidu-a-policy namespace: test1 spec: podSelector: matchLabels: run: busybox egress: - to: - ipBlock: cidr: 180.101.49.11/32 - to: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: UDP port: 53 $ kubectl -n test1 run busybox --labels=\"run=busybox\" --rm -ti --image=busybox /bin/sh / # wget www.baidu.com Connecting to www.baidu.com (180.101.49.11:80) saving to 'index.html' index.html 100% |*********************************************************************************************************************| 2381 0:00:00 ETA 'index.html' saved / # wget www.sohu.com Connecting to www.sohu.com (101.227.172.11:80) ^C / # wget https://github.com Connecting to github.com (13.250.177.223:443) ^C / # 参考链接 https://kubernetes.io/docs/concepts/services-networking/network-policies/#sctp-support https://docs.projectcalico.org/v3.8/security/kubernetes-network-policy#best-practice-create-deny-all-default-network-policy https://yq.aliyun.com/articles/640190 https://www.jianshu.com/p/c0d2618d2849 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/k8s-pressure-test.html":{"url":"origin/k8s-pressure-test.html","title":"k8s集群的压力测试","keywords":"","body":"K8S集群压力测试 参考 https://kubestone.io/en/latest/quickstart/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-serviceaccount-rbac.html":{"url":"origin/kubernetes-serviceaccount-rbac.html","title":"用户认证ServiceAccount与授权策略RBAC","keywords":"","body":"K8S安全之认证与授权策略机制 一、简介 官方文档：https://kubernetes.io/docs/reference/access-authn-authz/rbac/ 还是得先从kubernetes集群角色说起 ETCD：存储所有k8s资源状态数据 API Server：对外暴露操作ETCD等REST API接口 Kubernetes的API Server有众多的资源REST API接口，同时还有众多依赖API Server进行操作的集群组件，例如Controller Manager等。API Server为了保护API请求的合法性。API Serve内部需要先验证请求的权限。需要验证 目前Kubernetes支持的授权策略有： ABAC：基于属性的访问控制 RBAC：基于角色的访问控制 Webhook： Node AlwaysDeny：拒绝所有的请求，一般用于测试。 AlwaysAllow：表示允许所有的请求，不进行认证授权。（默认配置） 从1.6版本起，Kubernetes 默认启用RBAC访问控制策略。从1.8开始，RBAC已成为稳定的功能。API Server启用RABC需要设置启动参数–-authorization-mode=RBAC，。 二、RBAC API 资源对象 Kubernetes的RBAC认证授权策略使用rbac.authorization.k8s.io API组 Role ClusterRole RoleBinding ClusterRoleBind 三、应用 1、限制客户端用户只能访问或操作指定命名空间的特定资源 场景用户 用户角色 限制Namespace 限制资源对象 语义化的限制动作 开发者 developer test pod、configmap 只能查看pod/configmap能登录到pod中进行操作 ①创建RBAC相关的资源声明文件 ServiceAccount ClusterRole RoleBinding ClusterRoleBinding #=========================ServiceAccount====================== --- apiVersion: v1 kind: ServiceAccount metadata: name: developer namespace: default # 或者使用命令：kubectl -n default create sa developer #=========================ClusterRole====================== --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: developer rules: - apiGroups: - \"\" resources: - pods - pods/attach - pods/exec - pods/log - pods/status - configmaps verbs: - get - list - watch - apiGroups: - \"\" resources: - services verbs: - get - list - watch - apiGroups: - \"\" resources: - pods/exec verbs: - create # Portforward - apiGroups: - \"\" resources: - pods - pods/portforward verbs: - get - list - create #=========================RoleBinding====================== --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: developer namespace: test roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: developer subjects: - kind: ServiceAccount name: developer namespace: default # 或者使用命令：kubectl create rolebinding developer --clusterrole=developer --serviceaccount=default:developer --namespace=test #=========================ClusterRoleBinding====================== --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: developer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: developer subjects: - kind: ServiceAccount name: developer namespace: default ②获取serviceaccount的secret k -n default get secrets k -n default get secrets developer-token-** -oyaml 获得如下secrets的详细内容 apiVersion: v1 type: kubernetes.io/service-account-token kind: Secret metadata: namespace: default data: ca.crt: ******12345678******** # API Server服务端的CA证书 namespace: ZGVmYXVsdA== # 该字符串为“default”base64转码后的值 token: *******ABCDEF********* # 该token是经过base64处理的，需要进行解码处理 base64解码secret中的Token echo \"*******ABCDEF*********\" | base64 -d ③组装kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: ******12345678******** server: https://master.k8s117.curiouser.com:8443 name: k8s117 contexts: - context: cluster: k8s117 namespace: test user: k8s117-developer name: k8s117-developer current-context: k8s117-developer kind: Config preferences: {} users: - name: k8s117-developer user: token: \"*******ABCDEF*********base64解码后的值\" ④测试 看是否能获取所有的命名空间（不能） 看是否能查看test命名空间下的所有POD和ConfigMap（能） 看是否能登录到test命名空间下的POD（能） 看是否能使用kube-proxy端口转发test命名空间下POD的端口（能） Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-helm.html":{"url":"origin/kubernetes-helm.html","title":"K8S应用管理工具Helm","keywords":"","body":"Helm简介、安装、配置、使用 一、简介 Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。 Helm3之前是C/S架构的。主要分为客户端 helm 和服务端 Tiller。Tiller负责对charts的解析生成k8s资源声明文件，然后调用k8s api进行部署。同时还保存chart部署的版本信息。 Helm3移除了 Tiller，直接在客户端就对charts进行解析，调用k8s api部署资源声明文件。同时将charts release的版本信息保存至对应k8s应用部署所在命名空间下的secret中。(例如：名为sh.helm.release.v1.sentry-kubernetes-events.v1 helm.sh/release.v1类型的secret) 全面拥抱Helm3 二、安装 Github下载地址：https://github.com/helm/helm/releases 1、二进制包安装 下载二进制文件解压至系统环境路径下即可。 命令脚本 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh # 或者 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 2、包管理器安装 Brew brew install helm 3、源码编译安装 $ cd $GOPATH $ mkdir -p src/helm.sh $ cd src/helm.sh $ git clone https://github.com/helm/helm.git $ cd helm $ make 三、配置 helm3默认读取当前用户目录下～/.kube/config文件中的当前k8s环境上下文来配置部署charts到哪个k8s集群。相关权限跟随着kuectl配置的用户权限。（开箱即用的感觉） 1、配置helm的环境变量 Name Description $XDG_CACHE_HOME set an alternative location for storing cached files. $XDG_CONFIG_HOME set an alternative location for storing Helm configuration. $XDG_DATA_HOME set an alternative location for storing Helm data. $HELM_DRIVER set the backend storage driver. Values are: configmap, secret, memory $HELM_NO_PLUGINS disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins. $KUBECONFIG set an alternative Kubernetes configuration file (default \"~/.kube/config\") 2、Helm相关文件存储的默认路径 cached文件都存在$XDG_CACHE_HOME/helm 配置文件存在 $XDG_CONFIG_HOME/helm 数据文件存在$XDG_DATA_HOME/helm 3、各个操作操作系统的默认配置 操作系统 Cache文件路径 配置文件路径 数据文件路径 Linux $HOME/.cache/helm $HOME/.config/helm $HOME/.local/share/helm macOS $HOME/Library/Caches/helm $HOME/Library/Preferences/helm $HOME/Library/helm Windows %TEMP%\\helm %APPDATA%\\helm %APPDATA%\\helm 4、命令行的命令补全 helm completion zsh source 四、charts的管理 全局通用的命令行参数 --add-dir-header 添加文件路径到Header中 --alsologtostderr log to standard error as well as files --debug 输出Debug级别的日志 --kube-context string 指定使用哪个kubeconfig context --kubeconfig string 指定kubeconfig文件路径 --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log-dir string 指定日志输出到哪个路径下 --log-file string 指定日志输出到哪个文件中 --log-file-max-size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string 指定在哪个K8S命名空间下进行操作 --registry-config string path to the registry config file (default \"/Users/curiouser/Library/Preferences/helm/registry.json\") --repository-cache string path to the file containing cached repository indexes (default \"/Users/curiouser/Library/Caches/helm/repository\") --repository-config string path to the file containing repository names and URLs (default \"/Users/curiouser/Library/Preferences/helm/repositories.yaml\") --skip-headers If true, avoid header prefixes in the log messages --skip-log-headers If true, avoid headers when opening log files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging 1、远程Charts仓库的管理 添加远程charts仓库 helm repo add 远程仓库别名 https://kubernetes-charts-incubator.storage.googleapis.com/ 查看当前所有的远程charts仓库 helm repo list 删除指定的远程charts仓库 helm repo rm/remove 远程仓库别名 查看远程仓库中的所有charts helm search repo 查看Github中的所有charts helm search hub 2、Charts的管理 从远程仓库中下载Charts到本地 helm pull 远程仓库别名/chart名 参数项 # 参数项 --ca-file string verify certificates of HTTPS-enabled servers using this CA bundle --cert-file string identify HTTPS client using this SSL certificate file -d/--destination string location to write the chart. If this and tardir are specified, tardir is appended to this (default \".\") --devel use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored. -h/--help help for pull --key-file string identify HTTPS client using this SSL key file --keyring string location of public keys used for verification (default \"/Users/curiouser/.gnupg/pubring.gpg\") --password string chart repository password where to locate the requested chart --prov fetch the provenance file, but don't perform verification --repo string chart repository url where to locate the requested chart --untar 下载后解压 --untardir string 下载后解压到指定目录(默认是当前路径\".\") --username string chart repository username where to locate the requested chart --verify verify the package before installing it --version string specify the exact chart version to install. If this is not specified, the latest version is installed # 支持全局通用参数 五、部署Charts到k8s集群 命令格式 helm install [NAME] [CHART] [参数项] # 参数项 --atomic 原子部署。当charts部署失败时，所有操作进行回滚删除。同时如果设置该参数， 一并的\"--wait\"也会被设置 --ca-file string verify certificates of HTTPS-enabled servers using this CA bundle --cert-file string identify HTTPS client using this SSL certificate file --dependency-update 在部署前更新charts依赖 --description string 添加自定义描述 --devel use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored --disable-openapi-validation if set, the installation process will not validate rendered templates against the Kubernetes OpenAPI Schema --dry-run 模拟部署 -g, --generate-name generate the name (and omit the NAME parameter) -h, --help 显示帮助信息 --key-file string identify HTTPS client using this SSL key file --keyring string location of public keys used for verification (default \"/Users/curiouser/.gnupg/pubring.gpg\") --name-template string specify template used to name the release --no-hooks prevent hooks from running during install -o, --output format 指定日志输出的格式（可选项table, json, yaml 默认是table) --password string 远程chart仓库用户的密码 --post-renderer postrenderer the path to an executable to be used for post rendering. If it exists in $PATH, the binary will be used, otherwise it will try to look for the executable at the given path (default exec) --render-subchart-notes if set, render subchart notes along with the parent --replace re-use the given name, only if that name is a deleted release which remains in the history. This is unsafe in production --repo string 设置远程chart仓库的url --set stringArray 设置vaules。(覆盖values.yaml中的值可设置多个，以“,”分割。例如 key1=val1,key2=val2) --set-file stringArray 从文件中读取va luset values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --skip-crds if set, no CRDs will be installed. By default, CRDs are installed if not already present --timeout duration time to wait for any individual Kubernetes operation (like Jobs for hooks) (默认5分0秒) --username string 远程chart仓库的用户名 -f, --values strings 指定values文件或URL(可设置多个) --verify verify the package before installing it --version string specify the exact chart version to install. If this is not specified, the latest version is installed --wait 设置等待charts涉及的k8s资源变为ready状态的时间才认为部署成功。它的值等 同timeout设置的值例如Pods, PVCs, Services, Deployment的最少POD数, StatefulSet, or ReplicaSet ） # 支持全局通用参数 1、部署远程仓库中的charts到k8s集群 helm install 部署名 远程仓库别名/chart名 参数项 2、部署本地的Charts到k8s集群 helm install 部署名 -f values.yaml . 3、更新charts的部署 helm upgrade charts的部署名 -f values.yaml . # 参数项 --atomic 原子更新。当charts更新部署失败时，所有操作进行回滚删除。同时如果设置该参 数，一并的\"--wait\"也会被设置 --ca-file string verify certificates of HTTPS-enabled servers using this CA bundle --cert-file string identify HTTPS client using this SSL certificate file --cleanup-on-fail allow deletion of new resources created in this upgrade when upgrade fails --description string 添加自定义描述 --devel use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored --dry-run 模拟更新部署 --force force resource updates through a replacement strategy -h, --help 显示帮助信息 --history-max int limit the maximum number of revisions saved per release. Use 0 for no limit (default 10) -i, --install 如果指定的chart部署名不存在，就直接安装 --key-file string identify HTTPS client using this SSL key file --keyring string 指定验证时公钥的路径(默认当前用户路径下的.gnupg/pubring.gpg\") --no-hooks disable pre/post upgrade hooks -o, --output format 指定日志输出的格式（可选项table, json, yaml 默认是table) --password string 远程chart仓库用户的密码 --post-renderer postrenderer the path to an executable to be used for post rendering. If it exists in $PATH, the binary will be used, otherwise it will try to look for the executable at the given path (default exec) --render-subchart-notes if set, render subchart notes along with the parent --repo string 设置远程chart仓库的url --reset-values when upgrading, reset the values to the ones built into the chart --reuse-values when upgrading, reuse the last release's values and merge in any overrides from the command line via --set and -f. If '--reset-values' is specified, this is ignored --set stringArray 设置vaules。(覆盖values.yaml中的值可设置多个，以“,”分割。例如 key1=val1,key2=val2) --set-file stringArray set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --timeout duration time to wait for any individual Kubernetes operation (like Jobs for hooks) (默认5分0秒) --username string 远程chart仓库的用户名 -f, --values strings 指定values文件或URL(可设置多个) --verify verify the package before installing it --version string specify the exact chart version to install. If this is not specified, the latest version is installed --wait 设置等待charts涉及的k8s资源变为ready状态的时间才认为部署成功。它的值等 同timeout设置的值例如Pods, PVCs, Services, Deployment的最少POD数, StatefulSet, or ReplicaSet ） # 支持全局通用参数 4、删除部署charts的资源 默认删除charts涉及的所有资源和charts的发布版本 helm del/uninstall/del/delete/un charts的部署名 参数项 # 参数项 --description string 添加自定义描述 --dry-run 模拟删除 -h, --help 显示帮助信息 --keep-history 删除charts涉及的所有资源，然后标记该charts的发布为删除状态，但保留删除历史 --no-hooks prevent hooks from running during uninstallation --timeout duration time to wait for any individual Kubernetes operation (like Jobs for hooks) (默认5m0s) # 支持全局通用参数 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/Kubernetes-helm-charts编写规则.html":{"url":"origin/Kubernetes-helm-charts编写规则.html","title":"helm charts编写规则","keywords":"","body":"Helm Charts编写规则 一、Charts的文件目录结构 Prerequisite：初始化一个模板charts helm create charts名 以创建一个test charts为例进行说明 helm create test # 会在当前路径下产生以下目录结构的文件 test/ Chart.yaml # Charts的描述信息。例如作者信息，使用的k8s api版本等 LICENSE # Charts的许可证等信息(可选) README.md # Charts的README(可选) requirements.yaml # Charts的依赖管理文件(可选) values.yaml # charts的默认配置项 charts/ # 存放Charts所依赖的其他charts templates/ # 存放Charts的k8s资源声明文件模板 ｜-- NOTES.txt # 简短使用说明文件(可选) 二、Chart.yaml Chart.yaml 文件是编写Charts 所必需的。描述Chart的描述信息 apiVersion: The chart API version (必须) name: Chart名 (必须) version: Chart的版本信息 (必须) kubeVersion: A SemVer range of compatible Kubernetes versions (可选) description: 一句描述chart的话 (可选) type: chart类型 (可选) keywords: - chart关健字 (可选) home: The URL of this project's home page (可选) sources: - chart的源代码仓库URL(可选) dependencies: # 依赖的chart (可选) - name: 依赖的chart名 version: 依赖的chart版本 repository: The repository URL (\"https://example.com/charts\") or alias (\"@repo-name\") condition: (可选) A yaml path that resolves to a boolean, used for enabling/disabling charts (e.g. subchart1.enabled ) tags: # (可选) - Tags can be used to group charts for enabling/disabling together enabled: (可选) Enabled bool determines if chart should be loaded import-values: # (可选) - ImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items. alias: (可选) Alias usable alias to be used for the chart. Useful when you have to add the same chart multiple times maintainers: # (可选) - name: The maintainer's name (必须 for each maintainer) email: The maintainer's email (可选 for each maintainer) url: A URL for the maintainer (可选 for each maintainer) icon: A URL to an SVG or PNG image to be used as an icon (可选). appVersion: The version of the app that this contains (可选). This needn't be SemVer. 三、 参考 https://github.com/whmzsu/helm-doc-zh-cn/blob/master/chart/charts-zh_cn.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/Service与SpringBoot应用启动参数冲突的问题排查及解决方案.html":{"url":"origin/Service与SpringBoot应用启动参数冲突的问题排查及解决方案.html","title":"Service与SpringBoot应用启动参数冲突的问题排查及解决方案","keywords":"","body":"kubernetes Service与SpringBoot应用启动参数冲突 上下文 部署Springboot应用到kubernetes集群 创建了名为\"server\"的Service 描述 1.部署到kubernetes集群时,原因容器报错日志如下 ```bash Caused by: org.springframework.core.convert.ConversionFailedException: Failed to convert from type [java.lang.String] to type [java.lang.Integer] for value 'tcp://192.168.1.55:8080'; nested exception is java.lang.NumberFormatException: For input string: \"tcp://192.168.1.55:8080\" ``` 2.当单独使用Docker部署时,则不报错 3.报错表现为SpringBoot框架无法将字符串“tcp://192.168.1.55:8080”转换为数字类型 原因 SpringBoot应用会读取POD中系统环境变量\"SERVER_PORT\"的值作为应用监听的端口，值应为数字类型 而当该应用在kubernetes中的Service名字命名为\"server\"时,kubernetes会默认在该命名空间下所有的POD中注入一个名叫\"SERVER_PORT=tcp://service-ip地址:镜像Dockerfile暴露出来的端口\"环境变量(该值为字符类型，报错示例中的为\"SERVER_PORT=tcp://192.168.1.55:8080\")来进行POD间的服务发现 解决方案 禁止SpringBoot应用部署在kubernetes中的Service名字命名为“server”，建议命名为“项目名-应用名” 在部署Deployment声明文件中添加\"SERVER_PORT=8080\"进行覆盖默认值 附录: Kubernetes中的服务发现 创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。它支持使用Kubernetes Service环境变量以及与Docker的links兼容的变量。 简单来说，服务发现就是服务或者应用之间互相定位的过程。不过，服务发现并非什么新概念，传统的单体应用架构时代也会用到，只不过单体应用的动态性不强，更新和重新发布频度较低，通常以月甚至以年计，基本不会进行自动伸缩，因此服务发现的概念无须显性强调。在传统的单体应用网络位置发生变化时，由IT运维人员手工更新一下相关的配置文件基本就能解决问题。但在微服务应用场景中，应用被拆分成众多的小服务，它们按需创建且变动频繁，配置信息基本无法事先写入配置文件中并及时跟踪反映动态变化，服务发现的重要性便随之凸显。 服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费者则周期性地从注册中心获取服务提供者的最新位置信息从而“发现”要访问的目标服务资源。复杂的服务发现机制还能够让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。 实践中，根据其发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。 客户端发现：由客户端到服务注册中心发现其依赖到的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。 服务端发现：这种方式额外要用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。 由此可见，服务注册中心是服务发现得以落地的核心组件。事实上，DNS可以算是最为原始的服务发现系统之一，不过，在服务的动态性很强的场景中，DNS记录的传播速度可能会跟不上服务的变更速度，因此它不并适用于微服务环境。另外，传统实践中，常见的服务注册中心是ZooKeeper和etcd等分布式键值存储系统。不过，它们只能提供基本的数据存储功能，距离实现完整的服务发现机制还有大量的二次开发任务需要完成。另外，它们更注重数据一致性，这与有着更高的服务可用性要求的微服务发现场景中的需求不太相吻合。 Netflix的Eureka是目前较流行的服务发现系统之一，它是专门开发用来实现服务发现的系统，以可用性目前为先，可以在多种故障期间保持服务发现和服务注册的功能可用，其设计原则遵从“存在少量的错误数据，总比完全不可用要好”。另一个同级别的实现是Consul，它是由HashiCorp公司提供的商业产品，不过还有一个开源基础版本提供。它于服务发现的基础功能之外还提供了多数据中心的部署能力等一众出色的特性。 尽管传统的DNS系统不适于微服务环境中的服务发现，但SkyDNS项目（后来称kubedns）却是一个有趣的实现，它结合古老的DNS技术和时髦的Go语言、Raft算法并构建于etcd存储系统之上，为Kubernetes系统实现了一种服务发现机制。Service资源为Kubernetes提供了一个较为稳定的抽象层，这有点类似于服务端发现的方式，于是也就不存在DNS服务的时间窗口的问题。 Kubernetes自1.3版本开始，其用于服务发现的DNS更新为了kubeDNS，而类似的另一个基于较新的DNS的服务发现项目是由CNCF（Cloud Native Computing Foundation）孵化的CoreDNS，它基于Go语言开发，通过串接一组实现DNS功能的插件的插件链进行工作。自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。不过，Kubernetes依然支持使用环境变量进行服务发现。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-tools.html":{"url":"origin/kubernetes-tools.html","title":"辅助工具","keywords":"","body":"Kubernetes辅助工具 一、VSCode的Kubernetes插件 二、Electron公司的Kube Forwarder 下载地址：http://kube-forwarder.pixelpoint.io Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-api.html":{"url":"origin/jenkins-api.html","title":"Jenkins API","keywords":"","body":"Jenkins API 一、简介 Jenkins API支持以下3种格式： XML JSON并支持JSONP跨域访问 Python https://jenkins-host/api/ Jenkins API没有统一的入口，而是采用“…/api/” 的REST API样式，其中”…” 表示Jenkins资源的URL API类型 说明 JobsAPI 任务管理（任务信息、创建、修改） PluginManagerAPI 插件管理（插件信息、安装插件） QueueAPI 任务队列相关（队列状态） StatisticsAPI Jenkins统计信息 CrumbIssuerAPI 系统哈希值信息（用于防御CSRF攻击） SystemAPI Jenkins系统状态（版本、路径） Jenkins 使用 Baisc Auth 的权限验证方式，需要传入 username 和 api token 。 但在 Job 的远程触发中，可以设置用于远程触发的 token (在 Job 的配置页面设置)，这样在触发 Job 时就不需要传入 Basic Auth 了。 远程触发的 token 使用 urlencode 的方式放在请求的 body 中，其原始数据为： token= Basic Auth curl -X POST /view//job//build --user : Token curl -X POST /view//job//build --data-urlencode token= _class\": \"org.jenkinsci.plugins.workflow.job.WorkflowJob\" \"_class\": \"hudson.model.FreeStyleProject\" 二、API 1. 创建Job 2. 创建视图 3. 触发Jo构建 4. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-SharedLibraries.html":{"url":"origin/jenkins-SharedLibraries.html","title":"Jenkins共享库Shared Libraries","keywords":"","body":"Jenkins Pipeline共享库Shared Libraries 一、简介 随着Job的增多和pipeline的功能越来越复杂，Pipeline代码冗余度高。所以可以将一些公共的pipeline抽象做成模块代码，在各种项目pipeline之间共享核心实现，同时可以放到SVM中进行版本控制。以减少冗余并保证所有job在构建的时候会调用最新的共享库代码。这时就可以用到pipline的共享库Shared Libraries功能。 模块化 可重用性 二、共享库的目录结构 共享库根目录 |-- vars |-- test1.groovy |-- src |-- test2.groovy |-- resources vars: 依赖于Jenkins运行环境的Groovy脚本。其中的Groovy脚本被称之为全局变量。 src: 标准的Java源码目录结构,其中的Groovy脚本被称为类库(Library class)。该目录所有下的所有类都一次性静态被添加到类路径classpath下 resources: 目录允许从外部库中使用 libraryResource 步骤来加载有关的非 Groovy 文件。 目前，内部库不支持该特性 三、配置全局共享库 可在Jenkins中的Manage Jenkins –> Configure System　–> Global Pipeline Libraries 添加一个或多个全局的共享库，同时也可以在构建过程中的任何位置使用library step动作动态地配置引用共享库，详见动态引用共享库 四、引用共享库 1. 引用全局共享库 格式：@Library('my-shared-library-1@$Branch/Tag','my-shared-library-1@$Branch/Tag') _ #!groovy // 引用默认配置的共享库 @Library('demo-shared-library') _ // 引用指定分支、tag的共享库代码 @Library('demo-shared-library@1.0') _ // 引用多个指定分支tag的共享库 @Library('demo-shared-library@$Branch/Tag','demo-shared-library-test@$Branch/Tag') _ @Library('utils') import org.foo.Utilities @Library('utils') import static org.foo.Utilities.* 2. 动态引用共享库 2.7版本后的Shared Groovy Libraries插件，增加了一个library的setp,可以随时在构建过程中引用共享库 #!groovy library 'demo-shared-library@$BRANCH_NAME' library \"demo-shared-library@${params.LIB_VERSION}\" library('demo-shared-library').com.mycorp.pipeline.Utils.someStaticMethod() // 此时共享库的版本必须指定 library identifier: 'custom-lib@master', retriever: modernSCM( [$class: 'GitSCMSource', remote: 'git@git.mycorp.com:my-jenkins-utils.git', credentialsId: 'my-private-key']) 3. 调用第三方Java库 @Grab('org.apache.commons:commons-math3:3.4.1') import org.apache.commons.math3.primes.Primes 引用完的第三方Java库后会缓存在Jenkins Master节点的~/.groovy/grapes/ 目录下 五、全局变量和类库的编写规则和调用方法 1. /var下定义的全局变量 全局变量必须以全小写或驼峰（camelCased）命名以便于能够在流水线中正确的加载 /vars目录中的脚本根据需求以单例的方式实例化，这允许在单个.groovy` 文件中定义多个方法 /vars/*.groovy若实现call()方法，直接引用时默认执行其中的方法，该方法可以让全局变量以一种以类似于step的方式被调用 /vars/log.groovy #!groovy def call(String name = 'human') { echo \"Hello, ${name}.\" } def info(message) { echo \"INFO: ${message}\" } def warning(message) { echo \"WARNING: ${message}\" } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ log() // 输出\"Hello, human.\" log.info 'Starting' // 输出\"INFO: Starting\" log.warning 'Nothing to do!' // 输出\"WARNING: Nothing to do!\" 从2017年9月下旬发布的声明式 1.2开始，可以在全局变量中直接定义声明式流水线 /vars/evenOrOdd.groovy #!groovy def call(int buildNumber) { if (buildNumber % 2 == 0) { pipeline { agent any stages { stage('Even Stage') { steps { echo \"The build number is even\" } } } } } else { pipeline { agent any stages { stage('Odd Stage') { steps { echo \"The build number is odd\" } } } } } } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ evenOrOdd(currentBuild.getNumber()) 全局变量的传参 /vars/buildPlugin.groovy #!groovy def call(Map config) { node { git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\" sh 'mvn install' mail to: '...', subject: \"${config.name} plugin build\", body: '...' } } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ buildPlugin name: 'git' 声明式流水线不允许在script指令之外使用全局变量 Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ pipeline { agent none stage ('Example') { steps { script { log.info 'Starting' log.warning 'Nothing to do!' } } } } 2. /src下定义的类库 类库不能直接调用 sh或 git这样的步骤。 但是他们可以在封闭的类的范围之外实现方法，从而调用流水线步骤 /src/org/foo/Zot.groovy #!groovy package org.foo; def checkOutFrom(repo) { git url: \"git@github.com:jenkinsci/${repo}\" } return this Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ def z = new org.foo.Zot() z.checkOutFrom(repo) 类库中使用”class“声明父类 /src/org/foo/Utilities.groovy package org.foo class Utilities implements Serializable { def steps Utilities(steps) {this.steps = steps} def mvn(args) { steps.sh \"${steps.tool 'Maven'}/bin/mvn -o ${args}\" } } Jenkinsfile @Library('utils') import org.foo.Utilities def utils = new Utilities(this) node { utils.mvn 'clean package' } 类库中的方法访问流水线中的变量 /src/org/foo/Utilities.groovy package org.foo class Utilities { static def mvn(script, args) { script.sh \"${script.tool 'Maven'}/bin/mvn -s ${script.env.HOME}/jenkins.xml -o ${args}\" } } Jenkinsfile @Library('utils') import static org.foo.Utilities.* node { mvn this, 'clean package' } 六、Jenkins Pipeline生成器生成动态引用共享库的代码 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-声明式Declarative-pipeline语法.html":{"url":"origin/jenkins-声明式Declarative-pipeline语法.html","title":"声明式Declarative语法","keywords":"","body":"Jenkins声明式Declarative Pipeline 一、语法结构 Jenkins 2.5新加入的pipeline语法 声明式pipeline 基本语法和表达式遵循 groovy语法，但是有以下例外： 声明式pipeline 必须包含在固定格式的pipeline{}中 每个声明语句必须独立一行， 行尾无需使用分号 块(Blocks{}) 只能包含章节(Sections),指令（Directives）,步骤(Steps),或者赋值语句 属性引用语句被视为无参数方法调用。 如input() 一个声明式Pipeline中包含的元素 pipeline：声明这是一个声明式的pipeline脚本 agent：指定要执行该Pipeline的节点（job运行的slave或者master节点） stages：阶段集合，包裹所有的阶段（例如：打包，部署等各个阶段） stage：阶段，被stages包裹，一个stages可以有多个stage steps：步骤,为每个阶段的最小执行单元,被stage包裹 post：执行构建后的操作，根据构建结果来执行对应的操作 示例： pipeline{ // 指定pipeline在哪个slave节点上允许 agent { label 'jdk-maven' } // 指定pipeline运行时的一些配置 option { timeout(time: 1, unit: 'HOURS') } // 自定义的参数 parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } // 自定义的环境变量 environment { Gitlab_Deploy_KEY = credentials('gitlab-jenkins-depolykey') } // 定义pipeline的阶段任务 stages { stage (\"阶段1任务：拉代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段2任务：编译代码\") { steps { // 编译代码的具体命令 } } stage (\"阶段3任务：扫描代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段4任务：打包代码\") { steps { // 打包代码的具体命令 } } stage (\"阶段5任务：构建推送Docker镜像\") { steps { // 构建推送Docker镜像的具体命令 } } stage (\"阶段6任务：部署镜像\") { steps { // 部署镜像的具体命令 } } } post { success { // 当pipeline构建状态为\"success\"时要执行的事情 } always { // 无论pipeline构建状态是什么都要执行的事情 } } } 二、章节Sections 1、agent（必须） 指定整个Pipeline或特定阶段是在Jenkins Master节点还是Jenkins Slave节点上运行。可在顶级pipeline块和每个stage块中使用（在顶层pipeline{}中是必须定义的 ，但在阶段Stage中是可选的） 参数（以下参数值在顶层pipeline{}和stage{}中都可使用）： any：在任何可用的节点上执行Pipeline或Stage none：当在顶层pipeline{}中应用时，将不会为整个Pipeline运行分配全局代理，并且每个stage部分将需要包含其自己的agent部分 label node docker dockerfile kubernetes 公用参数： label customWorkspace reuseNode args 2、post 定义在Pipeline运行或阶段结束时要运行的操作。具体取决于Pipeline的状态 支持pipeline运行状态: always：无论Pipeline运行的完成状态如何都要运行 changed：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能运行 fixed：整个pipeline或者stage相对于上一次失败或不稳定Pipeline的状态有改变。才能运行 regression： aborted：只有当前Pipeline处于“中止”状态时，才会运行，通常是由于Pipeline被手动中止（通常在具有灰色指示的Web UI 中表示） failure：仅当当前Pipeline处于“失败”状态时才运行（通常在Web UI中用红色指示表示） success：仅当当前Pipeline在“成功”状态时才运行（通常在具有蓝色或绿色指示的Web UI中表示） unstable：只有当前Pipeline在不稳定”状态，通常由测试失败，代码违例等引起，才能运行（通常在具有黄色指示的Web UI中表示） unsuccessful： cleanup：无论Pipeline或stage的状态如何，在跑完所有其他的post条件后运行此条件下 的post步骤。 3、stages（必须） 至少包含一个用于执行任务的stage指令 pipeline{ }中只能有一个stages{} 4、steps（必须） 在stage指令中至少包含一个用于执行命令的steps 三、Jenkins中的变量 变量的来源 Jenkins内置的环境变量 构建任务相关的变量 构建状态相关的变量 插件提供的环境变量 pipeline中environment指令定义的变量 脚本自定义的变量 变量的引用 $变量名 ${变量名} ${env.变量名} 变量的处理 ${变量名[0..7]} 变量名.take(8) ${变量名.replace(' and counting', '')} The issue here is caused by the way Jenkins interprets $var inside sh block: if you use \"double quotes\", $var in sh \"... $var ...\" will be interpreted as Jenkins variable; if you use 'single quotes', $var in sh '... $var ...' will be interpreted as shell variable. 参考 https://stackoverflow.com/questions/16943665/how-to-get-git-short-hash-in-to-a-variable-in-jenkins-running-on-windows-2008 https://stackoverflow.com/questions/44007034/conditional-environment-variables-in-jenkins-declarative-pipeline/53771302 四、指令Directives 1、Environment环境变量 environment{…},使用键值对来定义一些环境变量并赋值。它的作用范围，取决environment{…}所写的位置。写在顶层环境变量，可以让所有stage下的step共享这些变量；也可以单独定义在某一个stage下，只能供这个stage去调用变量，其他的stage不能共享这些变量。一般来说，我们基本上上定义全局环境变量，如果是局部环境变量，我们直接用def关键字声明就可以，没必要放environment{…}里面。 同时，environment{…}支持credentials() 方法来访问预先在Jenkins保存的凭据，并赋值给环境变量 credentials() 支持的凭据类型： Secret Text Secret File Username and password：使用变量名_USR and 变量名_PSW 来获取其中的用户名和Password pipeline { agent any stages { stage('Example Username/Password') { environment { SERVICE_CREDS = credentials('my-prefined-username-password') } steps { sh 'echo \"Service user is $SERVICE_CREDS_USR\"' sh 'echo \"Service password is $SERVICE_CREDS_PSW\"' sh 'curl -u $SERVICE_CREDS https://myservice.example.com' } } } } SSH with Private Key pipeline { agent any stages { stage('Example Username/Password') { environment { SSH_CREDS = credentials('my-prefined-ssh-creds') } steps { sh 'echo \"SSH private key is located at $SSH_CREDS\"' sh 'echo \"SSH user is $SSH_CREDS_USR\"' sh 'echo \"SSH passphrase is $SSH_CREDS_PSW\"' } } } } 2、Parameters参数 pipeline{ }中只能有一个parameters{} 参数定义格式 parameters { 参数类型(name: '参数名', defaultValue: '默认值', description: '描述') } 参数类型 string text boobleanParam choice password 参数调用格式：${params.参数名} 示例： pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" echo \"Biography: ${params.BIOGRAPHY}\" echo \"Toggle: ${params.TOGGLE}\" echo \"Choice: ${params.CHOICE}\" echo \"Password: ${params.PASSWORD}\" } } } } 3、Options选项 pipeline{ }中只能有一个options{} buildDiscarder checkoutToSubdirectory disableConcurrentBuilds disableResume newContainerPerStage overrideIndexTriggers preserveStashes quietPeriod retry skipDefaultCheckout skipStagesAfterUnstable timeout timestamps parallelsAlwaysFailFast 4、Triggers触发器 pipeline{ }中只能有一个triggers {} 触发器类型 cron pollSCM upstream Jenkins的Cron语法 5、Stage阶段(至少有一个) 包含在stages{}中 至少有一个 6、Tools工具 包含在pipeline{}或stage{} 支持的工具： Maven JDK Gradle 7、Input用户输入 8、When条件 内置条件： branch Execute the stage when the branch being built matches the branch pattern given, for example: when { branch 'master' }. Note that this only works on a multibranch Pipeline. buildingTag Execute the stage when the build is building a tag. Example: when { buildingTag() } changelog Execute the stage if the build’s SCM changelog contains a given regular expression pattern, for example: when { changelog '.*^\\[DEPENDENCY\\] .+$' } changeset Execute the stage if the build’s SCM changeset contains one or more files matching the given string or glob. Example: when { changeset \"*/.js\" } By default the path matching will be case insensitive, this can be turned off with the caseSensitive parameter, for example: when { changeset glob: \"ReadMe.*\", caseSensitive: true } changeRequest Executes the stage if the current build is for a \"change request\" (a.k.a. Pull Request on GitHub and Bitbucket, Merge Request on GitLab or Change in Gerrit etc.). When no parameters are passed the stage runs on every change request, for example: when { changeRequest() }. By adding a filter attribute with parameter to the change request, the stage can be made to run only on matching change requests. Possible attributes are id, target, branch, fork, url, title, author, authorDisplayName, and authorEmail. Each of these corresponds to a CHANGE_* environment variable, for example: when { changeRequest target: 'master' }. The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison (the default), GLOB for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. Example: when { changeRequest authorEmail: \"[\\w_-.]+@example.com\", comparator: 'REGEXP' } environment Execute the stage when the specified environment variable is set to the given value, for example: when { environment name: 'DEPLOY_TO', value: 'production' } equals Execute the stage when the expected value is equal to the actual value, for example: when { equals expected: 2, actual: currentBuild.number } expression Execute the stage when the specified Groovy expression evaluates to true, for example: when { expression { return params.DEBUG_BUILD } } Note that when returning strings from your expressions they must be converted to booleans or return null to evaluate to false. Simply returning \"0\" or \"false\" will still evaluate to \"true\". tag Execute the stage if the TAG_NAME variable matches the given pattern. Example: when { tag \"release-*\" }. If an empty pattern is provided the stage will execute if the TAG_NAME variable exists (same as buildingTag()). The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison, GLOB (the default) for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. For example: when { tag pattern: \"release-\\d+\", comparator: \"REGEXP\"} not Execute the stage when the nested condition is false. Must contain one condition. For example: when { not { branch 'master' } } allOf Execute the stage when all of the nested conditions are true. Must contain at least one condition. For example: when { allOf { branch 'master'; environment name: 'DEPLOY_TO', value: 'production' } } anyOf Execute the stage when at least one of the nested conditions is true. Must contain at least one condition. For example: when { anyOf { branch 'master'; branch 'staging' } } triggeredBy Execute the stage when the current build has been triggered by the param given. For example: when { triggeredBy 'SCMTrigger' } when { triggeredBy 'TimerTrigger' } when { triggeredBy 'UpstreamCause' } when { triggeredBy cause: \"UserIdCause\", detail: \"vlinde\" } 未完待整理更新 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-07-13 15:16:10 "},"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html":{"url":"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html","title":"Kubernetes Plugin","keywords":"","body":"Jenkins在Kubernetes上使用Kubernetes插件动态创建Slave节点 一、Context 插件GIthub地址：https://github.com/jenkinsci/kubernetes-plugin Jenkins 分布式架构是由一个 Master 和多个 Slave Node组成的分布式架构。在 Jenkins Master 上管理你的项目，可以把你的一些构建任务分担到不同的 Slave Node 上运行，Master 的性能就提高了。Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行构建。一个master（jenkins服务所在机器）可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 传统的 Jenkins Slave 一主多从式会存在一些痛点。比如： 主 Master 发生单点故障时，整个流程都不可用了； 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致 管理起来非常不方便，维护起来也是比较费劲； 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态； 资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 而使用Kubernetes插件可以在Kubernetes上动态创建slave POD作为Slave节点。Jenkins Master 和 Slave 节点以 Docker Container 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。 这种方式的工作流程大致为：当 Jenkins Master 接收到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Docker Container 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且 Docker Container 也会自动删除，恢复到最初状态。这种方式带来的好处有很多： 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。 二、Jenkins与Slave的连接方式 Jenkins的Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。当job被分配到slave上运行的时候，此时master和slave其实是建立的双向字节流的连接，其中连接方法主要有如下几种： SSH：Jenkins内置有ssh客户端实现，可以用来与远程的sshd通信，从而启动slave agent。这是对unix系统的slave最方便的方法，因为unix系统一般默认安装有sshd。在创建ssh连接的slave的时候，你需要提供slave的host名字，用户名和ssh证书。创建public/private keys，然后将public key拷贝到slave的~/.ssh/authorized_keys中，将private key 保存到master上某ppk文件中。jenkins将会自动地完成其他的配置工作，例如copy slave agent的binary，启动和停止slave。 Java web start（JNLP：Java Network Lancher Protocol）：jnlp连接方式有个好处就是不用master和slave之间能够ssh连接，只需要能够ping即可。并且如果slave的机器是windows的话，也是可以的这个其实是非常实用的 WMI+DCOM：对于Windows的Slave，Jenkins可以使用Windows2000及以后内置的远程管理功能（WMI+DCOM），你只需要提供对slave有管理员访问权限的用户名和密码，jenkins将远程地创建windows service然后远程地启动和停止他们。对于windows的系统，这是最方便的方法，但是此方法不允许运行有显示交互的GUI程序。 在Kubernetes上的Jenkins通过Kubernetes插件动态创建的Slave POD节点是通过JNLP的方式与Jenkins Master进行通信的！ 三、Jenkins Kubernetes插件的安装配置 安装 配置 四、定制Slave镜像 Slave镜像中安装的软件信息 工具 版本 说明 Oracel JDK 1.8.0_171 Maven编译打包时使用 Apache Maven 3.6.1 在Slave容器中使用MAVEN编译打包源代码 helm v2.13.1 helm客户端 git 1.8.3.1 git命令 docker client 1.13.1 Dockers客户端，用于在Slave容器中构建应用镜像 sonar-scanner 3.3.0.1492 用于扫描源代码 FROM centos:7.4.1708 ENV TZ=Asia/Shanghai \\ LANG=en_US.UTF-8 \\ JDK_VERSION=Oracle_1.8.0_171 \\ MAVEN_VERSION=Apache_3.6.1 \\ HOME=/home/jenkins \\ MAVEN_HOME=/opt/apache-maven-3.6.1 \\ JAVA_HOME=/opt/jdk1.8.0_171 \\ SONARSCANNER_HOME=/opt/sonar-scanner-3.3.0.1492-linux COPY jdk1.8.0_171 /opt/jdk1.8.0_171 COPY apache-maven-3.6.1 /opt/apache-maven-3.6.1 COPY helm /usr/bin/helm COPY sonar-scanner-3.3.0.1492-linux /opt/sonar-scanner-3.3.0.1492-linux RUN curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo \\ && yum makecache \\ && yum install -y git docker-ce-cli make \\ && yum clean all \\ && groupadd -g 1000 jenkins \\ && useradd -c \"Jenkins user\" -d /home/jenkins -u 1000 -g 0 -m jenkins \\ && mkdir /home/jenkins/.m2 \\ && chown -R 1000.0 /home/jenkins \\ && ln -s /opt/jdk1.8.0_171/bin/java /usr/bin/java \\ && ln -s /opt/apache-maven-3.6.1/bin/mvn /usr/bin/mvn \\ && ln -s /opt/sonar-scanner-3.3.0.1492-linux/bin/sonar-scanner /usr/bin/sonar-scanner USER jenkins WORKDIR /home/jenkins COPY dumb-init /usr/bin/dumb-init ADD run-jnlp-client /usr/bin/ ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/usr/bin/run-jnlp-client\"] 五、Pipeline或者Job中使用验证 Job 创建一个自由风格的Job 点击构建后，会自动创建一个Slave POD，并通过JNLP协议与Jenkins Master的Agent端口5000进行通通信 Declarative Pipeline pipeline { agent { label 'maven' } stages { stage (\"代码编译\") { steps { configFileProvider([configFile(fileId: 'nexus-maven-settings', targetLocation: 'settings.xml')]){ sh 'mvn -s settings.xml compile' } } } stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.projectName=demo-springboot2 \\ -Dsonar.projectKey=demo-springboot2 \\ -Dsonar.sources=src \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=****** \\ -Dsonar.java.binaries=. \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.issuesReport.html.enable=true \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.gitlab.user_token=***** \\ -Dsonar.gitlab.url=http://gitlab.apps.okd311.curiouser.com/ \\ -Dsonar.gitlab.ignore_certificate=true \\ -Dsonar.gitlab.comment_no_issue=true \\ -Dsonar.gitlab.max_global_issues=1000 \\ -Dsonar.gitlab.unique_issue_per_inline=true\" } } stage (\"代码打包\") { steps { sh \"mvn -s settings.xml package\" } } stage(\"上传制品\"){ steps{ script{ def pomfile = readMavenPom file: 'pom.xml' sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u devops:**** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://nexus.apps.okd311.curiouser.com/repository/jenkins-product-repo/${pomfile.artifactId}-${pomfile.version}-${env.GIT_COMMIT}.${pomfile.packaging}\" } } } stage(\"构建应用镜像\"){ steps{ sh 'docker login -p ********** -u unused docker-registry-default.apps.okd311.curiouser.com' sh 'make' } } } post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 参考博客： https://github.com/easzlab/kubeasz/blob/master/docs/guide/jenkins.md https://www.qikqiak.com/k8s-book/docs/36.Jenkins%20Slave.html https://jenkins.io/blog/2018/09/14/kubernetes-and-secret-agents/ https://www.jianshu.com/p/1440b5b4b980 https://www.cnblogs.com/guguli/p/7827435.html https://blog.csdn.net/felix_yujing/article/details/78725142 https://jicki.me/kubernetes/2018/02/08/kubernetes-jenkins/ https://testerhome.com/topics/17251 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 16:33:01 "},"origin/jenkins-pipeline-utility-steps.html":{"url":"origin/jenkins-pipeline-utility-steps.html","title":"Pipeline Utility Steps","keywords":"","body":"Jenkins Pipeline-Utility-steps插件 一、Pipeline-Utility-steps插件简介 Jekins中的Pipeline-Utility-steps插件能让你在pipeline的Step中直接使用它的API方法进行某些操作，例如查找文件，读取YAML/JSON/Properties文件、读取Maven工程POM文件等。这些方法有一个前提，任何文件都需要放在jenkins的workspace下，执行的job才能去找到文件。 Github地址：https://github.com/jenkinsci/pipeline-utility-steps-plugin 相关文档：https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/ 二、Pipeline-Utility-steps插件的方法 1、文件操作 findFiles 根据一些字符串规则去查找文件，如果有匹配的查找，返回是一个fille数组对象。（文档） 参数 excludes(可选，参数类型为String) glob(可选，参数类型为String) 示例 def files = findFiles(glob: '**/TEST-*.xml') echo \"\"\"${files[0].name} ${files[0].path} ${files[0].directory} ${files[0].length} ${files[0].lastModified}\"\"\" touch 创建文件（如果文件不存在的话）并设置时间戳. Returns a FileWrapper representing the file that was touched. (文档) 参数 file(参数类型为String)：The path to the file to touch. timestamp(可选，参数类型为long)：The timestamp to set (number of ms since the epoc), leave empty for current system time. sha1 计算指定文件的SHA1 (文档) 参数 file(参数类型为String): The path to the file to hash. tee 将输出重定向到文件 参数 file(参数类型为String) Zip Files zip：创建Zip文件. (文档) 参数 zipFile(参数类型为String): The name/path of the zip file to create. archive(可选，参数类型为boolean): If the zip file should be archived as an artifact of the current build. The file will still be kept in the workspace after archiving. dir(可选，参数类型为String): The path of the base directory to create the zip from. Leave empty to create from the current working directory. glob(可选，参数类型为String): Ant style pattern of files to include in the zip. Leave empty to include all files and directories. unzip：解压或读取Zip文件 (文档) 参数 zipFile(参数类型为String): The name/path of the zip file to extract. charset(可选，参数类型为String): Specify which Charset you wish to use eg. UTF-8 dir(可选，参数类型为String): The path of the base directory to extract the zip to. Leave empty to extract in the current working directory. glob(可选，参数类型为String): Ant style pattern of files to extract from the zip. Leave empty to include all files and directories. quiet(可选，参数类型为boolean): Suppress the verbose output that logs every single file that is dealt with. E.g. unzip zipFile: 'example.zip', quiet: true read(可选，参数类型为boolean): Read the content of the files into a Map instead of writing them to the workspace. The keys of the map will be the path of the files read. E.g. def v = unzip zipFile: 'example.zip', glob: '*.txt', read: true String version = v['version.txt'] test(可选，参数类型为boolean): Test the integrity of the archive instead of extracting it. When this parameter is enabled, all other parameters (except for zipFile) will be ignored. The step will return true or false depending on the result instead of throwing an exception. 2、配置文件操作 readProperties Reads a file in the current working directory or a String as a plain text Java Properties file. The returned object is a normal Map with String keys. The map can also be pre loaded with default values before reading/parsing the data. (文档) 参数 defaults (可选，Nested Choice of Objects): An Map containing default key/values. These are added to the resulting map first. file(可选，参数类型为String): path to a file in the workspace to read the properties from. These are added to the resulting map after the defaults and so will overwrite any key/value pairs already present. interpolate (可选，参数类型为boolean): Flag to indicate if the properties should be interpolated or not. In case of error or cycling dependencies, the original properties will be returned. text (可选，参数类型为String): An String containing properties formatted data. These are added to the resulting map after file and so will overwrite any key/value pairs already present. 示例 def d = [test: 'Default', something: 'Default', other: 'Default'] def props = readProperties defaults: d, file: 'dir/my.properties', text: 'other=Override' assert props['test'] == 'One' assert props['something'] == 'Default' assert props.something == 'Default' assert props.other == 'Override' def props = readProperties interpolate: true, file: 'test.properties' assert props.url = 'http://localhost' assert props.resource = 'README.txt' // if fullUrl is defined to ${url}/${resource} then it should evaluate to http://localhost/README.txt assert props.fullUrl = 'http://localhost/README.txt' readManifest Reads a Jar Manifest file or text and parses it into a set of Maps. The returned data structure has two properties: main for the main attributes, and entries containing each individual section (except for main). (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): path to a file to read. It could be a plain text, .jar, .war or .ear. In the latter cases the manifest will be extracted from the archive and then read. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): text containing the manifest data. 示例 def man = readManifest file: 'target/my.jar' assert man.main['Version'] == '6.15.8' assert man.main['Application-Name'] == 'My App' assert man.entries['Section1']['Key1'] == 'value1-1' assert man.entries['Section2']['Key2'] == 'value2-2' readYaml Reads a file in the current working directory or a String as a plain text YAML file. It uses SnakeYAML as YAML processor. The returned objects are standard Java objects like List, Long, String, ...: bool: [true, false, on, off] int: 42 float: 3.14159 list: ['LITE', 'RES_ACID', 'SUS_DEXT'] map: {hp: 13, sp: 5}. (文档) 参数 file(可选，参数类型为String) text(可选，参数类型为String) 示例 // 读取单个YAML文件 def datas = readYaml text: \"\"\" something: 'my datas' size: 3 isEmpty: false \"\"\" assert datas.something == 'my datas' assert datas.size == 3 assert datas.isEmpty == false // 读取多个YAML文件 def datas = readYaml text: \"\"\" --- something: 'my first document' --- something: 'my second document' \"\"\" assert datas.size() == 2 assert datas[0].something == 'my first document' assert datas[1].something == 'my second document' // With file dir/my.yml containing something: 'my datas' : def datas = readYaml file: 'dir/my.yml', text: \"something: 'Override'\" assert datas.something == 'Override' writeYaml Write a YAML file from an object. (文档) 参数 file(参数类型为String): Mandatory path to a file in the workspace to write the YAML datas to. data: A Mandatory Object containing the data to be serialized. charset: Optionally specify the charset to use when writing the file. Defaults to UTF-8 if nothing else is specified. What charsets that are available depends on your Jenkins master system. The java specification tells us though that at least the following should be available: [ US-ASCII、ISO-8859-1、UTF-8、UTF-16BE、UTF-16LE、UTF-16] 示例 def amap = ['something': 'my datas', 'size': 3, 'isEmpty': false] writeYaml file: 'datas.yaml', data: amap def read = readYaml file: 'datas.yaml' assert read.something == 'my datas' assert read.size == 3 assert read.isEmpty == false readJSON Reads a file in the current working directory or a String as a plain text JSON file. The returned object is a normal Map with String keys or a List of primitives or Map. (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the JSON data. Data could be access as an array or a map. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the JSON formatted data. Data could be access as an array or a map. 示例 def props = readJSON file: 'dir/input.json' assert props['attr1'] == 'One' assert props.attr1 == 'One' def props = readJSON text: '{ \"key\": \"value\" }' assert props['key'] == 'value' assert props.key == 'value' def props = readJSON text: '[ \"a\", \"b\"]' assert props[0] == 'a' assert props[1] == 'b' writeJSON：Write a JSON file in the current working directory. That for example was previously read by readJSON. (文档) 参数 file(可选，参数类型为String): Path to a file in the workspace to write to. json(Nested Choice of Objects): The JSON object to write. pretty (可选，参数类型为int): Prettify the output with this number of spaces added to each level of indentation. 示例 def input = readJSON file: 'myfile.json' //Do some manipulation writeJSON file: 'output.json', json: input //or pretty print it, indented with a configurable number of spaces writeJSON file: 'output.json', json: input, pretty: 4 readCSV Reads a file in the current working directory or a String as a plain text. A List of CSVRecord instances is returned. (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the CSV data. Data is accessed as a List of String Array. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the CSV formatted data. Data is accessed as a List of String Arrays. format(可选，org.apache.commons.csv.CSVFormat) 示例 def records = readCSV file: 'dir/input.csv' assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b' assert records[0][0] == 'key' assert records[1][1] == 'b' // 进阶示例 def excelFormat = CSVFormat.EXCEL def records = readCSV file: 'dir/input.csv', format: excelFormat assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b', format: CSVFormat.DEFAULT.withHeader() assert records[1].get('key') == 'a' assert records[1].get('value') == 'b' writeCSV Write a CSV file in the current working directory. That for example was previously read by readCSV. See CSVPrinter for details.(文档) 参数 file(参数类型为String): Path to a file in the workspace to write to. records(java.lang.Iterable): The list of CSVRecord instances to write. format(可选，org.apache.commons.csv.CSVFormat):See CSVFormat for details. 示例 def records = [['key', 'value'], ['a', 'b']] writeCSV file: 'output.csv', records: records, format: CSVFormat.EXCEL 3、Maven项目 readMavenPom 读取Maven POM文件到一个Model数据结构中. (文档) 参数 file(可选，参数类型为String)：默认读取目前工作区下的POM.xml文件 示例 stage ('上传制品') { steps { script{ def pomfile = readMavenPom file: 'pom.xml' nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] } } } writeMavenPom Writes a Maven project file. That for example was previously read by readMavenPom. (文档) 参数 model(参数类型为org.apache.maven.model.Model): The Model object to write. file(可选，参数类型为String): Optional path to a file in the workspace to write to. If left empty the step will write to pom.xml in the current working directory. 示例 def pom = readMavenPom file: 'pom.xml' //Do some manipulation writeMavenPom model: pom Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-18 09:48:22 "},"origin/jenkins-Nexus-Platform的使用.html":{"url":"origin/jenkins-Nexus-Platform的使用.html","title":"Nexus Platform Plugin","keywords":"","body":"Preflight 官方插件文档：https://help.sonatype.com/integrations/nexus-and-continuous-integration/nexus-platform-plugin-for-jenkins 安装插件：Pipeline Utility Steps 功能： 一、安装 二、配置 系统管理--> 系统设置--> Sonatype Nexus 三、使用 上传构建后的制品到Nexus的Hosted类型仓库中 Job Declarative Pipeline ```bash stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://nexus-nexus.apps.okd311.curiouser.com/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } ``` 四、注意 如果Job再次构建，产生相同的Jar，上传信息还是一样的，Nexus的Release仓库需要设置为\"允许Redeploy\"。不然，仓库中已经相同版本信息的制品，会造成上传失败 参考链接 https://support.sonatype.com/hc/en-us/articles/115009108987-Jenkins-Publish-Using-Maven-Coordinates-from-the-pom-xml https://www.jianshu.com/p/29403ecf7fc2 https://stackoverflow.com/questions/37603619/extract-version-id-from-pom-in-a-jenkins-pipeline Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-配置SMTP邮箱服务.html":{"url":"origin/jenkins-配置SMTP邮箱服务.html","title":"Mail Plugin","keywords":"","body":"Jenkins配置SMTP邮箱服务 Prerequisite 自己邮箱运营商设置了开通SMTP服务 Jenkins 安装了Jenkins Mailers Plugin 一、Context Jenkins默认有个插件叫\"Mailer Plugin\"用来发送通知邮件。该插件使用的\"JavaMail \"来进行配置自定义个邮箱服务器 二、配置 系统管理-->系统设置 配置Jenkins的系统管理员邮箱地址 配置SMTP邮件服务器地址 三、使用 Job中 四、问题 当构建不成功时发送的邮件，内容包含构建的日志。 当初次构建成功时会发送邮件通知，当再次重复构建成功时，则不会发送邮件通知，得等到构建失败时才会再次发送通知邮件 功能太弱，可使用\"Mail Extension\"插件进行功能扩展。详见：jenkins-Mailer邮箱功能扩展插件Email-Extension 不知Jenkins的系统管理员邮箱时，发送会报错 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html":{"url":"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html","title":"Mail Extension","keywords":"","body":"Jenkins Mailer邮箱功能扩展插件Email-Extension 一、Context Jenkins自带的邮件插件功能太弱，有个邮箱扩展插件。 官方文档WIKI：https://wiki.jenkins.io/display/JENKINS/Email-ext+plugin 优势： 邮件格式改为HTML，更美观 使用模板来配置邮件内容 为不同的Job配置不一样的收件人 为不同的事件配置不一样的trigger 在Jenkins pipeline中集成发送邮件通知功能 二、插件安装配置 1、安装 2、配置 三、使用 1、Jobs中 2、Pipeline中 pipeline{ ...上文省略... post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 四、发送HTML格式的邮件 1、Pipeline中 Prerequisite 准备格式化好的HTML ${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志 Jenkins构建信息邮件，请勿回复！ 构建信息 项目名称： ${PROJECT_NAME} 构建编号： ${BUILD_NUMBER} 构建状态： ${BUILD_STATUS} 构建人员： ${GITLABUSERNAME} 构建日志： 见附件 Jenkins构建页面： ${BUILD_URL} 变更代码： ${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit} 单元测试报告： ${BUILD_URL}jacoco 测试报告 特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖 ${FILE,path=\"./target/site/jacoco/index.html\"} 使用pipeline语法生成器生成pipeline 压缩pipeline. (压缩HTML源代码的工具网站：http://tool.oschina.net/jscompress?type=2) pipeline{ ...上文省略... post { always { emailext attachLog: true, body: '''${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志Jenkins构建信息邮件，请勿回复！构建信息项目名称： ${PROJECT_NAME}构建编号： ${BUILD_NUMBER}构建状态： ${BUILD_STATUS}构建人员： ${GITLABUSERNAME}构建日志： 见附件Jenkins构建页面：${BUILD_URL}变更代码：${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit}单元测试报告：${BUILD_URL}jacoco测试报告特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖${FILE,path=\"./target/site/jacoco/index.html\"}''', mimeType: 'text/html', subject: '项目构建报告：$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-gitlab插件的使用.html":{"url":"origin/jenkins-gitlab插件的使用.html","title":"Gitlab","keywords":"","body":"Jenkins Gitlab插件的使用 一、Overviews Jenkins的Gitlab插件能接收gitlab仓库代码事件触发的Webhook来触发Jenkins Job/Pipeline的构建，并且能将构建状态同步到Gitlab中. 插件Github：https://github.com/jenkinsci/gitlab-plugin 利用此插件可实现如下效果： 二、安装 Jenkins -> 系统管理 ->插件管理->可用插件->搜索\"gitlab\" 插件下载地址：https://plugins.jenkins.io/gitlab-plugin 三、配置 1、Gitlab发送Webhook到Jenkins时的安全认证配置 Gitlab代码仓库配置web hook时需要Jenkins Gitlab插件的Token,来加密验证webhook的安全性 方式一：全局性的认证Token ① Jenkins创建新用户（授予Job/Build权限即可） ② 获取新用户的User ID和API Token ③ Gitlab配置web hook填写URL时，使用http://USERID:APITOKEN@JENKINS_URL/project/YOUR_JOB即可（Secret Token可忽略） 方式二(推荐)：单独项目Jenkins Job的认证Token 在Gitlab中配置Webhook时，将图中的Token填入即可 方式三(不推荐)：不使用认证 ① Manage Jenkins -> Configure System -> GitLab section -> 取消勾选 \"Enable authentication for '/project' end-point\" 2、Jenkins回写构建状态到Gitlab时需要的认证授权配置 Jenkins通过Gitlab API将构建状态回写到对应代码仓库时，需要有权限在代码仓库中创建评论，更改状态等 步骤一 ① Gitlab创建新用户（建议命名用户名时尽量见名知意，例如：Jenkins） ② 登陆新用户，获取Access Tokens,Token权限只要api即可(及时复制Token,只显示一次) ③ 在Jenkins中创建GitLab API token类型的凭据，在API token字段中保存上一步获取的Token ④ 在Jenkins Gitlab插件中配置gitlab Server相关信息 步骤二 对应代码仓库配置Jenkins Job时要将该Token所属的用户加入members中，授予Developers角色或有更高权限的角色 五、Jenkins Job中配置Webhook 1、配置Gitlab Buid Trigger 2、配置构建后Gitlab回写信息的动作 添加构建后动作\"Pushlish build status to Gitlab\" 之后会在Gitlab代码仓库的CI/CD-->Pipeline和Merge Request中看到构建状态图标 添加构建后动作\"Add note with build status on Gitlab merge request\" 之后会根据构建状态的不同在Gitlab的Merge Request中添加不同的comment评论 添加构建后动作\"Add Vote for build status on Gitlab merge request\" 之后会根据构建状态的不同在Gitlab的Merge Request中显示不同的投票 添加构建后动作\"Accept Gitlab merge request on success\" 该动作会在build构建成功后在Gitla上自动同意接受Merge Request 不添加构建后动作 Gitlab的事件只会触发Jenkins构建，不会回写任何信息到gitlab对应代码仓库中 五、Gitlab 代码仓库配置Webhook 详见：gitlab-配置代码仓库事件触发器Webhook 六、获取Webhook HTTP请求信息的变量 对于Gitlab发送过来的Webhook HTTP POST请求信息，可直接通过以下变量来获取。 gitlabBranch gitlabSourceBranch gitlabActionType gitlabUserName gitlabUserEmail gitlabSourceRepoHomepage gitlabSourceRepoName gitlabSourceNamespace gitlabSourceRepoURL gitlabSourceRepoSshUrl gitlabSourceRepoHttpUrl gitlabMergeRequestTitle gitlabMergeRequestDescription gitlabMergeRequestId gitlabMergeRequestIid gitlabMergeRequestState gitlabMergedByUser gitlabMergeRequestAssignee gitlabMergeRequestLastCommit gitlabMergeRequestTargetProjectId gitlabTargetBranch gitlabTargetRepoName gitlabTargetNamespace gitlabTargetRepoSshUrl gitlabTargetRepoHttpUrl gitlabBefore gitlabAfter gitlabTriggerPhrase Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-generic-webhook-trigger插件.html":{"url":"origin/jenkins-generic-webhook-trigger插件.html","title":"Generic Webhook Trigger","keywords":"","body":"Jenkins Generic Webhook Trigger插件 之前写过在Jenkins中使用Gitlab插件接收gitlab仓库代码指定事件触发的webhook来触发Job或Pipeline的执行构建,详见:Jenkins的Gitlab插件。这种方式在某种场景下有些限制，无法满足一些功能性复杂的webhook触发（例如只接受特定tag,分支或Merge的webhook触发，例如创建通配的pipeline，gitlab各个仓库配置固定的Webhook请求地址等等场景）。所以可以使用Jenkins Generic Webhook Trigger插件监听包含自定义设置的HTTP请求来触发job/pipeline的构建！对比Gitlab插件，有以下可自定义的特性： 暴露出来的回调API URL统一， 不同的job/pipeline使用不同的Token或者指定特殊的请求参数进行区分 可使用不同的方式提取HTTP请求中的各种信息，然后通过环境变量的形式传递给job/pipeline使用 可设置白名单，只允许接收指定IP地址的Webhook请求 一、简介 Generic Webhook Trigger是一款Jenkins插件，简称GWT，安装后会暴露出来一个公共API，GWT插件接收到 JSON 或 XML 的 HTTP POST 请求后，根据我们配置的规则决定触发哪个Jenkins项目 安装的话在Jenkins的插件管理中心直接搜索安装即可，下载HPI文件手动安装, 插件下载地址 插件Github地址：https://github.com/jenkinsci/generic-webhook-trigger-plugin 支持以下系统发送的Webhook: Bitbucket Cloud Bitbucket Server GitHub GitLab Gogs and Gitea Assembla Jira 二、GenericTrigger 触发器设置 GenericTrigger 触发条件分为5部分： 从 HTTP POST 请求中提取数据 Token, GWT 插件用于标识Jenkins项目的唯一性 根据请求参数值判断是否触发Jenkins项目的执行 日志打印控制 Webhook 响应控制 1. 从 HTTP POST 请求中匹配数据到环境变量中 GWT 插件可以从一个HTTP POST请求的Body、URL参数、header中提取数据并赋予指定的环境变量，后续任务可以直接引用。 在Job中设置数据的匹配 在Pipeline中设置数据的匹配 genericRequestVariables：从URL参数中提取值 genericVariables： 从HTTP POST的body 中提取值 genericHeaderVariables：从HTTP header 中提取值（用法和genericRequestVariables一样） genericVariables: [ [key: 'ref', value: '$.ref'], [key: 'before',value: '$.before', expressionType: 'JSONPath', //Optional, defaults to JSONPath regexpFilter: '', //Optional, defaults to empty string defaultValue: '' //Optional, defaults to empty string ] ], genericRequestVariables: [ [key: 'requestWithNumber', regexpFilter: '[^0-9]'], [key: 'requestWithString', regexpFilter: ''] ], genericHeaderVariables: [ [key: 'headerWithNumber', regexpFilter: '[^0-9]'], [key: 'headerWithString', regexpFilter: ''] ] 2.过滤符合条件的Webhook参数才能触发构建 例如只允许gitlab仓库Pipeline分支commit事件、带有特殊前缀的tag才能触发的Webhook才能触发构建，此时就可以先从webhook请求体中获取对应的值，放到指定的环境变量中，在插件的Option filter中设置正则表达式进行过滤，符合的才能触发pipeline，不符合的不触发。 3.Token 参数 当多个Jenkins Jobs中使用该插件时，Webhook Trigger都是同一个相同的URL。如果你只想触发一个特定的工作，可以 使用令牌参数为不同的作业提供不同的令牌。 可以再header或post中添加一些请求参数，并仅在该参数具有特定值时使用regexp筛选器触发。 发送Webhook请求时可使用以下方式在HTTP POST请求中携带Token： 请求URL参数: curl -vs http://localhost:8080/jenkins/generic-webhook-trigger/invoke?token=abc123 token的请求header: curl -vs -H \"token: abc123\" http://localhost:8080/jenkins/generic-webhook-trigger/invoke Bearer类型Authorization的请求header: curl -vs -H \"Authorization: Bearer abc123\" http://localhost:8080/jenkins/generic-webhook-trigger/invoke 4. 日志打印控制 Silent response： 当为true，只返回http 200 状态码，不返回触发状态等信息。 Print post content： 是否在构建日志中打印webhook 请求的内容 Print contributed variables： 是否在构建日志中打印提取后的变量 5. 显示触发信息 三、设置白名单 可在Jenkins Generic Webhook Trigger的全局配置中，配置IP地址白名单，指定、验证请求的来源IP地址，IP地址格式可为CIDR 或 ranges。 1.2.3.4 2.2.3.0/24 3.2.1.1-3.2.1.10 2001:0db8:85a3:0000:0000:8a2e:0370:7334 2002:0db8:85a3:0000:0000:8a2e:0370:7334/127 同时还支持HMAC验证(HMAC百度百科)。 Jenkins --> Manage Jenkins --> Configure System Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-29 12:18:20 "},"origin/gitlab-install.html":{"url":"origin/gitlab-install.html","title":"安装配置","keywords":"","body":"Gitlab的安装与配置 一、Docker Deb安装包下载地址：https://packages.gitlab.com/gitlab/ ARM 官方gitlab-ce暂没有ARM 架构的镜像,所以由第三方的镜像进行部署 参考：https://about.gitlab.com/handbook/engineering/development/enablement/distribution/maintenance/arm.html#why-dont-you-compile-arm32-bit-packages-on-arm64-for-speed GitHub：https://github.com/ulm0/gitlab DockerHub：https://hub.docker.com/r/ulm0/gitlab mkdir -p /data/gitlab/data /data/gitlab/logs /data/gitlab/config && \\ docker run -d \\ --hostname 192.168.1.8 \\ -e GITLAB_OMNIBUS_CONFIG=\"external_url 'http://192.168.1.1:38080';gitlab_rails['lfs_enabled'] = true; gitlab_rails['gitlab_shell_ssh_port'] = 30022 ; node_exporter['enable'] = true ;\" \\ -e RAILS_ENV=\"production\" \\ -e GITLAB_EMAIL_DISPLAY_NAME=\"Gitlab 13\" \\ -e GITLAB_EMAIL_FROM=\"*****@163.com\" \\ -e GITLAB_EMAIL_REPLY_TO=\"*****@163.com\" \\ -e GITLAB_EMAIL_SUBJECT_SUFFIX=\"Gitlab 13\" \\ -e GITLAB_ROOT_PASSWORD=\"*****\" \\ -p 38080:38080 \\ -p 30022:22 \\ --name gitlab \\ --restart always \\ --privileged \\ -v /data/gitlab/config:/etc/gitlab \\ -v /data/gitlab/logs:/var/log/gitlab \\ -v /data/gitlab/data:/var/opt/gitlab \\ ulm0/gitlab:13.2.6 手动构建新版本的arm gitlab docker镜像 git clone https://github.com/ulm0/gitlab.git gitlab-arm-docker cd gitlab-arm-docker echo \"13.8.1\" > VERSION make build 针对raspberry的deb包下载地址：https://packages.gitlab.com/gitlab/raspberry-pi2 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-24 18:37:50 "},"origin/gitlab-配置SMTP邮件服务.html":{"url":"origin/gitlab-配置SMTP邮件服务.html","title":"配置SMTP邮件服务","keywords":"","body":"1. 修改/etc/gitlab/gitlab.rb ### Email Settings gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = '******@163.com' gitlab_rails['gitlab_email_display_name'] = 'Curiouser163SMTPServer' gitlab_rails['gitlab_email_reply_to'] = '*****@163.com' gitlab_rails['gitlab_email_subject_suffix'] = 'Gitlab' gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.163.com\" gitlab_rails['smtp_port'] = 25 gitlab_rails['smtp_user_name'] = \"******@163.com\" gitlab_rails['smtp_password'] = \"******\" gitlab_rails['smtp_domain'] = \"163.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = false gitlab_rails['smtp_tls'] = false 注意： 当使用Docker镜像部署时，相关Gitlab参数可追加在环境变量“GITLAB_OMNIBUS_CONFIG”的值中进行配置。详见：https://docs.gitlab.com/omnibus/docker/ 及 gitlab的安装与配置 测试发送邮件 $ gitlab-rails console Loading production environment (Rails 4.2.10) $ irb(main):001:0> Notify.test_email('******@163.com','gitlab send mail test','gitlab test mail').deliver_now Notify#test_email: processed outbound mail in 539.6ms Sent mail to ******@163.com (397.8ms) Date: Thu, 04 Jul 2019 15:07:33 +0000 From: Curiouser163SMTPServer Reply-To: Curiouser163SMTPServer To:******@163.com Message-ID: Subject: gitlab send mail test Mime-Version: 1.0 Content-Type: text/html; charset=UTF-8 Content-Transfer-Encoding: 7bit Auto-Submitted: auto-generated X-Auto-Response-Suppress: All gitlab test mail=> #, >, >, , >, , , , , , > irb(main):002:0> 参考 https://docs.gitlab.com/omnibus/settings/smtp.html#smtp-settings Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-20 22:42:12 "},"origin/gitlab-配置代码仓库事件触发器Webhook.html":{"url":"origin/gitlab-配置代码仓库事件触发器Webhook.html","title":"代码仓库配置事件触发器Webhook","keywords":"","body":"Gitlab 代码仓库配置事件Webhoook触发器 一、Overviews Webhook与异步编程中\"订阅-发布模型\"非常类似，一端触发事件，一端监听执行。 WebHook就是一个接收HTTP POST（或GET，PUT，DELETE）的URL 通常来说，WebHook通过HTTP协议将请求数据发送到外部系统后，就不再处理响应数据啦！ 二、Gitlab事件触发器配置Webhook 当Gitlab中代码仓库发生了一些事件后，可设置触发器发送http通知到外部系统。通常配置Jenkins上的Webhook 配置Gitlab的网络限制 否则后续发送webhook请求时会报错\"Requests to the local network are not allowed\",原因详见：附录 配置代码代码仓库的Webhook 配置完可以模拟一些事件进行测试 查看详细的Webhook请求信息 附录：gitlab发送webhook请求时报错原因 If you have non-GitLab web services running on your GitLab server or within its local network, these may be vulnerable to exploitation via Webhooks. With Webhooks, you and your project maintainers and owners can set up URLs to be triggered when specific things happen to projects. Normally, these requests are sent to external web services specifically set up for this purpose, that process the request and its attached data in some appropriate way. Things get hairy, however, when a Webhook is set up with a URL that doesn't point to an external, but to an internal service, that may do something completely unintended when the webhook is triggered and the POST request is sent. Because Webhook requests are made by the GitLab server itself, these have complete access to everything running on the server (http://localhost:123) or within the server's local network (http://192.168.1.12:345), even if these services are otherwise protected and inaccessible from the outside world. If a web service does not require authentication, Webhooks can be used to trigger destructive commands by getting the GitLab server to make POST requests to endpoints like \"http://localhost:123/some-resource/delete\". To prevent this type of exploitation from happening, starting with GitLab 10.6, all Webhook requests to the current GitLab instance server address and/or in a private network will be forbidden by default. That means that all requests made to 127.0.0.1, ::1 and 0.0.0.0, as well as IPv4 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 and IPv6 site-local (ffc0::/10) addresses won't be allowed. This behavior can be overridden by enabling the option \"Allow requests to the local network from hooks and services\" in the \"Outbound requests\" section inside the Admin area under Settings (/admin/application_settings): Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/gitlab-backuprestore.html":{"url":"origin/gitlab-backuprestore.html","title":"代码仓库的备份与恢复","keywords":"","body":"Gitlab代码仓库的备份与恢复 一、Preface GItlab上的代码仓库需要进行定期的导出备份，并且可以随时进行恢复 二、API接口备份代码仓库数据 gitlab提供导出代码仓库指定资源的API接口 支持代码仓库导出的gitlab资源 不支持代码仓库导出的gitlab资源 Project and wiki repositories Build traces and artifacts Project uploads Container registry images Project configuration, including services CI variables Issues with comments, merge requests with diffs and comments, labels, milestones, snippets, and other project entities Webhooks Design Management files and data Any encrypted tokens LFS objects Merge Request Approvers Issue boards Push Rules Awards 1、通过API接口 ① 调用生成Export的接口 接口及参数 POST /projects/:id/export Attribute Type Required Description id integer/string yes The ID or URL-encoded path of the project owned by the authenticated user description string no Overrides the project description upload hash no Hash that contains the information to upload the exported project to a web server upload[url] string yes The URL to upload the project upload[http_method] string no The HTTP method to upload the exported project. Only PUT and POST methods allowed. Default is PUT curl --request POST --header \"PRIVATE-TOKEN: \" https://gitlab.example.com/api/v4/projects/1/export ②调用获取Export生成状态的接口 接口及参数 Attribute Type Required Description id integer/string yes The ID or URL-encoded path of the project owned by the authenticated user curl --header \"PRIVATE-TOKEN: \" https://gitlab.example.com/api/v4/projects/1/export ③下载Export 接口及参数 GET /projects/:id/export/download Attribute Type Required Description id integer/string yes The ID or URL-encoded path of the project owned by the authenticated user curl --header \"PRIVATE-TOKEN: \" --remote-header-name --remote-name https://gitlab.example.com/api/v4/projects/5/export/download ④执行脚本 #/bin/bash gitlab_url=http://gitlab.apps.okd311.curiouser.com/ gitlab_access_token=***** gitlab_api_version=\"api/v4\" for project_id in $(curl -s -XGET ''\"$gitlab_url\"''\"$gitlab_api_version\"'/projects?simple=true&order_by=id&sort=asc' -H 'private-token: '\"$gitlab_access_token\"'' | jq '.[].id') do curl -s -XPOST ''\"$gitlab_url\"''\"$gitlab_api_version\"'/projects/'\"$project_id\"'/export' -H 'private-token: '\"$gitlab_access_token\"'' > /dev/null if [[ `curl -s -XGET ''\"$gitlab_url\"''\"$gitlab_api_version\"'/projects/'\"$project_id\"'/export' -H 'private-token: '\"$gitlab_access_token\"'' | jq '.export_status' ` =~ \"finished\" ]];then curl -s -O -XGET ''\"$gitlab_url\"'/'\"$gitlab_api_version\"'/projects/'\"$project_id\"'/export/download' -H 'private-token: '\"$gitlab_access_token\"'' ; echo hahah ; else echo test; fi done 2、通过UI界面 ①UI路径 仓库页面-->Settings-->General-->Advanced-->Export project按钮 ②点击Export ③当Export生成后会发送带有Export下载链接的邮件给代码仓库的维护者 ④(可选)后续还可以在UI界面上点击下载Export 三、命令行备份gitlab实例数据 gitlab提供了相应的命令可以进行备份与恢复。 1、备份 ①命令 二进制方式或docker方式部署的 版本>= GitLab 12.2 sudo gitlab-backup create # 或者 docker exec -t gitlab-backup create 版本 gitlab-rake gitlab:backup:create # 或者 docker exec -t gitlab-rake gitlab:backup:create 源代码方式部署的 sudo -u git -H bundle exec rake gitlab:backup:create RAILS_ENV=production 使用helm部署在k8s中的 使用helm部署的，GitLab task runner容器中有个一个工具脚本可进行备份 kubectl exec -it backup-utility 生成的备份文件的信息 备份文件的路径：默认/var/opt/gitlab/backups，可在config/gitlab.yml配置文件设置backup_path进行控制 备份文件名规则：默认[TIMESTAMP]-版本_gitlab_backup.tar ②支持备份的gitlab实例数据 Database Attachments Git repositories data CI/CD job output logs CI/CD job artifacts LFS objects Container Registry images GitLab Pages content Snippets Group wikis ③命令行的备份策略参数 默认的备份策略实际上是使用Linux命令tar和gzip将数据从相应的数据位置流式传输到备份。在大多数情况下，这可以正常工作，但是在数据快速变化时可能会导致问题。例如： 当tar正在读取数据时更改数据时，错误文件可能会随着我们读取数据而发生更改，并导致备份过程失败。为了解决这个问题，8.17引入了一种称为副本的新备份策略。该策略在调用tar和gzip之前将数据文件复制到一个临时位置，从而避免了错误。副作用是备份过程最多占用额外的1X磁盘空间。该过程尽力在每个阶段清理临时文件，这样问题就不会复杂化，但是对于大型安装而言，这可能是一个相当大的变化。这就是为什么复制策略不是8.17中的默认策略 BACKUP：设置备份文件名，代替默认备份文件名规则中的时间戳字段，例如：[BACKUP的值]-版本_gitlab_backup.tar sudo gitlab-backup create BACKUP=dump # 版本 GZIP_RSYNCABLE：设置备份文件可被rsync传输 sudo gitlab-backup create BACKUP=dump GZIP_RSYNCABLE=yes # 版本 SKIP：设置备份时排除哪些gitlab资源，可选项有db(database)、uploads(attachments)、builds(CI job output logs)、artifacts(CI job artifacts)、lfs(LFS objects)、registry(Container Registry images)、pages(Pages content)、repositories(Git repositories data) sudo gitlab-backup create BACKUP=dump GZIP_RSYNCABLE=yes SKIP=builds,lfs # 版本 SKIP=tar：创建备份的最后一部分是生成包含所有部分的.tar文件。在某些情况下（例如，如果备份是由其他备份软件获取的），创建.tar文件可能会浪费精力，甚至直接有害，因此可以通过将tar添加到SKIP环境变量中来跳过此步骤。将tar添加到SKIP变量会将包含备份的文件和目录保留在用于中间文件的目录中。创建新备份时，这些文件将被覆盖，因此，应确保将它们复制到其他位置，因为系统上只能有一个备份。 GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY与GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY ： 设置导出代码仓库数据时使用多线程，GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY是设置同时最多可导出多少个代码仓库数据，默认为1。GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY 设置每个存储上同时备份的最大项目数。这样可以将存储库备份分散到各个存储中 sudo gitlab-backup create GITLAB_BACKUP_MAX_CONCURRENCY=4 GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY=1 # 源码安装时 sudo -u git -H bundle exec rake gitlab:backup:create GITLAB_BACKUP_MAX_CONCURRENCY=4 GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY=1 CRON：在使用crontab进定时备份时，设置为1是为了不显示备份进度信息，减少备份期间相关输出 0 2 * * * /opt/gitlab/bin/gitlab-backup create CRON=1 # 版本 ④备份相关的配置 备份时的一些配置除了在命令行中通过参数进行配置，还可以在配置文件/etc/gitlab/gitlab.rb或/home/git/gitlab/config/gitlab.yml中进行配置 设置备份文件的权限 gitlab_rails['backup_archive_permissions'] = 0644 # 源码安装时 backup: archive_permissions: 0644 # Makes the backup archives world-readable 设置备份文件的保存期限 如果特意指定备份文件名前缀，使用默认时间戳为前缀的。在执行备份时，可将比早于backup_keep_time时间的备份文件进行自动清除 ## Limit backup lifetime to 7 days - 604800 seconds gitlab_rails['backup_keep_time'] = 604800 # 源码安装时 backup: ## Limit backup lifetime to 7 days - 604800 seconds keep_time: 604800 ⑤配置文件的备份 GitLab提供的备份Rake任务是不会备份配置文件的 手动备份 直接手动备份整个/etc/gitlab目录。如果不想备份整个目录的话，至少也要备份以下文件 /etc/gitlab/gitlab-secrets.json /etc/gitlab/gitlab.rb 源代码安装时 /home/git/gitlab/config/secrets.yml /home/git/gitlab/config/gitlab.yml 命令行备份 当版本 >= 12.3时，可以使用命令sudo gitlab-ctl backup-etc命令备份配置文件。备份文件路径在/etc/gitlab/config_backup/,备份文件及目录都是只有root可读写。 同时可以设置配置备份文件的路径sudo gitlab-ctl backup-etc 配置文件的备份文件名默认规则：gitlab_config_时间戳_日期.tar 会备份以下配置文件 /etc/gitlab/ /etc/gitlab/ssh_host_rsa_key.pub /etc/gitlab/ssh_host_ecdsa_key.pub /etc/gitlab/trusted-certs/ /etc/gitlab/ssh_host_rsa_key /etc/gitlab/ssh_host_ecdsa_key /etc/gitlab/ssh_host_ed25519_key.pub /etc/gitlab/ssh_host_ed25519_key /etc/gitlab/gitlab.rb /etc/gitlab/gitlab-secrets.json 可设置定时任务，15 04 * * 2-6 gitlab-ctl backup-etc && cd /etc/gitlab/config_backup && cp $(ls -t | head -n1) /data/gitlab/config-backups/ ⑥上传备份到外部云存储 可在/etc/gitlab/gitlab.rb中设置参数，让命令行执行的脚本将备份文件上传到外部云存储中，支持的云存储：AWS S3, Google Cloud Storage, Azure Blob storage, 其他S3类型供应商 设置参数将备份上传至云存储时，可在执行命令后添加DIRECTORY参数，将上传文件路径前添加路径，便于备份 sudo gitlab-backup create DIRECTORY=daily sudo gitlab-backup create DIRECTORY=weekly Amazon S3 gitlab_rails['backup_upload_connection'] = { 'provider' => 'AWS', 'region' => 'eu-west-1', 'aws_access_key_id' => 'AKIAKIAKI', 'aws_secret_access_key' => 'secret123' # If using an IAM Profile, don't configure aws_access_key_id & aws_secret_access_key # 'use_iam_profile' => true } gitlab_rails['backup_upload_remote_directory'] = 'my.s3.bucket' Digital Ocean Spaces gitlab_rails['backup_upload_connection'] = { 'provider' => 'AWS', 'region' => 'ams3', 'aws_access_key_id' => 'AKIAKIAKI', 'aws_secret_access_key' => 'secret123', 'endpoint' => 'https://ams3.digitaloceanspaces.com' } gitlab_rails['backup_upload_remote_directory'] = 'my.s3.bucket' 如果上传时出现400 Bad Request，是因为默认备份文件时加密的，而Digital Ocean Spaces不支持上传加密文件，注释或删除掉['gitlab_rails['backup_encryption'] Google Cloud Storage gitlab_rails['backup_upload_connection'] = { 'provider' => 'Google', 'google_storage_access_key_id' => 'Access Key', 'google_storage_secret_access_key' => 'Secret', ## If you have CNAME buckets (foo.example.com), you might run into SSL issues ## when uploading backups (\"hostname foo.example.com.storage.googleapis.com ## does not match the server certificate\"). In that case, uncomnent the following ## setting. See: https://github.com/fog/fog/issues/2834 #'path_style' => true } gitlab_rails['backup_upload_remote_directory'] = 'my.google.bucket' 源码安装时 backup: upload: connection: provider: 'Google' google_storage_access_key_id: 'Access Key' google_storage_secret_access_key: 'Secret' remote_directory: 'my.google.bucket' Azure Blob storage gitlab_rails['backup_upload_connection'] = { 'provider' => 'AzureRM', 'azure_storage_account_name' => '', 'azure_storage_access_key' => '', 'azure_storage_domain' => 'blob.core.windows.net', # Optional } gitlab_rails['backup_upload_remote_directory'] = '' 源码安装时 backup: upload: connection: provider: 'AzureRM' azure_storage_account_name: '' azure_storage_access_key: '' remote_directory: '' 2、恢复备份 ①确定要恢复的备份文件在指定的目录下，git用户并拥有相应读写权限 备份文件要放到配置文件/etc/gitlab/gitlab.rb 中，配置项gitlab_rails['backup_path’] (默认是/var/opt/gitlab/backups)或者backup: path: \"tmp/backups\"(默认是/home/git/gitlab/tmp/backups)指定的路径下 sudo cp /data/gitlab-backups/11493107454_2018_04_25_10.6.4-ce_gitlab_backup.tar /var/opt/gitlab/backups/ sudo chown git.git /var/opt/gitlab/backups/11493107454_2018_04_25_10.6.4-ce_gitlab_backup.tar ②停掉连接数据库的进程 sudo gitlab-ctl stop unicorn sudo gitlab-ctl stop puma sudo gitlab-ctl stop sidekiq # 源码安装时 sudo service gitlab stop # 验证gitlab状态 sudo gitlab-ctl status ③开始恢复指定的备份 sudo gitlab-backup restore BACKUP=11493107454_2018_04_25_10.6.4-ce # 版本 恢复备份时的一些参数 GITLAB_ASSUME_YES=1：恢复备份时，恢复脚本可能跳出一些提示，设置这个环境变量可以跳过这些提示 sudo GITLAB_ASSUME_YES=1 gitlab-backup restore # 源码安装时 sudo -u git -H GITLAB_ASSUME_YES=1 bundle exec rake gitlab:backup:restore RAILS_ENV=production ④恢复数据加密用的秘钥文件 恢复/etc/gitlab/gitlab-secrets.json或/home/git/gitlab/.secret(源码安装时)文件。该文件包含数据库加密密钥，CI / CD变量以及用于两重身份验证的变量。如果您无法连同应用程序数据备份一起还原此加密密钥文件，则启用了双重身份验证的用户以及GitLab Runner都将失去对GitLab服务器的访问权限。 ⑤启动 sudo gitlab-ctl reconfigure sudo gitlab-ctl restart # 源码安装时 sudo service gitlab restart ⑥验证 sudo gitlab-rake gitlab:check SANITIZE=true sudo gitlab-ctl status 当版本 >= 13.1,可执行sudo gitlab-rake gitlab:doctor:secrets检查数据数据库中的数据是否解密啦 3、问题 ①压缩备份文件时报错 sudo /opt/gitlab/bin/gitlab-backup create ... Dumping ... ... gzip: stdout: Input/output error Backup failed 解决方案： 检查磁盘空间是否够用 如果存储的路径是NFS挂载点，检查挂载选项的timeout是否始终，默认是600，可设置大一点 四、通过git clone代码导出代码仓库 shell脚本 #!/bin/bash # A script to backup GitLab repositories. GLAB_BACKUP_DIR=${GLAB_BACKUP_DIR-\"gitlab_backup\"} # where to place the backup files GLAB_TOKEN=${GLAB_TOKEN-\"YOUR_TOKEN\"} # the access token of the account GLAB_GITHOST=${GLAB_GITHOST-\"gitlab.com\"} # the GitLab hostname GLAB_PRUNE_OLD=${GLAB_PRUNE_OLD-true} # when `true`, old backups will be deleted GLAB_PRUNE_AFTER_N_DAYS=${GLAB_PRUNE_AFTER_N_DAYS-7} # the min age (in days) of backup files to delete GLAB_SILENT=${GLAB_SILENT-false} # when `true`, only show error messages GLAB_API=${GLAB_API-\"https://gitlab.com/api/v3\"} # base URI for the GitLab API GLAB_GIT_CLONE_CMD=\"git clone --quiet --mirror git@${GLAB_GITHOST}:\" # base command to use to clone GitLab repos TSTAMP=`date \"+%Y%m%d\"` # The function `check` will exit the script if the given command fails. function check { \"$@\" status=$? if [ $status -ne 0 ]; then echo \"ERROR: Encountered error (${status}) while running the following:\" >&2 echo \" $@\" >&2 echo \" (at line ${BASH_LINENO[0]} of file $0.)\" >&2 echo \" Aborting.\" >&2 exit $status fi } # The function `tgz` will create a gzipped tar archive of the specified file ($1) and then remove the original function tgz { check tar zcf $1.tar.gz $1 && check rm -rf $1 } $GLAB_SILENT || (echo \"\" && echo \"=== INITIALIZING ===\" && echo \"\") $GLAB_SILENT || echo \"Using backup directory $GLAB_BACKUP_DIR\" check mkdir -p $GLAB_BACKUP_DIR $GLAB_SILENT || echo -n \"Fetching list of repositories ...\" GLAB_PROJ_API=\"${GLAB_API}/projects?private_token=${GLAB_TOKEN}&per_page=100&simple=true\" echo ${GLAB_PROJ_API} REPOLIST=`check curl --silent ${GLAB_PROJ_API} | check perl -p -e \"s/,/\\n/g\" | check grep \"\\\"path_with_namespace\\\"\" | check awk -F':\"' '{print $2}' | check sed -e 's/\"}//g'` $GLAB_SILENT || echo \"found `echo $REPOLIST | wc -w` repositories.\" $GLAB_SILENT || (echo \"\" && echo \"=== BACKING UP ===\" && echo \"\") for REPO in $REPOLIST; do $GLAB_SILENT || echo \"Backing up ${REPO}\" check ${GLAB_GIT_CLONE_CMD}${REPO}.git ${GLAB_BACKUP_DIR}/${GLAB_ORG}-${REPO}-${TSTAMP}.git && tgz ${GLAB_BACKUP_DIR}/${GLAB_ORG}-${REPO}-${TSTAMP}.git done if $GLAB_PRUNE_OLD; then $GLAB_SILENT || (echo \"\" && echo \"=== PRUNING ===\" && echo \"\") $GLAB_SILENT || echo \"Pruning backup files ${GLAB_PRUNE_AFTER_N_DAYS} days old or older.\" $GLAB_SILENT || echo \"Found `find $GLAB_BACKUP_DIR -name '*.tar.gz' -mtime +$GLAB_PRUNE_AFTER_N_DAYS | wc -l` files to prune.\" find $GLAB_BACKUP_DIR -name '*.tar.gz' -mtime +$GLAB_PRUNE_AFTER_N_DAYS -exec rm -fv {} > /dev/null \\; fi $GLAB_SILENT || (echo \"\" && echo \"=== DONE ===\" && echo \"\") $GLAB_SILENT || (echo \"GitLab backup completed.\" && echo \"\") 参考 https://docs.gitlab.com/omnibus/settings/backups.html https://docs.gitlab.com/ee/api/project_import_export.html https://docs.gitlab.com/ee/user/project/settings/import_export.html https://gist.github.com/devopstaku/7b1b2594ce657957206f3ec5f262eadb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-20 22:42:27 "},"origin/gitlab-upgrade.html":{"url":"origin/gitlab-upgrade.html","title":"版本升级","keywords":"","body":"Gitlab版本升级 一、简介 gitlab的版本规则是：主版本(Major).次版本(Minor).补丁版本(Patch) 主版本会在每年的五月22左右发布 次版本会在每月22号左右发布 补丁版本会不定时发布 版本升级说明 在同一个主要版本下，补丁版本和次要版本之间跳转是安全的。例如 12.7.5 -> 12.10.5 11.3.4 -> 11.11.1 12.0.4 -> 12.0.12 11.11.1 -> 11.11.8 而主版本升级，需要考虑版本是否可向后兼容与数据迁移，原则上先升级到当前主版本的最大次版本，然后在升级到下个主要版本最小次版本，依次往最新的版本进行升级 说明文档： https://docs.gitlab.com/ee/policy/maintenance.html https://docs.gitlab.com/ee/update/README.html#version-specific-upgrading-instructions 二、升级 1、版本升级路径 8.11.Z --> 8.12.0 --> 8.17.7 --> 9.5.10 --> 10.8.7 --> 11.11.8 --> 12.0.12 --> 12.1.17 --> 12.10.14 --> 13.0.14 --> 13.1.11 -- > latest 13.Y.Z 当前版本 目标版本 支持的版本升级路径 备注 12.9.2 13.4.3 12.9.2 -> 12.10.14 -> 13.0.14 -> 13.4.3 需要两个中间版本：最终的12.10版本以及13.0 11.5.0 13.2.10 11.5.0 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.10.14 -> 13.0.14 -> 13.2.10 需要五个中间版本：最终的11.11、12.0、12.1和12.10版本以及13.0 11.3.4 12.10.14 11.3.4 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.10.14 需要三个中间版本：最终的11.11和12.0版本，再加上12.1 10.4.5 12.9.5 10.4.5 -> 10.8.7 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.9.5 需要四个中间版本：10.8、11.11、12.0和12.1，然后是12.9.5 9.2.6 12.2.5 9.2.6 -> 9.5.10 -> 10.8.7 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.2.5 需要五个中间版本：9.5、10.8、11.11、12.0、12.1，然后是12.2。 8.13.4 11.3.4 8.13.4 -> 8.17.7 -> 9.5.10 -> 10.8.7 -> 11.3.4 8.17.7是版本8中的最新版本，9.5.10是版本9中的最新版本，10.8.7是版本10中的最新版本。 已实践的版本升级路线： 11.6.1 ----> 11.11.0-ce.0(关键点) ----> 12.0.1-ce.0(关键点) ----> 12.8.0-ce.0 ----> 12.10.0(关键点) ----> 13.0.0-ce.0(关键点) ----> latest(13.8.3) 2、升级步骤 升级的步骤取决于Gitlab的部署方式 二进制包安装 参考文档：https://docs.gitlab.com/omnibus/update/ 源码编译安装 参考文档：https://docs.gitlab.com/ee/update/upgrading_from_source.html Docker方式安装 参考文档：https://docs.gitlab.com/omnibus/docker/README.html#update Helm方式安装 参考文档：https://docs.gitlab.com/charts/installation/upgrade.html#steps Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-18 10:09:25 "},"origin/gitlab-package-registry.html":{"url":"origin/gitlab-package-registry.html","title":"Gitlab的制品仓库","keywords":"","body":"Gitlab的制品仓库 一、简介 二、配置 Personal Access Token CI Job Token Deploy Token 三、通用文件仓库 Gitlab Docs：https://docs.gitlab.com/ee/user/packages/generic_packages/ 开起 手动上传 curl --header \"PRIVATE-TOKEN: \" \\ --upload-file 要上传的文件 \\ \"http://gitlab地址/api/v4/projects/仓库ID/packages/generic/包名/版本/文件名\" Pipeline中上传 image: curlimages/curl:latest stages: - upload - download upload: stage: upload script: - 'curl --header \"JOB-TOKEN: $CI_JOB_TOKEN\" --upload-file path/to/file.txt \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/my_package/0.0.1/file.txt\"' download: stage: download script: - 'wget --header=\"JOB-TOKEN: $CI_JOB_TOKEN\" ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/my_package/0.0.1/file.txt' 制品命令 包名 只允许[a-zA-Z0-9][._-] 版本名 格式：X.Y.Z,包含[0-9][.]，符合/\\A\\d+\\.\\d+\\.\\d+\\z/正则表达式 文件名 只允许[a-zA-Z0-9][._-] 四、Docker Image仓库 1、配置 Gitlab Docs ：https://docs.gitlab.com/ee/administration/packages/container_registry.html 五、Maven Package仓库 Gitlab Docs：https://docs.gitlab.com/ee/user/packages/maven_repository/ settings.xml gitlab-maven Private-Token / Deploy-Token / Job-Token YOUR_PERSONAL_ACCESS_TOKEN / YOUR_DEPLOY_TOKEN / ${env.CI_JOB_TOKEN} pom.xml gitlab-maven https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven gitlab-maven https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven gitlab-maven https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven 在gitlab的pipeline中pom.xml文件 gitlab-maven ${env.CI_SERVER_URL}/api/v4/projects/${env.CI_PROJECT_ID}/packages/maven gitlab-maven ${env.CI_SERVER_URL}/api/v4/projects/${env.CI_PROJECT_ID}/packages/maven gitlab-maven ${env.CI_SERVER_URL}/api/v4/projects/${env.CI_PROJECT_ID}/packages/maven 发布命令 mvn deploy # 如果成功后会显示一下信息 Uploading to gitlab-maven: https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven/com/mycompany/mydepartment/my-project/1.0-SNAPSHOT/my-project-1.0-20200128.120857-1.jar Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-28 16:08:31 "},"origin/gitlab-runner.html":{"url":"origin/gitlab-runner.html","title":"Gitlab Runner","keywords":"","body":"Gitlab Runner 一、简介 Gitlab Runner是Gitlab Pipeline中各阶段Job任务中脚本执行的承载者。 二、原理 1、Runner说明 所有runner注册到gitlab时都要选择一个执行器，而执行器决定了pipeline中job任务的运行环境 支持的runner执行器： SSH Shell Parallels VirtualBox Docker Docker Machine (auto-scaling) Kubernetes Custom runner类型 Shared runners 共享runner：所有仓库可以使用 Group runners 组runner：所有仓库可以使用 Specific runners 特殊runner：个别指定的仓库可以使用 2、Runner执行器工作流 执行器为kubernetes的runner的工作流 三、注册Runner 1、在Gitlab中获取Runner的注册信息 2、Runner注册流程 Runtime platform arch=amd64 os=linux pid=48 revision=2ebc4dc4 version=13.9.0 Running in system-mode. Enter the GitLab instance URL (for example, https://gitlab.com/): # 输入gitlab实例的地址 Enter the registration token: # 输入向gitlab实例注册的Token Enter a description for the runner: [759510becaba]: # 输入对当前gitlab runner的描述信息 Enter tags for the runner (comma-separated): # 输入当前gitlab runner的标签 Registering runner... succeeded runner=Db2NbZ2M Enter an executor: docker-ssh+machine, kubernetes, custom, docker, parallels, virtualbox, docker+machine, docker-ssh, shell, ssh: # 输入当前gitlab runner的执行器类型 docker Enter the default Docker image (for example, ruby:2.6): # 输入当前gitlab runner的执行器默认使用的docker镜像 alpine:latest Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! 四、Runner部署及配置 1、Docker 部署runner docker run -d \\ --name gitlab-runner \\ --restart always \\ -v /srv/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest 注册runner docker exec -it gitlab-runner gitlab-runner register \\ --non-interactive \\ --url # gitlab地址 \\ --registration-token # gitlab的runner注册Token \\ --executor # 执行器 \\ --description # \"runner的详细描述\" \\ --docker-image # 默认镜像 \\ --docker-privileged \\ --docker-pull-policy # \"镜像拉取策略\" \\ --tag-list # \"runner 标签\" 2、二进制 部署runner # Linux x86-64 / Linux x86 / Linux arm / Linux arm64 / Linux s390x arch=[ amd64,386,arm,arm64,s390x ] curl -L -o /usr/local/bin/gitlab-runner \"https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-$arch\" && \\ chmod +x /usr/local/bin/gitlab-runner && \\ useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash && \\ gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner && \\ gitlab-runner start 注册 runner gitlab-runner register \\ --non-interactive \\ --url # gitlab地址 \\ --registration-token # gitlab的runner注册Token \\ --executor # 执行器 \\ --description # \"runner的详细描述\" \\ --docker-image # 默认镜像 \\ --docker-privileged \\ --docker-pull-policy # \"镜像拉取策略\" \\ --tag-list # \"runner 标签\" 3、Kubernetes helm repo add gitlab https://charts.gitlab.io helm update 编写helm charts values.yaml配置文件 helm install gitab-runner --namespace gitlab -f values.yaml gitlab/gitlab-runner 五、Runner与Pipeline的流程图 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-04 23:44:11 "},"origin/gitlab-pipeline.html":{"url":"origin/gitlab-pipeline.html","title":"Gitlab Pipeline","keywords":"","body":"Gitlab CI/CD Pipeline 一、简介 Stages 所有 Stages 会按照顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始。 只有当所有 Stages 完成后，该构建任务 (Pipeline) 才会成功 如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务 (Pipeline) 失败 有.pre、.post和default Jobs：构建任务，表示某个 Stage 里面执行的工作 相同 Stage 中的 Jobs 会并行执行 相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功 如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务 (Pipeline) 失败 二、Stage定义及语法 三、Job定义及语法 变量variable 脚本script image Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-04 23:40:11 "},"origin/gitlab-api.html":{"url":"origin/gitlab-api.html","title":"Gitlab Restful API","keywords":"","body":"Gitlab Restful API 一、简介 GItlab拥有Restful的API接口来操作其中的资源对象。 Gitlab Restful API Docs：https://docs.gitlab.com/ee/api/README.html#status-codes 1、接口请求方式 Endpoint：http://gitlab地址/api/api版本/资源对象/操作URI?参数1&参数2 目前接口版本为V4，V3在11.0版本后已经移除 接口需要认证，支持的验证方式： OAuth2 tokens curl --header \"Authorization: Bearer OAUTH-TOKEN\" \"https://gitlab.example.com/api/v4/projects\" # 或者 curl \"https://gitlab.example.com/api/v4/projects?access_token=OAUTH-TOKEN\" Personal access tokens curl --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects\" # 或者 curl \"https://gitlab.example.com/api/v4/projects?private_token=\" # 或者 curl --header \"Authorization: Bearer \" \"https://gitlab.example.com/api/v4/projects\" Project access tokens curl --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects\" # 或者 curl \"https://gitlab.example.com/api/v4/projects?private_token=\" # 或者 curl --header \"Authorization: Bearer \" \"https://gitlab.example.com/api/v4/projects\" Session cookie GitLab CI/CD job token 接口返回的是JSON数据格式（在Linux下可以使用jq进行处理） curl --header \"PRIVATE-TOKEN: \" \\ \"https://gitlab.example.com/api/v4/projects\" | jq -r '.[].id' 如果某些资源对象的URL操作中包含特殊字符，需要做URL编码处理 例如：资源对象路径包含的/可以使用%2F进行处理 GET /api/v4/projects/1/repository/files/src%2FREADME.md?ref=master GET /api/v4/projects/1/branches/my%2Fbranch/commits GET /api/v4/projects/1/repository/tags/my%2Ftag 请求资源对象参数中的数组和hash 数组 curl --request POST --header \"PRIVATE-TOKEN: \" \\ -d \"import_sources[]=github\" \\ -d \"import_sources[]=bitbucket\" \\ \"https://gitlab.example.com/api/v4/some_endpoint\" Hash curl --request POST --header \"PRIVATE-TOKEN: \" \\ --form \"namespace=email\" \\ --form \"path=impapi\" \\ --form \"file=@/path/to/somefile.txt\" --form \"override_params[visibility]=private\" \\ --form \"override_params[some_other_param]=some_value\" \\ \"https://gitlab.example.com/api/v4/projects/import\" Hash数组 curl --globoff --request POST --header \"PRIVATE-TOKEN: \" \\ \"https://gitlab.example.com/api/v4/projects/169/pipeline?ref=master&variables[][key]=VAR1&variables[][value]=hello&variables[][key]=VAR2&variables[][value]=world\" curl --request POST --header \"PRIVATE-TOKEN: \" \\ --header \"Content-Type: application/json\" \\ --data '{ \"ref\": \"master\", \"variables\": [ {\"key\": \"VAR1\", \"value\": \"hello\"}, {\"key\": \"VAR2\", \"value\": \"world\"} ] }' \\ \"https://gitlab.example.com/api/v4/projects/169/pipeline\" 请求分页 对于一些有大量返回数据的资源请求，gitlab为了保持性能，是有默认分页操作的。支持的分页方式 基于Offset的分页(所有接口都支持) 参数 | Parameter | Description | | :--------- | :------------------------------------------ | | page | 页数，默认为1 | | per_page | 一页显示的对象个数 (默认: 20,最大: 100) | curl --request PUT --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/namespaces?per_page=50\" 分页返回的Header curl --head --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects/9/issues/8/notes?per_page=3&page=2\" HTTP/2 200 OK cache-control: no-cache content-length: 1103 content-type: application/json date: Mon, 18 Jan 2016 09:43:18 GMT link: ; rel=\"prev\", ; rel=\"next\", ; rel=\"first\", ; rel=\"last\" status: 200 OK vary: Origin x-next-page: 3 x-page: 2 x-per-page: 3 x-prev-page: 1 x-request-id: 732ad4ee-9870-4866-a199-a9db0cde3c86 x-runtime: 0.108688 x-total: 8 x-total-pages: 3 | Header | Description | | :-------------- | :------------- | | x-next-page | 下一页的索引 | | x-page | 当前页的索引 | | x-per-page | 每页的对象个数 | | X-prev-page | 上一页的索引 | | x-total | 所有的对象格式 | | x-total-pages | 所有页的个数 | 基于Keyset的分页(仅支持部分接口 ) 参数 | Parameter | Description | | :----------- | :------------------------------------------ | | pagination | keyset (开启分页使用keyset). | | per_page | 一页显示的对象个数 (默认: 20,最大: 100) | 分页返回的Header curl --request GET --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects?pagination=keyset&per_page=50&order_by=id&sort=asc\" HTTP/1.1 200 OK ... Links: ; rel=\"next\" Link: ; rel=\"next\" Status: 200 OK ... 接口返回状态码 | 状态返回码 | 描述 | | :----------------------- | :----------------------------------------------------------- | | 200 OK | GET,PUT或者DELETE请求已处理完成，已返回JSON结构的数据 | | 204 No Content | 接口已处理完成了请求，但是没有什么数据可返回的 | | 201 Created | POST 请求已处理完成，已返回JSON结构的数据 | | 304 Not Modified | 自上次请求以来，该资源尚未修改。 | | 400 Bad Request | 请求参数不完整 | | 401 Unauthorized | 用户未认证，请求需要认证Token | | 403 Forbidden | 请求中认证Token没有相应的权限对请求中的对象进行操作 | | 404 Not Found | 请求操作的资源没发现。可能是参数不对，没有匹配到准确的对象 | | 405 Method Not Allowed | 请求中的资源对象不支持该请求方法 | | 409 Conflict | 请求的资源操作跟已有的现有对象有冲突。 | | 412 | 请求的资源操作被拒绝啦。如果尝试删除资源时提供了If-Unmodified-Since标头，则可能会发生这种情况。 | | 422 Unprocessable | The entity couldn’t be processed. | | 429 Too Many Requests | 请求被限制了！ | | 500 Server Error | 在服务端处理请求过程中出现了错误 | 二、Project API DQL 1、列出所有仓库 Endpoint GET /projects 文档 https://docs.gitlab.com/ee/api/projects.html#list-all-projects 参数 参数 类型 是否必须 描述 archived boolean No 是否列出archived状态的 id_after integer No Limit results to projects with IDs greater than the specified ID. id_before integer No Limit results to projects with IDs less than the specified ID. last_activity_after datetime No Limit results to projects with last_activity after specified time. Format: ISO 8601 YYYY-MM-DDTHH:MM:SSZ last_activity_before datetime No Limit results to projects with last_activity before specified time. Format: ISO 8601 YYYY-MM-DDTHH:MM:SSZ membership boolean No Limit by projects that the current user is a member of. min_access_level integer No Limit by current user minimal access level. order_by string No Return projects ordered by id, name, path, created_at, updated_at, or last_activity_at fields. repository_size, storage_size, packages_size or wiki_size fields are only allowed for admins. Default is created_at. owned boolean No Limit by projects explicitly owned by the current user. repository_checksum_failed boolean No Limit projects where the repository checksum calculation has failed (Introduced in GitLab Premium 11.2). repository_storage string No Limit results to projects stored on repository_storage. (admins only) search_namespaces boolean No Include ancestor namespaces when matching search criteria. Default is false. search string No Return list of projects matching the search criteria. simple boolean No Return only limited fields for each project. This is a no-op without authentication as then only simple fields are returned. sort string No Return projects sorted in asc or desc order. Default is desc. starred boolean No Limit by projects starred by the current user. statistics boolean No Include project statistics. visibility string No Limit by visibility public, internal, or private. wiki_checksum_failed boolean No Limit projects where the wiki checksum calculation has failed (Introduced in GitLab Premium 11.2). with_custom_attributes boolean No Include custom attributes in response. (admins only) with_issues_enabled boolean No Limit by enabled issues feature. with_merge_requests_enabled boolean No Limit by enabled merge requests feature. with_programming_language string No Limit by projects which use the given programming language. 示例 GET /projects?custom_attributes[key]=value&custom_attributes[other_key]=other_value Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-22 21:49:06 "},"origin/nexus-简介.html":{"url":"origin/nexus-简介.html","title":"Nexus","keywords":"","body":"一、Nexus简介 Nexus是Sonatype公司出的一款目前最为流程的构件仓库管理软件，主要用于局域网内部的构件管理，代理访问外部仓库等。例如对于公司私有的Java制品Jar包，可上传至Nexus的Maven类型仓库中进行集中管理；代理访问阿里云Maven仓库，缓存加速获取互联网上的Java制品。Nexus使用Lucene提供了强大的构件搜索功能，拥有丰富的RestFul API接口用于管理控制，支持WebDAV和LDAP安全身份认证，基于RBAC的权限访问控制等功能。市面上同类产品有Apache的Archiva和JFrog的Artifatory。 Nexus分为免费开源版OSS和收费商业版Professional 二、仓库类型 Proxy 类型仓库主要用于代理缓存访问外网上其他公开的仓库，将每次从代理仓库拉取的制品缓存到nexus文件系统中，下次再拉取相同版本制品时就不需再次从外网拉取，起到代理访问缓存的功能 Hosted 类型的仓库主要用于存放各个项目组产出的、用于共享、不能放到公网上、私有的制品。有两种版本策略，一种是Snapshots版本策略类型的，对于相同版本制品的上传，nexus会自动追加时间戳加以区分；一种是Release版本策略类型的，对于相同的制品，要明确版本，不能存放相同版本。可以理解为snapshots仓库存放一些内容变更频繁的制品，这样不管上传还是使用时不用频繁变更版本号就能拉取到最新版本。而release仓库存放一些内容稳定变更少的制品，使用时指定好版本就行，无需经常变动 Group 类型仓库主要用于组合其他仓库，统一对外使用方式。可设置组仓库组合其他仓库的顺序。例如组合顺序为先拉取maven格式aliyun代理仓库中的制品，如果其中没有想要的制品，再去拉取maven格式Central代理仓库中的制品。如果还没有，就去maven格式hosted类型仓库中拉取，直到遍历完所有的组合仓库。同时，拉取使用时不需要配置那么多的仓库地址，只需要配置group仓库地址就行 三、官方高可用方案 官方收费版的高可用方案 四、Kubernetes部署 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: replicas: 1 selector: matchLabels: app: nexus3 strategy: type: Recreate template: metadata: labels: app: nexus3 spec: imagePullSecrets: - name: harbor-secrets initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /nexus-data'] volumeMounts: - name: nexus3-data mountPath: /nexus-data containers: - env: - name: CONTEXT_PATH value: / - name: TZ value: 'Asia/Shanghai' image: docker.io/sonatype/nexus3:3.15.2 imagePullPolicy: IfNotPresent readinessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 name: nexus3 ports: - containerPort: 8081 protocol: TCP resources: limits: memory: 4096Mi requests: memory: 2048Mi terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /nexus-data name: nexus3-data dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: nexus3-data persistentVolumeClaim: claimName: nexus3-pv --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: nexus3 name: nexus3-pv namespace: nexus3 spec: accessModes: - ReadWriteMany resources: requests: storage: 1000Gi --- apiVersion: v1 kind: Service metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: ports: - name: 8081-tcp port: 8081 protocol: TCP targetPort: 8081 selector: app: nexus3 sessionAffinity: None type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nexus3 namespace: nexus3 spec: rules: - host: nexus.curiouser.com http: paths: - path: / backend: serviceName: nexus3 servicePort: 8081 五、常见仓库配置 YUM格式制品 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库 yum-ansible：https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ yum-ustc： http://mirrors.ustc.edu.cn/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库 yum-hosted Maven格式制品 Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库 maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库 maven-snapshots maven-releases NPM格式制品 Group类型仓库 npm npm-taobao npm-cnpm npm-hosted Proxy类型仓库 npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库 npm-hosted（上传权限需修改realms) Docker格式制品 Group类型仓库 docker（设置http-port:8082） Proxy类型仓库 docker-io： https://registry-1.docker.io Hosted类型仓库 docker-hosted（设置http-port:8083） Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-使用OrientDB Console在DB层面修改配置.html":{"url":"origin/nexus-使用OrientDB Console在DB层面修改配置.html","title":"使用OrientDB Console在DB层面修改配置","keywords":"","body":"Nexus使用OrientDB Console在DB层面修改配置 一、Context 在配置Nexus对接LDAP过程中，原先的用户（admin和原来创建的）也是可以登录的，为了测试只让LDAP用户登录，将安全域中的Local Authorizing Realm和Local Authenticating Realm给去掉了，导致admin用户登陆不上，LDAP上的用户能登陆，但没有管理Nexus的权限。换句话说，这个Nexus成了僵尸。如果Nexus的admin密码忘了,情形也类似。都需要修改Nexus数据库中的用户数据才能修改相关配置（Nexus是将用户、配置等相关信息存放在OrientDB 数据库中，而不是在配置文件中，所以改密码或者在没有权限的情况下改配置都是要操作OrientDB 中的数据） 相关信息 Nexus是在Openshift(Kubernetes也一样)中容器化部署的 Nexus版本：3.15.2 Nexus的数据目录是单独使用NFS类型的PV挂载的 openshift上Nexus容器的执行用户没有权限操作除持久化目录之外的目录。（以root用户起的Nexus进程可直接使用Nexus容器中的”/opt/nexus/lib/support/nexus-orient-console.jar“连接OrientDB ） 问题解决思路 将Nexus的PV数据目录挂载到物理节点一个临时目录下 使用Nexus相同版本安装包中的OrientDB Console连接到临时数据目录中的DB，然后修改相关信息 Note: 如果容器中执行Nexus进程的用户和物理节点上的用户不一样导致权限不够的话，可暂时将临时数据目录中的所有文件权限设置为777，修改完再改回来 再将Nexus的PV数据目录挂载到openshift上Nexus容器中，重启Nexus 二、Preflight Nexus的PV数据目录已挂载到/roor/test 下载相同版本的Nexus到/opt/目录下 /roor/test下的所有文件及文件夹权限临时修改为777 三、操作 1. 使用官方的OrientDB Console连接到数据目录中的DB java -jar /opt/nexus-3.15.2-01/lib/support/nexus-orient-console.jar 此时会进入OrientDB 控制台 # OrientDB console v.2.2.36 (build d3beb772c02098ceaea89779a7afd4b7305d3788, branch 2.2.x) https://www.orientdb.com # Type 'help' to display all the supported commands. orientdb> #输入exit退出 2. 连接临时数据目录中的DB orientdb> connect plocal:/root/test/db/security admin admin 此时会连到一个security的Database 3. 重置Realm 如果从活动列表中删除了缺省安全域，则缺省管理员用户(即使用户名密码正确)也无法进行身份验证 orientdb> delete from realm Note: 重置后。默认的安全域将被激活，任何自定义的安全域都将被删除。后续再UI界面进行添加 4.(可选) 重置admin用户密码为默认值\"admin123\" orientdb> update user SET password=\"$shiro1$SHA-512$1024$NE+wqQq/TmjZMvfI7ENh/g==$V4yPw8T64UQ6GfJfxYq2hLsVrBY8D1v+bktfOxGdt4b/9BthpWPNUy/CBk6V9iA0nHpzYzJFWO8v/tZFtES8CA==\" UPSERT WHERE id=\"admin\" 5.(可选) 重置admin为管理员 如果admin用户不在是“nx-admin”管理员角色 orientdb> select * from user_role_mapping where userID = \"admin\" orientdb> update user_role_mapping set roles = [\"nx-admin\"] where userID = \"admin\" orientdb> select status from user where id = \"admin\" orientdb> update user set status=\"active\" upsert where id=\"admin\" 6. 退出OrientDB 控制台，修改回临时数据目录所有文件的原始权限，重新将数据目录挂载到容器上，然后重启Nexus Bazinga，admin用户能正常，又重新夺回Nexus的管理权限，所有的仓库配置和数据没有丢失，重新将LDAP的安全域加回去，一切恢复原样。 附录：Nexus使用的OrientDB 1. What is the OrientDB Console Nexus 3 uses several OrientDB databases. In very specific circumstances, these databases can be manipulated as advised by Sonatype support. This article describes how to open a special command-line interface for connecting to and working with the databases used by Nexus. This interface is known as the OrientDB Console. Caution: Using the console incorrectly can cause irreparable harm to the databases. 2. Launching the OrientDB Console on Nexus 3.2.1 and Newer Nexus 3.2.1+ includes a single jar executable which can launch the OrientDB console. As the operating system user account that typically owns the Nexus Repository Manager process, start a terminal session on the host where Nexus is installed. Change directories to your application directory. Launch the console using the same version of Java executable that Nexus is using : Unix java -jar ./lib/support/nexus-orient-console.jar Windows java -jar lib\\support\\nexus-orient-console.jar Mac .install4j/jre.bundle/Contents/Home/jre/bin/java -jar ./lib/support/nexus-orient-console.jar You should be presented with a command-line interface such as this, ready to accept your commands: # OrientDB console v.2.2.16 www.orientdb.com #Type 'help' to display all the supported commands. orientdb> # When you are done your commands, type exit to quit the console. The nexus-orient-console.jar sets up the correct classpath to successfully launch the console. Launching the jar from another location is not supported 参考连接 https://support.sonatype.com/hc/en-us/articles/115002930827-Accessing-the-OrientDB-Console https://support.sonatype.com/hc/en-us/articles/213467158-How-to-reset-a-forgotten-admin-password-in-Nexus-3-x Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-设置SMTP邮件服务.html":{"url":"origin/nexus-设置SMTP邮件服务.html","title":"设置SMTP邮件服务","keywords":"","body":" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-maven仓库的配置与使用.html":{"url":"origin/nexus-maven仓库的配置与使用.html","title":"Maven","keywords":"","body":"Maven仓库的配置与使用 一、Overview 有了Maven仓库之后，当Maven需要下载构件时，直接请求Nexus，Maven仓库上存在则下载到本地仓库；Maven仓库上不存在的话，Nexus请求外部的远程仓库，将构件下载到Maven仓库，再提供给本地仓库下载。 Maven格式制品仓库配置 Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库 maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库 maven-snapshots maven-releases maven格式的仓库有两种典型的使用场景，一个是顺序拉取group组仓库中组合仓库里的制品，一个是上传制品到hosted类型的仓库中，例如maven-snapshots仓库。而maven配置nexus中的仓库有三个地方，配置最终生效的顺序为全局配置文件--->用户配置文件---->POM文件： maven的全局配置文件settings.xml中,该配置文件在maven安装目录conf文件夹下 maven的用户配置文件settings.xml中,该配置文件在用户目录.m2文件夹下 项目的POM文件 二、代理仓库的使用 在Maven用户配置文件setting.xml中添加maven格式group仓库的地址 .....上文省略...... curiouser-maven microservices microservices用户的密码 curiouser-maven * The Maven repository of curiouser http://nexus-ip地址:8081/repository/maven/ .....下文省略...... 三、发布制品到Maven的Hosted仓库 1、mvn deploy 在Maven用户配置文件setting.xml中添加snapshot、release仓库的id .....上文省略...... maven-releases microservices microservices用户的密码 maven-snapshots microservices microservices用户的密码 .....下文省略...... 在项目POM.xml文件中添加Snapshots仓库或Release仓库的地址 .....上文省略...... maven-releases User Project Release http://nexus-ip地址:8081/repository/maven-releases/ maven-snapshots User Project SNAPSHOTS http://nexus-ip地址:8081/repository/maven-snapshots/ .....下文省略...... 然后mvn deploy 2、Curl手动上传 curl -v -u microservices:microservices用户的密码 --upload-file springboot2-0.0.0.jar http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.jar curl -v -u microservices:microservices用户的密码 --upload-file pom.xml http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.pom 3、mvn命令手动上传 前提是Maven用户配置文件setting.xml中已经添加了snapshot、release仓库的id Linux mvn deploy:deploy-file \\ #要上传的Jar包路径 \\ -Dfile=/root/websocket-server-9.4.11.v20180605.jar \\ #Jar包Maven坐标的GroupID \\ -DgroupId=org.eclipse.jetty.websocket \\ #Jar包Maven坐标的ArtifactID \\ -DartifactId=websocket-server \\ #Jar包Maven坐标的Version \\ -Dversion=9.4 \\ #要上传到仓库的制品类型，该值还可以是pom -Dpackaging=jar \\ #maven私服hosted类型仓库的地址 -Durl=http://nexus-ip地址:8081/repository/maven-releases/ \\ #maven私服hosted类型仓库的repositoryid -DrepositoryId=maven-releases Windows mvn deploy:deploy-file ^ -Dfile=D:\\websocket-server-9.4.11.v20180605.jar ^ -DgroupId=org.eclipse.jetty.websocket ^ -DartifactId=websocket-server ^ -Dversion=9.4 ^ -Dpackaging=jar ^ -Durl=http://nexus-ip地址:8081/repository/maven-releases/ ^ -DrepositoryId=maven-releases 4、UI界面上传 5、使用Postman上传 注意事项 Jar包上传只能上传hosted类型的仓库中，Proxy和Group类型的无法上传。同时，注意有些仓库设置禁止了上传 hosted类型的Snapshot仓库默认设置的上传版本控制为只能上传以“-snapshot”结尾版本的制品。所以版本为非“-snapshot”结尾的Jar包无法上传到snapshot仓库中 如果Nexus禁止匿名用户访问时，匿名用户是被禁止拉取maven仓库的Jar包。所以可以创建一个对maven仓库类型有读取权限的用户，在settings.xml文件进行配置。 附录：如何在maven的配置文件settings.xml中使用加密过的用户密码 在Maven的settings.xml中，往往要配置访问远程库所在的服务器的username/password。但是明文的密码总是显得那么扎眼，必欲除之而后快。Apache Maven项目提供了便捷的密码加密机制，该机制的最近更新时间为2018-03-06。该机制目前只支持在命令行下的操作，如生成密码的密文。此外，用户还需要在${user.home}/.m2目录下配置settings-security.xml文件，其中包含： 用以加密其他密码的master password（此处也是密文） 或指向另一个保密文件的完整路径 在该加密机制中有两个概念，一个是master password，即用以加密其他密码的密码，另一个就是实际使用的服务器访问密码password。master password的密文配置在settings-security.xml文件中，而服务器访问密码password的密文就可以大大方方地配置在settings.xml中。具体用法如下： 生成Master password的密文 mvn --encrypt-master-password 根据提示输入Master password: 就可以生成密文{iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} 配置${user.home}/.m2/settings-security.xml文件(如果没有就手动创建) {iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} # 如果settings-security.xml文件被保存到U盘，则配置${user.home}/.m2/settings-security.xml文件如下： /my_u_volume/my_path/settings-security.xml 加密访问服务器的密码 mvn --encrypt-password 根据提示输入Password: ​ 就可以生成密文{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=} 配置settings.xml文件 my.server myfoo add_any_comment or \\{\\{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=\\}\\} add_any_comment 参考链接 https://blog.csdn.net/taiyangdao/article/details/79500507 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-npm仓库的配置与使用.html":{"url":"origin/nexus-npm仓库的配置与使用.html","title":"NPM","keywords":"","body":"NPM仓库的配置与使用 一、npm仓库的配置信息 Group类型仓库 npm-taobao npm-cnpm npm-hosted Proxy类型仓库 npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库 npm-hosted 二、使用NPM仓库 npm config set registry http://nexus-ip-address:8081/repository/npm/ 三、发布制品到NPM的Hosted仓库 当使用 npm login 或npm adduser 等NPM客户端使用Token进行登录认证到Nexus的NPM仓库时，Nexus默认仅支持Local Authenticating Realm ,认证不了NPM相关token。所以配置Nexus添加npm的认证域。 echo \"hello\" >> test npm init # package name: (test) sadsada # version: (1.0.0) # description: # entry point: (index.js) test # test command: # git repository: # keywords: # author: # license: (ISC) # About to write to /root/test/package.json: # { # \"name\": \"sadsada\", # \"version\": \"1.0.0\", # \"description\": \"\", # \"main\": \"f\", # \"scripts\": { # \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" # }, # \"author\": \"\", # \"license\": \"ISC\" # } npm login -registry http://nexus-ip-address:8081/repository/npm-hosted/ # Username: admin #Password: ***** # Email: (this IS public) asdad@sada.com npm publish -registry http://nexus-ip-address:8081/repository/npm-hosted/ 四、使用nrm工具切换npm仓库 Github地址：https://github.com/Pana/nrm 帮助快速切换npm仓库源。默认已经配置了npm、yarn、taobao、cnpm、nj、npmMirror、edunpm等常见的仓库源。 1. 安装 npm install nrm -g 2. 命令详解 $ nrm -h Usage: nrm [options] [command] Options: -V, --version output the version number -h, --help output usage information Commands: ls List all the registries current Show current registry name use Change registry to registry add [home] Add one custom registry set-auth [options] [value] Set authorize information for a custom registry with a base64 encoded string or username and pasword set-email Set email for a custom registry set-hosted-repo Set hosted npm repository for a custom registry to publish packages del Delete one custom registry home [browser] Open the homepage of registry with optional browser publish [options] [|] Publish package to current registry if current registry is a custom registry. if you're not using custom registry, this command will run npm publish directly test [registry] Show response time for specific or all registries help Print this help 3. 常用命令 查看默认支持的npm 仓库 $ nrm ls * npm -------- https://registry.npmjs.org/ yarn ------- https://registry.yarnpkg.com/ cnpm ------- http://r.cnpmjs.org/ taobao ----- https://registry.npm.taobao.org/ nj --------- https://registry.nodejitsu.com/ npmMirror -- https://skimdb.npmjs.com/registry/ edunpm ----- http://registry.enpmjs.org/ # \"*\"编注的仓库代表当前使用的仓库 添加私有的npm仓库 nrm add okd-nexus http://nexus-ip-address:8081/repository/npm-hosted/ 切换npm仓库 nrm use 仓库名 删除仓库 nrm del 仓库名 测试仓库速度 nrm test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-yum仓库的配置与使用.html":{"url":"origin/nexus-yum仓库的配置与使用.html","title":"YUM","keywords":"","body":"YUM仓库的配置与使用 一、Overviews 之前搭建内网YUM源仓库都是使用HTTP和createrepo做的，每次还要手动去同步外网镜像源最新的RPM包，繁琐耗时。最近发现新版的Nexus已经有可以做YUM源的功能，能像Maven仓库那样，没有相关资源的时候去外网拉，有的话就不去外网啦。即节省存储，又加快速度，还能自动更新RPM版本。 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库 yum-ustc：http://mirrors.ustc.edu.cn/ yum-tuna：https://mirrors.tuna.tsinghua.edu.cn/ yum-163：http://mirrors.163.com/ yum-ansible：https://releases.ansible.com/ansible/rpm/releasepel-7-x86_64/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库 yum-hosted（\"Repodata Depth\"设置为“1”） 二、手动上传RPM包到Hosted仓库 上传RPM包到Hosted仓库，相关的RPM元信息文件夹repodata会自动生成，不再需要createrepo生成啦（如果没有自动生成，点击仓库的Rebuild Index，然后稍等即可） $ ls -l | grep ^[^d] | awk '{print $9}' mysql-community-client-5.7.19-1.el7.x86_64.rpm mysql-community-common-5.7.19-1.el7.x86_64.rpm mysql-community-devel-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-compat-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-devel-5.7.19-1.el7.x86_64.rpm mysql-community-libs-5.7.19-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.19-1.el7.x86_64.rpm mysql-community-minimal-debuginfo-5.7.19-1.el7.x86_64.rpm mysql-community-server-5.7.19-1.el7.x86_64.rpm mysql-community-server-minimal-5.7.19-1.el7.x86_64.rpm mysql-community-test-5.7.19-1.el7.x86_64.rpm #一次只能上传一个RPM文件， #为了一个Host仓库存放不同软件的不同版本，上传时需要指定上传到Hosted仓库的哪个目录下，以文件夹名来区分当前上传的RPM包是哪个版本的。 $ for i in `ls *rpm` ;do curl -v --user 'admin:admin123' --upload-file $i http://nexus-ip地址:8081/repository/yum-nexus-MySQL/mysql5.7.19/;done 三、YUM仓库的使用 [centos] name=Nexus Yum baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/os/$basearch/ enabled=1 gpgcheck=0 [centos-extras] name=Nexus Yum Extras baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/extras/$basearch/ enabled=1 gpgcheck=0 [epel] name=Nexus Epel baseurl=http://nexus-ip地址:8081/repository/yum-nexus/epel/$releasever/$basearch/ enabled=1 gpgcheck=0 [openshift] name=Nexus openshift 3.11 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/paas/$basearch/openshift-origin311/ enabled=1 gpgcheck=0 [mysql5.7] name=Nexus MySQL 5.7.19 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/mysql5.7.19 enabled=1 gpgcheck=0 注意 不要代理多个外网的YUM源。例如同时代理缓存网易的163镜像源和清华大学的镜像源，这些外网镜像源中的RPM都大差无几，配置多个的话，使用group去代理拉RPM的时候，会优先选择group中靠前的外网代理镜像源。有时（偶尔其中一个外网镜像源不能用）会出现相同的RPM存在于每一个proxy仓库中，浪费存储。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-composer.html":{"url":"origin/nexus-composer.html","title":"Composer","keywords":"","body":"Nexus支持Composer仓库 一、简介 Nexus 3 默认暂不支持PHP Composer仓库，但是可以通过第三方插件支持 插件Github地址：https://github.com/sonatype-nexus-community/nexus-repository-composer Group类型仓库 composer-public composer-packagist composer-hosted Proxy类型仓库 composer-packagist：https://packagist.org/ Hosted类型仓库 composer-hosted 二、编译、安装插件 git clone https://github.com/sonatype-nexus-community/nexus-repository-composer.git mvn clean package # 编译成功后，生成的Jar包在nexus-repository-composer/target下 将插件jar包放到/system/org/sonatype/nexus/plugins/nexus-repository-composer/0.0.2/nexus-repository-composer-0.0.2.jar 编辑/system/org/sonatype/nexus/assemblies/nexus-core-feature/3.x.y/nexus-core-feature-3.x.y-features.xml，添加 nexus-repository-rubygems + nexus-repository-composer nexus-repository-gitlfs + + org.sonatype.nexus.plugins:nexus-repository-composer + mvn:org.sonatype.nexus.plugins/nexus-repository-composer/0.0.2 + 重启Nexus，看是否支持创建Composer仓库了 三、仓库配置 四、使用 1、安装Composer curl -sS https://getcomposer.org/installer | php mv composer.phar /usr/local/bin/composer composer --version 2、Composer配置使用私有源 方式一：配置Composer全局使用私有源 ①手动 composer config -g repo.packagist composer http://Neuxs-IP:8081/repository/composer-public/ # Composer默认使用ssl连接代理源，使用私有仓库源时，使用的HTTP，需要关闭SSL composer config -g -- disable-tls true composer config -g -- secure-http false ②使用crm工具 #安装 composer global require slince/composer-registry-manager ^2.0 # 添加公司内部的私有源 composer repo:add synology-nxus http://Neuxs-IP:8081/repository/composer-public/ # 查看所有的私有源 composer repo:ls # 切换到私有源 composer repo:use synology-nxus 方式二：配置项目级别配置使用私有源 编写项目根目录下的composer.json { \"packagist.org\": false , \"repositories\": { \"packagist\": { \"type\": \"composer\", \"url\": \"http://Neuxs-IP:8081/repository/composer-public/\" } }, \"config\": { \"secure-http\": false }, \"require\": { \"monolog/monolog\": \"1.0.*\", \"pugx/shortid-php\":\"v0.5.1\" } } 3、安装依赖 composer会根据当前路径下composer.json中写的下载依赖 composer install # 依赖会被下载当前目录下的vendor文件夹中 手动添加依赖 composer require monolog/monolog 4、查看Nexus仓库中是否已经缓存了依赖包 5、上传包到Hosted类型的仓库中 编辑composer.json，添加项目包的信息，像名字，描述，版本号，维护者等信息。 { \"version\": \"1.0\", \"name\": \"php-composer-test\", \"description\": \"this is a demo composer repo\", \"authors\": [{ \"name\": \"curiouser\", \"email\": \"12345678@qq.com\" }], \"packagist.org\": false , \"repositories\": { \"packagist\": { \"type\": \"composer\", \"url\": \"http://Neuxs-IP:8081/repository/composer-public/\" } }, \"config\": { \"secure-http\": false }, \"require\": { \"monolog/monolog\": \"1.0.*\", \"pugx/shortid-php\":\"v0.5.1\" } } 归档项目 composer archive --format=zip 上传归档项目包 curl -v --user 'user:pass' --upload-file example.zip http://Neuxs-IP:8081/repository/composer-hosted/packages/upload/项目名/组件名/版本号 查看项目包是否已上传到Nexus中 下载引用上传的包 composer require curiouser/test:1.0 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-pypi.html":{"url":"origin/nexus-pypi.html","title":"Pypi","keywords":"","body":"Nexus Pypi仓库的使用 一、简介 Nexus针对Python制品仓库的管理与使用 官方文档：https://help.sonatype.com/repomanager3/formats/pypi-repositories Group类型仓库 pypi-public Proxy类型仓库 pypi-aliyun：http://mirrors.aliyun.com/pypi/ pupi-python：https://pypi.python.org/ Hosted类型仓库 pypi-hosted 二、客户端使用 1、pip3配置仓库源 ①全局配置 编辑 ~/.pip/pip.conf [global] # 用户名密码已配置在URL中。如果密码中包含特殊字符，使用特殊字符对应ASCII码的16进制进行代替。例如：密码中包含的“#”的ASCII为35,对应的16进制表示%23。密码中包含的“/”的ASCII为47,对应的16进制表示%2F。 index-url = http://pypi:******@nexus-ip:8081/repository/pypi-public/simple trusted-host = 192.168.150.88 ②临时配置 pip3 install flask \\ -i http://pypi:******@nexus-ip:8081/repository/pypi-public/simple \\ --trusted-host 192.168.150.88 注意 使用pip3 config list 可查看pip配置 官方已不再推荐支持 easy_install ，详见一下链接 https://setuptools.readthedocs.io/en/latest/easy_install.html https://packaging.python.org/discussions/pip-vs-easy-install/ 2、使用twine上传python制品到私有仓库 ①安装twine pip3 install twine \\ -i http://pypi:******@nexus-ip:8081/repository/pypi-public/simple \\ --trusted-host 192.168.150.88 ②编写setup.py import setuptools import os import requests # 将README.md中的描述文字作为制品的详细描述 with open(\"README.md\", \"r\") as fh: long_description = fh.read() # 将requirements.txt中依赖模块的版本信息作为制品的依赖描述 if os.path.exists(\"requirements.txt\"): install_requires = io.open(\"requirements.txt\").read().split(\"\\n\") else: install_requires = [] setuptools.setup( # 项目命名 name=\"demotest\", # 版本 version=\"0.0.1\", # 作者 author=\"curiouser\", # 作者邮箱 author_email=\"*******@163.com\", # 项目制品包的简要描述 description=\"test\", # 项目制品包的详细描述 long_description=long_description, # 制品包详细描述的格式 long_description_content_type=\"text/markdown\", # 项目代码仓库地址 url=\"https://github.com/test\", packages=setuptools.find_packages(), classifiers=[ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\", ], install_requires = install_requires, # 是否打包文件夹内的所有数据 include_package_data=true, package_data = { # If any package contains *.txt or *.rst files, include them: 'chinesename': ['source/*.txt', \"source/*.json\"], }, # 如果需要支持脚本方法运行，可以配置入口点 entry_points={ 'console_scripts': [ 'chinesename = chinesename.run:main' ] } ) 编写README.md setuptools官方文档：https://packaging.python.org/guides/distributing-packages-using-setuptools/ setup.py推荐规则：https://github.com/pypa/sampleproject/blob/master/setup.py ③打包项目 python3 setup.py sdist bdist_wheel # 打完包，会在当前目录下的dist目录下产生源文件tar.gz，分发文件.whl两个项目包 ④上传项目包到hosted仓库 # 检测包 twine check dist/* # 上传包 twine upload dist/* \\ --repository-url http://nexus-ip:8081/repository/pypi-hosted/ ⑤验证 ⑥下载使用 sudo pip3 install demotest==0.0.1 # 或者 sudo pip3 install demotest==0.0.1 \\ -i http://pypi:******@nexus-ip:8081/repository/pypi-public/simple \\ --trusted-host 192.168.150.88 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-helm.html":{"url":"origin/nexus-helm.html","title":"Helm","keywords":"","body":"Nexus Helm的配置与使用 一、简介 Nexus从3.21.1开始正式支持Helm charts仓库的管理。之前一直是使用第三方插件的形式支持的。 目前暂时支持Hosted和Proxy类型的Helm仓库 二、配置 Proxy helm-google https://kubernetes-charts-incubator.storage.googleapis.com/ Hosted helm-hosted 三、使用 1、Helm客户端添加Nexus上Proxy类型的远程charts仓库 添加Nexus上的Helm代理类型的仓库 helm repo add 远程Charts仓库别名 http://nexus-ip:8081/repository/helm-google/ --username admin --password=\"*****\" 查看添加的远程仓库 helm repo list 搜索远程仓库 helm search repo 从远程仓库中下载charts到本地，并解压 helm pull 远程Charts仓库别名/chart名 --untar 从远程仓库中下载指定版本的charts到本地 helm fetch 远程Charts仓库别名/chart名 --version 1.3.0 2、上传Helm charts到Nexus上Hosted类型的仓库 curl -u 用户名:密码 http://nexus-ip:8081/repository/helm-hosted/ --upload-file sentry-kubernetes-0.2.3.tgz 3、手动在Nexus UI界面上传.gz格式的Chart到Helm Hosted仓库 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-数据的备份恢复.html":{"url":"origin/nexus-数据的备份恢复.html","title":"数据备份恢复","keywords":"","body":"Nexus的数据备份与恢复 Nexus的备份分为两个部分。一个是元信息和配置信息数据库的备份，一个是Blob存储的备份 一、配置DB的备份操作 二、Blob存储的备份操作 Nexus的blob stores可以简单的理解为一个文件夹，存放着各种制品的原始文件，例如原始的Java制品Jar包，POM文件，War包等文件。Blob的类型可以是文件系统的一个文件夹，也可以是S3对象存储的一个存储Buckets。（默认是文件系统类型，仅支持S3对象存储）一个blob存储可以被一个或者多个仓库组使用 三、Kubernetes上Nexus的数据备份 由于nexus在Kubernetes上的实例使用了CephFS类型的PVC存储作为Volume挂载在到数据目录下。所以，只要将Nexus容器持久化卷的PV对应Ceph路径挂载到某个目录下，拷贝其中的数据库DB目录和Blob目录进行数据备份。 mkdir test nexus3-k8s-2019-5-6-bak mount -t ceph jk1:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-25c75aeb-6f2b-11e9-ac5b-9209ee33fdea test -o name=admin,secret=AQCxFKtcDGd1AhAAvytw5KlaeuApSi1a3G2iwA== cp -r test/db test/blob nexus3-k8s-2019-5-6-bak umount test 四、数据恢复操作 数据恢复时使用的版本与旧版本的差异仅限于小版本号 暂停旧的Nexus POD 挂载旧的Nexus CephFS PV 到本地目录 将备份提拷贝新的CephFS PV中 修改备份目录的权限 重启新的Nexus POD 五、备份策略优化 hosted类型的仓库可使用单独的Blob存储，备份时只备份该Blob。Proxy类型的仓库可不用备份。 可添加执行脚本类型的定时任务做备份，将新增Blob同步到其他Nexus实例中 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-api.html":{"url":"origin/nexus-api.html","title":"API","keywords":"","body":"Nexus API 一、Context 官网API文档：https://help.sonatype.com/repomanager3/rest-and-integration-api 二、Search API Search API用于搜索component和asset。 GET /service/rest/v1/search 1、Search Components 例如在maven-central仓库中搜索\"group=org.osgi\"的component $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search?repository=maven-central&group=org.osgi' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDoyZTQ3ZGRhMGYxYjU1NWUwNzE1OWRjOWY5ZGQzZmVmNA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.osgi\", \"name\" : \"org.osgi.core\", \"version\" : \"4.3.1\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ] } ], \"continuationToken\" : null } 2、Search Assets 例如在maven-central仓库中搜索\"group=org.osgi\"的assets $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets?repository=maven-central&group=org.osgi' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ], \"continuationToken\" : null } 3、Search and Download Asset 用于搜索一个资产，然后将请求重定向到该资产的downloadUrl GET /service/rest/v1/search../assets/download 例如获取一个maven坐标为\"groupId=com.curiosuer，artifactId=SpringBoot2，version=0.0.0\"Jar包的下载链接 $ curl -v -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets/download?maven.groupId=com.curiosuer&maven.artifactId=SpringBoot2&maven.baseVersion=0.0.0&maven.extension=jar&maven.classifier' 浏览器中 三、Repositories API 1、List Repositories 获取用户能访问到的Repository仓库列表 GET /service/rest/v1/repositories $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/repositories' [ { \"name\" : \"YUM-Hosted\", \"format\" : \"yum\", \"type\" : \"hosted\", \"url\" : \"http://localhost:8081/repository/YUM-Hosted\" }, ... ] 此endpoint返回所有存储库，并且不允许分页。 注意，存储库的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 四、Assets API 1、List Assets 列出指定Repository仓库中包含的Assets GET /service/rest/v1../assets 例如列出maven-central仓库中的Assets $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets?repository=Maven-Releases' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/Maven-Releases/com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"path\" : \"com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"id\" : \"TWF2ZW4tUmVsZWFzZXM6MzZlM2RlYzhkZTUyOGM5YmRkYTdhZTNjZjlmYjFiNTY\", \"repository\" : \"Maven-Releases\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"c1ab61e9f407cbabaa8f3b377a76afa1f8afa4f1\", \"md5\" : \"5474cd7fc95a581eb6b6a3319c8aa6ba\" }, ... ], \"continuationToken\" : \"3f5cae01760233b6506547dc7be10e0b\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。 注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 2、Get Asset GET /service/rest/v1../assets/{id} This endpoint allows us to get the details of an individual asset. $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"path\" : \"org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a3bf672b3ea844575acba3b84790e76ed86a7c66\", \"md5\" : \"49e439c814c3098450dc4bbee952463f\" }} 3、Delete Asset DELETE /service/rest/v1../assets/{id} This endpoint can be used to delete an individual asset. $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:41:47 GMT ... 五、Components API 1、List Components 遍历仓库中的Components GET /service/rest/v1/components $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components?repository=Maven-Central' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjMyNjhmMjIwZTQ1ZDdkZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"com.google.guava\", \"name\" : \"guava\", \"version\" : \"21.0\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MzA4OThiZjZmZTFkOTE2NA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"3a3d111be1be1b745edfa7d91678a12d7ed38709\", \"md5\" : \"ddc91fd850fa6177c91aab5d4e4d1fa6\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5MDc0MDFlYzRjNjVlNjU5OQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a1ff60cb911e1f64801c03d03702044d10c9bdd3\", \"md5\" : \"e34b8695ede1677ba262411d757ea980\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlOWJjNDgzOGE1MzM2OGZlZg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"fe4fa08a8c0897f9896c7e278fb397ede4a2feed\", \"md5\" : \"5c10f97af2ce9db54fa6c2ea6997a8d7\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmZDA3NDdkNjlhZDNmZjI5Nw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"992b43ab7b3a061be47767e910cab58180325abc\", \"md5\" : \"33aed29aa0bb4e03ea7854066a5b4738\" } } ] }, ... ], \"continuationToken\" : \"88491cd1d185dd136f143f20c4e7d50c\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。 注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 2、Get Component 获取仓库中component的详细信息 GET /service/rest/v1/components/{id} $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.apache.httpcomponents\", \"name\" : \"httpcomponents-client\", \"version\" : \"4.3.5\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2YTFhOGUxOGQxZmFkOGM3Mw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"95d80a44673358a5dcbcc2f510770b9f93fe5eba\", \"md5\" : \"f4769c4e60799ede664414c26c6c5c9d\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5ZDU3YjFlYjE0MzM1ZTcwMQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"6b98f5cef5d7102f8f45215bdcf48dc843d060af\", \"md5\" : \"f3b3ac640853fcb887621d13029a1747\" } } ] } 3、Upload Component 上传Component到指定仓库中，一些格式的仓库允许上传的Component中包含多个Assets。 endpoint的上传参数取决于要上传Component到那个仓库的格式 POST /service/rest/v1/components $ curl -v -u admin:admin123 \\ -X POST 'http://localhost:8081/service/rest/v1/components?repository=maven-releases' \\ -F maven2.groupId=com.google.guava \\ -F maven2.artifactId=guava \\ -F maven2.version=24.0-jre \\ -F maven2.asset1=@guava-24.0-jre.jar \\ -F maven2.asset1.extension=jar \\ -F maven2.asset2=@guava-24.0-jre-sources.jar \\ -F maven2.asset2.classifier=sources \\ -F maven2.asset2.extension=jar HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... ①Maven Maven format allows multiple assets to be uploaded as part of a single component. To upload multiple assets just follow the information from a table describing the given format and replace assetN with multiple instances of it (e.g. asset1, asset2, etc.): Field name Field type Required? Description maven2.groupId String Yes, unless a POM asset is included in the upload Group ID of the component maven2.artifactId String Yes, unless a POM asset is included in the upload Artifact ID of the component maven2.version String Yes, unless a POM asset is included in the upload Version of the component maven2.generate-pom Boolean No Whether the Nexus Repository Manager should generate a POM file based on above component coordinates provided maven2.packaging String No Define component packaging (e.g. jar, ear) maven2.assetN File Yes, at least one Binary asset maven2.assetN.extension String Yes Extension of the corresponding assetN asset maven2.assetN.classifier String No Classifier of the corresponding assetN asset Examples：Uploading a jar and Automatically Creating a pom File $ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=true\" \\ -F \"maven2.groupId=com.example\" \\ -F \"maven2.artifactId=commercial-product\" \\ -F \"maven2.packaging=jar\" \\ -F \"version=1.0.0\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/product.jar;type=application/java-archive\" \\ -F \"maven2.asset1.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-third-party\" Upload a POM and associated JAR File $ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=false\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/pom.xml\" \\ -F \"maven2.asset1.extension=pom\" \\ -F \"maven2.asset2=@/absolute/path/to/the/local/file/product-1.0.0.jar;type=application/java-archive\" \\ -F \"maven2.asset2.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-releases\" ②Raw Raw supports multiple assets within a single component. Field name Field type Required? Description raw.directory String Yes Destination for upload files (e.g. /path/to/files) raw.assetN File Yes, at least one Binary asset raw.assetN.filename String Yes Filename to be used for the corresponding assetN asset ③PyPI Field name Field type Required? Description pypi.asset File Yes Binary asset ④RubyGems Field name Field type Required? Description rubygems.asset File Yes Binary asset ⑤NuGet Field name Field type Required? Description nuget.asset File Yes Binary asset ⑥NPM Field name Field type Required? Description npm.asset File Yes Binary asset 4、Delete Component：删除仓库中的component DELETE /service/rest/v1/components/{id} $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... 六、Metrics API 示例 https://raw.githubusercontent.com/OpenShiftDemos/nexus/master/scripts/nexus-functions ################################################################# # Functions for Managing Sonatype Nexus # # # # Authors: # # - Jorge Morales https://github.com/jorgemoralespou # # - Siamak Sadeghianfar https://github.com/siamaksade # # # ################################################################# ​ # # add_nexus2_repo [repo-id] [repo-url] [nexus-username] [nexus-password] [nexus-url] # ​ function add_nexus2_repo() { local _REPO_ID=$1 local _REPO_URL=$2 local _NEXUS_USER=$3 local _NEXUS_PWD=$4 local _NEXUS_URL=$5 ​ read -r -d '' _REPO_JSON Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html":{"url":"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html","title":"Jenkins相关插件","keywords":"","body":"使用jenkins插件上传CI流程制品到Nexus仓库 一、Overviews 现在Nexus各个格式仓库中的制品大多数都是在Jenkins的持续集成CI流水线中生成的，每次流水线构建都需要其制品上传到Nexus中进行管理。Nexus针对Jenkins有Nexus Platform的插件来简化上传步骤，该插件主要用来上传Maven格式制品到Hosted类型的仓库中。同时，Jenkins CI Pipeline中除了可以使用该插件来上传Maven制品到Maven格式仓库，原始Curl也是可以的。 插件Github：https://github.com/jenkinsci/nexus-platform-plugin 二、Jenkins使用Nexus Platform上传maven格式制品 1、安装 2、配置 系统管理--> 系统设置--> Sonatype Nexus 3、Jenkins Job 4、Jenkins Pipeline .....上文省略...... stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://Nexus-IP地址:8081/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } .....下文省略...... 5、Jenkins使用Curl命令手动上传maven制品到Nexus仓库中 stage(\"上传制品\"){ steps{ script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用curl命令通过Nexus API接口上传制品到RAW仓库。下载URL既是上传URL sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u admin:****** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://Nexus-IP地址:8081/repository/jenkins-product-repository/${pomfile.artifactId}-${pomfile.version}-${params.BUILD_VERSION}-${params.BUILD_ID}.${pomfile.packaging}\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/SonarQube静态代码扫描分析简介.html":{"url":"origin/SonarQube静态代码扫描分析简介.html","title":"SonarQube静态代码扫描分析","keywords":"","body":"一、静态代码分析 Why：在软件开发过程中，开发团队往往要花费大量的时间和精力发现并修改代码缺陷。而 发现BUG越晚，修复的成本越大 缺陷引入的大部分是在编码阶段，但发现更多的是在单元测试、集成测试、功能测试阶段 30% 至 70% 的代码逻辑设计和编码缺陷是可以通过静态代码分析来发现和修复的 How：在编码阶段，可以通过以下手段发现源代码问题，从源头及时规避，保证代码质量 静态代码扫描工具 Code Review What：Code Review往往要求大量的时间消耗和相关知识的积累，因此对于软件开发团队来说，使用静态代码分析工具自动化执行代码检查和分析，能够极大地提高软件可靠性并节省软件开发和测试成本。 帮助程序开发人员自动执行静态代码分析，快速定位代码隐藏错误和缺陷 助代码设计人员更专注于分析和解决代码设计缺陷 显著减少在代码逐行检查上花费的时间，提高软件可靠性并节省软件开发和测试成本 常见的一些静态分析工具 Checkstyle：SourceForge 的开源项目，通过检查对代码编码格式，命名约定，Javadoc，类设计等方面进行代码规范和风格的检查，从而有效约束开发人员更好地遵循代码编写规范 FindBugs：由马里兰大学提供的一款开源 Java 静态代码分析工具。FindBugs 通过检查类文件或 JAR 文件，将字节码与一组缺陷模式进行对比从而发现代码缺陷，完成静态代码分析 PMD：由 DARPA 在 SourceForge 上发布的开源 Java 代码静态分析工具。PMD 通过其内置的编码规则对 Java 代码进行静态检查，主要包括对潜在的 bug，未使用的代码，重复的代码，循环体创建新对象等问题的检验 二、Sonar简介 Sonar是一个用于代码质量管理的开源平台，可以从 七个维度检测代码质量 Sonar可以通过PMD、CheckStyle、Findbugs等代码规则检测工具来检测你的代码，帮助你发现代码的漏洞，Bug，异味等信息。 Sonar最大的特点就是插件化，可以根据不同的场景需求进行插件化安装，可以同时可以检测Python、C++等多种语言。 Sonar客户端可以采用IDE插件、Sonar-Scanner插件、Ant插件和Maven插件等多种方式，并通过各种不同的分析机制对项目源代码进行分析和扫描，并把分析扫描后的结果上传到sonar的数据库，通过sonar web界面对分析结果进行管理 Sonar的架构体系 Project：是需要被分析的源码 SonarQube Scanner：用于执行代码分析的工具，SonarQube Scanner分析完毕之后，会将结果上报到指定的SonarQube Server。 SonarQube Server：显示分析结果的Web Server，在SonarQube Scanner第一次将一个工程的分析结果上报给SonarQube Server后，Server上会自动创建一个工程显示分析的结果，可以在Server上设置代码质量管理相关的各种配置，如设置代码检查规则（Rule）和质量门限（Quality Gate）等。SonarQube Server包含三个子进程（web服务（界面管理）、搜索服务、计算引擎服务（写入数据库）） SonarQube Database：保存SonarQube服务端的权限配置，插件配置，项目快照，项目视图等 三、自动化扫描分析源代码的流程 官方推荐的自动化扫描流程 自动化静态代码扫描流程 本地开发：JetBrains Intellij IDEA 、Eclipse安装阿里巴巴的代码检查规范插件，可在编写代码时提示规范信息；安装使用sonarlint插件在本地运行代码扫描 Gitlab：Gitlab代码仓库可设置事件监听器，例如PUSH事件、Merge Request事件等。发送Web-hook到外部系统 Jenkins：Jenkins中可安装Gitlab插件，用于设置特定的Web-hook后端监听器来触发当前任务。 Jenkins Pipeline：在Jenkins Pipeline中获取Web-hook信息来拉取代码，然后编译、执行Sonar Scanner扫描源代码文件或二进制文件，最后将扫描的结果发送SnarQube进行存储、展示、管理等操作 SonarQube： 四、SonarQube服务端配置 1. 配置代码规则插件 2. 配置全局参数 3. 管理扫描结果 4. 质量门禁 五、Sonar体系中的配置参数生效优先级 UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数 例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数 例如sonar scanner Maven插件在settings.xml中配置的参数 项目分析客户端命令行运行时配置的参数，例如sonar-scanner二进制命令行运行时以“-D”开头的配置参数 六、扫描器 当SonarQube服务端搭建配置好了，Sonar提供了各种插件形式的Sonar Scanner扫描器供你选择来扫描你的源代码。 SonarScanner：二进制客户端 SonarScanner for Maven：Maven插件 SonarScanner for Jenkins：Jenkins插件 SonarScanner for Gradle：Gradle插件 SonarScanner for Ant：Ant插件 SonarScanner扫描参数 官方文档说明 参数 描述 默认值 是否必要 sonar.host.url SonarQube服务端地址 http://localhost:9000 是 sonar.projectKey 项目的唯一标识。以字母,-,_,:,至少有一个非数字 对于Maven插件的话,默认值是 :其他形式插件不提供默认值 是 sonar.projectName 在SonarQube Web UI上面显示的项目名 对于Maven插件形式,默认值是其他形式插件不提供默认值 否 sonar.projectVersion 项目的扫描版本 对于Maven插件形式,默认值是其他形式插件不提供默认值 否 sonar.login 发送扫描结果到SonarQube时的认证方式之一。值类型可为用户生成的认证Token，用户名 是 sonar.password 当sonar.login值类型为认证Token时，则不填 是 sonar.ws.timeout 等待服务端响应的最大秒数 60 否 sonar.projectDescription 项目描述。用于在项目Web UI中显示项目的描述 对于Maven插件形式,默认值是 否 sonar.links.homepage 项目地址。用于在项目Web UI中显示项目访问链接 对于Maven插件形式,默认值是 否 sonar.links.issue 项目代码Issue管理地址。用于在项目Web UI中显示Issue管理链接 对于Maven插件形式,默认值是 否 sonar.links.scm 项目源代码仓库地址。用于在项目Web UI中显示源代码仓库链接 for Maven projects 否 sonar.sources 以逗号分割的main源代码文件夹路径 否 sonar.tests 以逗号分割的测试源代码文件夹路径 否 sonar.sourceEncoding 源代码文件的编码格式，例如：UTF-8, MacRoman, Shift_JIS 系统的编码格式 否 sonar.externalIssuesReportPaths 否 sonar.projectDate 格式： yyyy-MM-dd, 例如: 2010-12-01 Current date 否 sonar.projectBaseDir 针对多模块项目时，指定要扫描源代码的目录路径 否 sonar.working.directory 指定Sonarscanner的工作空间。必须是不存在的，相对路径。针对MSBuild的插件，此参数不兼容 ~/.scannerwork 否 sonar.scm.provider 否 sonar.scm.forceReloadAll 否 sonar.scm.exclusions.disabled 否 sonar.scm.revision 否 sonar.buildString 100 否 sonar.analysis.[yourKey] 10 否 sonar.log.level 控制Sonarscanner输出日志的级别 INFO 否 sonar.verbose 输出更多Sonarscanner客户端和Sonarqube服务的扫描信息 false 否 sonar.showProfiling 否 sonar.scanner.dumpToFile 输出扫描期间所有的配置参数到文件中 否 sonar.scanner.metadataFilePath 指定report-task.txt文件的生成路径 等于sonar.working.directory的值 否 1. SonarScanner 下载地址：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/ 配置文件 全局配置文件路径：$安装目录/conf/sonar-scanner.properties 项目配置文件路径：$项目根目录/sonar-project.properties CLI命令参数 $ sonar-scanner usage: sonar-scanner [options] 参数: -D,--define Define property -h,--help Display help information -v,--version Display version information -X,--debug Produce execution debug output If you need more debug information you can add one of the following to your command line: -X, --verbose, or -Dsonar.verbose=true. 2. SonarScanner for Maven 官方文档：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-maven/ 注意： 从maven-sonar-plugin 3.4.0.905开始，不再支持SonarQube 从maven-sonar-plugin 3.1开始，不再支持Maven 全局参数 详见SonarQube服务端配置 配置Maven的setting.xml org.sonarsource.scanner.maven sonar true http://myserver:9000 f6eedc3d8bfa850a15f2ffcd 在项目pom.xml中配置项目扫描参数 ....上文省略.... com.curiosuer springboot2 0.0.1 用于演示Spring Boot2的一些功能 Curiouser-Demo-SpringBoot2-${gitlabBranch} http://springboot2-demo.apps.okd311.curiouser.com/swagger-ui.html ${gitlabSourceRepoHomepage}/commit/${gitlabMergeRequestLastCommit} ${gitlabSourceRepoHomepage}/issues ${BUILD_URL} UTF-8 UTF-8 1.8 指定的值 --> ${gitlabMergeRequestLastCommit} src/main/ 1 ${gitlabMergeRequestLastCommit} ${gitlabMergeRequestLastCommit} reuseReports jacoco ....下文省略.... 注意：pom.xml中有些参数的值是可以在Jenkins CI流水线中通过环境变量获取的。 执行扫描命令 mvn test sonar:sonar -Dspring.profiles.active=local 默认参数 sonar.projectKey ==> POM中的: sonar.projectName ==> POM中的 sonar.projectVersion ==> POM中的 sonar.projectDescription ==> POM中的 sonar.links.homepage ==> POM中的 sonar.links.ci ==> POM中的 sonar.links.issue ==> POM中的 sonar.links.scm ==> POM中的 七、扫描结果解析 附录 1、Sonar检查代码质量的七个维度 复杂度分布（complexity）：代码复杂度过高将难以理解、难以维护 重复代码（duplications）：程序中包含大量复制粘度的代码是质量低下的表现 单元测试（unit tests）：统计并展示单元测试覆盖率 编码规范（coding rules）：通过Findbugs、PMD、CheckStyle等规范代码编写 注释（commments）：少了可读性差，多了看起来费劲 潜在的Bug（potential bugs）：通过Findbugs、PMD、CheckStyle等检测潜在bug 结构与设计（architecture & design）：依赖i、耦合等 2、常见检查分析工具的内置规范 Checkstyle：分析源代码文件 Javadoc 注释：检查类及方法的 Javadoc 注释 命名约定：检查命名是否符合命名规范 标题：检查文件是否以某些行开头 Import 语句：检查 Import 语句是否符合定义规范 代码块大小，即检查类、方法等代码块的行数 空白：检查空白符，如 tab，回车符等 修饰符：修饰符号的检查，如修饰符的定义顺序 块：检查是否有空块或无效块 代码问题：检查重复代码，条件判断，魔数等问题 类设计：检查类的定义是否符合规范，如构造函数的定义等问题 FindBugs：分析字节码文件 Bad practice 坏的实践：常见代码错误，用于静态代码检查时进行缺陷模式匹配 Correctness 可能导致错误的代码，如空指针引用等 国际化相关问题：如错误的字符串转换 可能受到的恶意攻击，如访问权限修饰符的定义等 多线程的正确性：如多线程编程时常见的同步，线程调度问题。 运行时性能问题：如由变量定义，方法调用导致的代码低效问题。 PMD：：分析源代码文件 可能的 Bugs：检查潜在代码错误，如空 try/catch/finally/switch 语句 未使用代码（Dead code）：检查未使用的变量，参数，方法 复杂的表达式：检查不必要的 if 语句，可被 while 替代的 for 循环 重复的代码：检查重复的代码 循环体创建新对象：检查在循环体内实例化新对象 资源关闭：检查 Connect，Result，Statement 等资源使用之后是否被关闭掉 Jtest 可能的错误：如内存破坏、内存泄露、指针错误、库错误、逻辑错误和算法错误等 未使用代码：检查未使用的变量，参数，方法 初始化错误：内存分配错误、变量初始化错误、变量定义冲突 命名约定：检查命名是否符合命名规范 Javadoc 注释：检查类及方法的 Javadoc 注释 线程和同步：检验多线程编程时常见的同步，线程调度问题 国际化问题： 垃圾回收：检查变量及 JDBC 资源是否存在内存泄露隐患 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-22 12:08:49 "},"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html":{"url":"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html","title":"SonarScanner-将扫描结果以comment的形式回写到gitlab","keywords":"","body":"SonarScanner使用Sonarqube的Gitlab插件将扫描结果以gitlab comment的形式回写到Gitlab 一、Context 在Jenkins中做CI过程中,有一个步骤是代码编译完,使用sonar scanner扫描代码,检查静态代码中的语法错误等,然后将扫描结果发送到sonarqube,供项目经理查看代码质量. sonarqube可以安装插件gitlab,让sonarscanner扫描完代码,将结果以gitlab注释的方式回写到提交的commit中.方便开发人员排查代码. 以下操作过程各组件的版本 sonarqube: 7.3 (build 15553) sonarscanner: 3.3.0.1492 sonarqube gitlab插件: 4.0.0 gitlab: 10.8.4 ce jenkins: 2.150.2 Jenkins CI流水线是在使用Jenkins Slave(Kubernetes插件动态生成Slave POD)节点中来运行的,所以Sonarscanner,Maven等工具都是在Kubernetes Jenkins Slave镜像中已经安装好的. 二、操作 1、安装sonar-gitlab-plugin插件 插件Github:https://github.com/gabrie-allaigre/sonar-gitlab-plugin/ 2、生成用户访问Token 该Token用于客户端调用SonarQube API上传扫描代码时使用 3、gitlab创建sonarscanner的用户,并生成AccessKey 在sonarqube 服务端分析生成结果后，会使用该用户的身份在对应代码下以评论的形式显示扫描结果。所以需要该用户的AccessKey有访问Gitlab API的权限。 4、在gitlab中将sonarqube加入到对应项目仓库的Members中 设置sonarqube用户在该仓库的角色起码是开发者 5、Sonarqube中编辑gitlab插件的全局配置 扫描项目时，扫描参数生效优先级如下： UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数（例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数） 项目分析客户端命令行中配置的参数 所以可以在UI界面全局配置中配置一些通用、不经常变动的、由管理员控制的参数。例如：gitlab插件的通用配置、gitlab地址等参数 6、Jenkins Pipeline中使用sonarscanner扫描代码 stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=6a6fa6f1702ae42f8d0a0fe14166d9a2 \\ -Dsonar.projectName=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectKey=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectVersion=$GIT_COMMIT \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.sources=src/main \\ -Dsonar.test=src/test \\ -Dsonar.java.binaries=target/classes \\ -Dsonar.java.test.binaries='target/test-classes/*/*.class' \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.java.coveragePlugin=jacoco \\ -Dsonar.dynamicAnalysis=reuseReports \" } } 三、效果 四、Soanarscanner Gitlab插件参数详解 Variable Comment Type Version sonar.gitlab.url GitLab url Administration, Variable >= 1.6.6 sonar.gitlab.max_global_issues Maximum number of anomalies to be displayed in the global comment Administration, Variable >= 1.6.6 sonar.gitlab.user_token Token of the user who can make reports on the project, either global or per project Administration, Project, Variable >= 1.6.6 sonar.gitlab.project_id Project ID in GitLab or internal id or namespace + name or namespace + path or url http or ssh url or url or web Project, Variable >= 1.6.6 sonar.gitlab.commit_sha SHA of the commit comment Variable >= 1.6.6 sonar.gitlab.ref Branch name or reference of the commit Variable sonar.gitlab.ref_name Branch name or reference of the commit Variable >= 1.6.6 sonar.gitlab.max_blocker_issues_gate Max blocker issue for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_critical_issues_gate Max critical issues for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_major_issues_gate Max major issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_minor_issues_gate Max minor issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_info_issues_gate Max info issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.ignore_certificate Ignore Certificate for access GitLab, use for auto-signing cert (default false) Administration, Variable >= 2.0.0 sonar.gitlab.comment_no_issue Add a comment even when there is no new issue (default false) Administration, Variable >= 2.0.0 sonar.gitlab.disable_inline_comments Disable issue reporting as inline comments (default false) Administration, Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_file Show issue for commit file only (default false) Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_line Show issue for commit line only (default false) Variable >= 2.1.0 sonar.gitlab.build_init_state State that should be the first when build commit status update is called (default pending) Administration, Variable >= 2.0.0 sonar.gitlab.disable_global_comment Disable global comment, report only inline (default false) Administration, Variable >= 2.0.0 sonar.gitlab.failure_notification_mode Notification is in current build (exit-code) or in commit status (commit-status) (default commit-status) Administration, Variable >= 2.0.0 sonar.gitlab.global_template Template for global comment in commit Administration, Variable >= 2.0.0 sonar.gitlab.ping_user Ping the user who made an issue by @ mentioning. Only for default comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.unique_issue_per_inline Unique issue per inline comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.prefix_directory Add prefix when create link for GitLab Variable >= 2.1.0 sonar.gitlab.api_version GitLab API version (default v4 or v3) Administration, Variable >= 2.1.0 sonar.gitlab.all_issues All issues new and old (default false, only new) Administration, Variable >= 2.1.0 sonar.gitlab.json_mode Create a json report in root for GitLab EE (codeclimate.json or gl-sast-report.json) Project, Variable >= 3.0.0 sonar.gitlab.query_max_retry Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.query_wait Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.quality_gate_fail_mode Quality gate fail mode: error, warn or none (default error) Administration, Variable >= 3.0.0 sonar.gitlab.issue_filter Filter on issue, if MAJOR then show only MAJOR, CRITICAL and BLOCKER (default INFO) Administration, Variable >= 3.0.0 sonar.gitlab.load_rules Load rules for all issues (default false) Administration, Variable >= 3.0.0 sonar.gitlab.disable_proxy Disable proxy if system contains proxy config (default false) Administration, Variable >= 4.0.0 sonar.gitlab.merge_request_discussion Allows to post the comments as discussions (default false) Project, Variable >= 4.0.0 sonar.gitlab.ci_merge_request_iid The IID of the merge request if it’s pipelines for merge requests Project, Variable >= 4.0.0 五、问题 1、当项目是私有仓库时 2、获取项目仓库的ProjectID 3、gitlab插件4.0.0无法兼容Sonarqube 7.6-community至7.9-community的版本 报错如下！插件GIthub的原始Issue：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/issues/213 [ERROR] Failed to execute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed toexecute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 原因 解决方案 已经修改编译好的插件Jar包：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/releases/download/4.1.0-SNAPSHOT/sonar-gitlab-plugin-4.1.0-SNAPSHOT.jar 参考链接 https://gitlab.com/gitlab-org/gitlab-ce/issues/28342 https://www.cnblogs.com/amyzhu/p/8988519.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-22 11:58:01 "},"origin/sonarqube-gitlab-auth.html":{"url":"origin/sonarqube-gitlab-auth.html","title":"Sonarqube使用Gitlab登录","keywords":"","body":"Sonarqube对接Gitlab认证 一、操作 1、Sonarqube安装Gitlab Auth插件 2、Gitlab Application配置 3、Sonarqube配置Gitlab认证插件 4、认证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/sonarqube-ide-scanner.html":{"url":"origin/sonarqube-ide-scanner.html","title":"IDE本地插件扫描检查","keywords":"","body":"SonarQube本地IDE插件扫描 一、简介 SonarQube扫描代码的步骤是：不管是使用Maven插件还是通用的SonarScanner命令行工具，都是将项目的代码发送到SonarQube服务端进行处理的。由于SonarQube底层组件会用到ElasticSearch服务，如果服务器性能不够好或同时多个项目同时进行了扫描，会造成SonarQube服务端的后台处理任务加重，耗时严重，降低CICD流水线中集成扫描步骤的效率。所以有以下可建议的措施： 提高服务器性能 减少扫描无用的代码（例如服务端客户端进行exclude配置，只扫关注程度高的代码） 在合适的CICD环节再加入代码，增加代码扫描结果的重视程度 例如在发布到STG环境的CI流水线中加入代码扫描。如果是加在发布到生产环境的CI流水线中，会影响发版上线计划，开发人员来不及修改。如果是加在发布到测试环境的CI流水线中，会因为代码修改频繁，降低代码自动集成效果。而STG环境代码更新没那么频繁，离上线计划时间还有缓存进行修改代码。 开发人员在本地开发时先扫描一遍，提前规避掉大部分的代码错误 所以以Jetbrains公司的系列IDE IntelliJIDEA为例进行本地代码扫描。 二、IDEA安装配置插件 在IDE的插件管理中心安装名为\"SonarQube Community Plugin\"的插件，然后重启IDE。 插件GitHub地址：https://github.com/sonar-intellij-plugin/sonar-intellij-plugin 1、插件中心安装插件 2、配置插件添加sonarqueb服务端地址 扫描添加SonarQube服务端已有的项目 三、扫描代码（以扫描单个源代码文件为例） 扫描前会先从服务端下载服务端项目中配置的代码规则 会得到以下扫描结果 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-17 18:16:49 "},"origin/ldap-Jenkins对接LDAP.html":{"url":"origin/ldap-Jenkins对接LDAP.html","title":"Jenkins","keywords":"","body":"一. Context OpenLDAP的条目组织形式 二. Jenkins配置 1. Jenkins安装LDAP插件 安装插件有两种方法： 方法一：后台插件管理里直接安装 优点：简单方便，不需要考虑插件依赖问题 缺点：因为网络等各种问题安装不成功 安装方法：登录Jenkins --> 系统管理 --> 插件管理 --> 可选插件 --> 搜索LDAP --> 选中 --> 直接安装 --> 安装完成重启 方法二：官网下载安装文件后台上传 优点：一定可以安装成功的 缺点：麻烦，要去官网找插件并解决依赖 安装方法：官网下载插件 --> 登录Jenkins --> 系统管理 --> 插件管理 --> 高级 --> 上传插件 --> 选择文件 --> 上传 --> 安装完成后重启 LDAP插件下载地址：https://updates.jenkins.io/download/plugins/ldap/ 2. 登录Jenkins --> 系统管理 --> 全局安全配置 root DN：这里的root DN只是指搜索的根，并非LDAP服务器的root dn。由于LDAP数据库的数据组织结构类似一颗大树，而搜索是递归执行的，理论上，我们如果从子节点（而不是根节点）开始搜索，因为缩小了搜索范围那么就可以获得更高的性能。这里的root DN指的就是这个子节点的DN，当然也可以不填，表示从LDAP的根节点开始搜索 User search base：这个配置也是为了缩小LDAP搜索的范围，例如Jenkins系统只允许ou为Admin下的用户才能登陆，那么你这里可以填写ou=Admin，这是一个相对的值，相对于上边的root DN，例如你上边的root DN填写的是dc=domain,dc=com，那么user search base这里填写了ou=Admin，那么登陆用户去LDAP搜索时就只会搜索ou=Admin,dc=domain,dc=com下的用户了 User search filter：这个配置定义登陆的“用户名”对应LDAP中的哪个字段，如果你想用LDAP中的uid作为用户名来登录，那么这里可以配置为uid={0}（{0}会自动的替换为用户提交的用户名），如果你想用LDAP中的mail作为用户名来登录，那么这里就需要改为mail={0}。在测试的时候如果提示你user xxx does not exist，而你确定密码输入正确时，就要考虑下输入的用户名是不是这里定义的这个值了 Group search base：参考上边User search base解释 Group search filter：这个配置允许你将过滤器限制为所需的objectClass来提高搜索性能，也就是说可以只搜索用户属性中包含某个objectClass的用户，这就要求你对你的LDAP足够了解，一般我们也不配置 Group membership：没配置，没有详细研究 Manager DN：这个配置在你的LDAP服务器不允许匿名访问的情况下用来做认证（详细的认证过程参考文章LDAP落地实战（二）：SVN集成OpenLDAP认证中关于LDAP服务器认证过程的讲解），通常DN为cn=admin,dc=domain,dc=com这样 Manager Password：上边配置dn的密码 Display Name LDAP attribute：配置用户的显示名称，一般为显示名称就配置为uid，如果你想显示其他字段属性也可以这里配置，例如mail Email Address LDAP attribute：配置用户Email对应的字段属性，一般没有修改过的话都是mail，除非你用其他的字段属性来标识用户邮箱，这里可以配置 3. 登录验证 参考链接 https://mp.weixin.qq.com/s/S5ozDJSh4yTSfP_glNoiOQ https://plugins.jenkins.io/ldap https://wiki.jenkins.io/display/JENKINS/LDAP+Plugin#LDAPPlugin-Groupmembership https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_liunx_52_ldap_for_jenkins.html https://blog.csdn.net/wanglei_storage/article/details/52935312 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-SonarQube对接LDAP.html":{"url":"origin/ldap-SonarQube对接LDAP.html","title":"SonarQube","keywords":"","body":"一、Context OpenLDAP的条目组织形式 Sonaeqube官方文档的操作步骤 二、操作 1、Sonarqube安装LDAP插件 配置--> 应用市场 2、修改配置文件/opt/sonarqube/conf/sonar.properties 如果sonarqube的部署实例是使用Dockers的话，则可通过环境变量的方式注入以下配置 sonar.security.realm=LDAP sonar.forceAuthentication=true ldap.authentication=simple ldap.url=ldap://openldap-service.openldap.svc:389 ldap.bindDn=cn=admin,dc=curiouser,dc=com ldap.bindPassword=****** # User Configuration ldap.user.baseDn=ou=employee,dc=curiouser,dc=com ldap.user.request=(&(memberOf=cn=sonarqube,ou=applications,dc=curiouser,dc=com)(cn={0})) ldap.user.realNameAttribute=sn ldap.user.emailAttribute=mail 相关配置 Property Description Default value Required Example sonar.security.realm Set this to LDAP authenticate first against the external sytem. If the external system is not reachable or if the user is not defined in the external system, authentication will be performed against SonarQube's internal database. none Yes LDAP (only possible value) sonar.authenticator.downcase Set to true when connecting to a LDAP server using a case-insensitive setup. false No ldap.url URL of the LDAP server. If you are using ldaps, you should install the server certificate into the Java truststore. none Yes ldap://localhost:10389 ldap.bindDn The username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory. none No cn=sonar,ou=users,o=mycompany ldap.bindPassword The password of the user to connect with. Leave this blank for anonymous access to the LDAP directory. none No secret ldap.authentication Possible values: simple, CRAM-MD5, DIGEST-MD5, GSSAPI. See the tutorial on authentication mechanisms simple No ldap.realm See Digest-MD5 Authentication, CRAM-MD5 Authentication none No example.org ldap.contextFactoryClass Context factory class. com.sun.jndi.ldap.LdapCtxFactory No ldap.StartTLS Enable use of StartTLS false No ldap.followReferrals Follow referrals or not. See Referrals in the JNDI true 用户配置 Property Description Default value Required Example ldap.user.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for users. None Yes cn=users,dc=example,dc=org ldap.user.request LDAP user request. (&(objectClass=inetOrgPerson)(uid={login})) No (&(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute Attribute in LDAP defining the user’s real name. cn No ldap.user.emailAttribute Attribute in LDAP defining the user’s email. mail No Group Mapping Only groups are supported (not roles). Only static groups are supported (not dynamic groups). For the delegation of authorization, groups must be first defined in SonarQube. Then, the following properties must be defined to allow SonarQube to automatically synchronize the relationships between users and groups. Property Description Default value Required Example for Active Directory ldap.group.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for groups. none No cn=groups,dc=example,dc=org ldap.group.request LDAP group request. (&(objectClass=groupOfUniqueNames)(uniqueMember={dn})) No (&(objectClass=group)(member={dn})) ldap.group.idAttribute Property used to specifiy the attribute to be used for returning the list of user groups in the compatibility mode. cn No sAMAccountName 重启Sonarqube，启动过程中如果出现以下日志，则证明LDAP连接成功 INFO org.sonar.INFO Security realm: LDAP ... INFO o.s.p.l.LdapContextFactory Test LDAP connection: OK 3、登录验证 4、权限控制 将admin用户的管理员权限删除，赋予另一个用户 参考链接 https://hub.docker.com/_/sonarqube?tab=description https://docs.sonarqube.org/latest/instance-administration/delegated-auth/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-Gitlab对接LDAP.html":{"url":"origin/ldap-Gitlab对接LDAP.html","title":"Gitlab","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、配置 1. 修改/etc/gitlab/gitlab.rb ..................省略............................. gitlab_rails['ldap_enabled'] = true ###! **remember to close this block with 'EOS' below** gitlab_rails['ldap_servers'] = YAML.load 三、测试登录 四、注意 当用户第一次使用LDAP登录GitLab时，如果其LDAP电子邮件地址是现有GitLab用户的电子邮件地址时，那么LDAP DN用户将与现有gitlab用户相关联。如果在GitLab的数据库中没有找到LDAP电子邮件属性，就会创建一个新用户。 换句话说，如果现有的GitLab用户希望自己启用LDAP登录，那么他们应该检查他们的GitLab电子邮件地址是否匹配LDAP电子邮件地址，然后通过他们的LDAP凭证登录GitLab。 https://docs.gitlab.com/ee/administration/auth/ldap.html#enabling-ldap-sign-in-for-existing-gitlab-users 参考链接 https://blog.csdn.net/tongdao/article/details/52538365 https://docs.gitlab.com/ee/administration/auth/ldap.html#configuration https://docs.gitlab.com/ee/administration/auth/how_to_configure_ldap_gitlab_ce/index.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-Nexus对接LDAP.html":{"url":"origin/ldap-Nexus对接LDAP.html","title":"Nexus","keywords":"","body":"Preflight Nexus 3 OpenLDAP 3.15.2-01 1.2.4 一、Context OpenLDAP的条目组织形式 二、Nexus设置 1. Nexus开启认证Realm 2. 配置LDAP Name：Enter a unique name for the new configuration. LDAP server address：Enter Protocol, Hostname, and Port of your LDAP server. Protocol：Valid values in this drop-down are ldap and ldaps that correspond to the Lightweight Directory Access Protocol and the Lightweight Directory Access Protocol over SSL. Hostname：The hostname or IP address of the LDAP server. Port：The port on which the LDAP server is listening. Port 389 is the default port for the ldap protocol, and port 636 is the default port for the ldaps. Search base：The search base further qualifies the connection to the LDAP server. The search base usually corresponds to the domain name of an organization. For example, the search base could be dc=example,dc=com. Note: If the values in your search base contain spaces, escape them with \"%20\", as in \"dc=example%20corp,dc=com\" You can configure one of four authentication methods to be used when connecting to the LDAP Server with the Authentication method drop-down. Simple Authentication：Simple authentication consists of a Username and Password. Simple authentication is not recommended for production deployments not using the secure ldaps protocol as it sends a clear-text password over the network. Anonymous Authentication：The anonymous authentication uses the server address and search base without further authentication. Digest-MD5：This is an improvement on the CRAM-MD5 authentication method. For more information, see RFC-2831. CRAM-MD5：The Challenge-Response Authentication Method (CRAM) is based on the HMAC-MD5 MAC algorithm. In this authentication method, the server sends a challenge string to the client. The client responds with a username followed by a Hex digest that the server compares to an expected value. For more information, see RFC-2195.For a full discussion of LDAP authentication approaches, see RFC-2829 and RFC-2251. SASL Realm：The Simple Authentication and Security Layer (SASL) realm used to connect to the LDAP server. It is only available if the authentication method is Digest-MD5 or CRAM-MD5. Username or DN：Username or DN (Distinguished Name) of an LDAP user with read access to all necessary users and groups. It is used to connect to the LDAP server. Password：Password for the Username or DN configured above. Base DN：Corresponds to the collection of distinguished names used as the base for user entries. This DN is relative to the Search Base. For example, if your users are all contained in ou=users,dc=sonatype,dc=com and you specified a Search Base of dc=sonatype,dc=com, you use a value of ou=users. User subtree：Check the box if True. Uncheck if False. Values are true if there is a tree below the Base DN that can contain user entries and false if all users are contain within the specified Base DN. For example, if all users are in ou=users,dc=sonatype,dc=com this field should be False. If users can appear in organizational units within organizational units such as ou=development,ou=users,dc=sonatype,dc=com, this field should be True . Object class：This value is a standard object class defined in RFC-2798. It specifies the object class for users. Common values are inetOrgPerson, person, user, or posixAccount. User filter：This allows you to configure a filter to limit the search for user records. It can be used as a performance improvement. User ID attribute：This is the attribute of the object class specified above, that supplies the identifier for the user from the LDAP server. The repository manager uses this attribute as the User ID value. Real name attribute：This is the attribute of the Object class that supplies the real name of the user. The repository manager uses this attribute when it needs to display the real name of a user similar to usage of the internal First name and Last name attributes. Email attribute：This is the attribute of the Object class that supplies the email address of the user. The repository manager uses this attribute for the Email attribute of the user. It is used for email notifications of the user. Password attribute：It can be used to configure the Object class, which supplies the password (\"userPassword\"). If this field is blank the user will be authenticated against a bind with the LDAP server. The password attribute is optional. When not configured authentication will occur as a bind to the LDAP server. Otherwise this is the attribute of the Object class that supplies the password of the user. The repository manager uses this attribute when it is authenticating a user against an LDAP server. Group Base DN：This field is similar to the Base DN field described for User Element Mapping, but applies to groups instead of users. For example, if your groups were defined under ou=groups,dc=sonatype,dc=com, this field would have a value of ou=groups. Group subtree：This field is similar to the User subtree field described for User Element Mapping, but configures groups instead of users. If all groups are defined under the entry defined in Base DN, set the field to false. If a group can be defined in a tree of organizational units under the Base DN, set the field to true. Group object class：This value in this field is a standard object class defined in RFC-2307. The class is simply a collection of references to unique entries in an LDAP directory and can be used to associate user entries with a group. Examples are groupOfUniqueNames, posixGroup or custom values. Group ID attribute：Specifies the attribute of the object class that specifies the group identifier. If the value of this field corresponds to the ID of a role, members of this group will have the corresponding privileges. Group member attribute：Specifies the attribute of the object class which specifies a member of a group. An example value is uniqueMember. Group member format：This field captures the format of the Group Member Attribute, and is used by the repository manager to extract a username from this attribute. An example values is ${dn} . 3. 分配Nexus管理员的角色\"nx-admin\"给LDAP上的一个用户，作为nexus新的管理员。然后将admin用户禁用。 参考链接 https://help.sonatype.com/repomanager3/security/ldap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-Grafana对接LDAP.html":{"url":"origin/ldap-Grafana对接LDAP.html","title":"Grafana","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、操作 1、修改/etc/grafana/grafana.ini .............省略............. [auth.ldap] enabled = true config_file = /etc/grafana/ldap.toml allow_sign_up = true .............省略............. 2、修改/etc/grafana/ldap.toml .............省略............. # To troubleshoot and get more log info enable ldap debug logging in grafana.ini # [log] # filters = ldap:debug [[servers]] # Ldap server host (specify multiple hosts space separated) host = \"openldap-service.openldap.svc\" # Default port is 389 or 636 if use_ssl = true port = 389 # Set to true if ldap server supports TLS use_ssl = false # Set to true if connect ldap server with STARTTLS pattern (create connection in insecure, then upgrade to secure connection with TLS) start_tls = false # set to true if you want to skip ssl cert validation ssl_skip_verify = false # set to the path to your root CA certificate or leave unset to use system defaults # root_ca_cert = \"/path/to/certificate.crt\" # Authentication against LDAP servers requiring client certificates # client_cert = \"/path/to/client.crt\" # client_key = \"/path/to/client.key\" # Search user bind dn bind_dn = \"cn=admin,dc=curiouser,dc=com\" # Search user bind password，If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" bind_password = '*********' # User search filter, for example \"(cn=%s)\" or \"(sAMAccountName=%s)\" or \"(uid=%s)\" search_filter = \"(&(memberOf=cn=grafana,ou=applications,dc=curiouser,dc=com))\" # An array of base dns to search through search_base_dns = [\"ou=employee,dc=curiouser,dc=com\"] ## For Posix or LDAP setups that does not support member_of attribute you can define the below settings。Please check grafana LDAP docs for examples # group_search_filter = \"(&(objectClass=posixGroup)(memberUid=%s))\" # group_search_base_dns = [\"ou=groups,dc=grafana,dc=org\"] # group_search_filter_user_attribute = \"uid\" # Specify names of the ldap attributes your ldap uses [servers.attributes] name = \"sn\" username = \"cn\" member_of = \"memberOf\" email = \"mail\" # Map ldap groups to grafana org roles [[servers.group_mappings]] #group_dn = \"cn=admins,dc=grafana,dc=org\" #org_role = \"Admin\" # To make user an instance admin (Grafana Admin) uncomment line below # grafana_admin = true # The Grafana organization database id, optional, if left out the default org (id 1) will be used # org_id = 1 [[servers.group_mappings]] group_dn = \"cn=users,dc=grafana,dc=org\" org_role = \"Editor\" [[servers.group_mappings]] # If you want to match all (or no ldap groups) then you can use wildcard group_dn = \"*\" #org_role = \"Viewer\" .............省略............. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-jira接LDAP.html":{"url":"origin/ldap-jira接LDAP.html","title":"Jira","keywords":"","body":"Jira对接LDAP 一、上下文 二、Jira配置LDAP 三、测试LDAP用户 四、同步用户 五、将LDAP用户添加至管理员用户组 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jira-部署.html":{"url":"origin/jira-部署.html","title":"Jira的部署","keywords":"","body":"Jira的安装与部署 一、简介 镜像：https://hub.docker.com/r/atlassian/jira-software 第三方镜像：https://hub.docker.com/r/blacklabelops/jira 破解文件：atlassian-extras-3.2.jar 什么是JIRA? JIRA 是目前比较流行的基于Java架构的管理系统，由于Atlassian公司对很多开源项目实行免费提供缺陷跟踪服务，因此在开源领域，其认知度比其他的产品要高得多，而且易用性也好一些。同时，开源则是其另一特色，在用户购买其软件的同时，也就将源代码也购置进来，方便做二次开发。JIRA功能全面，界面友好，安装简单，配置灵活，权限管理以及可扩展性方面都十分出色。 JIRA的主要功能 问题追踪和管理：用它管理项目，跟踪任务、bug、需求，通过jira的邮件通知功能进行协作通知，在实际工作中使工作效率提高很多 问题跟进情况的分析报告：可以随时了解问题和项目的进展情况 项目类别管理功能：可以将相关的项目分组管理 组件/模块负责人功能：可以将项目的不同组件/模块指派相应的负责人，来处理所负责的组件的Issues 项目email地址功能：每个项目可以有不同的email（该项目的通知邮件从该地址发出） 无限制的工作流：可以创建多个工作流为不同的项目使用 JIRA的主要特点 JIRA的优点 用它管理项目，跟踪任务、bug，通过JIRA的邮件通知功能进行协作通知，在实际工作中使工作效率提高很多，效果非常不错！安全性、可扩展性方面发挥到了极致！ JIRA不仅仅是一个缺陷跟踪系统，通过Jira，可以整合客户、开发人员、测试人员，各人各司其职，信息很快得到交流和反馈，让大家感到软件开发在顺利快速的进行，朝意想的目标迈进。eclipse和IDEA下的Jira插件，主要为开发人员服务，实时将信息反馈给开发人员，开发人员同时迅速地将修复的结果信息反馈到跟踪系统中，最后通过持续集成，软件迅速地完成了更新，这些方便便捷的操作会极大地鼓舞软件开发中的各方人员，甚至包括客户，及时响应，相信是每一个客户都会欣赏的。 跟同类软件产品TestTracker、ClearQuest、TestDirector相比，JIRA的性价比最好！ JIRA的缺点 对于测试需求、测试用例等都没有提供直接的方式进行管理。不过可以利用JIRA的Issue Type的可定制性,来进行需求和测试用例方面的管理,也可以与Testlink集成。 相关版本 JIRA 4.0版本之后，不再按照功能区分版本。取消了以前的标准版，专业版，企业版之分；取而代之的是按照用户数量来划分：25、50、100、无限制用户。 所有的版本都具有之前企业版的功能！JIRA不限制创建项目数和Issue的数量，购买之后可以永久使用；并且一年内免费更新版本。 二、安装 0. 拉去镜像 docker pull docker.io/atlassian/jira-software:8.2.0 1. 部署PostgreSQL 省略 2. 创建破解文件的ConfigMap oc create configmap crack-jar --from-file=atlassian-extras-3.2.jar --from-literal=text=./atlassian-extras-3.2.jar 3. 创建其他资源 创建PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jira-data namespace: jira spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 创建ServiceAccount oc create serviceaccount jira 创建RBAC相关资源 4. OKD部署Deployment声明文件 apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: jira name: jira namespace: jira spec: replicas: 1 selector: app: jira deploymentconfig: jira strategy: type: Recreate template: metadata: labels: app: jira deploymentconfig: jira spec: containers: - env: - name: JVM_MINIMUM_MEMORY value: 800m - name: JVM_MAXIMUM_MEMORY value: 1024m - name: TZ value: Asia/Shanghai image: docker.io/atlassian/jira-software:8.2.0 imagePullPolicy: IfNotPresent name: jira ports: - containerPort: 8080 protocol: TCP resources: limits: cpu: '1' memory: 1500Mi requests: cpu: 500m memory: 500Mi readinessProbe: failureThreshold: 3 initialDelaySeconds: 40 periodSeconds: 30 successThreshold: 1 tcpSocket: port: 8080 timeoutSeconds: 2 livenessProbe: failureThreshold: 3 initialDelaySeconds: 40 periodSeconds: 30 successThreshold: 1 tcpSocket: port: 8080 timeoutSeconds: 1 volumeMounts: - mountPath: /var/atlassian/application-data/jira name: jira-data - mountPath: /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/atlassian-extras-3.2.jar name: crack-jar readOnly: true subPath: atlassian-extras-3.2.jar dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: jira serviceAccountName: jira terminationGracePeriodSeconds: 30 volumes: - name: jira-data persistentVolumeClaim: claimName: jira-data - configMap: defaultMode: 420 name: crack-jar name: crack-jar 5. 创建Service并创建HTTP访问Route oc expose dc jira --port=8080 oc expose service jira --name=jira --port=8080 --hostname=jira.apps.okd311.curiouser.com 6. 页面配置 配置页面语言-->选择手动配置 配置数据库 设置应用程序的属性 申请试用License 7. 配置管理员用户 8. 配置SMTP邮箱通知 9. 查看许可证 三、配置LDAP 见链接: Jira接LDAP 四、项目示例 项目类型 创建示例Scrum敏捷项目 项目的发布 五、问题 1. Unable to create and acquire lock file for jira.home directory '/var/atlassian/application-data/jira 解决：删除jira_home目录下的lock文件（.jira-home.lock），是一个隐藏文件，然后重启jira服务即可。 2. Unable to clean the cache directory: /var/atlassian/application-data/jira/plugins/.osgi-plugins/feli 解决：先停止jira服务，然后删除$JIRA_HOME/plugins/.osgi-plugins/felix/，然后启动jira服务即可 3. There is/are [1] thread(s) in total that are monitored by this Valve and may be stuck. 解决方案：等等就好了 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-19 17:45:44 "},"origin/redmine-backuprestore.html":{"url":"origin/redmine-backuprestore.html","title":"redmine的备份与恢复","keywords":"","body":"Redmine的备份与恢复 一、简介 要备份的数据类型 数据库中的数据 用户上传的附件 二、备份 Redmine backups should include: Database Attachments (stored in the files directory under the installation directory by default) 备份数据库数据 MySQL The mysqldump command can be used to backup the contents of your MySQL database to a text file. For example: /usr/bin/mysqldump -u -p -h > /path/to/backup/db/redmine.sql You can find ,, , and in the file config/database.yml. `` may not be required depending on your installation of the database. PostgreSQL The pg_dump command can be used to backup the contents of a PostgreSQL database to a text file. Here is an example: /usr/bin/pg_dump -U -h -Fc --file=redmine.sqlc You can find ,, and in the file `config/database.yml`. may not be required depending on your installation of the database. The pg_dump command will prompt you to enter the password when necessary. SQLite SQLite databases are all contained in a single file, so you can back them up by copying the file to another location. You can determine the file name of SQLite database by looking at config/database.yml. 备份用户上传的附件 All file uploads are stored in attachments_storage_path (defaults to the files/ directory). You can copy the contents of this directory to another location to easily back it up. WARNING: attachments_storage_path may point to a different directory other than files/. Be sure to check the setting in config/configuration.yml to avoid making a useless backup. 备份脚本 Here is a simple shell script that can be used for daily backups (assuming you're using a MySQL database): # Database /usr/bin/mysqldump -u -p | gzip > /path/to/backup/db/redmine_`date +%Y-%m-%d`.gz # Attachments rsync -a /path/to/redmine/files /path/to/backup/files 三、恢复 恢复数据库 MySQL For example if you have a gziped dump file with the name 2018-07-30.gz, then the database can be restored with the following command: gunzip -c 2018-07-30.gz | mysql -u --password Enter password: PostgreSQL When the option -Fc of the command pg_dump is used like it is at the above example then you need to use the command pg_restore: pg_restore -U -h -d redmine.sqlc otherwise a text file can be restored with psql: psql SQLite Copy the database file from the backup location. 参考 https://www.redmine.org/boards/2/topics/2442?r=18660 https://www.redmine.org/projects/redmine/wiki/RedmineBackupRestore Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/logging-日志系统技术概览简介.html":{"url":"origin/logging-日志系统技术概览简介.html","title":"日志系统技术概览简介","keywords":"","body":"日志系统概览简介 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/logging-日志系统数据在个组件中的流转格式.html":{"url":"origin/logging-日志系统数据在个组件中的流转格式.html","title":"日志系统数据在个组件中的流转格式","keywords":"","body":"原始日志 2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\"business\":\"curiouser\",\"currentTime\":\"2019-09-24 09:12:39.052\",\"data\":\"{\"args\": {\"AuthQueryDTO\": {\"clientId\":\"ppush-platform\",\"clientSecret\":\"Jygv8V4TerC5rDxO\"},},\"result\": {\"expireTime\":-1,\"token\":\"77ff1cd2d1985b6d2d99bd54453bbc5f\",\"type\":\"1\"}}\",\"datatype\":0,\"interface1\":\"com.curiouser.auth.center.controller.ClientApiController\",\"level\":\"INFO\",\"method\":\"serverAuth\",\"module\":\"curiouser-auth-center\",\"reqTime\":8,\"requestId\":\"req-bf1bcc406dfa4d35b9062e06fbad78cd\",\"thread\":\"XNIO-1 task-14\",\"urlPath\":\"/client/server/token\"} This is a test log ! hahaha {\"datatype\":0,\"business\":\"alert\",\"module\":\"alert-rule\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"level\":\"WARN \",\"method\":\"isConnectionAlive\",\"thread\":\"XNIO-1 task-20\",\"requestId\":\"req-498fe711243b444e9b73ed6d5dc20a20\",\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\"} 经过\"原始日志+Filebeat\"处理过的日志 {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"fields\":{\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\",\"CLUSTER\":\"cluster_dev\"}} {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"This is a test log ! hahaha\",\"fields\":{\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"req2fe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer\"流程处理过的日志（内容没变，字段顺序变了，消息顺序变了） {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"This is a test log ! hahaha\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.336Z\",\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cenoller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"r8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.332Z\",\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka \"流程处理过的日志(内容没变，字段顺序变了，消息顺序变了) {\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cencuriouseroller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-cencuriousereqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":0},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"message\":\"This is a test log ! hahaha\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":645},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer\"流程处理过的日志(内容没变，字段顺序变了，消息顺序变了) {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"This is a test log ! hahaha\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer--->elasticsearch\"流程处理过的日志（将logstash-consumer发送的日志数据放在”_source“字段下，同时） # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"s_ZaY20BY8hT6jLicmK4\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 675 }, \"@version\": \"1\", \"data\": { \"business\": \"alert\", \"interface\": \"com.zaxxer.hikari.pool.PoolBase\", \"method\": \"isConnectionAlive\", \"requestId\": \"reqfe711243b444e9b73ed6d5dc20a20\", \"data\": \"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\", \"datatype\": 0, \"thread\": \"XNIO-1 task-20\", \"level\": \"WARN \", \"module\": \"alert-rule\", \"currentTime\": \"2019-09-24 20:50:00,056\" }, \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"1fUgY20BY8hT6jLiJAfF\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"This is a test log ! hahaha\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"@timestamp\": \"2019-09-24T11:34:29.741Z\", \"fields\": { \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"CLUSTER\": \"cluster_dev\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"PROJECTNAME\": \"test\" }, \"@version\": \"1\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 645 } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T11:34:29.741Z\" ] }, \"sort\": [ 1569324869741 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"DOlaY20B_ehr23pid9GM\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf106dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 0 }, \"@version\": \"1\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"ENV\": \"dev\", \"TEMPLATE\": 2019082110, \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } 附录 1、filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /root/logs/test.log exclude_files: [\"/root/logs/_filebeat\", \".gz$\"] recursive_glob.enabled: true setup.template.settings: index.number_of_shards: 3 processors: - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"data\" overwrite_keys: false - drop_fields: fields: [\"agent\", \"tags\", \"input\", \"ecs\"] fields: NAMESPACE: \"test\" PROJECTNAME: \"test\" CLUSTER: cluster_dev ENV: dev CANARY: sit0 TEMPLATE: 2019082110 output.logstash: hosts: [\"localhost:5044\"] #output.file: # path: \"/root/logs/output\" # filename: filebeat.log 2、logstash_producer配置 input { beats { id => \"logstash_producer_input_beats\" port => 5044 } } output { #file{ # path => \"/root/logs/output/logstah-producer.log\" #} kafka { id => \"logstash_producer_output_kafka\" codec => json topic_id => \"logs\" bootstrap_servers => \"localhost:9092\" compression_type => \"snappy\" } } 3、logstash_consumer配置 input { kafka { id => \"logstash_consumer_input_kafka\" bootstrap_servers => \"localhost:9092\" topics => \"logs\" group_id => \"applications_logs_group\" codec => \"json\" auto_offset_reset => \"earliest\" } } output { #file{ # path => \"/root/logs/output/logstah-consumer.log\" #} elasticsearch { id => \"logstash_consumer_output_elasticsearch\" hosts => [\"localhost:9092\"] index=>\"%{[fields][NAMESPACE]}-%{[fields][PROJECTNAME]}-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"logstash-pipeline\" password => \"logstash-pipeline\" } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kafka-origin.html":{"url":"origin/kafka-origin.html","title":"原理","keywords":"","body":"kafka原理 一、数据存储的目录结构 例如一个kafka集群有五个Broker节点，创建一个有2个分区1个副本的Topic主题， Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 20:12:58 "},"origin/logging-kafka基础知识.html":{"url":"origin/logging-kafka基础知识.html","title":"基础知识","keywords":"","body":"一、Kafka 与 Zookeeper 1、Zookeeper在Kafka集群分布式消息中的作用 1.1、选举Controller Kafka是高可用的分布式消息系统，首先要解决的就是资源协调分配和多副本状态维护的问题。解决这些问题通常就是两种思路，一是依靠Zookeeper来协调，二是设定一个中心节点，让这个中心节点来协调。如果依靠Zookeeper来协调，会存在大量的竞争条件，对Zookeeper的访问压力增大，而且如果Zookeeper出现了问题（比如网络抖动），系统很容易出现紊乱。Kafka采用的是第二种思路，即选举一个中心节点来进行资源协调与多副本状态维护，这个中心节点被称作Controller（一个特殊的Broker），这个选举过程依靠Zookeeper来完成。 Broker启动时，会竞争创建临时\"/controller\"。如果创建成功，则成为Controller，并把Broker的id等信息写入这个节点。同时会全程监控\"/controller\"的数据变化，如果旧的Controller挂掉，则开启新一轮的竞争过程。 1.2、注册Broker Kafka要进行资源协调，第一件需要知道的事情就是各个Broker的存活状态，这个问题利用Zookeeper可以很容易做到。 假设某个Broker，id为0，它启动时，会创建\"/brokers/ids/0\"临时节点，并把端口等信息写进去。Controller会监控\"/brokers/ids\"的节点变化，以实时感知各broker的状态，进行资源协调。 1.3、协调topic的创建、调整与销毁 在Kafka这个多副本分区的消息系统里，创建一个topic，至少需要以下3个步骤： 持久化topic的多副本分区信息 为每个分区挑选一个副本leader 将上述信息发送给对应的Broker，以完成实际的日志文件创建过程 Controller的存在，可以很容易完成上面的b和c步骤，但a步骤不行，如果Controller挂掉，则这些信息会不可用。Kafka把这些信息保存在Zookeeper中，依靠其高可用特性来保证这些信息的高可用。假设某个topic名字为mytopic，创建时，其分区信息保存在\"/brokers/topics/mytopic\"中。Controller全程监控\"/brokers/topics\"的孩子节点变动，实时感知这些信息，以完成后续步骤。 创建完成之后，后续往往会有分区调整和topic删除等需求。普通青年可能会觉得这两个问题很简单，给Controller发个相关请求就可以了。事实远非如此！ 拿分区调整来说，假设某分区有三个副本，分别位于Broker-1、Broker-2和Broker-3，leader为1，现在扩容增加了Broker-4、Broker-5、Broker-6，为了平衡机器间压力，需要将副本1 2 3移到4 5 6，至少经历以下步骤： 修改该分区的副本信息为1 2 3 4 5 6，leader为1 等待4 5 6副本追赶1 2 3的进度直至大家都同步(in sync) 从4 5 6中挑选一个新的副本leader，假设为4 修改该分区的副本信息为4 5 6，leader为4 以上每个步骤都有可能失败，如何才能保证这次调整顺利进行呢？ 首先，我们不能直接修改该分区的副本信息为 4 5 6，原因很简单，需要等待4 5 6的追赶过程以便产生新leader。其次，操作未完全成功的命令需要保存下来，如果操作过程中，Controller挂掉，则新的Controller可以从头开始直至成功。Kafka怎么做的呢？ 通常是Admin控制台）把调整命令写入\"/admin/reassign_partitions\"节点 Controller监控\"/admin/reassign_partitions\"，拿到调整命令，执行上述步骤 如果操作成功则删除该节点；如果Controller挂掉，新的Controller还会拿到这个命令并从头开始执行 当然，这里一次只能有一个调整命令，但一个调整命令可以同时调整多个topic的多个分区。 在这个过程中，Zookeeper的作用是：持久化操作命令并实时通知操作者，是不是只有Zookeeper可以做这个事情呢，不是，但Zookeeper可以做得很好，保证命令高可用。 类似的操作还有topic删除，副本的leader变更等，都是沿用上面的套路。 1.4. 保存topic级别和client级别的配置信息 Broker的集群中有全局配置信息，但如果想针对某个topic或者某个client进行配置呢，Kafka把这些信息保存在Zookeeper中，各个Broker实时监控以更新。 1.5、脑裂问题 脑裂问题是指，在一个设有中心节点的系统中，出现了两个中心节点。两个中心同时传达命令，自然会造成系统的紊乱。 Kafka利用Zookeeper所做的第一件也是至关重要的一件事情是选举Controller，那么自然就有疑问，有没有可能产生两个Controller呢？ 首先，Zookeeper也是有leader的，它有没有可能产生两个leader呢？答案是不会。 quorum机制可以保证，不可能同时存在两个leader获得大多数支持。假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。 Kafka的Controller也采用了epoch，具体机制如下: 所有Broker监控\"/controller\"，节点被删除则开启新一轮选举，节点变化则获取新的epoch Controller会注册SessionExpiredListener，一旦因为网络问题导致Session失效，则自动丧失Controller身份，重新参与选举 收到Controller的请求，如果其epoch小于现在已知的controller_epoch，则直接拒绝 理论上来说，如果Controller的SessionExpired处理成功，则可以避免双leader，但假设SessionExpire处理意外失效的情况：旧Controller假死，新的Controller创建。旧Controller复活，SessionExpired处理意外失效，仍然认为自己是leader。 这时虽然有两个leader，但没有关系，leader只会发信息给存活的broker（仍然与Zookeeper在Session内的），而这些存活的broker则肯定能感知到新leader的存在，旧leader的请求会被拒绝。 1.6、如果Zookeeper挂了会怎样 每个Broker有一个metaDataCache，缓存有topic和partition的基本信息，可以正常的生产和消费信息，但不能进行topic的创建、调整和删除等操作。 此外，Broker会不断重试连接。 1.7、Zookeeper用量估计 假设Broker数目为B，topic数目为T，所有topic总partition数目为P，Client数目为C，以下数值均为峰值： qps: 100以内 连接数: B watcher数目：3 * B + 2 * T + 6 Zookeeper节点数（叶子节点）: B + P + T + C + 8 2、kafka注册到zookeeper中的数据存储结构 Zookeeper路径的创建者与监听者 路径 创建者 监听者 类型 /controller 各个broker竞争创建 所有broker全程监控data change 临时节点 /controller_epoch controller 无 永久节点 /brokers/ids broker启动时检查并确保存在 controller全程监控child change 永久节点 /brokers/ids/{id} id对应的broker 无 临时节点 /brokers/topics broker启动时检查确保存在 controller全程监控child change 永久节点 /brokers/topics/{topic} controller收到创建请求，或者broker启用自动创建topic时，或admin工具 controller全程监控data change 永久节点 /brokers/topics/{topic}/{partition}/state partiton的leader partition reassign时，controller临时监控data change 永久节点 /config/changes broker启动时检查并确保存在 所有broker全程监控child change 永久节点 /config/topics broker启动时检查并确保存在 无 永久节点 /config/clients broker启动时检查并确保存在 无 永久节点 /brokers/seqid broker启动时检查并确保存在 待确认 永久节点 /admin/delete_topics broker启动时检查并确保存在 controller全程监控child change 永久节点 /isr_change_notification broker启动时检查并确保存在 controller全程监控child change 永久节点 /admin/reassign_partitions admin 工具 controller全程监控data change 永久节点，reassign结束后会删除 /admin/preferred_replica_election admin 工具 controller全程监控data change 永久节点，replica election结束后会删除 2.1、Topic注册信息 /brokers/topics/[topic] 存储某个topic的partitions所有分配信息 { \"version\": \"版本编号目前固定为数字1\", \"partitions\": { \"partitionId编号\": [ 同步副本组brokerId列表 ], \"partitionId编号\": [ 同步副本组brokerId列表 ], ....... } } 2.2、Partition状态信息 /brokers/topics/[topic]/partitions/[partition-Id]/state Schema: { \"controller_epoch\": 表示kafka集群中的中央控制器选举次数, \"leader\": 表示该partition选举leader的brokerId, \"version\": 版本编号默认为1, \"leader_epoch\": 该partition leader选举次数, \"isr\": [同步副本组brokerId列表] } 2.3、Broker注册信息 /brokers/ids/[0...N] 每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL) Schema: { \"jmx_port\": jmx端口号, \"timestamp\": kafka broker初始启动时的时间戳, \"host\": 主机名或ip地址, \"version\": 版本编号默认为1, \"port\": kafka broker的服务端端口号,由server.properties中参数port确定 } 2.4、Controller epoch /controller_epoch -> int (epoch) 此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1 2.5、Controller注册信息 /controller -> int (broker id of the controller) 存储center controller中央控制器所在kafka broker的信息 { \"version\": 版本编号默认为1, \"brokerid\": kafka集群中broker唯一编号, \"timestamp\": kafka broker中央控制器变更时的时间戳 } 2.6、Consumer注册信息 /consumers/[groupId]/ids/[consumerIdString] 每个consumer都有一个唯一的ID(consumerId可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.这是一个临时的znode,此节点的值为请看consumerIdString产生规则,即表示此consumer目前所消费的topic + partitions列表. Schema: { \"version\": 版本编号默认为1, \"subscription\": { //订阅topic列表 \"topic名称\": consumer中topic消费者线程数 }, \"pattern\": \"static\", \"timestamp\": \"consumer启动时的时间戳\" } 2.7、Consumer offset /consumers/[groupId]/offsets/[topic]/[partitionId] -> long (offset) 用来跟踪每个consumer目前所消费的partition中最大的offset.此znode为持久节点,可以看出offset跟group_id有关,以表明当消费者组(consumer group)中一个消费者失效,重新触发balance,其他consumer可以继续消费 2.8、admin管理信息 二、Kafka中的消费者与消费者组 1、消费者组里面的消费者消费Topic Partition的消息时流程 每个consumer客户端被创建时,会向zookeeper注册自己的信息.主要是为了\"负载均衡\" 同一个Consumer Group中的Consumers，Kafka将相应Topic中的每个消息只发送给其中一个Consumer。 Consumer Group中的每个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer； 一个Consumer group的多个consumer的所有线程依次有序地消费一个topic的所有partitions,如果Consumer group中所有consumer总线程大于partitions数量，则会出现空闲情况 举例说明： kafka集群中创建一个topic为report-log，4个partitions 索引编号为0,1,2,3。假如有目前有三个消费者node（注意：一个consumer中一个消费线程可以消费一个或多个partition） 如果每个consumer创建一个consumer thread线程,各个node消费情况如下，node1消费索引编号为0,1分区，node2费索引编号为2,node3费索引编号为3 如果每个consumer创建2个consumer thread线程，各个node消费情况如下(是从consumer node先后启动状态来确定的)，node1消费索引编号为0,1分区；node2费索引编号为2,3；node3为空闲状态 总结：从以上可知，Consumer Group中各个consumer是根据先后启动的顺序有序消费一个topic的所有partitions的。如果Consumer Group中所有consumer的总线程数大于partitions数量，则可能consumer thread或consumer会出现空闲状态。 2、Consumer均衡算法 当一个group中,有consumer加入或者离开时,会触发partitions均衡(均衡的最终目的,是提升topic的并发消费能力) 假如topic1,具有如下partitions: P0,P1,P2,P3 加入group中,有如下consumer: C0,C1 首先根据partition索引号对partitions进行排序，假设排序: P0,P1,P2,P3 根据(consumer.id + '-'+ thread序号)对消费者进行排序,假设排序: C0,C1 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i M),P((i + 1) M -1)] 3、Consumer启动流程 首先进行\"Consumer Id注册\"; 然后在\"Consumer id 注册\"节点下注册一个watch用来监听当前group中其他consumer的\"退出\"和\"加入\";只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions). 在\"Broker id 注册\"节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance. 三、kafka的版本 Kafka版本规则 在Kafka 1.0.0之前基本遵循4位版本号，比如Kafka 0.8.2.0、Kafka 0.11.0.3等。而从1.0.0开始Kafka就告别了4位版本号，遵循 Major.Minor.Patch 的版本规则，其中Major表示大版本，通常是一些重大改变，因此彼此之间功能可能会不兼容；Minor表示小版本，通常是一些新功能的增加；最后Patch表示修订版，主要为修复一些重点Bug而发布的版本。比如Kafka 2.1.1，大版本就是2，小版本是1，Patch版本为1，是为修复Bug发布的第1个版本。 Kafka版本演进 Kafka总共发布了7个大版本，分别是0.7.x、0.8.x、0.9.x、0.10.x、0.11.x、1.x及2.x版本。截止目前，最新版本是Kafka 2.6.0.，也是最新稳定版本 kafka的offset保存位置分为两种情况 0.9.0.0版本之前默认保存在zookeeper当中 ，0.9.0.0版本之后保存在broker对应的topic当中 参考链接 http://blog.csdn.net/lizhitao/article/details/23744675 https://www.jianshu.com/p/5bef1f9f74cd Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-17 18:16:20 "},"origin/logging-kafka常用操作.html":{"url":"origin/logging-kafka常用操作.html","title":"kafka常用操作","keywords":"","body":"Apache Kafka常用操作 一、Topic管理 1. 列出所有Topic kafka-topics.sh --zookeeper 127.0.0.1:2181 --list kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list 2. 创建一个topic kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 2 --partitions 3 --topic Test #--replication-factor参数指定Topic的数据副本个数 #--partitions参数指定Topic的分区个数 3. 删除Topic kafka-topics.sh --delete --zookeeper 127.0.0.1:2181 --topic Test 或者 #只会删除zookeeper中的元数据，消息文件须手动删除 kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper 127.0.0.1:2181 --topic Test 4. 查看Topic的详细信息 kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic Test Topic:Test PartitionCount:2 ReplicationFactor:1 Configs: Topic: Test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: Test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 #第一行，列出了topic的名称，分区数(PartitionCount),副本数(ReplicationFactor)以及其他的配置(Config.s) #Leader:1 表示为做为读写的broker的节点编号 #Replicas:表示该topic的每个分区在那些borker中保存 #Isr:表示当前有效的broker, Isr是Replicas的子集 5. 增加Topic分区个数（只能增加扩容） kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --partitions 2 6. 给Topic增加配置 kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --config flush.messages=1 7. 删除Topic的配置 kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --delete-config flush.messages=1 8. 查看消费者组 kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --list 9. 查看Topic各个分区的消息偏移量最大（小）值 kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 127.0.0.1:9092 --time -1 --topic Test # time为-1时表示最大值，time为-2时表示最小值 10. 查看Topic中指定consumer组内消息消费的offset kafka的offset保存位置分为两种情况 0.9.0.0版本之前默认保存在zookeeper当中 ，0.9.0.0版本之后保存在broker对应的topic当中 kafka-consumer-offset-checker.sh --zookeeper 127.0.0.1:2181 --group logstash-group --topic Test GROUP TOPIC PID OFFSET LOGSIZE LAG Ower 消费者组 话题id 分区id 当前已消费的条数 总条数 未消费的条数 所有者 console-consumer-98995 Test 0 112 318084 317972 none console-consumer-98995 Test 1 -1 318088 unknown none 方式二： $ kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --offsets --group Group-Name TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID basic_log_k8s 1 127509 333334 205825 logstash-0-490058d7-154f-4111-b514-57de254ecae8 /192.168.3.72 logstash-0 basic_log_k8s 0 127317 333333 206016 logstash-0-2bd85bcc-282e-41a0-a9c3-6d0dbefd547f /192.168.0.40 logstash-0 11. 修改指定消费者分组对应topic的offset 第一种情况offset信息保存在topic中 $ bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 #参数解析： #--bootstrap-server 代表你的kafka集群 你的offset保存在topic中 #--group 代表你的消费者分组 #--topic 代表你消费的主题 #--execute 代表支持复位偏移 #--reset-offsets 代表要进行偏移操作 #--to-offset 代表你要偏移到哪个位置 是long类型数值，只能比前面查询出来的小 #还有其他的--to- ** 方式可以自己验证 本人验证过--to-datetime 没有成功 第二种方式offset信息保存在zookeeper当中 $ bin/kafka-consumer-groups.sh --zookeeper kafka_zk1:2181 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 12. 修改topic副本因子数 官方文档：https://kafka.apache.org/21/documentation.html#replication ① 先查看Topic的信息 $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 ② 准备JSON文件 { \"version\": 1, \"partitions\": [ { \"topic\": \"test\", \"partition\": 0, \"replicas\": [2, 1, 3] }, { \"topic\": \"test\", \"partition\": 1, \"replicas\": [3, 2, 1] }] } ③ kafka-reassign-partitions命令增加topic分区副本数 $ kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file replication.json --execute Current partition replica assignment {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3]}]} Save this to use as the --reassignment-json-file option during rollback Successfully started reassignment of partitions {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2,1,3]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3,2,1]}]} ④ 使用verify参数来检查副本数据是否复制分配完成 $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] is still in progress Reassignment of partition [test,1] is still in progress $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] completed successfully Reassignment of partition [test,1] completed successfully $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:iteblog PartitionCount:2 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Topic: test Partition: 1 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 13. 均衡Topic分区到新增Broker节点 重新分配官方文档地址：http://kafka.apache.org/documentation/#basic_ops_cluster_expansion 翻译官方文档中文地址：http://orchome.com/36 参考文章：https://blog.csdn.net/forrest_ou/article/details/79141391 ① 确定要重启分配分区的主题，新建topics-to-move.json json文件 { \"topics\": [ {\"topic\": \"foo1\"}, {\"topic\": \"foo2\"} ], \"version\":1 } // foo1 foo2 为要重新分配的主题 ② 使用 bin/kafka-reassign-partitions.sh重新分配工具生成分配规则的json语句分配到 5，6机器 kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list \"5,6\" –generate ③ 有分配规则的json语句输出到控制台，复制到新建的json文件expand-cluster-reassignment.json中，例如： {\"version\":1, \"partitions\":[{\"topic\":\"foo1\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":2,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":2,\"replicas\":[5,6]}] } //描述分配之后分区的分布情况 ④ 执行命令，开始分区重新分配 kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –execute ⑤ 验证是否完成 kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –verify //当输出全部都是completed successfully表明移动已经完成. 注意 kafka新建主题时的分区分配策略：随机选取第一个分区节点，然后往后依次增加。例如第一个分区选取为1，第二个分区就 是2，第三个分区就是3. 1，2，3是brokerid。不会负载均衡，所以要手动重新分配分区操作，尽量均衡。 在生产的同时进行数据迁移会出现重复数据。所以迁移的时候避免重复生产数据，应该停止迁移主题的生产。同时消费不会，同时消费之后出现短暂的leader报错，会自动恢复。 新增了broker节点，如果有主题的分区在新增加的节点上，生产和消费的客户端都应该在hosts配置文件中增加新增的broker节点，否则无法生产消费，但是也不报错。 可以不需要第一步和第二步，自己手动新建分配的json文件 14. 查询Topic不可用的分区 kafka-topics.sh --describe --unavailable-partitions --zookeeper localhost:2181 二、其他 1. 自带测试生产者 kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic Test 2. 自带测试消费者 kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --from-beginning --topic Test 3. 自带性能测试 位于bin/kafka-producer-perf-test.sh.主要参数有以下: messages 生产者发送总的消息数量 message-size 每条消息大小（单位为b） batch-size 每次批量发送消息的数量 topics 生产者发送的topic threads 生产者使用几个线程同时发送 例如 kafka-producer-perf-test.sh --messages 100000 --message-size 1000 --batch-size 10000 --topics test --threads 4 --broker-list 127.0.0.1:9092 start.time, end.time, compression, message.size, batch.size, total.data.sent.in.MB, MB.sec, total.data.sent.in.nMsg, nMsg.sec 2015-10-15 18:56:27:542, 2015-10-15 18:56:30:880, 0, 1000, 10000, 95.37, 28.5702, 100000, 29958.0587 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kafka-client-connect-procedure-tools.html":{"url":"origin/kafka-client-connect-procedure-tools.html","title":"kafka连接调试脚本","keywords":"","body":"kafka连接测试工具 一、简介 为了测试kafka客户端连接k8s上Kafka Bootstrap返回的信息，有一个Python脚本可显示Broker地址，并产生测试数据验证生产消费是否正常 GItHub：https://github.com/rmoff/kafka-listeners/blob/master/python/python_kafka_test_client.py 二、脚本使用 Python代码 from confluent_kafka.admin import AdminClient from confluent_kafka import Consumer from confluent_kafka import Producer from sys import argv from datetime import datetime topic='test_topic' def Produce(source_data): print('\\n') p = Producer({'bootstrap.servers': bootstrap_server}) def delivery_report(err, msg): \"\"\" Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). \"\"\" if err is not None: print('❌ Message delivery failed: {}'.format(err)) else: print('✅ 📬 Message delivered: \"{}\" to {} [partition {}]'.format(msg.value().decode('utf-8'),msg.topic(), msg.partition())) for data in source_data: p.poll(0) p.produce(topic, data.encode('utf-8'), callback=delivery_report) r=p.flush(timeout=5) if r>0: print('❌ Message delivery failed ({} message(s) still remain, did we timeout sending perhaps?)\\n'.format(r)) def Consume(): print('\\n') c = Consumer({ 'bootstrap.servers': bootstrap_server, 'group.id': 'rmoff', 'auto.offset.reset': 'earliest' }) c.subscribe([topic]) try: msgs = c.consume(num_messages=1,timeout=30) if len(msgs)==0: print(\"❌ No message(s) consumed (maybe we timed out waiting?)\\n\") else: for msg in msgs: print('✅ 💌 Message received: \"{}\" from topic {}\\n'.format(msg.value().decode('utf-8'),msg.topic())) except Exception as e: print(\"❌ Consumer error: {}\\n\".format(e)) c.close() try: bs=argv[1] print('\\n🥾 bootstrap server: {}'.format(bs)) bootstrap_server=bs except: # no bs X-D bootstrap_server='localhost:9092' print('⚠️ No bootstrap server defined, defaulting to {}\\n'.format(bootstrap_server)) a = AdminClient({'bootstrap.servers': bootstrap_server}) try: md=a.list_topics(timeout=10) print(\"\"\" ✅ Connected to bootstrap server(%s) and it returned metadata for brokers as follows: %s --------------------- ℹ️ This step just confirms that the bootstrap connection was successful. ℹ️ For the consumer to work your client will also need to be able to resolve the broker(s) returned in the metadata above. ℹ️ If the host(s) shown are not accessible from where your client is running you need to change your advertised.listener configuration on the Kafka broker(s). \"\"\" % (bootstrap_server,md.brokers)) try: Produce(['foo / ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S')]) Consume() except: print(\"❌ (uncaught exception in produce/consume)\") except Exception as e: print(\"\"\" ❌ Failed to connect to bootstrap server. 👉 %s ℹ️ Check that Kafka is running, and that the bootstrap server you've provided (%s) is reachable from your client \"\"\" % (e,bootstrap_server)) 安装脚本依赖 python3 -m pip install -r requirements.txt 测试命令 python3 python_kafka_test_client.py localhost:9092 测试输出，显示了kafka bootstrap返回给客户端的broker连接地址 🥾 bootstrap server: localhost:9092 ✅ Connected to bootstrap server(localhost:9092) and it returned metadata for brokers as follows: {0: BrokerMetadata(0, curiouser:9092)} --------------------- ℹ️ This step just confirms that the bootstrap connection was successful. ℹ️ For the consumer to work your client will also need to be able to resolve the broker(s) returned in the metadata above. ℹ️ If the host(s) shown are not accessible from where your client is running you need to change your advertised.listener configuration on the Kafka broker(s). ✅ 📬 Message delivered: \"foo / 2020-12-23 18:19:24\" to test_topic [partition 0] ✅ 💌 Message received: \"foo / 2020-12-23 18:19:24\" from topic test_topic 参考： https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-25 14:10:42 "},"origin/zk-kafka-ui.html":{"url":"origin/zk-kafka-ui.html","title":"Zookeeper和kafka的WebUI工具","keywords":"","body":"ZK与Kafka的WebUI工具 一、ZK的WebUI管理工具 功能： 权限隔离，使用admin用户登录后方可操作zk 连接数据不保存在服务端，而是存在浏览器cookie中 可对ZK进行增删改查 GitHub地址：https://github.com/qiuxiafei/zk-web Docker镜像地址：https://hub.docker.com/r/tobilg/zookeeper-webui Docker部署 docker run -d \\ -p 8080:8080 \\ -e USER=admin \\ -e PASSWORD=12356789 \\ --name zookeeper-web-ui \\ -t tobilg/zookeeper-webui k8s部署 kubectl -n tools run zookeeper-web-ui --restart='Always' --env=\"USER=admin\" --env=\"PASSWORD=12356789\" --image tobilg/zookeeper-webui kubectl -n tools expose deployment zookeeper-web-ui --port=80 --target-port=8080 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-18 16:39:15 "},"origin/kafka-shell-kaf.html":{"url":"origin/kafka-shell-kaf.html","title":"命令行Kafka工具","keywords":"","body":"kafka命令行工具kaf 一、简介 如果是使用kafka原生bin目录下的二进制命令的话，每一次命令要打好多参数，参数还无法自动补全，甚是麻烦，而GitHub中有个项目，可以像docker、kubectl命令一样，快速操作kafka。 GitHub地址：https://github.com/birdayz/kaf 二、安装配置 1、安装 Go go get -u github.com/birdayz/kaf/cmd/kaf 二进制 直接在GitHub的releases页面下载对应操作系统的二进制文件到可执行路径下 2、配置 ①命令行参数 Kafka Command Line utility for cluster management Usage: kaf [command] Available Commands: completion Generate bash completion script for bash or zsh config Handle kaf configuration consume Consume messages group Display information about consumer groups. groups List groups help Help about any command node Describe and List nodes nodes List nodes in a cluster produce Produce record. Reads data from stdin. query Query topic by key topic Create and describe topics. topics List topics Flags: -b, --brokers strings Comma separated list of broker ip:port pairs -c, --cluster string set a temporary current cluster --config string config file (default is $HOME/.kaf/config) -h, --help help for kaf --schema-registry string URL to a Confluent schema registry. Used for attempting to decode Avro-encoded messages -v, --verbose Whether to turn on sarama logging Use \"kaf [command] --help\" for more information about a command. ②命令行补全 Bash Linux kaf completion bash > /etc/bash_completion.d/kaf Bash MacOS kaf completion bash > /usr/local/etc/bash_completion.d/kaf Zsh kaf completion zsh > \"${fpath[1]}/_kaf\" Fish kaf completion fish > ~/.config/fish/completions/kaf.fish Powershell Invoke-Expression (@(kaf completion powershell) -replace \" ''\\)$\",\" ' ')\" -join \"`n\") 3、使用 ①配置kafka连接 kaf config add-cluster local -b localhost:9092 连接配置会写在~/.kaf/config文件中 ②选择对应的kafka连接 kaf config select-cluster ③列出kafka broker节点的详细信息 kaf node ls ④列出所有的Topic及其分区、副本信息 kaf topics ⑤列出指定Topic的详细信息 kaf topics describe test_topic ⑥列出所有的消费者组 kaf groups ⑦列出指定消费者组的详细信息 kafa group describe dispatcher ⑧从标准输入写消息到指定Topic echo test | kaf produce test_topic ⑨消费指定Topic中的消息 kaf consume test_topic -f Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-11 14:40:03 "},"origin/filebeat-简介安装配置.html":{"url":"origin/filebeat-简介安装配置.html","title":"简介安装配置","keywords":"","body":"Filebeat的简介、安装、配置、Pipeline 一. 简介 Filebeat由两个主要组件组成： Inputs： 负责管理harvester并找到所有要读取的文件来源。如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。每个Inputs都在自己的Go协程中运行 每个prospector类型可以定义多次 Harvesters： 一个harvester负责读取一个单个文件的内容，每个文件启动一个harvester。harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会续读这个文件。这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。默认情况下，Filebeat保存文件打开的状态直到close_inactive到达。 关闭harvester会产生以下结果： 如果在harvester仍在读取文件时文件被删除，则关闭文件句柄，释放底层资源。 文件的采集只会在scan_frequency过后重新开始 如果在harvester关闭的情况下移动或移除文件，则不会继续处理文件 二. 安装 默认的安装文件路径 Type Description Default Location Config Option home Home of the Filebeat installation. path.home bin The location for the binary files. {path.home}/bin config The location for configuration files. {path.home} path.config data The location for persistent data files. {path.home}/data path.data logs The location for the logs created by Filebeat. {path.home}/logs path.logs YUM/RPM [elastic-7.x] name=Elastic repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install filebeat-7.4.0 RPM下载地址：https://www.elastic.co/cn/downloads/beats/filebeat yum localinstall -y filebeat-7*.rpm 安装文件路径 Type Description Location home Home of the Filebeat installation. /usr/share/filebeat bin The location for the binary files. /usr/share/filebeat/bin config The location for configuration files. /etc/filebeat data The location for persistent data files. /var/lib/filebeat logs The location for the logs created by Filebeat. /var/log/filebeat 二进制文件 zip, tar.gz, tgz 压缩格式的二进制安装包，下载地址：https://www.elastic.co/cn/downloads/beats/filebeat 安装文件路径 Type Description Location home Home of the Filebeat installation. {extract.path} bin The location for the binary files. {extract.path} config The location for configuration files. {extract.path} data The location for persistent data files. {extract.path}/data logs The location for the logs created by Filebeat. {extract.path}/logs Filebeat命令行启动 /usr/share/filebeat/bin/filebeat Commands SUBCOMMAND [FLAGS] Commands 描述 export 导出配置到控制台，包括index template, ILM policy, dashboard help 显示帮助文档 keystore 管理secrets keystore. modules 管理配置Modules run Runs Filebeat. This command is used by default if you start Filebeat without specifying a command. setup 设置初始环境。包括index template, ILM policy, write alias, Kibana dashboards (when available), machine learning jobs (when available). test 测试配置文件 version 显示版本信息 Global Flags 描述 -E \"SETTING_NAME=VALUE\" 覆盖配置文件中的配置项 --M \"VAR_NAME=VALUE\" 覆盖Module配置文件的中配置项 -c FILE 指定filebeat的配置文件路径。路径要相对于`path.config -d SELECTORS -e --path.config --path.data --path.home --path.logs --strict.perms 示例： /usr/share/filebeat/bin/filebeat --modules mysql -M \"mysql.slowlog.var.paths=[/root/slow.log]\" -e /usr/share/filebeat/bin/filebeat -e -E output.console.pretty=true --modules mysql -M \"mysql.slowlog.var.paths=[\"/root/mysql-slow-sql-log/mysql-slowsql.log\"]\" -M \"mysql.error.enabled=false\" -E output.elasticsearch.enabled=false SystemD启动 systemctl enable filebeat systemctl start filebeat systemctl stop filebeat systemctl status filebeat journalctl -u filebeat.service systemctl daemon-reload systemctl restart filebeat Filebeat的SystemD配置文件 $ /usr/lib/systemd/system/filebeat.service [Unit] Description=Filebeat sends log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat.yml\" Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target Variable Description Default value BEAT_LOG_OPTS Log options -e BEAT_CONFIG_OPTS Flags for configuration file path -c /etc/filebeat/filebeat.yml BEAT_PATH_OPTS Other paths -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat 三. Docker镜像 docker pull docker.elastic.co/beats/filebeat:7.4.0 docker pull filebeat:7.4.0 镜像中的安装文件路径 Type Description Location home Home of the Filebeat installation. /usr/share/filebeat bin The location for the binary files. /usr/share/filebeat config The location for configuration files. /usr/share/filebeat data The location for persistent data files. /usr/share/filebeat/data logs The location for the logs created by Filebeat. /usr/share/filebeat/logs Kubernetes部署 默认部署到kube-system命名空间 部署类型是Daemonset，会部署到每一个Node上 每个Node上的/var/lib/docker/containers目录会挂载到filebeat容器中 默认Filebeat会将日志吐到kube-system命名空间下的elasticsearch中，如果需要指定吐到其他elasticsearch中，修改环境变量 - name: ELASTICSEARCH_HOST value: elasticsearch - name: ELASTICSEARCH_PORT value: \"9200\" - name: ELASTICSEARCH_USERNAME value: elastic - name: ELASTICSEARCH_PASSWORD value: changeme curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml kubectl create -f filebeat-kubernetes.yaml kubectl --namespace=kube-system get ds/filebeat OKD部署 curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml 修改部署文件 securityContext: runAsUser: 0 privileged: true oc adm policy add-scc-to-user privileged system:serviceaccount:kube-system:filebeat 四. 配置 Filebeat的配置文件路径：/etc/filebeat/filebeat.yml 配置语法为YAML 配置项 描述 示例 processors.* Processors配置 processors:- include_fields: fields: [\"cpu\"]- drop_fields: fields: [\"cpu.user\", \"cpu.system\"] filebeat.modules: Module配置 filebeat.modules:- module: mysql error: enabled: true filebeat.inputs: Input配置 filebeat.inputs:- type: log enabled: false paths: - /var/log/*.log output.*: Output配置 output.console: enabled: true path.* 组件产生文件的位置配置 path.home: /usr/share/filebeatpath.data: ${path.home}/datapath.logs: ${path.home}/logs setup.template.* Template配置 logging.* 日志配置 logging.level: infologging.to_stderr: falselogging.to_files: true monitoring.* X-Pack监控配置 monitoring.enabled: falsemonitoring.elasticsearch.hosts: [\"localhost:9200\"] http.* HTTP Endpoint配置 http.enabled: falsehttp.port: 5066http.host: localhost filebeat.autodiscover.* Filebeat自动发现配置 通用配置 全局配置项 queue.* 缓存队列设置 全局配置项 配置项 默认值 描述 示例 registry.path ${path.data}/registry 注册表文件的根路径 filebeat.registry.path: registry registry.file_permissions 0600 注册表文件的权限。Window下该配置项无效 filebeat.registry.file_permissions: 0600 registry.flush 0s filebeat.registry.flush: 5s registry.migrate_file filebeat.registry.migrate_file: /path/to/old/registry_file config_dir filebeat.config_dir: path/to/configs shutdown_timeout 5s filebeat.shutdown_timeout: 5s 通用配置项 配置项 默认值 描述 示例 name name: \"my-shipper\" tags tags: [\"service-X\", \"web-tier\"] fields fields: {project: \"myproject\", instance-id: \"57452459\"} fields_under_root 如果该选项设置为true，则新增fields会放在根路径下，而不是放在fields路径下。自定义的field会覆盖filebeat默认的field。 fields_under_root: true processors 该配置项可配置以下Processors，详见 max_procs 配置示例 # Modules配置项 filebeat.modules: - module: system # 通用配置项 fields: level: debug review: 1 fields_under_root: false # Processors配置项 processors: - decode_json_fields: # Input配置项 filebeat.inputs: - type: log # Output配置项 output.elasticsearch: output.logstash: 五. Input插件类型 Input类型 类型 描述 配置示例 Log 从日志文件中读取每一行 filebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log Stdin filebeat.inputs: - type: stdin Container filebeat.inputs: - type: container paths: - '/var/lib/docker/containers//.log' Kafka filebeat.inputs: - type: kafka hosts: - kafka-broker-1:9092 - kafka-broker-2:9092 topics: [\"my-topic\"] group_id: \"filebeat\" Redis filebeat.inputs: - type: redis hosts: [\"localhost:6379\"] password: \"${redis_pwd}\" UDP filebeat.inputs: - type: udp max_message_size: 10KiB host: \"localhost:8080\" Docker filebeat.inputs: - type: docker containers.ids: - 'e067b58476dc57d6986dd347' TCP filebeat.inputs: - type: tcp max_message_size: 10MiB host: \"localhost:9000\" Syslog filebeat.inputs: - type: syslog protocol.udp: host: \"localhost:9000\" s3 filebeat.inputs: - type: s3 queue_url: https://test.amazonaws.com/12/test access_key_id: my-access-key secret_access_key: my-secret-access-key NetFlow Google Pub/Sub 六. Output插件类型 类型 描述 配置样例 Elasticsearch output.elasticsearch: hosts: [\"https://localhost:9200\"] protocol: \"https\" index: \"filebeat-%{[agent.version]}-%{+yyyy.MM.dd}\" ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] ssl.certificate: \"/etc/pki/client/cert.pem\" ssl.key: \"/etc/pki/client/cert.key\" username: \"filebeat_internal\" password: \"YOUR_PASSWORD\" Logstash output.logstash: hosts: [\"127.0.0.1:5044\"] Kafka output.kafka: hosts: [\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"] topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 Redis output.redis: hosts: [\"localhost\"] password: \"my_password\" key: \"filebeat\" db: 0 timeout: 5 File output.file: path: \"/tmp/filebeat\" filename: filebeat #rotate_every_kb: 10000 #number_of_files: 7 #permissions: 0600 Console output.console: pretty: true Cloud 七. Processors插件 配置语法 processors: - if: then: - : - : ... else: - : - : 可以再Input中添加Processor - type: processors: - : when: 条件语法 equals equals: http.response.code: 200 contains contains: status: \"Specific error\" regexp regexp: system.process.name: \"foo.*\" range：The condition supports lt, lte, gt and gte. The condition accepts only integer or float values. range: http.response.code: gte: 400 network network: source.ip: private destination.ip: '192.168.1.0/24' destination.ip: ['192.168.1.0/24', '10.0.0.0/8', loopback] has_fields has_fields: ['http.response.code'] or or: - - - ... ----------------------------- or: - equals: http.response.code: 304 - equals: http.response.code: 404 and and: - - - ... ----------------------------- and: - equals: http.response.code: 200 - equals: status: OK ----------------------------- or: - - and: - - not not: -------------- not: equals: status: OK 支持的Processors 类型 作用 配置样例 add_cloud_metadata add_docker_metadata processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" add_fields processors:- add_fields: target: project fields: name: myproject id: '574734885120952459' add_host_metadata processors: - add_host_metadata: netinfo.enabled: false cache.ttl: 5m geo: name: nyc-dc1-rack1 location: 40.7128, -74.0060 continent_name: North America country_iso_code: US region_name: New York region_iso_code: NY city_name: New York add_kubernetes_metadata processors: - add_kubernetes_metadata: host: kube_config: ~/.kube/config default_indexers.enabled: false default_matchers.enabled: false indexers: - ip_port: matchers: - fields: lookup_fields: [\"metricset.host\"] add_labels processors:- add_labels: labels: number: 1 with.dots: test nested: with.dots: nested array: - do - re - with.field: mi add_locale processors:- add_locale: ~processors:- add_locale: format: abbreviation add_observer_metadata add_process_metadata add_tags processors:- add_tags: tags: [web, production] target: \"environment\" community_id convert processors: - convert: fields: - {from: \"src_ip\", to: \"source.ip\", type: \"ip\"} - {from: \"src_port\", to: \"source.port\", type: \"integer\"} ignore_missing: true fail_on_error: false decode_base64_field decode_cef decode_csv_fields decode_json_fields decompress_gzip_field dissect processors:- dissect: tokenizer: \"%{key1} %{key2}\" field: \"message\" target_prefix: \"dissect\" dns drop_event processors:- drop_event: when: condition drop_fields processors:- drop_fields: when: condition fields: [\"field1\", \"field2\", ...] ignore_missing: false extract_array processors: - extract_array: field: my_array mappings: source.ip: 0 destination.ip: 1 network.transport: 2 include_fields processors: - include_fields: when: condition fields: [\"field1\", \"field2\", ...] registered_domain rename processors: - rename: fields: - from: \"a.g\" to: \"e.d\" ignore_missing: false fail_on_error: true script timestamp 八. 采集注册文件解析 采集注册文件路径：/var/lib/filebeat/registry/filebeat/data.json [{\"source\":\"/root/mysql-slow-sql-log/mysql-slowsql.log\",\"offset\":1365442,\"timestamp\":\"2019-10-11T09:29:35.185399057+08:00\",\"ttl\":-1,\"type\":\"log\",\"meta\":null,\"FileStateOS\":{\"inode\":2360926,\"device\":2051}}] source # 记录采集日志的完整路径 offset # 已经采集的日志的字节数;已经采集到日志的哪个字节位置 timestamp # 日志最后一次发生变化的时间戳 ttl # 采集失效时间，-1表示只要日志存在，就一直采集该日志 type: meta filestateos # 操作系统相关 　　inode # 日志文件的inode号 　　device # 日志所在磁盘的磁盘编号 硬盘格式化的时候，操作系统自动将硬盘分成了两个区域。 一个是数据区，用来存放文件的数据信息 一个是inode区，用来存放文件的元信息，比如文件的创建者、创建时间、文件大小等等 每一个文件都有对应的inode，里边包含了与该文件有关的一些信息，可以用stat命令查看文件的inode信息 > stat /var/log/messages File: ‘/var/log/messages’ Size: 56216339 Blocks: 109808 IO Block: 4096 regular file Device: 803h/2051d Inode: 1053379 Links: 1 Access: (0600/-rw-------) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-10-06 03:20:01.528781081 +0800 Modify: 2019-10-12 13:59:13.059112545 +0800 Change: 2019-10-12 13:59:13.059112545 +0800 Birth: - 2051为十进制数，对应十六进制数803 参考链接 https://www.cnblogs.com/micmouse521/p/8085229.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/filebeat-多实例部署.html":{"url":"origin/filebeat-多实例部署.html","title":"多实例安装部署","keywords":"","body":"Filebeat的多实例部署 一、上下文 一台服务器为了充分利用资源使用，通常安装了多个系统的组件。例如既是MySQL集群的节点，又是Redis集群的节点。如果该服务器之前已经部署了一个FIlebeat实例用来采集MySQL的慢查询日志，输出日志到指定的Logstash进行处理。而此时，有需求要收集该服务器上另外一个系统的组件日志数据。FIlebeat的配置中无法使用条件判断设置多个不同的输出目的地。此时可以直接部署多实例的filebeat。 二、部署配置 以采集API网关Kong的日志为例. 创建新的filebeat配置文件/etc/filebeat/filebeat-kong.yml 创建Filebeat系统服务启动配置文件 /usr/lib/systemd/system/filebeat-kong.service [Unit] Description=Filebeat sends kong log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" # 指定Filebeat配置文件 Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat-kong.yml\" # 不同实例filebeat的'path.data'一定要不一样 Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat-kong -path.logs /var/log/filebeat-kong\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target 启动filebeat服务 sudo systemctl daemon-reload sudo systemctl start filebeat-kong.service sudo systemctl status filebeat-kong.service -l Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/filebeat-modules模块.html":{"url":"origin/filebeat-modules模块.html","title":"Filebeat Modules","keywords":"","body":"Filebeat的Modules模块 一、简介 Filebeat采集日志文件除了可以自定义配置Input采集器、Processor处理器、输出目的地等，还提供大量的模板配置Modules来快速地配置采集通用格式的日志文件。例如Nginx标准格式的日志文件。 Filebeat的Module简化了常见日志格式的收集、解析和可视化。 一个典型的Module(例如，对于Nginx日志的Module)由一个或多个Fileset组成(对于Nginx，则是access和error日志文件)。Fileset包含以下内容: Filebeat输入配置：其中包含查找日志文件的默认路径，而这些默认路径取决于操作系统。Filebeat配置还负责在需要时将多行事件拼接在一起。 Elasticsearch Ingest节点Pipeline定义：用于解析日志行。 字段定义：用于为每个字段配置Elasticsearch的正确类型。它们还包含每个字段的简短描述。 Kibana表盘样本：可以用来可视化日志文件。 Filebeat支持的Modules Filebeat模块需要Elasticsearch 5.2或更高版本。 类型 Modules overview Apache module Auditd module AWS module CEF module Cisco module Coredns Module Elasticsearch module Envoyproxy Module Google Cloud module haproxy module IBM MQ module Icinga module IIS module Iptables module Kafka module Kibana module Logstash module MongoDB module MSSQL module MySQL module nats module NetFlow module Nginx module Osquery module Palo Alto Networks module PostgreSQL module RabbitMQ module Redis module Santa module Suricata module System module Traefik module Zeek (Bro) Module 二、Module配置 1. 加载Modules 在/etc/filebeat/filebeat.yml中配置加载Modules filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s 指定特殊的filebeat全局配置文件来配置加载Modules filebeat -c /etc/filebeat/filebeat-kong.yaml modules enable nginx 2. 查看所有Module filebeat modules list 3. 启用Module Filebeat的Modules配置文件通常在/etc/filebeat/modules.d下 Filebeat提供了几种启用模块的不同方法： 命令行启用module filebeat modules enable module名 在配置文件/etc/filebeat/filebeat.yml中启用Modules filebeat.modules: - module: nginx - module: mysql - module: system 在运行时使用Modules filebeat --modules nginx,mysql,system 4. 配置Module变量参数 Filebeat的Modules配置文件通常在/etc/filebeat/modules.d下。当module不启用时，自带的Module配置文件是以.disabled后缀的。启用后，会自动去掉.disabled后缀，此时可以修改module的默认变量参数 在运行时配置Module变量参数 filebeat -e --modules 模块名 -M \"nginx.access.var.paths=[/var/log/nginx/access.log*]\" 在Modules的配置文件中配置变量参数 以Nginx模块的配置文件为例/etc/filebeat/modules.d/nginx.yml - module: nginx # 设置Nginx访问日志fileset access: input: close_eof: true enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ] # 设置Nginx错误日志fileset error: enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [\"/var/log/nginx/error.log\"] 注意：例如想要给Nginx模块的fileset添加一个参数close_eof: true，可使用以下参数 配置文件 - module: nginx access: input: close_eof: true 命令行 filebeat -e --modules nginx -M \"*.*.input.close_eof=true\" # 或者 filebeat -e --modules nginx -M \"nginx.*.input.close_eof=true\" # 或者 filebeat -e --modules nginx -M \"nginx.access.input.close_eof=true\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/filbeat-nginx-module.html":{"url":"origin/filbeat-nginx-module.html","title":"Nginx Module","keywords":"","body":"Filebeat的Nginx模块 一、简介 Filebeat的Nginx Module模块可直接用来处理Nginx标准格式的访问日志和错误日志。 二、启用Nginx模块 1. 加载Modules 在/etc/filebeat/filebeat.yml中配置加载Modules filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s 指定特殊的filebeat全局配置文件来配置加载Modules filebeat -c /etc/filebeat/filebeat-kong.yaml modules enable nginx 2. 启用Nginx 模块 filebeat modules enable nginx 三、配置Nginx模块变量参数 1. 配置Nginx 模块变量参数的方式 Nginx module的配置文件/etc/filebeat/modules.d/nginx.yml - module: nginx # 设置Nginx访问日志 access: input: close_eof: true enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ] # 设置Nginx错误日志 error: enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [\"/var/log/nginx/error.log\"] 在运行时 filebeat -e \\ --modules nginx \\ -M \"nginx.access.var.paths=[ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ]\" \\ -M \"nginx.error.var.paths=[\"/var/log/nginx/error.log\"]\" \\ -M \"nginx.access.input.close_eof=true\" 四、Nginx模块配置文件详解 1. Nginx模块配置目录结构 Nginx模块中所有的配置文件在/usr/share/filebeat/module/nginx路径下 /usr/share/filebeat/module/nginx ├── access │ ├── config │ │ └── nginx-access.yml │ ├── ingest │ │ └── default.json │ └── manifest.yml ├── error │ ├── config │ │ └── nginx-error.yml │ ├── ingest │ │ └── pipeline.json │ └── manifest.yml └── module.yml 2. module.yml dashboards: - id: 55a9e6e0-a29e-11e7-928f-5dbe6f6f5519 file: Filebeat-nginx-overview.json - id: 046212a0-a2a1-11e7-928f-5dbe6f6f5519 file: Filebeat-nginx-logs.json - id: ML-Nginx-Access-Remote-IP-Count-Explorer file: ml-nginx-access-remote-ip-count-explorer.json - id: ML-Nginx-Remote-IP-URL-Explorer file: ml-nginx-remote-ip-url-explorer.json 3. access/manifest.yml module_version: \"1.0\" var: - name: paths default: - /var/log/nginx/access.log* os.darwin: - /usr/local/var/log/nginx/access.log* os.windows: - c:/programdata/nginx/logs/*access.log* ingest_pipeline: ingest/default.json input: config/nginx-access.yml machine_learning: - name: response_code job: machine_learning/response_code.json datafeed: machine_learning/datafeed_response_code.json min_version: 5.5.0 - name: low_request_rate job: machine_learning/low_request_rate.json datafeed: machine_learning/datafeed_low_request_rate.json min_version: 5.5.0 - name: remote_ip_url_count job: machine_learning/remote_ip_url_count.json datafeed: machine_learning/datafeed_remote_ip_url_count.json min_version: 5.5.0 - name: remote_ip_request_rate job: machine_learning/remote_ip_request_rate.json datafeed: machine_learning/datafeed_remote_ip_request_rate.json min_version: 5.5.0 - name: visitor_rate job: machine_learning/visitor_rate.json datafeed: machine_learning/datafeed_visitor_rate.json min_version: 5.5.0 requires.processors: - name: user_agent plugin: ingest-user-agent - name: geoip plugin: ingest-geoip 4. access/config/nginx-access.yml type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ 5. access/ingest/default.json { \"description\": \"Pipeline for parsing Nginx access logs. Requires the geoip and user_agent plugins.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\"?(?:%{IP_LIST:nginx.access.remote_ip_list}|%{DATA:source.address}) - %{DATA:user.name} \\\\[%{HTTPDATE:nginx.access.time}\\\\] \\\"%{DATA:nginx.access.info}\\\" %{NUMBER:http.response.status_code:long} %{NUMBER:http.response.body.bytes:long} \\\"%{DATA:http.request.referrer}\\\" \\\"%{DATA:user_agent.original}\\\"\" ], \"pattern_definitions\": { \"IP_LIST\": \"%{IP}(\\\"?,?\\\\s*%{IP})*\" }, \"ignore_missing\": true } }, { \"grok\": { \"field\": \"nginx.access.info\", \"patterns\": [ \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\", \"\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"nginx.access.info\" } }, { \"split\": { \"field\": \"nginx.access.remote_ip_list\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"split\": { \"field\": \"nginx.access.origin\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"set\": { \"field\": \"source.ip\", \"value\": \"\" } }, { \"script\": { \"lang\": \"painless\", \"source\": \"boolean isPrivate(def dot, def ip) { try { StringTokenizer tok = new StringTokenizer(ip, dot); int firstByte = Integer.parseInt(tok.nextToken()); int secondByte = Integer.parseInt(tok.nextToken()); if (firstByte == 10) { return true; } if (firstByte == 192 && secondByte == 168) { return true; } if (firstByte == 172 && secondByte >= 16 && secondByte 6. error/manifest.yml module_version: \"1.0\" var: - name: paths default: - /var/log/nginx/error.log* os.darwin: - /usr/local/var/log/nginx/error.log* os.windows: - c:/programdata/nginx/logs/error.log* ingest_pipeline: ingest/pipeline.json input: config/nginx-error.yml 7. error/config/nginx-error.yml type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ 8. error/ingest/pipeline.json { \"description\": \"Pipeline for parsing the Nginx error logs\", \"processors\": [{ \"grok\": { \"field\": \"message\", \"patterns\": [ \"%{DATA:nginx.error.time} \\\\[%{DATA:log.level}\\\\] %{NUMBER:process.pid:long}#%{NUMBER:process.thread.id:long}: (\\\\*%{NUMBER:nginx.error.connection_id:long} )?%{GREEDYDATA:message}\" ], \"ignore_missing\": true } }, { \"rename\": { \"field\": \"@timestamp\", \"target_field\": \"event.created\" } }, { \"date\": { \"field\": \"nginx.error.time\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy/MM/dd H:m:s\"], \"ignore_failure\": true } }, { \"date\": { \"if\": \"ctx.event.timezone != null\", \"field\": \"nginx.error.time\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy/MM/dd H:m:s\"], \"timezone\": \"{{ event.timezone }}\", \"on_failure\": [{\"append\": {\"field\": \"error.message\", \"value\": \"{{ _ingest.on_failure_message }}\"}}] } }, { \"remove\": { \"field\": \"nginx.error.time\" } }], \"on_failure\" : [{ \"set\" : { \"field\" : \"error.message\", \"value\" : \"{{ _ingest.on_failure_message }}\" } }] } 五、示例： 1. Filebeat Nginx模块配置采集API网关Kong日志 Kong日志数据采集处理流程：kong节点 + filbeat ----> Kubernetes上的Logstash ----> Kubernetes上的Elasticsearch Kong使用了Nginx作为基础组件，它的日志也主要是Nginx格式的日志，分为两种：访问日志和错误日志。它的Nginx是安装了Lua模块的，而Lua模块的错误日志和Nginx的错误日志混合在一起。Lua的错误日志格式有的是多行。这就造成整个Nginx错误日志中既有单行错误日志，又有多行错误日志。 直接使用Filebeat的Nginx模块采集日志文件。对于标准格式的Kong访问日志是没有问题的，关键点是错误日志，要修改filebeat的Nginx模块对错误日志文件进行多行采集，设置过滤关键词，将关键词之间的多行合并为一个采集事件。 Kong的日志输出目录：/usr/local/kong/logs。目录下有两种格式的日志文件 Nginx标准日志格式的访问日志文件：/usr/local/kong/logs/admin_access.log /usr/local/kong/logs/access.log 172.17.18.169 - - [21/Oct/2019:11:47:42 +0800] \"GET /oalogin.php HTTP/1.1\" 494 46 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\" 带有Lua模块Nginx的错误日志文件 ：/usr/local/kong/logs/error.log ```bash 2019/10/21 10:58:56 [warn] 14716#0: 17345670 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET / HTTP/1.1\", host: \"172.17.18.169\" 2019/10/21 10:59:05 [warn] 14717#0: 17346563 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET /routes HTTP/1.1\", host: \"172.17.18.169\" 2019/10/21 11:00:09 [error] 14716#0: *17348732 lua coroutine: runtime error: don't know how to respond to POST stack traceback: coroutine 0: [C]: ? coroutine 1: [C]: in function 'resume' /usr/local/share/lua/5.1/lapis/application.lua:397: in function 'handler' /usr/local/share/lua/5.1/lapis/application.lua:130: in function 'resolve' /usr/local/share/lua/5.1/lapis/application.lua:167: in function [C]: in function 'xpcall' /usr/local/share/lua/5.1/lapis/application.lua:173: in function 'dispatch' /usr/local/share/lua/5.1/lapis/nginx.lua:230: in function 'serve' /usr/local/share/lua/5.1/kong/init.lua:1113: in function 'admin_content' content_by_lua(nginx-kong.conf:190):2: in main chunk, client: 172.17.18.169, server: kong_admin, request: \"POST /routes/smsp-route HTTP/1.0\", host: \"local.api.kong.curouser.com:80\" 2019/10/21 11:06:38 [warn] 14713#0: *17362982 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET /upstream HTTP/1.1\", host: \"172.17.18.169\" ``` 修改Nginx模块采集错误日志文件的方式 Filebeat的安装，Nignx模块启用，模块参数配置等操作步骤省略。这里只写针对Nginx错误日志配置进行的修改。 编辑/usr/share/filebeat/module/nginx/error/config/nginx-error.yml # ===========================修改前的====================================== type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ # ===========================修改后的====================================== type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] multiline.pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after Logstash针对FIlebeat发送过来的日志事件进行分割处理的Pipelines #=======================接收Filebeat发送过来的日志事件==================== input { beats { id => \"logstash_kong_beats\" port => 5044 } } #=======================过滤、拆分、转换日志事件============================== filter { if [fileset][name] == \"access\" { grok { match => { \"message\" => [\"%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \\\"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\\\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \\\"%{DATA:[nginx][access][referrer]}\\\" \\\"%{DATA:[nginx][access][agent]}\\\"\"] } remove_field => \"message\" } mutate { add_field => { \"read_timestamp\" => \"%{@timestamp}\" } } date { match => [ \"[nginx][access][time]\", \"dd/MMM/YYYY:H:m:s Z\" ] remove_field => \"[nginx][access][time]\" } useragent { source => \"[nginx][access][agent]\" target => \"[nginx][access][user_agent]\" remove_field => \"[nginx][access][agent]\" } geoip { source => \"[nginx][access][remote_ip]\" target => \"[nginx][access][geoip]\" } } else if [fileset][name] == \"error\" { grok { match => { \"message\" => [\"%{DATA:[nginx][error][time]} \\[%{DATA:[nginx][error][level]}\\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}\"] } remove_field => \"message\" } mutate { rename => { \"@timestamp\" => \"read_timestamp\" } } date { match => [ \"[nginx][error][time]\", \"YYYY/MM/dd H:m:s\" ] remove_field => \"[nginx][error][time]\" } } } #=======================根据日志事件类型的不同输出到不同elasticsearch索引中==================== output { if [fileset][name] == \"access\" { elasticsearch { id => \"logstash_kong_access_log\" hosts => [\"elasticsearch.elk.svc\"] index => \"kong-accesslog-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true template_name => \"logstash-logger\" user => \"logstash-user\" password => \"logstash-password\" } }else if [fileset][name] == \"error\"{ elasticsearch { id => \"logstash_kong_error_log\" hosts => [\"elasticsearch.elk.svc\"] index => \"kong-errorlog-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true template_name => \"logstash-curiouser\" user => \"logstash-user\" password => \"logstash-password\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/logstash-简介安装配置Pipeline.html":{"url":"origin/logstash-简介安装配置Pipeline.html","title":"简介安装配置Pipeline","keywords":"","body":"Logastash的简介、安装、配置、Pipeline、插件 一. 简介 官方文档：https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html Logstash是一个开源数据收集引擎，具有实时管道功能。 Logstash可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地 Logstash 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，还有强大的插件功能，常用于日志处理。 Logstash耗资源较大，运行占用CPU和内存高。另外没有消息队列缓存，存在数据丢失隐患 Logstash使用Ruby语言编写的运行在Java虚拟机上的具有收集、分析和转发数据流功能的工具 Logstash使用Pipeline方式进行日志的搜集，处理和输出 Event：logstash将数据流中的每一条数据在input处被转换为event，在output处event再被转换为目标格式的数据 Inputs：用于从数据源获取Event。每个Input启动一个线程，从对应数据源获取数据，将数据写入一个队列 Filters：用于过滤、修改Event Outputs：负责输出Event到其他系统中 Logstash使用Pipeline流水线的形式来处理数据Event事件，大致流程如下 其中inputs和outputs支持codecs（coder&decoder）在1.3.0 版之前，logstash 只支持纯文本形式输入，然后用filter处理它。但现在，我们可以在输入期间处理不同类型的数据。所以现在的数据处理流程 箭头代表数据流向。可以有多个input。中间的queue负责将数据分发到不通的pipline中，每个pipline由batcher，filter和output构成。batcher的作用是批量从queue中取数据（可配置）。 logstash数据流历程 首先有一个输入数据，例如是一个web.log文件，其中每一行都是一条数据。file imput会从文件中取出数据，然后通过json codec将数据转换成logstash event。 这条event会通过queue流入某一条pipline处理线程中，首先会存放在batcher中。当batcher达到处理数据的条件（如一定时间或event一定规模）后，batcher会把数据发送到filter中，filter对event数据进行处理后转到output，output就把数据输出到指定的输出位置。 输出后还会返回ACK给queue，包含已经处理的event，queue会将已处理的event进行标记。 queue分类 In Memory： 在内存中，固定大小，无法处理进程crash. 机器宕机等情况，会导致数据丢失。 Persistent Queue：可处理进程crash情况，保证数据不丢失。保证数据至少消费一次；充当缓冲区，可代替kafka等消息队列作用。 Dead Letter Queues：存放logstash因数据类型错误等原因无法处理的Event Persistent Queue（PQ）处理流程 一条数据经由input进入PQ，PQ将数据备份在disk，然后PQ响应input表示已收到数据； 数据从PQ到达filter/output，其处理到事件后返回ACK到PQ； PQ收到ACK后删除磁盘的备份数据； 二. 安装 1. 安装Java环境 在一些Linux环境下，必须设置JAVA_HOME环境变量，否则Logstash在安装期间没有检测到JAVA_HOME环境变量，会报错并且启动不起来服务。如果JDK目录在/opt下，则 在/usr/bin/下建立软连接指向JAVA_HOME/bin路径下的java 2. 安装Logstash YUM/RPM [elasticsearch-7.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install -y logstash-7.2.0 手动下载RPM安装，官方下载链接：https://www.elastic.co/downloads/logstash yum localinstall -y logstash-7*.rpm RPM包安装后各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. /usr/share/logstash bin Binary scripts including logstash to start Logstash and logstash-plugin to install plugins /usr/share/logstash/bin settings Configuration files, including logstash.yml, jvm.options, and startup.options /etc/logstash path.settings conf Logstash pipeline configuration files /etc/logstash/conf.d/*.conf See /etc/logstash/pipelines.yml logs Log files /var/log/logstash path.logs plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. /usr/share/logstash/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. /var/lib/logstash path.data 二进制包 二进制包中各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. {extract.path}- Directory created by unpacking the archive bin Binary scripts, including logstash to start Logstash and logstash-plugin to install plugins {extract.path}/bin settings Configuration files, including logstash.yml and jvm.options {extract.path}/config path.settings logs Log files {extract.path}/logs path.logs plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. {extract.path}/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. {extract.path}/data path.data 3. 启动 以服务形式或命令启动Logstash systemctl start logstash #后台会起一个名叫org.jruby.Main的Java后台进程，用jps -l查看 jps -l 使用二进制执行文件启动 /user/share/logstash/bin/logstash -f logstash.conf --config.reload.automatic #-f 指定配置文件路径 #--config.reload.automatic 自动检测加载配置文件，该参数在有-e参数是不生效 #--config.reload.interval 设置多少秒检测一次配置文件 如果Logstash启动时没有配置自动加载配置文件，重启进程时加上。 4. 验证 /usr/share/logstash/bin/logstash -e 'input { stdin { } } output { stdout {} }' #参数-e：直接从命令行定义配置信息 #配置从标准输入读取输入，然后输出到标准输出 stdin > hello world stdout> 2013-11-21T01:22:14.405+0000 0.0.0.0 hello world #Logstash会在消息上添加时间戳和IP地址 #Ctrl+D 退出Logstash 5. 命令行参数 参数 描述 默认值 -r, --config.reload.automatic Monitor configuration changes and reload whenever it is changed. NOTE: use SIGHUP to manually reload the config false -n, --node.name NAME Specify the name of this logstash instance, if no value is given it will default to the current hostname. 当前主机名 -f, --path.config CONFIG_PATH Load the logstash config from a specific file or directory. If a directory is given, all files in that directory will be concatenated in lexicographical order and then parsed as a single config file. You can also specify wildcards (globs) and any matched files will be loaded in the order described above. -e, --config.string CONFIG_STRING Use the given string as the configuration data. Same syntax as the config file. If no input is pecified, then the following is used as the default input: \"input { stdin { type => stdin } }\" and if no output is specified, then the following is used as the default output: \"output { stdout { codec => rubydebug } }\" If you wish to use both defaults, please use the empty string for the '-e' flag. nil --log.level LEVEL Set the log level for logstash. Possible values are: fatal error warn info debug trace (default: \"info\") -l, --path.logs PATH Write logstash internal logs to the given file. Without this flag, logstash will emit logs to standard output. /usr/share/logstash/logs -t, --config.test_and_exit Check configuration for valid syntax and then exit. false --config.reload.interval RELOAD_INTERVAL How frequently to poll the configuration location for changes, in seconds 3000000000 --http.host HTTP_HOST Web API binding host 127.0.0.1 --http.port HTTP_PORT Web API http port 9600..9700 --log.format FORMAT Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text (using Ruby's Object#inspect) plain --path.settings SETTINGS_DIR Directory containing logstash.yml file. This can also be set through the LS_SETTINGS_DIR environment variable /usr/share/logstash/config -p, --path.plugins PATH A path of where to find plugins. This flag can be given multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: 'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin. [] --path.data PATH This should point to a writable directory. Logstash will use this directory whenever it needs to store data. Plugins will also have access to this path. /usr/share/logstash/data -u, --pipeline.batch.delay DELAY_IN_MS When creating pipeline batches, how long to wait while polling for the next event. 50 --pipeline.id ID Sets the ID of the pipeline. main -b, --pipeline.batch.size SIZE Size of batches the pipeline is to work in. 125 -V, --version Emit the version of logstash and its friends, then exit. -M, --modules.variable MODULES_VARIABLE Load variables for module template. Multiple instances of '-M' or '--modules.variable' are supported. Ignored if '--modules' flag is not used. Should be in the format of '-M \"MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE\"' as in '-M \"example.var.filter.mutate.fieldname=fieldvalue\"' --modules MODULES Load Logstash modules. Modules can be defined using multiple instances '--modules module1 --modules module2', or comma-separated syntax '--modules=module1,module2' Cannot be used in conjunction with '-e' or '-f' Use of '--modules' will override modules declared in the 'logstash.yml' file. --setup Load index template into Elasticsearch, and saved searches, index-pattern, visualizations, and dashboards into Kibana when running modules. false -w, --pipeline.workers COUNT Sets the number of pipeline workers to run. 20 --config.debug Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled). WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false --pipeline.unsafe_shutdown Force logstash to exit during shutdown even if there are still inflight events in memory. By default, logstash will refuse to quit until all received events have been pushed to the outputs. false --java-execution Use Java execution engine. true -i, --interactive SHELL Drop to shell instead of running as normal. Valid shells are \"irb\" and \"pry\" --verbose Set the log level to info. 三. Docker镜像 docker pull docker.elastic.co/logstash/logstash:7.4.0 docker pull logstash:7.4.0 镜像中各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. /usr/share/logstash bin Binary scripts, including logstash to start Logstash and logstash-plugin to install plugins /usr/share/logstash/bin settings Configuration files, including logstash.yml and jvm.options /usr/share/logstash/config path.settings conf Logstash pipeline configuration files /usr/share/logstash/pipeline path.config plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. /usr/share/logstash/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. /usr/share/logstash/data path.data Note：基于该镜像启动的容器，日志是直接输出到控制台的，无法直接输出到日志文件中 docker镜像是基于.tar.gz格式的二进制包创建的 将pipeline文件挂载到/usr/share/logstash/pipeline/下启动 docker run --rm -it \\ -v ./test.conf:/usr/share/logstash/pipeline/test.conf \\ docker.elastic.co/logstash/logstash:7.4.0 默认pipeline文件：/usr/share/logstash/pipeline/logstash.conf input { beats { port => 5044 } } output { stdout { codec => rubydebug } } 也就是说如果不配置挂载pipeline文件就直接启动容器，logstash将启动一个最小化的pipeline：Beat Input ---> Stdout Output 可通过设置环境变量的形式配置logstash。 docker run --rm -it -e PIPELINE_WORKERS:2 docker.elastic.co/logstash/logstash:7.4.0。例如以下环境变量对应的logstash配置 Environment Variable Logstash Setting PIPELINE_WORKERS pipeline.workers LOG_LEVEL log.level XPACK_MONITORING_ENABLED xpack.monitoring.enabled logstash docker 镜像中的默认配置 http.host 0.0.0.0 xpack.monitoring.elasticsearch.hosts http://elasticsearch:9200 四. 配置 Logstash配置文件中配置项的格式是基于YAML语法，例如： pipeline: batch: size: 125 delay: 50 也可以使用平级格式 pipeline.batch.size: 125 pipeline.batch.delay: 50 配置项的值可以引用系统级别的环境变量 pipeline.batch.size: ${BATCH_SIZE} pipeline.batch.delay: ${BATCH_DELAY:50} node.name: \"node_${LS_NODE_NAME}\" path.queue: \"/tmp/${QUEUE_DIR:queue}\" 如果设置多个自定义的配置项时，推荐使用以下格式 modules: - name: MODULE_NAME1 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE var.PLUGIN_TYPE2.PLUGIN_NAME2.KEY1: VALUE var.PLUGIN_TYPE3.PLUGIN_NAME3.KEY1: VALUE - name: MODULE_NAME2 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE 常见的logstash配置 Setting Description Default value node.name A descriptive name for the node. Machine’s hostname path.data The directory that Logstash and its plugins use for any persistent needs. LOGSTASH_HOME/data pipeline.id The ID of the pipeline. main pipeline.java_execution Use the Java execution engine. true pipeline.workers The number of workers that will, in parallel, execute the filter and output stages of the pipeline. If you find that events are backing up, or that the CPU is not saturated, consider increasing this number to better utilize machine processing power. Number of the host’s CPU cores pipeline.batch.size The maximum number of events an individual worker thread will collect from inputs before attempting to execute its filters and outputs. Larger batch sizes are generally more efficient, but come at the cost of increased memory overhead. You may need to increase JVM heap space in the jvm.options config file. See Logstash Configuration Files for more info. 125 pipeline.batch.delay When creating pipeline event batches, how long in milliseconds to wait for each event before dispatching an undersized batch to pipeline workers. 50 pipeline.unsafe_shutdown When set to true, forces Logstash to exit during shutdown even if there are still inflight events in memory. By default, Logstash will refuse to quit until all received events have been pushed to the outputs. Enabling this option can lead to data loss during shutdown. false pipeline.plugin_classloaders (Beta) Load Java plugins in independent classloaders to isolate their dependencies. false path.config The path to the Logstash config for the main pipeline. If you specify a directory or wildcard, config files are read from the directory in alphabetical order. Platform-specific. See Logstash Directory Layout. config.string A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as the config file. None config.test_and_exit When set to true, checks that the configuration is valid and then exits. Note that grok patterns are not checked for correctness with this setting. Logstash can read multiple config files from a directory. If you combine this setting with log.level: debug, Logstash will log the combined config file, annotating each config block with the source file it came from. false config.reload.automatic When set to true, periodically checks if the configuration has changed and reloads the configuration whenever it is changed. This can also be triggered manually through the SIGHUP signal. false config.reload.interval How often in seconds Logstash checks the config files for changes. 3s config.debug When set to true, shows the fully compiled configuration as a debug log message. You must also set log.level: debug. WARNING: The log message will include any password options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false config.support_escapes When set to true, quoted strings will process the following escape sequences: \\n becomes a literal newline (ASCII 10). \\r becomes a literal carriage return (ASCII 13). \\t becomes a literal tab (ASCII 9). \\\\ becomes a literal backslash \\. \\\" becomes a literal double quotation mark. \\' becomes a literal quotation mark. false modules When configured, modules must be in the nested YAML structure described above this table. None queue.type The internal queuing model to use for event buffering. Specify memory for legacy in-memory based queuing, or persisted for disk-based ACKed queueing (persistent queues). memory path.queue The directory path where the data files will be stored when persistent queues are enabled (queue.type: persisted). path.data/queue queue.page_capacity The size of the page data files used when persistent queues are enabled (queue.type: persisted). The queue data consists of append-only data files separated into pages. 64mb queue.max_events The maximum number of unread events in the queue when persistent queues are enabled (queue.type: persisted). 0 (unlimited) queue.max_bytes The total capacity of the queue in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both queue.max_events and queue.max_bytes are specified, Logstash uses whichever criteria is reached first. 1024mb (1g) queue.checkpoint.acks The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (queue.type: persisted). Specify queue.checkpoint.acks: 0 to set this value to unlimited. 1024 queue.checkpoint.writes The maximum number of written events before forcing a checkpoint when persistent queues are enabled (queue.type: persisted). Specify queue.checkpoint.writes: 0 to set this value to unlimited. 1024 queue.checkpoint.retry When enabled, Logstash will retry once per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. false queue.drain When enabled, Logstash waits until the persistent queue is drained before shutting down. false dead_letter_queue.enable Flag to instruct Logstash to enable the DLQ feature supported by plugins. false dead_letter_queue.max_bytes The maximum size of each dead letter queue. Entries will be dropped if they would increase the size of the dead letter queue beyond this setting. 1024mb path.dead_letter_queue The directory path where the data files will be stored for the dead-letter queue. path.data/dead_letter_queue http.host The bind address for the metrics REST endpoint. \"127.0.0.1\" http.port The bind port for the metrics REST endpoint. 9600 log.level 设置Logstash日志输出级别 可用值：fatal error warn info debug trace info log.format The log format. Set to json to log in JSON format, or plain to use Object#.inspect. plain path.logs The directory where Logstash will write its log to. LOGSTASH_HOME/logs path.plugins Where to find custom plugins. You can specify this setting multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: PATH/logstash/TYPE/NAME.rb where TYPE is inputs, filters, outputs, or codecs, and NAME is the name of the plugin. Platform-specific. See Logstash Directory Layout. 五. Pipeline 1. 配置项结构 Logstash Pipeline文件的配置项分为三个部分： input{ input插件{ 插件配置项 } } filter{ filter插件{ 插件配置项 } } output{ output插件{ 插件配置项 } } Note: 如果在filter中添加了多种处理规则，则按照它的顺序一一处理，但是有一些插件并不是线程安全的。 如果在filter中指定了两个一样的的插件，这两个任务并不能保证准确的按顺序执行，因此官方也推荐避免在filter中重复使用插件。 2. 插件的条件控制 官方文档：https://www.elastic.co/guide/en/logstash/6.7/event-dependent-configuration.html#conditionals 有时需要在特定条件下过滤或输出事件。为此，您可以使用条件（conditional）来决定filter和output处理特定的事件。比如在elk系统中想要添加一个type类型的关键字来根据不同的条件赋值，最后好做统计。条件语支持if，else if和else语句并且可以嵌套。 条件语法 if EXPRESSION { ... } else if EXPRESSION { ... } else { ... } 操作符 比较操作： 相等: ==, !=, , >, , >= 正则: `=~(匹配正则), !~(不匹配正则) 包含:in(包含), not in(不包含) 布尔操作： and(与), or(或), nand(非与), xor(非或) 一元运算符： !(取反) ()(复合表达式), !()(对复合表达式结果取反) 示例 filter { if [foo] in [foobar] { mutate { add_tag => \"field in field\" } } if [foo] in \"foo\" { mutate { add_tag => \"field in string\" } } if \"hello\" in [greeting] { mutate { add_tag => \"string in field\" } } if [foo] in [\"hello\", \"world\", \"foo\"] { mutate { add_tag => \"field in list\" } } if [missing] in [alsomissing] { mutate { add_tag => \"shouldnotexist\" } } if !(\"foo\" in [\"hello\", \"world\"]) { mutate { add_tag => \"shouldexist\" } } if [message] =~ /\\w+\\s+\\/\\w+(\\/learner\\/course\\/)/ { mutate { add_field => { \"learner_type\" => \"course\" } } } mutate { add_field => { \"show\" => \"This data will be in the output\" } } mutate { add_field => { \"[@metadata][test]\" => \"Hello\" } } mutate { add_field => { \"[@metadata][no_show]\" => \"This data will not be in the output\" } } } output { if \"_grokparsefailure\" not in [tags] { elasticsearch { ... } } if [@metadata][test] == \"Hello\" { stdout { codec => rubydebug } } if [loglevel] == \"ERROR\" and [deployment] == \"production\" { pagerduty { ... } } } 注意： 如果if[foo] in \"String\"在执行这样的语句时无法把该字段值转化成String类型。所以最好要加field if exist判断 if [\"foo\"] { mutate { add_field => \"bar\" => \"%{foo}\" } } 3. 引用event中的字段 直接引用字段，使用[]，嵌套字段使用多层[][]即可 { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- if [b] =~ \"2\" { .......... } if [c][c1] == \"3\" { ........... } 在字符串中以sprintf方式引用,使用%{} { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- add_field => { \"test\" => \"test: %{b}\" } add_field => { \"test\" => \"test: %{[c][c1]}\" } 六. Input插件 插件一览表 Plugin Description Github repository azure_event_hubs Receives events from Azure Event Hubs azure_event_hubs beats Receives events from the Elastic Beats framework logstash-input-beats cloudwatch Pulls events from the Amazon Web Services CloudWatch API logstash-input-cloudwatch couchdb_changes Streams events from CouchDB’s _changes URI logstash-input-couchdb_changes dead_letter_queue read events from Logstash’s dead letter queue logstash-input-dead_letter_queue elasticsearch Reads query results from an Elasticsearch cluster logstash-input-elasticsearch exec Captures the output of a shell command as an event logstash-input-exec file Streams events from files logstash-input-file ganglia Reads Ganglia packets over UDP logstash-input-ganglia gelf Reads GELF-format messages from Graylog2 as events logstash-input-gelf generator Generates random log events for test purposes logstash-input-generator github Reads events from a GitHub webhook logstash-input-github google_cloud_storage Extract events from files in a Google Cloud Storage bucket logstash-input-google_cloud_storage google_pubsub Consume events from a Google Cloud PubSub service logstash-input-google_pubsub graphite Reads metrics from the graphite tool logstash-input-graphite heartbeat Generates heartbeat events for testing logstash-input-heartbeat http Receives events over HTTP or HTTPS logstash-input-http http_poller Decodes the output of an HTTP API into events logstash-input-http_poller imap Reads mail from an IMAP server logstash-input-imap irc Reads events from an IRC server logstash-input-irc java_generator Generates synthetic log events core plugin java_stdin Reads events from standard input core plugin jdbc Creates events from JDBC data logstash-input-jdbc jms Reads events from a Jms Broker logstash-input-jms jmx Retrieves metrics from remote Java applications over JMX logstash-input-jmx kafka Reads events from a Kafka topic logstash-input-kafka kinesis Receives events through an AWS Kinesis stream logstash-input-kinesis log4j Reads events over a TCP socket from a Log4j SocketAppender object logstash-input-log4j lumberjack Receives events using the Lumberjack protocl logstash-input-lumberjack meetup Captures the output of command line tools as an event logstash-input-meetup pipe Streams events from a long-running command pipe logstash-input-pipe puppet_facter Receives facts from a Puppet server logstash-input-puppet_facter rabbitmq Pulls events from a RabbitMQ exchange logstash-input-rabbitmq redis Reads events from a Redis instance logstash-input-redis relp Receives RELP events over a TCP socket logstash-input-relp rss Captures the output of command line tools as an event logstash-input-rss s3 Streams events from files in a S3 bucket logstash-input-s3 salesforce Creates events based on a Salesforce SOQL query logstash-input-salesforce snmp Polls network devices using Simple Network Management Protocol (SNMP) logstash-input-snmp snmptrap Creates events based on SNMP trap messages logstash-input-snmptrap sqlite Creates events based on rows in an SQLite database logstash-input-sqlite sqs Pulls events from an Amazon Web Services Simple Queue Service queue logstash-input-sqs stdin Reads events from standard input logstash-input-stdin stomp Creates events received with the STOMP protocol logstash-input-stomp syslog Reads syslog messages as events logstash-input-syslog tcp Reads events from a TCP socket logstash-input-tcp twitter Reads events from the Twitter Streaming API logstash-input-twitter udp Reads events over UDP logstash-input-udp unix Reads events over a UNIX socket logstash-input-unix varnishlog Reads from the varnish cache shared memory log logstash-input-varnishlog websocket Reads events from a websocket logstash-input-websocket wmi Creates events based on the results of a WMI query logstash-input-wmi xmpp Receives events over the XMPP/Jabber protocol logstash-input-xmpp 插件通用配置项 参数 参数值类型 必须 默认值 详解 add_field hash No {} 向事件添加字段。 codec codec No plain 用于输入数据的编解码器，在输入数据之前，输入编解码器是一种方便的解码方法，不需要在你的Logstash管道中使用单独的过滤器 enable_metric boolean No true 禁用或启用这个特定插件实例的指标日志，默认情况下，我们记录所有我们可以记录的指标，但是你可以禁用特定插件的指标集合。 id string No 向插件配置添加唯一的ID，如果没有指定ID，则Logstash将生成一个，强烈建议在配置中设置此ID，当你有两个或多个相同类型的插件时，这一点特别有用。例如，如果你有两个log4j输入，在本例中添加一个命名ID将有助于在使用监视API时监视Logstash。input { kafka { id => \"my_plugin_id\" }} tags array No 向事件添加任意数量的标记，这有助于以后的处理。 type string No 向该输入处理的所有事件添加type字段，类型主要用于过滤器激活，该type作为事件本身的一部分存储，因此你也可以使用该类型在Kibana中搜索它。如果你试图在已经拥有一个type的事件上设置一个type（例如，当你将事件从发送者发送到索引器时），那么新的输入将不会覆盖现有的type，发送方的type集在其生命周期中始终与该事件保持一致，甚至在发送到另一个Logstash服务器时也是如此。 七. Filter插件 插件一览表 Plugin Description Github repository aggregate Aggregates information from several events originating with a single task logstash-filter-aggregate alter Performs general alterations to fields that the mutate filter does not handle logstash-filter-alter bytes Parses string representations of computer storage sizes, such as \"123 MB\" or \"5.6gb\", into their numeric value in bytes logstash-filter-bytes cidr Checks IP addresses against a list of network blocks logstash-filter-cidr cipher Applies or removes a cipher to an event logstash-filter-cipher clone Duplicates events logstash-filter-clone csv Parses comma-separated value data into individual fields logstash-filter-csv date Parses dates from fields to use as the Logstash timestamp for an event logstash-filter-date de_dot Computationally expensive filter that removes dots from a field name logstash-filter-de_dot dissect Extracts unstructured event data into fields using delimiters logstash-filter-dissect dns Performs a standard or reverse DNS lookup logstash-filter-dns drop Drops all events logstash-filter-drop elapsed Calculates the elapsed time between a pair of events logstash-filter-elapsed elasticsearch Copies fields from previous log events in Elasticsearch to current events logstash-filter-elasticsearch environment Stores environment variables as metadata sub-fields logstash-filter-environment extractnumbers Extracts numbers from a string logstash-filter-extractnumbers fingerprint Fingerprints fields by replacing values with a consistent hash logstash-filter-fingerprint geoip Adds geographical information about an IP address logstash-filter-geoip grok Parses unstructured event data into fields logstash-filter-grok http Provides integration with external web services/REST APIs logstash-filter-http i18n Removes special characters from a field logstash-filter-i18n java_uuid Generates a UUID and adds it to each processed event core plugin jdbc_static Enriches events with data pre-loaded from a remote database logstash-filter-jdbc_static jdbc_streaming Enrich events with your database data logstash-filter-jdbc_streaming json Parses JSON events logstash-filter-json json_encode Serializes a field to JSON logstash-filter-json_encode kv Parses key-value pairs logstash-filter-kv memcached Provides integration with external data in Memcached logstash-filter-memcached metricize Takes complex events containing a number of metrics and splits these up into multiple events, each holding a single metric logstash-filter-metricize metrics Aggregates metrics logstash-filter-metrics mutate Performs mutations on fields logstash-filter-mutate prune Prunes event data based on a list of fields to blacklist or whitelist logstash-filter-prune range Checks that specified fields stay within given size or length limits logstash-filter-range ruby Executes arbitrary Ruby code logstash-filter-ruby sleep Sleeps for a specified time span logstash-filter-sleep split Splits multi-line messages into distinct events logstash-filter-split syslog_pri Parses the PRI (priority) field of a syslog message logstash-filter-syslog_pri threats_classifier Enriches security logs with information about the attacker’s intent logstash-filter-threats_classifier throttle Throttles the number of events logstash-filter-throttle tld Replaces the contents of the default message field with whatever you specify in the configuration logstash-filter-tld translate Replaces field contents based on a hash or YAML file logstash-filter-translate truncate Truncates fields longer than a given length logstash-filter-truncate urldecode Decodes URL-encoded fields logstash-filter-urldecode useragent Parses user agent strings into fields logstash-filter-useragent uuid Adds a UUID to events logstash-filter-uuid xml Parses XML into fields logstash-filter-xml 插件通用配置项 Setting Input type Required add_field hash No add_tag array No enable_metric boolean No id string No periodic_flush boolean No remove_field array No remove_tag array No 八. Output插件 插件一览表 Plugin Description Github repository boundary Sends annotations to Boundary based on Logstash events logstash-output-boundary circonus Sends annotations to Circonus based on Logstash events logstash-output-circonus cloudwatch Aggregates and sends metric data to AWS CloudWatch logstash-output-cloudwatch csv Writes events to disk in a delimited format logstash-output-csv datadog Sends events to DataDogHQ based on Logstash events logstash-output-datadog datadog_metrics Sends metrics to DataDogHQ based on Logstash events logstash-output-datadog_metrics elastic_app_search Sends events to the Elastic App Search solution logstash-output-elastic_app_search elasticsearch Stores logs in Elasticsearch logstash-output-elasticsearch email Sends email to a specified address when output is received logstash-output-email exec Runs a command for a matching event logstash-output-exec file Writes events to files on disk logstash-output-file ganglia Writes metrics to Ganglia’s gmond logstash-output-ganglia gelf Generates GELF formatted output for Graylog2 logstash-output-gelf google_bigquery Writes events to Google BigQuery logstash-output-google_bigquery google_cloud_storage Uploads log events to Google Cloud Storage logstash-output-google_cloud_storage google_pubsub Uploads log events to Google Cloud Pubsub logstash-output-google_pubsub graphite Writes metrics to Graphite logstash-output-graphite graphtastic Sends metric data on Windows logstash-output-graphtastic http Sends events to a generic HTTP or HTTPS endpoint logstash-output-http influxdb Writes metrics to InfluxDB logstash-output-influxdb irc Writes events to IRC logstash-output-irc java_sink Discards any events received core plugin java_stdout Prints events to the STDOUT of the shell core plugin juggernaut Pushes messages to the Juggernaut websockets server logstash-output-juggernaut kafka Writes events to a Kafka topic logstash-output-kafka librato Sends metrics, annotations, and alerts to Librato based on Logstash events logstash-output-librato loggly Ships logs to Loggly logstash-output-loggly lumberjack Sends events using the lumberjack protocol logstash-output-lumberjack metriccatcher Writes metrics to MetricCatcher logstash-output-metriccatcher mongodb Writes events to MongoDB logstash-output-mongodb nagios Sends passive check results to Nagios logstash-output-nagios nagios_nsca Sends passive check results to Nagios using the NSCA protocol logstash-output-nagios_nsca opentsdb Writes metrics to OpenTSDB logstash-output-opentsdb pagerduty Sends notifications based on preconfigured services and escalation policies logstash-output-pagerduty pipe Pipes events to another program’s standard input logstash-output-pipe rabbitmq Pushes events to a RabbitMQ exchange logstash-output-rabbitmq redis Sends events to a Redis queue using the RPUSH command logstash-output-redis redmine Creates tickets using the Redmine API logstash-output-redmine riak Writes events to the Riak distributed key/value store logstash-output-riak riemann Sends metrics to Riemann logstash-output-riemann s3 Sends Logstash events to the Amazon Simple Storage Service logstash-output-s3 sns Sends events to Amazon’s Simple Notification Service logstash-output-sns solr_http Stores and indexes logs in Solr logstash-output-solr_http sqs Pushes events to an Amazon Web Services Simple Queue Service queue logstash-output-sqs statsd Sends metrics using the statsd network daemon logstash-output-statsd stdout Prints events to the standard output logstash-output-stdout stomp Writes events using the STOMP protocol logstash-output-stomp syslog Sends events to a syslog server logstash-output-syslog tcp Writes events over a TCP socket logstash-output-tcp timber Sends events to the Timber.io logging service logstash-output-timber udp Sends events over UDP logstash-output-udp webhdfs Sends Logstash events to HDFS using the webhdfs REST API logstash-output-webhdfs websocket Publishes messages to a websocket logstash-output-websocket xmpp Posts events over XMPP logstash-output-xmpp zabbix Sends events to a Zabbix server logstash-output-zabbix 插件通用配置项 Setting Input type Required codec codec No enable_metric boolean No id string No 九. Codec插件 插件一览表 Plugin Description Github repository avro Reads serialized Avro records as Logstash events logstash-codec-avro cef Reads the ArcSight Common Event Format (CEF). logstash-codec-cef cloudfront Reads AWS CloudFront reports logstash-codec-cloudfront cloudtrail Reads AWS CloudTrail log files logstash-codec-cloudtrail collectd Reads events from the collectd binary protocol using UDP. logstash-codec-collectd dots Sends 1 dot per event to stdout for performance tracking logstash-codec-dots edn Reads EDN format data logstash-codec-edn edn_lines Reads newline-delimited EDN format data logstash-codec-edn_lines es_bulk Reads the Elasticsearch bulk format into separate events, along with metadata logstash-codec-es_bulk fluent Reads the fluentd msgpack schema logstash-codec-fluent graphite Reads graphite formatted lines logstash-codec-graphite gzip_lines Reads gzip encoded content logstash-codec-gzip_lines jdots Renders each processed event as a dot core plugin java_line Encodes and decodes line-oriented text data core plugin java_plain Processes text data with no delimiters between events core plugin json Reads JSON formatted content, creating one event per element in a JSON array logstash-codec-json json_lines Reads newline-delimited JSON logstash-codec-json_lines line Reads line-oriented text data logstash-codec-line msgpack Reads MessagePack encoded content logstash-codec-msgpack multiline Merges multiline messages into a single event logstash-codec-multiline netflow Reads Netflow v5 and Netflow v9 data logstash-codec-netflow nmap Reads Nmap data in XML format logstash-codec-nmap plain Reads plaintext with no delimiting between events logstash-codec-plain protobuf Reads protobuf messages and converts to Logstash Events logstash-codec-protobuf rubydebug Applies the Ruby Awesome Print library to Logstash events logstash-codec-rubydebug 十. 插件管理 Logstash 插件是使用 Ruby开发的，Logstash 从很早的1.5.0+版开始，其插件模块和核心模块便分开维护，其插件使用的是 RubyGems包管理器来管理维护。所以 Logstash插件本质上就是自包含的RubyGems。 RubyGems（简称 gems）是一个用于对 Ruby组件进行打包的 Ruby 打包系统。 它提供一个分发 Ruby 程序和库的标准格式，还提供一个管理程序包安装的工具。 插件的名字格式：logstash-{input/output/filter}-插件名 示例：filter中的date插件：logstash-filter-date 1. 安装插件 #以安装dissect插件为例 /usr/share/logstash/bin/logstash-plugin install 插件名 #参数详解： --path.plugins 指定安装路径 2. 查看已安装的插件 /usr/share/logstash/bin/logstash-plugin list #参数详解： --verbose 查看插件的版本 --verbose 查看组（input, filter, codec, output）下面的所有插件。例如查看filter下的所有插件 3. 更新插件 #更新某个插件 /usr/share/logstash/bin/logstash-plugin update 插件名 #更新全部插件 /usr/share/logstash/bin/logstash-plugin update 4. 卸载插件 /usr/share/logstash/bin/logstash-plugin remove 插件名 5. 给插件管理器设置代理 export HTTP_PROXY=http://127.0.0.1:3128 6. 修改插件仓库地址 Logstash插件默认仓库地址是：http://rubygems.org 有一些开源的插件仓库： Geminabox：https://github.com/geminabox/geminabox Gemirro：https://github.com/PierreRambaud/gemirro Gemfury：https://gemfury.com/ Artifactory：http://www.jfrog.com/open-source/ 编辑/usr/share/logstash/Gemfile，将source \"https://rubygems.org\"改为source \"https://my.private.repository\" 十一、监控信息 1、查看pipeline运行监控信息 curl -XGET 'http://logstash实例地址:9600/_node/stats/pipelines/pipeline实例名?pretty' Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-30 17:40:34 "},"origin/logstash-采集MySQL慢查询日志到Elasticsearch.html":{"url":"origin/logstash-采集MySQL慢查询日志到Elasticsearch.html","title":"Pipeline示例--采集MySQL慢查询日志到Elasticsearch","keywords":"","body":"Logstash采集MySQL慢查询日志到Elasticsearch 一、原始日志数据 ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- /opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with: Tcp port: 0 Unix socket: /opt/logs/mysql/mysql.sock Time Id Command Argument /opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with: Tcp port: 3306 Unix socket: /opt/logs/mysql/mysql.sock Time Id Command Argument ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-08-20T05:08:37.928071Z # User@Host: root[root] @ [172.17.89.18] Id: 1038 # Query_time: 2.100371 Lock_time: 1.263743 Rows_sent: 0 Rows_examined: 0 SET timestamp=1566277715; -- Dumping database structure for curiouser_alert_rule DROP DATABASE IF EXISTS `curiouser_alert_rule`; ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-09-02T02:56:05.166482Z # User@Host: root[root] @ [172.17.88.142] Id: 38433 # Query_time: 0.526962 Lock_time: 0.000101 Rows_sent: 1000 Rows_examined: 1000 SET timestamp=1567392964; /* ApplicationName=DataGrip 2019.2.1 */ select * from curiouser_notification.notif_send_records order by id desc limit 1000; ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-09-24T04:52:37.816164Z # User@Host: root[root] @ [172.17.0.115] Id: 88405 # Query_time: 0.622215 Lock_time: 0.000149 Rows_sent: 0 Rows_examined: 1 SET timestamp=1569300757; UPDATE xxl_job_registry SET `update_time` = NOW() WHERE `registry_group` = 'EXECUTOR' AND `registry_key` = 'metadata' AND `registry_value` = '192.168.215.94:9999'; ---------------------------------------标识分隔符（源文件中不存在）-------------------------------------------------- # Time: 2019-09-02T04:45:00.840024Z # User@Host: root[root] @ [172.17.0.113] Id: 38663 # Query_time: 0.557195 Lock_time: 0.000000 Rows_sent: 0 Rows_examined: 0 use curiouser_alert_rule; SET timestamp=1567399500; commit; Note：只摘录了几种典型格式的日志 二、Pipeline Note：Input插件将指定分割的多行数据变成一行放到一个Event的message中供filter插件处理 input { file{ path => \"/root/logs/mysql-log/test.log\" start_position => \"beginning\" codec => multiline { # 以\"# Time:\"为分隔符，中间的所有多行内容归为一行并填充到Event时间中 pattern => \"^# Time:\" negate => true what => \"previous\" # 指定最多读取多少行，默认500行（以防执行初始数据库数据sql语句超过默认行） max_lines => 20000 } } } filter { grok { # 在使用codec/multiline搭配使用的时候，需要注意，grok和普通正则一样默认是不支持匹配回车换行的。就像你需要=～//m一样也需要单独指定，具体写法是在表达式开始位置加(?m)标记 match => { \"message\" => \"(?m)^# Time:.*\\s+#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n\\s*(?(?\\w+)\\b.*)$\" } # 对于能匹配上面Grok正则的message就删除掉，不能匹配会原始保留 remove_field => [ \"message\" ] } #mutate { # gsub => [ \"sql\", \"\\n# Time: \\d+\\s+\\d+:\\d+:\\d+\", \"\" ] #} date { match => [ \"timestamp\", \"UNIX\" ] remove_field => [ \"timestamp\" ] } } output { #stdout { } elasticsearch { id => \"logstash_mysqlslowsql\" hosts => [\"localhost:9200\"] index=>\"mysql-test-log-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"elastic\" password => \"elastic\" } #file{ # path => \"/root/logs/mysql-log/test-out.log\" #} } # ---------------------------------如果以\"^# User@Host:“为分隔符时的Grok正则表达式------------------------------ match => { \"message\" => \"(?m)^# User@Host: %{USER:User}\\[[^\\]]+\\] @ (?:(?\\S*) )?\\[(?:%{IP:Client_IP})?\\]\\s.*# Query_time: %{NUMBER:Query_Time:float}\\s+Lock_time: %{NUMBER:Lock_Time:float}\\s+Rows_sent: %{NUMBER:Rows_Sent:int}\\s+Rows_examined: %{NUMBER:Rows_Examined:int}\\s*(?:use %{DATA:Database};\\s*)?SET timestamp=%{NUMBER:timestamp};\\s*(?(?\\w+)\\s+.*)\\n# Time:.*$\" } match => { \"(?m)^#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n-{0,2}\\s*(?(?\\w+)\\b.*;)\\s*(?:\\n#\\s+Time)?.*$\" } 三、日志数据经logstash处理后的数据格式 {\"message\":\"/opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with:\\nTcp port: 0 Unix socket: /opt/logs/mysql/mysql.sock\\nTime Id Command Argument\\n/opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with:\\nTcp port: 3306 Unix socket: /opt/logs/mysql/mysql.sock\\nTime Id Command Argument\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.402Z\",\"@version\":\"1\"} {\"message\":\"# Time: 2019-08-20T05:08:37.928071Z\\n# User@Host: root[root] @ [172.17.89.18] Id: 1038\\n# Query_time: 2.100371 Lock_time: 1.263743 Rows_sent: 0 Rows_examined: 0\\nSET timestamp=1566277715;\\n-- Dumping database structure for curiouser_alert_rule\\nDROP DATABASE IF EXISTS `curiouser_alert_rule`;\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.444Z\",\"@version\":\"1\"} {\"message\":\"# Time: 2019-09-02T02:56:05.166482Z\\n# User@Host: root[root] @ [172.17.88.142] Id: 38433\\n# Query_time: 0.526962 Lock_time: 0.000101 Rows_sent: 1000 Rows_examined: 1000\\nSET timestamp=1567392964;\\n/* ApplicationName=DataGrip 2019.2.1 */ select * from curiouser_notification.notif_send_records order by id desc limit 1000;\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.447Z\",\"@version\":\"1\"} {\"host\":\"allinone.tools.curiouser.com\",\"row_id\":88405,\"tags\":[\"multiline\"],\"clientip\":\"172.17.0.115\",\"@timestamp\":\"2019-09-24T04:52:37.000Z\",\"@version\":\"1\",\"user\":\"root\",\"sql\":\"UPDATE xxl_job_registry\\n SET `update_time` = NOW()\\n WHERE `registry_group` = 'EXECUTOR'\\n AND `registry_key` = 'metadata'\\n AND `registry_value` = '192.168.215.94:9999';\",\"Query_time\":0.622215,\"path\":\"/root/logs/mysql-log/test.log\",\"action\":\"UPDATE\",\"Row_sent\":0,\"lock_time\":1.49E-4,\"Rows_examined\":1} {\"host\":\"allinone.tools.curiouser.com\",\"row_id\":38663,\"tags\":[\"multiline\"],\"clientip\":\"172.17.0.113\",\"@timestamp\":\"2019-09-02T04:45:00.000Z\",\"@version\":\"1\",\"user\":\"root\",\"sql\":\"commit;\",\"database\":\"curiouser_alert_rule\",\"Query_time\":0.557195,\"path\":\"/root/logs/mysql-log/test.log\",\"action\":\"commit\",\"Row_sent\":0,\"lock_time\":0.0,\"Rows_examined\":0} 四、日志数据在Elasticsearch中存储的结构 { \"_index\": \"mysql-test-log-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"klPvuG0BV8kuVZccHbmj\", \"_version\": 1, \"_score\": null, \"_source\": { \"host\": \"allinone.tools.curiouser.com\", \"row_id\": 88405, \"tags\": [ \"multiline\" ], \"clientip\": \"172.17.0.115\", \"@timestamp\": \"2019-09-24T04:52:37.000Z\", \"@version\": \"1\", \"user\": \"root\", \"sql\": \"UPDATE xxl_job_registry\\n SET `update_time` = NOW()\\n WHERE `registry_group` = 'EXECUTOR'\\n AND `registry_key` = 'metadata'\\n AND `registry_value` = '192.168.215.94:9999';\", \"Query_time\": 0.622215, \"path\": \"/root/logs/mysql-log/test.log\", \"action\": \"UPDATE\", \"Row_sent\": 0, \"lock_time\": 0.000149, \"Rows_examined\": 1 }, \"fields\": { \"@timestamp\": [ \"2019-09-24T04:52:37.000Z\" ] }, \"sort\": [ 1569300757000 ] } 五、问题： 有几种特殊格式的日志能采集到，但无法格式解析(时间戳会是以采集时间为准) 日志文件中最后一条最新的日志在logstash退出时才进行了收集 附录： 1、filter中grok插件的正则 ^ : 匹配输入字符串的开始位置 \\s : 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v] \\S : 匹配任何可见字符。等价于 \\f\\n\\r\\t\\v \\n : 匹配一个换行符。等价于\\x0a和\\cJ \\b : 匹配一个单词边界，也就是指单词和空格间的位置（即正则表达式的“匹配”有两种概念，一种是匹配字符，一种是匹配位置，这里的\\b就是匹配位置的）。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er” \\w : 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的\"单词\"字符使用Unicode字符集 默认的正则匹配模式：%{NUMBER:row_id:int} 匹配模式:字段名:数值类型 自定义的正则匹配模式： (?the pattern here) 2、MySQL开启慢查询日志 # slow_query_log 慢查询开启状态 # slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） # long_query_time 查询超过多少秒才记录 # 查看慢查询相关参数 mysql> show variables like 'slow_query%'; mysql> show variables like 'long_query_time'; # 全局变量设置 mysql> set global slow_query_log='ON'; mysql> set global slow_query_log_file='/usr/local/mysql/data/slow.log'; mysql> set global long_query_time=1; # 修改配置文件my.cnf [mysqld] slow_query_log = ON slow_query_log_file = /usr/local/mysql/data/slow.log long_query_time = 1 5.5版本慢查询日志 # Time: 180810 8:45:12 # User@Host: select[select] @ [10.63.253.59] # Query_time: 1.064555 Lock_time: 0.000054 Rows_sent: 1 Rows_examined: 319707 SET timestamp=1533861912; SELECT COUNT(*) FROM hs_forum_thread t WHERE t.`fid`='50' AND t.`displayorder`>='0'; 5.6版本慢查询日志 # Time: 160928 18:36:08 # User@Host: root[root] @ localhost [] Id: 4922 # Query_time: 5.207662 Lock_time: 0.000085 Rows_sent: 1 Rows_examined: 526068 use db_name; SET timestamp=1475058968; select count(*) from redeem_item_consume where id 5.7版本慢查询日志 # Time: 2018-07-09T10:04:14.666231Z # User@Host: bbs_code[bbs_code] @ [10.82.9.220] Id: 9304381 # Query_time: 5.274805 Lock_time: 0.000052 Rows_sent: 0 Rows_examined: 2 SET timestamp=1531130654; SELECT * FROM pre_common_session WHERE sid='Ba1cSC' OR lastactivity 慢查询日志异同点： 每个版本的Time字段格式都不一样 相较于5.6、5.7版本，5.5版本少了Id字段 use db语句不是每条慢日志都有的 可能会出现像下边这样的情况，慢查询块# Time：下可能跟了多个慢查询语句 # Time: 160918 2:00:03 # User@Host: dba_monitor[dba_monitor] @ [10.63.144.82] Id: 968 # Query_time: 0.007479 Lock_time: 0.000181 Rows_sent: 172 Rows_examined: 344 SET timestamp=1474135203; SELECT table_schema as 'DB',table_name as 'TABLE',CONCAT(ROUND(( data_length + index_length ) / ( 1024 * 1024 *1024 ), 2), '') as 'TOTAL',TABLE_COMMENT FROM information_schema.TABLES ORDER BY data_length + index_length DESC; # User@Host: dba_monitor[dba_monitor] @ [10.63.144.82] Id: 969 # Query_time: 0.003303 Lock_time: 0.000395 Rows_sent: 233 Rows_examined: 233 SET timestamp=1474135203; select TABLE_SCHEMA,TABLE_NAME,COLUMN_NAME,ORDINAL_POSITION,COLUMN_TYPE,ifnull(COLUMN_COMMENT,0) from COLUMNS where table_schema not in ('mysql','information_schema','performance_schema','test'); 参考链接 https://my.oschina.net/lics/blog/916618 https://blog.51cto.com/fengwan/1758920 https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-10-27 17:45:39 "},"origin/logstash-filter-grok.html":{"url":"origin/logstash-filter-grok.html","title":"grok插件","keywords":"","body":"Logstash Grok插件 一、简介 Logstash提供了一系列filter过滤插件来处理收集到的log event，根据log event的特征去切分所需要的字段，方便kibana做visualize和dashboard的data analysis。 官方文档：https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html 插件Github：https://github.com/logstash-plugins/logstash-filter-grok 二、默认内置的匹配模式 Grok模块提供了默认内嵌了一些基本匹配模式。可使用以下方式查看支持的匹配模式 https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns http://grokdebug.herokuapp.com/patterns Logstash安装目录：/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns 内置的匹配模式 USERNAME [a-zA-Z0-9._-]+ USER %{USERNAME} EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+ EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME} INT (?:[+-]?(?:[0-9]+)) BASE10NUM (?[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))) NUMBER (?:%{BASE10NUM}) BASE16NUM (?(?\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``)) UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12} # URN, allowing use of RFC 2141 section 2.3 reserved characters URN urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+ # Networking MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC}) CISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4}) WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2}) COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2}) IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)? IPV4 (?[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+ URIPROTO [A-Za-z]([A-Za-z0-9+\\-.]+)+ URIHOST %{IPORHOST}(?::%{POSINT:port})? # uripath comes loosely from RFC1738, but mostly from what Firefox # doesn't turn into %XX URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+ #URIPARAM \\?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)? URIPARAM \\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]* URIPATHPARAM %{URIPATH}(?:%{URIPARAM})? URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})? # Months: January, Feb, 3, 03, 12, December MONTH \\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b MONTHNUM (?:0?[1-9]|1[0-2]) MONTHNUM2 (?:0[1-9]|1[0-2]) MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]) # Days: Monday, Tue, Thu, etc... DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?) # Years? YEAR (?>\\d\\d){1,2} HOUR (?:2[0123]|[01]?[0-9]) MINUTE (?:[0-5][0-9]) # '60' is a leap second in most time standards and thus is valid. SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?) TIME (?! HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT} # Shortcuts QS %{QUOTEDSTRING} # Log formats SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}: # Log Levels LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?) 使用语法 %{SYNTAX:SEMANTIC} # %{内置的匹配模式:存储该值的变量字段名:数值类型} # 例如 %{NUMBER:row_id:int} # 如：3会被NUBER模式所匹配 三、自定义的匹配模式 方式一：直接在pipeline中定义使用 直接在pipeline中自定义匹配模式的语法规则 (?the pattern here) 示例 filter { grok { patterns_dir => [\"./patterns\"] match => { \"message\" => \"%{IP:client_id_address} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:http_response_time} (?[0-9A-F]{10,11})}\" } } } 方式二：创建自定义pattern文件 创建文件./patterns/postfix POSTFIX_QUEUEID [0-9A-F]{10,11} 在pipeline中使用 filter { grok { patterns_dir => [\"./patterns\"] match => { \"message\" => \"%{IP:client_id_address} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:http_response_time} %{POSTFIX_QUEUEID:queue_id}\" } } } 四、Debug http://grokdebug.herokuapp.com/ Kibana中的开发工具 五、示例 1、Filebeat采集多行MySQL慢查询日志到ES MySQL(5.6)慢查询日志 # Time: 200317 17:29:17 # User@Host: test[test] @ [192.168.1.1] Id: 5 # Query_time: 9.717266 Lock_time: 0.000167 Rows_sent: 3 Rows_examined: 101693 use test; SET timestamp=1584437357; SELECT * FROM test WHERE name like 'aaa' ORDER BY id DESC LIMIT 1000; # Time: 200317 17:32:08 # User@Host: test[test] @ [192.168.10.2] Id: 97 # Query_time: 4.375731 Lock_time: 0.000151 Rows_sent: 25 Rows_examined: 6049071 SET timestamp=1584437528; select * from test where uid='35001' limit 100; Filebeat配置 filebeat.inputs: - type: log paths: - /data/mysql/log/slow.log exclude_files: [\"_filebeat\", \".gz$\"] multiline.pattern: '^# Time:' multiline.negate: true multiline.match: after multiline.max_lines: 20000 Filebeat采集发到logstash的日志格式 { \"@timestamp\": \"2020-03-18T02:52:57.139Z\", \"@metadata\":{ \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.5.1\" }, \"host\":{ \"name\": \"test.mysql\" }, \"log\": { \"offset\": 525, \"file\": { \"path\": \"/data/mysql/logs/slow.log\" }, \"flags\": [\"multiline\"] }, \"message\": \"# Time: 200317 17:29:17\\n# User@Host: test[test] @ [192.168.1.1] Id: 5\\n# Query_time: 9.717266 Lock_time: 0.000167 Rows_sent: 3 Rows_examined: 101693\\nuse test;\\nSET timestamp=1584437357;\\nSELECT * FROM test WHERE name like 'aaa' ORDER BY id DESC LIMIT 1000;\" } Logstash filter grok配置 filter { grok { match => { \"message\" => \"(?m)^#\\s*Time:.*\\s+#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n\\s*(?(?\\w+)\\b.*)$\" } remove_field => [ \"message\" ] } date { match => [ \"timestamp\", \"UNIX\" ] remove_field => [ \"timestamp\" ] } } ES存储的数据格式 { \"_index\": \"mysql-slowlog-2020-03-17\", \"_type\": \"_doc\", \"_id\": \"6nNo6HABfk0PUyuv\", \"_version\": 1, \"_score\": null, \"_source\": { \"lock_time\": 0.000167, \"action\": \"SELECT\", \"user\": \"test\", \"Row_sent\": 3, \"database\": \"test\", \"fields\": { \"env\": \"test\" }, \"Rows_examined\": 101693, \"sql\": \"select * from test where uid='35001' limit 100\", \"@timestamp\": \"2020-03-17T09:29:17.000Z\", \"row_id\": 5, \"host\": { \"name\": \"test.mysql\" }, \"clientip\": \"192.168.1.1\", \"tags\": [ \"mysql-slow-log\", \"beats_input_codec_plain_applied\" ], \"Query_time\": 9.717266, \"@version\": \"1\", \"log\": { \"file\": { \"path\": \"/data/mysql/logs/slow.log\" }, \"flags\": [ \"multiline\" ], \"offset\": 525 } }, \"fields\": { \"@timestamp\": [ \"2020-03-17T09:29:17.000Z\" ] }, \"sort\": [ 1584437357000 ] } 参考 https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html https://www.cnblogs.com/toSeek/p/6117845.html https://www.jianshu.com/p/49ae54a411b8 https://blog.csdn.net/qq_21989939/article/details/79524640 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/logstash-filter-summary.html":{"url":"origin/logstash-filter-summary.html","title":"常用filter实现的功能","keywords":"","body":"Logstash常用filter实现的功能 1、截取带有文件路径字段中的文件名 filter{ grok { match => { \"[log][file][path]\" => \"%{GREEDYDATA}/%{GREEDYDATA:app}-access.log\" } } } 2、删除json字段 filter{ mutate { remove_field => [ \"@timestamp\" , \"headers\" , \"response.data\"] gsub => [\"message\", \"\\\\\\\", \"\"] } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-21 16:58:50 "},"origin/elk-install.html":{"url":"origin/elk-install.html","title":"ELK系列安装部署","keywords":"","body":"部署ELK 一、Elasticsearch Docker 镜像信息 Docker Hub：https://hub.docker.com/_/elasticsearch 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html 数据目录：/usr/share/elasticsearch/data docker run -d \\ --name elasticsearch \\ -p 9200:9200 \\ -e TZ=Asia/Shanghai \\ -e \"cluster.name=docker-desktop\" \\ -e \"bootstrap.memory_lock=true\" \\ -e \"discovery.type=single-node\" \\ -e ES_JAVA_OPTS=\"-Xms2g -Xmx2g\" \\ -e \"xpack.monitoring.collection.enabled=true\" \\ -e \"xpack.security.authc.api_key.enabled=true\" \\ -e \"xpack.security.enabled=true\" \\ -e ELASTIC_PASSWORD=Curiouser \\ elasticsearch:7.10.1 Docker Compose version: '2.2' services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data01:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local networks: elastic: driver: bridge Ansible二进制 Ansible脚本GitHub地址：https://github.com/elastic/ansible-elasticsearch 二、Kibana Docker docker run -d \\ --name kibana \\ --link elasticsearch:elasticsearch \\ -p 5601:5601 \\ -e TZ=Asia/Shanghai \\ -e ELASTICSEARCH_USERNAME=elastic \\ -e ELASTICSEARCH_PASSWORD=Curiouser \\ -e I18N_LOCALE=zh-CN \\ -e XPACK_SECURITY_ENABLED=TRUE \\ -e XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=ZjdlNDE1ZjJiM2M4ZGI0MjdkZDRlYzQ0 \\ -e XPACK_SECURITY_ENABLED=true \\ -e XPACK_SECURITY_AUTHC_API_KEY_ENABLED=true \\ kibana:7.10.1 三、Logstash Docker docker run -d \\ --name logstash \\ --link elasticsearch:elasticsearch \\ -p 9600:9600 \\ -p 5044:5044 \\ -e TZ=Asia/Shanghai \\ -e XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic \\ -e XPACK_MONITORING_ELASTICSEARCH_PASSWORD=Curiouser \\ -e MONITORING_ENABLED=true \\ -v ~/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\ logstash:7.5.1 四、其他 验证 curl -u elastic:Curiouser http://127.0.0.1:9200/_cat/nodes?v Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-18 16:07:13 "},"origin/elasticsearch-基础知识.html":{"url":"origin/elasticsearch-基础知识.html","title":"基础知识","keywords":"","body":"Elasticsearch基础知识 Document 文档 Elasticsearch 是面向Document文档的，文档是所有可搜索数据的最小单位 文档会被序列化JSON格式，保存在Elasticsearch中 JSON对象由字段组成 每个字段都有对应的字段类型 (字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID 手动指定ID 通过Elasticsearch自动生成 一个文档由Meta Data元数据与Source Data原始数据组成 { \"_index\": \"***\", # 文档所存在的索引名 \"_type\": \"_doc\", # 文档所属的类型名 \"_id\": \"***\", # 文档的唯一ID \"_version\": 1, # 文档的版本信息 \"_score\": null, # 文档相关性打分 \"_source\": { .... }, # 文档的原始JSON数据 \"fields\": { \"***\": [ \"***\" ] }, # 额外添加的字段 \"sort\": [ 1575256044058 ] # 排序 } Index 索引 索引，即一系列documents的集合。 Shard 分片 分片是独立的，对于一个Search Request的行为，每个分片都会执行这个Request。 分片分为两种类型：主分片（Primary Shard）和副本分片（Replica Shard） 主分片：用以解决数据水平扩展的问题，通过主分片，可以将数据分布到集群内的所有节点上(主从复制) 主分片在索引创建时指定，后续不允许修改，除非reindex 一个分片是一个运行的Lucene实例，Integer.MAX_VALUE - 128 = 2,147,483,519 个docs。 副本分片：用于解决数据高可用的问题，是主分片的拷贝（可以提高读吞吐量） 副本分片数，可动态调整 主分片和备分片不会出现在同一个节点上（防止单点故障 集群节点类型 一个节点就是一个ElasticSearch的实例，本质上就是一个Java进程。 每个节点都有名字，通过配置文件，或者启动时候 -E node.name = node1指定 每一个节点在启动之后，会分配一个UID，保存在data目录下 生产环境中一个节点应该设置单一的角色（意味着节点可以多角色） 节点类型 配置参数 默认值 作用 备注 master eligible node.master true 每个节点启动后，默认就是一个Master eligible节点（可以设置node.master:false 禁止）,Master-eligible节点可以参与选主流程，成为Master节点每个节点上都保存了集群的状态信息(所有节点信息，所有的索引和其相关的Mapping和Setting信息，分片路由信息)，只有Master节点可以修改集群状态信息 可以参加选主 data node.data true 当第一个节点启动，它会将自己选举成Master节点保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于内存、CPU、IO密集型，对硬件资源要求高。 存储数据 ingest node.ingest True ingest节点可以运行一些pipeline的脚本 Coordinating 无 负责接收Client请求，将请求分发到合适的节点，最终把结果汇聚在一起返回给客户端。每个节点默认都起到了Coordinating Node的职责 每个节点默认都是coordinating节点，设置其他类型全部为false machine learning node.ml true(需要enable x-pack) 机器学习 集群状态 ES集群状态有三种： Green：所有主分片和备份分片都准备就绪（分配成功），即使有一台机器挂了（假设一台机器一个实例），数据都不会丢失，但会变成Yellow状态 Yellow：所有主分片准备就绪，但存在至少一个主分片（假设是A）对应的备份分片没有就绪，此时集群属于警告状态，意味着集群高可用和容灾能力下降，如果刚好A所在的机器挂了，并且你只设置了一个备份（已处于未就绪状态），那么A的数据就会丢失（查询结果不完整），此时集群进入Red状态 Red：：至少有一个主分片没有就绪（直接原因是找不到对应的备份分片成为新的主分片）,此时查询的结果会出现数据丢失（不完整） Elasticsearch的写入请求 Elasticsearch的写入请求主要包括：index、create、update、delete、bulk。bulk是实现对前四种的批量操作。 在6.x版本以后实际上走的都是bulk接口了。 create/index是直接新增doc，delete是直接根据_id删除doc。 ES的任意节点都可以作为协调节点(coordinating node)接受请求，当协调节点接受到请求后进行一系列处理，然后通过_routing字段找到对应的primary shard，并将请求转发给primary shard, primary shard完成写入后，将写入并发发送给各replica， raplica执行写入操作后返回给primary shard， primary shard再将请求返回给协调节点 Elasticsearch写入过程 Elasticsearch中每个index由多个shard组成，默认是5个，每个shard分布在不同的机器上。shard分为主分片和副本分片。 ​ 红色：Client Node（客户端节点）绿色：Primary Node（主分片节点）蓝色：Replica Node（副本分片节点） Elasticsearch索引过程 Elasticsearch搜索过程 Elasticsearch的准实时 Elasticsearch的核心优势就是近乎实时，为什么说是近乎实时而非真实意义上的实时呢，因为Elasticsearch能够做到准实时，而并不是完全的实时。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch--_cat-API.html":{"url":"origin/elasticsearch--_cat-API.html","title":"_cat","keywords":"","body":"Elasticsearch _cat APIs 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html 查看_cat API支持的所有Endpoint GET /_cat curl -XGET http://127.0.0.1:9200/_cat /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates 查询Endpoint参数 GET /_cat/health?help curl -XGET \"http://127.0.0.1:9200/_cat/health?help\" 　 　 # 参数全称 | 参数缩写 | 参数详解 ---------------------------------------------------------------------------------------------------- epoch | t,time | seconds since 1970-01-01 00:00:00 timestamp | ts,hms,hhmmss | time in HH:MM:SS cluster | cl | cluster name status | st | health status node.total | nt,nodeTotal | total number of nodes node.data | nd,nodeData | number of nodes that can store data shards | t,sh,shards.total,shardsTotal | total number of shards pri | p,shards.primary,shardsPrimary | number of primary shards relo | r,shards.relocating,shardsRelocating | number of relocating nodes init | i,shards.initializing,shardsInitializing | number of initializing nodes unassign | u,shards.unassigned,shardsUnassigned | number of unassigned shards pending_tasks | pt,pendingTasks | number of pending tasks max_task_wait_time | mtwt,maxTaskWaitTime | wait time of longest task pending active_shards_percent | asp,activeShardsPercent | active number of shards in percent 使用参数控制查询条件 GET /_cat/health?h=st,t #带表头 GET /_cat/health?v&h=st,t 控制查询的输出排序 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date #查询出来的Index将会以store.size的大小降序输出。只输出Index名，store.size大小，创建时间戳 curl -XGET \"http://elasticsearch-service.logger.svc:9200/_cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date\" 控制查询的输出格式 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date&format=yaml yaml - index: \"test-test-2019.05.21\" store.size: \"4.1gb\" creation.date: \"1558432572904\" - index: \".monitoring-es-7-2019.06.17\" store.size: \"1.2gb\" creation.date: \"1560729605158\" json [ { \"index\" : \"test-test-2019.05.21\", \"store.size\" : \"4.1gb\", \"creation.date\" : \"1558432572904\" }, { \"index\" : \".monitoring-es-7-2019.06.17\", \"store.size\" : \"1.2gb\", \"creation.date\" : \"1560729605158\" } ] text (default) index store.size creation.date test-test-2019.05.21 4.1gb 1558432572904 .monitoring-es-7-2019.06.17 1.2gb 1560729605158 cbor smile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-index-api.html":{"url":"origin/elasticsearch-index-api.html","title":"index","keywords":"","body":"Elasticsearch的Index API 一、DDL 数据定义(索引的创建与删除) 数据定义语言：Data Definition Language 1. 创建索引 PUT /index_name?pretty ======================================================== curl -sk -u username:userpassword -XPUT \"http://localhost:9200/index_name?pretty\" 2. 删除Index DELET /index_name ======================================================== curl -sk -u username:userpassword -XDELETE \"http://127.0.0.1:9200/index_name\" 二、DCL 数据控制(索引的配置) 数据控制语言：Data Control Language 1. 查看索引的设置 GET /index_name/_settings ======================================================== curl -sk -u username:userpassword \"http://127.0.0.1:9200/index_name/_settings\" 2. 查看索引的Mapping GET /index_name/_mapping ======================================================== curl -sk -u username:userpassword \"http://127.0.0.1:9200/index_name/_mapping\" 3. 设置索引Mapping PUT /index_name { \"mappings\": { \"index_name\": { \"dynamic\":\"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"prd_id\": { \"type\": \"long\" }, \"mer_id\": { \"type\": \"long\" }, \"data_status\": { \"type\": \"text\" }, \"datachange_createtime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" }, \"datachange_lasttime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } } ======================================================== curl -sk -u username:userpassword -XPUT \"http://127.0.0.1:9200/index_name\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"index_name\": { \"dynamic\":\"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"prd_id\": { \"type\": \"long\" }, \"mer_id\": { \"type\": \"long\" }, \"data_status\": { \"type\": \"text\" }, \"datachange_createtime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" }, \"datachange_lasttime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } }' 三、DML 数据操作(文档的增、删、改) 数据操作语言：Data Manipulation Language 1. 向索引中插入一个文档 向索引中插入一个ID为1的文档 PUT /index_name/_doc/1 { \"name\": \"test\" } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d'{ \"name\": \"test\" }' 2. 向索引中批量插入文档 详见Elasticsearch索引文档批量操作 3. 更新指定文档 PUT /index_name/_doc/1?pretty { \"doc\": {\"name\": \"test1\"} } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"doc\": {\"name\": \"test1\"} } ' 4. 指定文档新增字段 PUT /index_name/_doc/1?pretty { \"doc\": {\"name\": \"test1\"，\"new_field\": \"testN\"} } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"doc\": {\"name\": \"test1\"，\"new_field\": \"testN\"} } ' 三、DQL 数据查询 (文档的查询) 数据查询语言：Data Query Language 1. 查询索引中的所有文档 只显示前十条 GET /index_name/_search?pretty ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_search?pretty\" 2. 查询_id为1的文档 GET /index_name/_doc/1?pretty ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_doc/1?pretty\" 3. 查询_id为1的文档的元数据 GET index_name/_doc/1/_source ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_doc/1/_source?pretty\" 4. 查询符合指定条件的文档 GET /index_name/_search?q=name:test1 ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_search?q=name:test1\" 5. 复杂查询 GET /employee/_doc/_search { \"query\" : { \"bool\": { \"must\": { \"match\" : { \"last_name\" : \"smith\" } }, \"filter\": { \"range\" : { \"age\" : { \"gt\" : 30 } } } } } } #这条语句翻译成sql：select * from employee where last_name='smith' and age > 40 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-bulk-api.html":{"url":"origin/elasticsearch-bulk-api.html","title":"bulk","keywords":"","body":"Elasticsearch索引文档的批量操作API：_bulk 一、简介 官方文档 1. API请求URL格式 POST /_bulk { \"index动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"字段名\" : \"字段值\" } { \"delete动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } } { \"create动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"字段名\" : \"字段值\" } { \"update动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"doc\" : { \"字段名\" : \"字段值\" } } POST /索引名/_bulk {\"index\":{\"_id\":\"文档ID\"}}{ \"字段名\": \"字段值\" } {\"index\":{\"_id\":\"文档ID\"}}{ \"字段名\": \"字段值\" } 2. 支持的文档操作动作 index 如果索引中已经存在具有相同名称的文档，则创建失败，索引将根据需要添加或替换文档 create 如果索引中已经存在具有相同名称的文档，则创建失败，索引将根据需要添加或替换文档 delete 不期望下一行有文档数据。具有与标准delete API相同的语义 update 期望在下一行中指定部分文档、upsert和脚本及其选项 3. 将文档操作数据存储在文本 文本格式 动作及元数据\\n 数据\\n 动作及元数据\\n 数据\\n .... 动作及元数据\\n 数据\\n 例如操作数据文本test.json数据如下： {\"index\": {\"_index\": \"test\", \"_type\": \"_doc\", \"_id\": 1}} {\"doc\": {\"name\": \"test1\"}} {\"index\": {\"_index\": \"test\", \"_type\": \"_doc\", \"_id\": 2}} {\"doc\": {\"name\": \"test2\"}} ======================================================================== {\"index\":{\"_id\":\"1\"}} { \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}} { \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}} { \"name\": \"test3\" } 操作API的Curl命令 curl -X POST \"localhost:9200/_bulk\" -H 'Content-Type: application/json' --data-binary @test.json ======================================================================== curl -X POST \"localhost:9200/test/_bulk\" -H 'Content-Type: application/json' --data-binary @test.json 4. 注意事项 批量操作的响应可能是很大的JSON数据，其中包含执行的每个操作的结果，显示的顺序与请求中出现的操作顺序相同。单个操作的失败不会影响其余操作。 批量操作的响应中没有标识操作成功的计数字段 二、API请求的参数 三、Update动作的参数 doc (partial document) upsert doc_as_upsert script params (for script) lang (for script) _source POST _bulk { \"update\" : {\"_id\" : \"1\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"doc\" : {\"field\" : \"value\"} } { \"update\" : { \"_id\" : \"0\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"script\" : { \"source\": \"ctx._source.counter += params.param1\", \"lang\" : \"painless\", \"params\" : {\"param1\" : 1}}, \"upsert\" : {\"counter\" : 1}} { \"update\" : {\"_id\" : \"2\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"doc\" : {\"field\" : \"value\"}, \"doc_as_upsert\" : true } { \"update\" : {\"_id\" : \"3\", \"_index\" : \"index1\", \"_source\" : true} } { \"doc\" : {\"field\" : \"value\"} } { \"update\" : {\"_id\" : \"4\", \"_index\" : \"index1\"} } { \"doc\" : {\"field\" : \"value\"}, \"_source\": true} 四、操作示例 1. 向指定索引批量插入文档 Kibana Dev Tools Console POST _bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"name\" : \"test1\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"2\" } }{ \"name\" : \"test2\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"name\" : \"test3\" } ======================================================================== POST /test/_bulk {\"index\":{\"_id\":\"1\"}}{ \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}}{ \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}}{ \"name\": \"test3\" } Curl命令 curl -XPOST \"http://localhost:9200/_bulk\" \\ -H 'Content-Type: application/json' \\ -d ' { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"name\" : \"test1\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"2\" } }{ \"name\" : \"test2\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"name\" : \"test3\" } ' ======================================================================== curl -XPOST \"http://localhost:9200/test/_bulk\" \\ -H 'Content-Type: application/json' \\ -d ' {\"index\":{\"_id\":\"1\"}}{ \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}}{ \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}}{ \"name\": \"test3\" } ' 2. 针对索引文档进行批量操作 Kibana Dev Tools Console POST _bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"field1\" : \"value1\" } { \"delete\" : { \"_index\" : \"test\", \"_id\" : \"2\" } } { \"create\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"field1\" : \"value3\" } { \"update\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"doc\" : { \"field2\" : \"value2\"} } Curl命令 curl -X POST \"localhost:9200/_bulk?pretty\" \\ -H 'Content-Type: application/json' \\ -d ' { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"field1\" : \"value1\" } { \"delete\" : { \"_index\" : \"test\", \"_id\" : \"2\" } } { \"create\" : { \"_index\" : \"test\", \"_id\" : \"3\" } } { \"field1\" : \"value3\" } { \"update\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"doc\" : {\"field2\" : \"value2\"} } ' Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-ingest节点.html":{"url":"origin/elasticsearch-ingest节点.html","title":"Ingest节点","keywords":"","body":"ElasticSearch的Ingest角色节点 一、简介 Elasticsearch集群中的每一个节点有着各自的角色，不同的功能，共同保证集群存储分片、分词索引、聚合搜索等功能。 Master节点：负责集群相关的操作，例如创建或删除索引，跟踪哪些节点是集群的一部分，以及决定将哪些分片分配给哪些节点。 拥有稳定的主节点是衡量集群健康的重要标志。 Data节点：保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于内存、CPU、IO密集型，对硬件资源要求高。 Coordinating节点： 每一个节点都默认设置为了协调节点。 搜索请求或大容量索引请求可能涉及不同数据节点上的数据。例如，搜索请求是分两个阶段执行的，由接收客户端请求的节点(即协调节点)进行协调。在分散阶段，协调节点将请求转发给持有数据的数据节点。每个数据节点在本地执行请求并将结果返回给协调节点。在收集阶段，协调节点将每个数据节点的结果简化为单个全局结果集。 Ingest节点：可以看作是数据前置处理转换的节点。在实际的文档索引发生之前，Ingest节点会拦截批量和索引请求，然后使用ingest Pipeline对文档进行过滤、转换等数据转换预处理操作，然后将文档传递回索引或批量API。类似于 logstash 中 filter 的作用。 Ingest是5.X版本就有的特性 Ingest节点是通过包含多个processor的pipeline对文档进行预处理操作，processor是实际处理数据的插件。 默认情况下，所有节点都启用Ingest角色，因此任何节点都可以处理Ingest任务 可以创建专用的Ingest节点 要禁用节点的Ingest功能，需要在elasticsearch.yml 设置\"node.ingest：false\" 二、Ingest Pipeline与Logstash Filter Logstash处理数据的流程：logstash在pipeline filter中设置不同的插件对从Input传过来的数据进行加工处理，再输出带output中。 Easticsearch Ingest Pipeline节点处理数据的流程：Ingest Pipeline是Ingest节点上用于 Logstash Filter Ingest Pipeline 支持的数据源 大量的输入和输出插件（比如：kafka，redis等）可供使用 不能从外部来源（例如消息队列或数据库）提取数据，必须批量bulk或索引index请求将数据推送到 Elasticsearch 应对数据激增的能力不同 Logstash 可在本地对数据进行缓冲以应对采集骤升情况。Logstash 支持与大量不同的消息队列类型进行集成。 极限情况下会出现：在长时间无法联系上 Elasticsearch 或者 Elasticsearch 无法接受数据的情况下，均有可能会丢失数据。 处理能力不同 支持的插件和功能点较Ingest节点多很多。 支持为数不多处理器操作。Ingest节点管道只能在单一事件的上下文中运行。Ingest通常不能调用其他系统或者从磁盘中读取数据。 排他式功能支持不同 支持采集附件处理器插件，此插件可用来处理和索引常见格式（例如 PPT、XLS 和 PDF）的附件。 不支持如上文件附件类型。 三、Ingest Pipeline 1. Ingest Pipeline的定义及使用 Ingest Pipeline中每个processor实现了对文档的某种转换，如移除某个字段，重命名某个字段等操作。pipeline定义语法格式如下： PUT _ingest/pipeline/my-pipeline-id { \"description\" : \"...\", # Pipeline功能描述(必须，string类型) \"version\" : 123, # 用于管理ingest pipeline的版本号(可选，Integer类型) \"processors\" : [ ... ] # 指定1个或多个processor(必须，数组类型) } 要使用某个pipeline，只需要在请求中简单的指定pipeline的id就可以了： PUT my-index/_doc/doc_id?pipeline=my_pipeline_id { \"a\": \"b\", \"foo\": \"bar\" } 2. Ingest Pipeline的管理API ① Put 添加或更新Pipeline PUT /_ingest/pipeline/my-pipeline-id { \"description\" : \"describe pipeline\", \"version\" : 123, \"processors\" : [ { \"set\" : { \"field\": \"foo\", \"value\": \"bar\" } } ] } ② Get 查看指定的Pipeline GET _ingest/pipeline/my-pipeline-id 查看Pipeline的指定参数，例如查看Pipeline的版本号字段 GET /_ingest/pipeline/my-pipeline-id?filter_path=*.version ③ Delete 删除指定Pipeline DELETE /_ingest/pipeline/my-pipeline-id 删除模糊匹配的Pipeline DELETE /_ingest/pipeline/pipeline-* 删除所有Pipeline DELETE /_ingest/pipeline/* ④ Simulate 调用Ingest pipeline对指定的文档进行模拟测试。可以指定一个现有的Ingest pipeline来对提供的文档进行模拟测试，也可以在请求体中提供Ingest pipeline定义。 POST _ingest/pipeline/_simulate { \"pipeline\": { \"description\": \"template\", \"processors\": [ { \"set\": { \"field\": \"\", \"value\": \" \" } }, { \"set\": { \"field\": \"time\", \"value\": \"\" } } ] }, \"docs\": [ { \"_index\": \"simulate_test\", \"_source\": { \"name\": \"kyle\", \"age\": 18, \"birth\": \"1993-09-01\" } }, { \"_index\": \"simulate_test\", \"_source\": { \"name\": \"reason\", \"age\": 20, \"birth\": \"1990-02-03\" } } ] } 模拟测试调用已经创建的Ingest Pipeline POST /_ingest/pipeline/my-pipeline-id/_simulate { \"docs\": [ { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"foo\": \"bar\" } }, { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"foo\": \"rab\" } } ] } docs(必须, 数组)字段支持的参数: _index：(可选, string类型) 包含文档的索引名 _id：(可选, string类型) 文档的唯一标识 _source：(必须, json对象) 文档的JSON数据 3. Index Setting设置索引默认Ingest Pipeline 可在Index Setting中设置“index.default_pipeline”参数指定默认Ingest Pipeline。如果Index Setting设置了默认Ingest Pipeline,但Ingest Pipeline不存在，索引请求将会失败。参数值设为_none则表示不使用Ingest Pipeline进行文档预处理 PUT test/_settings { \"number_of_replicas\": 0, \"index\":{ \"analysis.analyzer.default.type\":\"ik_max_word\", \"analysis.search_analyzer.default.type\":\"ik_smart\", \"default_pipeline\": \"my-pipeline-id\" } } 四、Ingest Pipeline中的Processors Processor的配置格式如下 { \"PROCESSOR_NAME\" : { ... processor configuration options ... } } 所有Processors支持以下通用参数 tag：只是Pipeline中特定Processors实例化的字符串标识符。tag字段不影响处理Processors的行为，但是对于特定Processors的记录和跟踪错误非常有用。 on_failure：用于设置Pipeline或Processor中的异常处理。详情见Ingest Pipeline的异常处理 if：设置判断条件来决定Processors是否处理符合条件的文档。详情见Processor中的条件判断 1. Processor获取、处理文档中的字段数据 获取文档_source 原始数据中的字段 { \"set\": { \"field\": \"my_field\", \"value\": 582.1 } } # 或者 { \"set\": { \"field\": \"_source.my_field\", \"value\": 582.1 } } 获取文档Metadata元数据中的字段 Processor可直接处理文档Metadata元数据中_index,_type,_id,_routing Elasticsearch不允许原始数据_source中的字段与Metadata元数据中的字段相同 { \"set\": { \"field\": \"_id\", \"value\": \"1\" } } 获取Ingest的元数据字段 除了文档Metadata元数据和_source原始数据中的字段外，Processor可以在文档处理过程中添加与Ingest相关的元数据。Ingest元数据是暂时的，在文档被管道处理之后就会丢失，因此不会被索引。 例如ingest会在在_ingest下添加了ingest时间戳，用于标识对文档进行预处理的时间，获取方式如下： # 该示例添加了一个名称为received的字段。该值是es收到index 或 bulk 请求预处理文档的时间。 { \"set\": { \"field\": \"received\", \"value\": \"\" } } 2. Processor中的条件判断 Ingest pipeline的processor支持if判断来决定是否处理指定条件的文档。if字段必须包含返回布尔值的脚本。如果脚本的计算结果为true，那么将为给定的文档执行Processor，否则将跳过它。 Ingest pipeline processor中的if判断语句会被解释为elasticsearch官方支持的“Painless script”格式脚本 if字段使用脚本选项中定义的脚本字段作为对象，并通过脚本处理程序中脚本使用的相同的ctx变量访问文档的只读版本。 ① 在判断条件中获取文档中的嵌套字段 在文档中原始数据_source下有大量的嵌套JSON数据，那如何在Processor中的条件获取中嵌套较深的字段数据呢？可使用“a.b.c”这种形式获取。 如果原始数据中没有a.b存在，条件语句会抛出“NullPointerExceptions”的异常，可在Processor的条件判断引用字段时使用“?.” PUT _ingest/pipeline/drop_guests_network { \"processors\": [ { \"drop\": { \"if\": \"ctx.network?.name == 'Guest'\" } } ] } ② 复杂的条件判断 例如可以在drop processor中，判断原始数据某个数组类型的字段中是否包含\"prod\"特殊字符 PUT _ingest/pipeline/not_prod_dropper { \"processors\": [ { \"drop\": { \"if\": \"\"\" Collection tags = ctx.tags; if(tags != null){ for (String tag : tags) { if (tag.toLowerCase().contains('prod')) { return false; } } } return true; \"\"\" } } ] } # 以下文档会被丢弃 POST test/_doc/1?pipeline=not_prod_dropper { \"tags\": [\"application:myapp\", \"env:Stage\"] } # 以下文档不会被丢弃 POST test/_doc/2?pipeline=not_prod_dropper { \"tags\": [\"application:myapp\", \"env:Production\"] } ③ 判断条件的正则表达式 如果要在if条件中使用正则表达式，需要在elasticsearch.yml中设置script.painless.regex.enabled: true PUT _ingest/pipeline/check_url { \"processors\": [ { \"set\": { \"if\": \"ctx.href?.url =~ /^http[^s]/\", \"field\": \"href.insecure\", \"value\": true } } ] } PUT _ingest/pipeline/check_url { \"processors\": [ { \"set\": { \"if\": \"ctx.href?.url != null && ctx.href.url.startsWith('http://')\", \"field\": \"href.insecure\", \"value\": true } } ] } ④ Pipeline Processor中的条件判断 可在Pipeline Processor中设置判断条件来决定是否调用其他Pipeline PUT _ingest/pipeline/logs_pipeline { \"description\": \"A pipeline of pipelines for log files\", \"version\": 1, \"processors\": [ { \"pipeline\": { \"if\": \"ctx.service?.name == 'apache_httpd'\", \"name\": \"httpd_pipeline\" } }, { \"pipeline\": { \"if\": \"ctx.service?.name == 'syslog'\", \"name\": \"syslog_pipeline\" } }, { \"fail\": { \"message\": \"This pipeline requires service.name to be either `syslog` or `apache_httpd`\" } } ] } 3. 内置的Processors Append Processor Bytes Processor Circle Processor Convert Processor Date Processor Date Index Name Processor Dissect Processor Dot Expander Processor Drop Processor Fail Processor Foreach Processor GeoIP Processor Grok Processor Gsub Processor HTML Strip Processor Join Processor JSON Processor KV Processor Lowercase Processor Pipeline Processor Remove Processor Rename Processor Script Processor Set Processor Set Security User Processor Split Processor Sort Processor Trim Processor Uppercase Processor URL Decode Processor User Agent processor 4. 自定义processors 自定义的processors必须让所有elasticsearch节点都要安装，在elasticsearch.yml中添加“plugin.mandatory：ingest-attachment” 五、Ingest Pipeline的异常处理 针对一些比较复杂的Pipeline，其中可能定义了多个Processor进行文档处理，而这些Processor是按照顺序执行，如果在执行过程中一个遇到了异常，后续processor将不会执行，这是不可取的。 可以在pipeline或processor语法块中使用on_failure参数进行异常捕获。 如果在processor语法块指定了on_failure配置，不管它是否为空，processor抛出的任何异常都会被捕获，而Pipeline将继续执行其他的processor。 因为可以在on_failure语句的范围内定义更多的处理器，所以可以嵌套失败处理。 同时也可以设置\"on_failure\": true进行忽略异常，而不做任何处理 { \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ { \"rename\" : { \"field\" : \"foo\", \"target_field\" : \"bar\", \"ignore_failure\" : true } } ] } 以下Ingest Pipeline在rename processor中设置了当文档中没有指定字段\"foo\"时，会在异常处理参数中使用set processor添加\"error\"字段 { \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ { \"rename\" : { \"field\" : \"foo\", \"target_field\" : \"bar\", \"on_failure\" : [ { \"set\" : { \"field\" : \"error\", \"value\" : \"field \\\"foo\\\" does not exist, cannot rename to \\\"bar\\\"\" } } ] } } ] } 以下Ingest Pipeline在全局定义块中设置了当匹pipeline其中processor处理抛出异常，整个pipeline出错时，会在异常处理参数中使用set processor添加\"_index\"字段 { \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ ... ], \"on_failure\" : [ { \"set\" : { \"field\" : \"_index\", \"value\" : \"failed-\" } } ] } 六、Filebeat Modules模块的Ingest Pipeline 以Filebeat Nginx模块处理访问日志的Ingest PIpeline为例，文件路径：/usr/share/filebeat/module/nginx/access/ingest/default.json { \"description\": \"Pipeline for parsing Nginx access logs. Requires the geoip and user_agent plugins.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\"?(?:%{IP_LIST:nginx.access.remote_ip_list}|%{DATA:source.address}) - %{DATA:user.name} \\\\[%{HTTPDATE:nginx.access.time}\\\\] \\\"%{DATA:nginx.access.info}\\\" %{NUMBER:http.response.status_code:long} %{NUMBER:http.response.body.bytes:long} \\\"%{DATA:http.request.referrer}\\\" \\\"%{DATA:user_agent.original}\\\"\" ], \"pattern_definitions\": { \"IP_LIST\": \"%{IP}(\\\"?,?\\\\s*%{IP})*\" }, \"ignore_missing\": true } }, { \"grok\": { \"field\": \"nginx.access.info\", \"patterns\": [ \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\", \"\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"nginx.access.info\" } }, { \"split\": { \"field\": \"nginx.access.remote_ip_list\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"split\": { \"field\": \"nginx.access.origin\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"set\": { \"field\": \"source.ip\", \"value\": \"\" } }, { \"script\": { \"lang\": \"painless\", \"source\": \"boolean isPrivate(def dot, def ip) { try { StringTokenizer tok = new StringTokenizer(ip, dot); int firstByte = Integer.parseInt(tok.nextToken()); int secondByte = Integer.parseInt(tok.nextToken()); if (firstByte == 10) { return true; } if (firstByte == 192 && secondByte == 168) { return true; } if (firstByte == 172 && secondByte >= 16 && secondByte 参考文档 https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html https://blog.csdn.net/laoyang360/article/details/93376355 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-数据的分配路由.html":{"url":"origin/elasticsearch-数据的分配路由.html","title":"数据的路由分配","keywords":"","body":"Elasticsearch数据的分配路由 一、简介 二、索引分片的路由分配 2、给节点打上标签 elasticsearch.yml node.attr.size: medium 或启动命令 ./bin/elasticsearch -Enode.attr.size=medium 3. PUT test/_settings { \"index.routing.allocation.include.size\": \"big\", \"index.routing.allocation.include.rack\": \"rack1\" } 4. ](https://github.com/elastic/elasticsearch/edit/7.5/docs/reference/index-modules/allocation/filtering.asciidoc) index.routing.allocation.include.{attribute}`** Assign the index to a node whose {attribute} has at least one of the comma-separated values. index.routing.allocation.require.{attribute} Assign the index to a node whose {attribute} has all of the comma-separated values. index.routing.allocation.exclude.{attribute} Assign the index to a node whose {attribute} has none of the comma-separated values. 内置的attribute： _name Match nodes by node name _host_ip Match nodes by host IP address (IP associated with hostname) _publish_ip Match nodes by publish IP address _ip Match either _host_ip or _publish_ip _host Match nodes by hostname _id Match nodes by node id PUT test/_settings { \"index.routing.allocation.include._ip\": \"192.168.2.*\" } 5. PUT loginmac-201905/_settings { \"index\": { \"routing\": { \"allocation\": { \"require\": { \"box_type\": \"warm\" } } } } } 6. POST /_cluster/reroute { \"commands\": [ { \"move\": { \"index\": \"loginmac-201905\", \"shard\": 2, \"from_node\": \"node-248\", \"to_node\": \"node-12\" } } ] } 参考 https://www.elastic.co/guide/en/elasticsearch/reference/7.5/shard-allocation-filtering.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/es-thread-pool.html":{"url":"origin/es-thread-pool.html","title":"进程池","keywords":"","body":"ElasticSearch的进程池 一、简介 每个Elasticsearch节点内部都维护着多个线程池，每一类型的操作都被分配于不同的线程池 二、线程信息 1、查询节点上的热点线程 GET /_nodes/hot_threads ::: {log-node1}{w_AbxWEDSqa11saW1WXjEQ}{M2p9u125R_2wiVznDV2-ug}{192.168.1.6}{192.168.1.6:9300}{dilm}{ml.machine_memory=16656637952, rack=r1, xpack.installed=true, ml.max_open_jobs=20} Hot threads at 2020-12-15T09:48:50.840Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 2.9% (14.3ms out of 500ms) cpu usage by thread 'elasticsearch[log-node1][refresh][T#1]' 8/10 snapshots sharing following 2 elements java.base@13.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@13.0.1/java.lang.Thread.run(Thread.java:830) 2.4% (12.1ms out of 500ms) cpu usage by thread 'elasticsearch[log-node1][refresh][T#2]' 8/10 snapshots sharing following 2 elements java.base@13.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@13.0.1/java.lang.Thread.run(Thread.java:830) ::: {log-node2}{fllcX9yABXqLYabqgUi7Bw}{YJmD534TlykN_xDx11duA}{192.168.1.7}{192.168.1.7:9300}{dilm}{ml.machine_memory=33566380032, rack=r1, ml.max_open_jobs=20, xpack.installed=true} Hot threads at 2020-12-15T09:48:50.840Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 3.4% (17.1ms out of 500ms) cpu usage by thread 'elasticsearch[log-node2][refresh][T#3]' 8/10 snapshots sharing following 2 elements java.base@13.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@13.0.1/java.lang.Thread.run(Thread.java:830) 2、查询节点上的线程详细信息 GET /_nodes/thread_pool { \"_nodes\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"cluster_name\" : \"log\", \"nodes\" : { \"fllcX9yf455LYabqgUi7Bw\" : { \"name\" : \"log-node2\", \"transport_address\" : \"192.168.1.7:9300\", \"host\" : \"192.168.1.7\", \"ip\" : \"192.168.1.7\", \"version\" : \"7.5.1\", \"build_flavor\" : \"default\", \"build_type\" : \"rpm\", \"build_hash\" : \"3ae9ac9a93c9512551cf95d88e1e18d96\", \"roles\" : [ \"ingest\", \"master\", \"data\", \"ml\" ], \"attributes\" : { \"ml.machine_memory\" : \"33566380032\", \"rack\" : \"r1\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"thread_pool\" : { \"watcher\" : { \"type\" : \"fixed\", \"size\" : 40, \"queue_size\" : 1000 }, \"force_merge\" : { \"type\" : \"fixed\", \"size\" : 1, \"queue_size\" : -1 }, # .....省略..... } } } } 三、线程池类型 fixed 有着固定个数的线程池，个数由size参数指定。允许你指定一个队列（大小使用queue_size属性指定，默认是-1，即无限制）用来保存请求，直到有一个空闲的线程来执行请求。如果Elasticsearch无法把请求放到队列中（队列满了），该请求将被拒绝。 threadpool.write.size: 8 threadpool.write.queue_size: 100 scaling 动态个数的线程池，个数与工作负载成比例，在core参数与max参数之间浮动，同样keep_alive参数指定了闲置线程被回收的时间。 threadpool.warmer.core: 1 threadpool.warmer.max: 8 threadpool.warmer.keep_alive: 2m fixed_auto_queue_size 此功能是试验性的，在将来的版本中可能会完全更改或删除。 Elastic会尽力解决所有问题，但是实验性功能不受官方GA功能的支持SLA约束。不推荐使用[7.7.0，不推荐使用实验性fixed_auto_queue_size线程池类型，该类型将在8.0中删除。 固定个数但大小浮动的线程池，个数由size参数指定，大小在min_queue_size与max_queue_size之间浮动 thread_pool.search.queue_size: 500 #queue_size允许控制没有线程执行它们的挂起请求队列的初始大小。 thread_pool.search.size: 200 #size参数控制线程数，默认为核心数乘以5。 thread_pool.search.min_queue_size:10 #min_queue_size设置控制queue_size可以调整到的最小量。 thread_pool.search.max_queue_size: 1000 #max_queue_size设置控制queue_size可以调整到的最大量。 thread_pool.search.auto_queue_frame_size: 2000 #auto_queue_frame_size设置控制在调整队列之前进行测量的操作数。它应该足够大，以便单个操作不会过度偏向计算。 thread_pool.search.target_response_time: 6s #target_response_time是时间值设置，指示线程池队列中任务的目标平均响应时间。如果任务通常超过此时间，则将调低线程池队列以拒绝任务。 四、核心线程池 线程 类型 作用 默认配置 generic scaling 用于一些通用操作，如node discovery search fixed_auto_queue_size 索引的count/search/sugges操作 size = int((可用cpu核心数*3)/2)+ 1queue = 1000 search_throttled fixed_auto_queue_size search_throttled类型索引的count/search/suggest/get操作 size =1queue = 100 write fixed 对单个文档的index/delete/update操作和bulk批量插入操作 size = 可用cpu核心数(最多多1个)queue = 1000 get fixed get操作 size = 可用cpu核心数queue = 1000 analyze fixed analyze操作 size = 1queue = 16 snapshot scaling 对索引的snapshot/restore操作 keep-alive of 5m and a max of min(5, (可用cpu核心数)/2). system_write fixed 系统索引的写操作 maximum size of min(5, (可用cpu核心数/ 2) system_read fixed 系统索引的读操作 maximum size of min(5, (可用cpu核心数/ 2) refresh scaling refresh操作 keep-alive of 5m and a max of min(10, (可用cpu核心数)/2). flush scaling flush，synced flush，translog fsync等操作 keep-alive of 5m and a max of min(5, (可用cpu核心数)/2). force_merge fixed force merge操作 size = 1 ，队列没有大小限制 fetch_shard_store scaling 监控分片的存储 keep-alive of 5m and 最大大小为(2*可用cpu核心数) fetch_shard_started scaling 监控分片的状态 keep-alive of 5m and 最大大小为(2*可用cpu核心数) listener scaling 设置为true时，主要用于为Java客户端执行动作 max of min(10, (可用cpu核心数) / 2) 从ElasticSearch5.0 开始，无法通过api更改线程池的配置，需要更改elasticsearch.yml并重启才能生效配置 参考 https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html#fixed-auto-queue-size Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-16 13:22:07 "},"origin/elasticsearch-benchmarks.html":{"url":"origin/elasticsearch-benchmarks.html","title":"elasticsearch性能测试","keywords":"","body":"ElasticSearch性能测试esrally 一、简介 官方文档：https://esrally.readthedocs.io/en/stable/ GitHub：https://github.com/elastic/rally 二、安装配置esrally 1、安装esrally pip3 install esrally brew install pbzip2 2、配置 三、命令详解 1、命令格式 esrally [-h] [--version] {race,list,info,create-track,generate,compare,download,install,start,stop} 可选参数: -h, --help show this help message and exit --version show program's version number and exit 子命令: {race,list,info,create-track,generate,compare,download,install,start,stop} race Run a benchmark list List configuration options info Show info about a track create-track Create a Rally track from existing data generate Generate artifacts compare Compare two races download Downloads an artifact install Installs an Elasticsearch node locally start Starts an Elasticsearch node locally stop Stops an Elasticsearch node locally Find out more about Rally at https://esrally.readthedocs.io/en/2.2.0/ 2、子命令 ①列出内置的测试数据 $ esrally list tracks 测试数据 测试数据描述 文档个数 压缩后大小 未压缩大小 Default Challenge All Challenges geonames POIs from Geonames 11,396,503 252.9 MB 3.3 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts,significant-text percolator Percolator benchmark based on AOL queries 2,000,000 121.1 kB 104.9 MB append-no-conflicts append-no-conflicts http_logs HTTP server log data 247,249,096 1.2 GB 31.1 GB append-no-conflicts append-no-conflicts,runtime-fields,append-no-conflicts-index-only,append-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only geoshape Shapes from PlanetOSM 60,523,283 13.4 GB 45.4 GB append-no-conflicts append-no-conflicts metricbeat Metricbeat data 1,079,600 87.7 MB 1.2 GB append-no-conflicts append-no-conflicts geopoint Point coordinates from PlanetOSM 60,844,404 482.1 MB 2.3 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts nyc_taxis Taxi rides in New York in 2015 165,346,692 4.5 GB 74.3 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-only,update,append-ml,date-histogram geopointshape Point coordinates from PlanetOSM indexed as geoshapes 60,844,404 470.8 MB 2.6 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts so Indexing benchmark using up to questions and answers from StackOverflow 36,062,278 8.9 GB 33.1 GB append-no-conflicts append-no-conflicts eventdata This benchmark indexes HTTP access logs generated based sample logs from the elastic.co website using the generator available in https://github.com/elastic/rally-eventdata-track 20,000,000 756.0 MB 15.3 GB append-no-conflicts append-no-conflicts,transform eql EQL benchmarks based on endgame index of SIEM demo cluster 60,782,211 4.5 GB 109.2 GB default default nested StackOverflow Q&A stored as nested docs 11,203,029 663.3 MB 3.4 GB nested-search-challenge nested-search-challenge,index-only noaa Global daily weather measurements from NOAA 33,659,481 949.4 MB 9.0 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,top_metrics,aggs pmc Full text benchmark with academic papers from PMC 574,199 5.5 GB 21.7 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts ②创建测试实例 esrally create-track \\ --track=http_logs \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****'\" \\ --indices=\"products,companies\" \\ --output-path=~/tracks track.json contains the actual Rally track. For details see the track reference. companies.json and products.json contain the mapping and settings for the extracted indices. *-documents.json(.bz2) contains the sources of all the documents from the extracted indices. The files suffixed with -1k contain a smaller version of the document corpus to support test mode. ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ 三、性能测试 1、安装esrally pip3 install esrally 2、创建测试任务和数据 esrally create-track \\ --track=http_logs \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****\" \\ --indices=\"products,companies\" \\ --output-path=~/tracks 3、 esrally race --distribution-version=6.0.0 --track=geopoint --challenge=append-fast-with-conflicts 三、 esrally list tracks esrally list races esrally create-track \\ --track=http_logs \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****'\" \\ --indices=\"products,companies\" \\ --output-path=~/tracks esrally race \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****'\" \\ --track=geonames Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-04 21:09:17 "},"origin/elasticsearch-7.1的xpack权限控制.html":{"url":"origin/elasticsearch-7.1的xpack权限控制.html","title":"Xpack","keywords":"","body":"一、Context 之前ELK套装安装X-Pack的安全功能时，只有安装30天的试用许可证时间，以允许访问所有功能。 当许可证到期时，X-Pack将以降级模式运行。可以购买订阅以继续使用X-Pack组件的全部功能（https://www.elastic.co/subscriptions）。但是,最近官方从6.8.0和7.1.0开始免费提供安全功能. 本次实验,所有ELK组件版本均为7.1.0,以容器单节点运行 二. Elasticsearch开启Xpack elasticsearch的容器化部署参考笔记: ElasticSearch的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数 xpack.monitoring.collection.enabled(开启自我监控): true path.repo(设置snapshot存储仓库的路径): /usr/share/elasticsearch/snapshots-repository discovery.type(设置当前节点为单节点模式): single-node cluster.name(设置elasticsearch的集群名): curiouser bootstrap.memory_lock: 'true' TZ(设置时区): Asia/Shanghai ES_JAVA_OPTS(设置elasticsearch的JVM堆栈大小): '-Xms1g -Xmx2g' ELASTIC_USERNAME: \"kibana\" ELASTIC_PASSWORD: \"kibana\" xpack.security.enabled: 'true' xpack.security.transport.ssl.enabled: \"true\" xpack.security.transport.ssl.verification_mode: \"certificate\" xpack.security.transport.ssl.keystore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.transport.ssl.truststore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.http.ssl.enabled: \"false\" 查看elasticsearch是否开启xpack的安全验证 curl -XGET 'localhost:9200/_cat/health?v&pretty' # curl -XGET \"http://127.0.0.1:9200/_cat/health?v&pretty\" # 使用上述命令会返回401,提示未授权验证,使用以下命令进行安全验证地访问 curl --user kibana:****kibana用户的密码**** -XGET 'localhost:9200/_cat/health?v&pretty' 三、Kibana开启Xpack kibana的容器化部署详见笔记: Kibana的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数: ELASTICSEARCH_USERNAME: kibana用户 ELASTICSEARCH_PASSWORD: kibana用户的随机密码 TZ(设置时区): Asia/Shanghai 镜像中默认指定的elasticsearch地址为:http://elasticsearch:9200,刚好在open shift中部署的elasticsearch的svc名为\"elasticsearch\",它的访问方式为:http://elasticsearch:9200或者http://elasticsearch.命名空间.svc:9200 登录Kibana进行验证 使用elastic 超级用户进行登录，密码来自 setup-passwords 命令输出的结果 四、Logstash开启Xpack 配置logstash发送监控数据到elasticsearch xpack.monitoring.elasticsearch.hosts: \"http://elasticsearch:9200\" xpack.monitoring.enabled: \"true\" xpack.monitoring.elasticsearch.username: \"logstash_system\" xpack.monitoring.elasticsearch.password: \"***logstash_system用户的密码****\" 在kibana中查看logstash的监控数据 在kibana中创建logstash-pipeline角色,授予\"manage_index_template\",\"monitor\"的集群权限和\"write\",\"delete\",\"create_index\",\"manage_ilm\",\"manage\"的Index权限,然后绑定到logstash-pipeline用户上,用以创建Index并向其中写入数据 在pipeline的elasticsearch output插件中设置用户和密码 output{ elasticsearch{ hosts => \"elasticsearch:9200\" index => \"%{AppID}-%{+YYYY.MM.dd}\" user => \"logstash-pipeline\" password => \"****logstash-pipeline用户密码****\" } } 查看logstash的pipeline是否将数据写入的elasticsearch 附录：Kibana上的角色权限 Cluster相关的角色权限 角色权限 权限描述 all Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. create_snapshot Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. manage Builds on monitor and adds cluster operations that change values in the cluster. This includes snapshotting, updating settings, and rerouting. It also includes obtaining snapshot and restore status. This privilege does not include the ability to manage security. manage_ccr All cross-cluster replication operations related to managing follower indices and auto-follow patterns. It also includes the authority to grant the privileges necessary to manage follower indices and auto-follow patterns. This privilege is necessary only on clusters that contain follower indices. manage_data_frame_transforms All operations on index templates. manage_ilm All operations on index templates. manage_index_templates All operations on index templates. manage_ingest_pipelines All operations on ingest node pipelines. manage_ml All machine learning operations, such as creating and deleting datafeeds, jobs, and model snapshots.Note：Datafeeds that were created prior to version 6.2 or created when security features were disabled run as a system user with elevated privileges, including permission to read all indices. Newer datafeeds run with the security roles of the user who created or updated them. manage_pipeline All operations on ingest pipelines. manage_rollup All rollup operations, including creating, starting, stopping and deleting rollup jobs. manage_saml Enables the use of internal Elasticsearch APIs to initiate and manage SAML authentication on behalf of other users. manage_security All security-related operations such as CRUD operations on users and roles and cache clearing. manage_token All security-related operations on tokens that are generated by the Elasticsearch Token Service. manage_watcher All watcher operations, such as putting watches, executing, activate or acknowledging.Note：Watches that were created prior to version 6.1 or created when the security features were disabled run as a system user with elevated privileges, including permission to read and write all indices. Newer watches run with the security roles of the user who created or updated them. monitor All cluster read-only operations, like cluster health and state, hot threads, node info, node and cluster stats, and pending cluster tasks. monitor_data_frame_transforms All read-only operations related to data frames. monitor_ml All read-only machine learning operations, such as getting information about datafeeds, jobs, model snapshots, or results. monitor_rollup All read-only rollup operations, such as viewing the list of historical and currently running rollup jobs and their capabilities. monitor_watcher All read-only watcher operations, such as getting a watch and watcher stats. read_ccr All read-only cross-cluster replication operations, such as getting information about indices and metadata for leader indices in the cluster. It also includes the authority to check whether users have the appropriate privileges to follow leader indices. This privilege is necessary only on clusters that contain leader indices. read_ilm All read-only index lifecycle management operations, such as getting policies and checking the status of index lifecycle management transport_client All privileges necessary for a transport client to connect. Required by the remote cluster to enable Cross Cluster Search. Index相关的角色权限 角色权限 权限描述 all Any action on an index create Privilege to index documents. Also grants access to the update mapping action.NoteThis privilege does not restrict the index operation to the creation of documents but instead restricts API use to the index API. The index API allows a user to overwrite a previously indexed document. create_index Privilege to create an index. A create index request may contain aliases to be added to the index once created. In that case the request requires the manage privilege as well, on both the index and the aliases names. delete Privilege to delete documents. delete_index Privilege to delete an index. index Privilege to index and update documents. Also grants access to the update mapping action. manage All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate). manage_follow_index All actions that are required to manage the lifecycle of a follower index, which includes creating a follower index, closing it, and converting it to a regular index. This privilege is necessary only on clusters that contain follower indices. manage_ilm All index lifecycle management operations relating to managing the execution of policies of an index This includes operations like retrying policies, and removing a policy from an index. manage_leader_index All actions that are required to manage the lifecycle of a leader index, which includes forgetting a follower. This privilege is necessary only on clusters that contain leader indices. monitor All actions that are required for monitoring (recovery, segments info, index stats and status). read Read-only access to actions (count, explain, get, mget, get indexed scripts, more like this, multi percolate/search/termvector, percolate, scroll, clear_scroll, search, suggest, tv). read_cross_cluster Read-only access to the search action from a remote cluster. view_index_metadata Read-only access to index metadata (aliases, aliases exists, get index, exists, field mappings, mappings, search shards, type exists, validate, warmers, settings, ilm). This privilege is primarily available for use by Kibana users. write Privilege to perform all write operations to documents, which includes the permission to index, update, and delete documents as well as performing bulk operations. Also grants access to the update mapping action. 参考链接 https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html https://www.elastic.co/guide/en/elastic-stack-overview/7.1/get-started-logstash-user.html https://www.elastic.co/guide/en/logstash/current/ls-security.html https://www.elastic.co/guide/en/logstash/current/docker-config.html#docker-env-config Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticSearch-索引的快照备份与恢复.html":{"url":"origin/elasticSearch-索引的快照备份与恢复.html","title":"Snapshots","keywords":"","body":"一、Context shared file system：NFS S3 HDFS 二、使用NFS作为快照仓库后端存储 1. 在es集群中的某一个节点创建NFS文件系统，ES集群节点进行挂载 yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"/data/es/Elastic-SnapShots 172.16.3.0/24(rw,sync,no_root_squash,no_subtree_check) \" >> /etc/exports ;\\ export -r ;\\ showmount -e 127.0.0.1 2. 集群其他节点挂载NFS共享目录 yum install nfs-utils -y ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"172.16.3.5:/data/es/Elastic-SnapShots /data/es/Elastic-SnapShots nfs defaults 0 0\" >> /etc/fstab ;\\ mount -a ;\\ df -mh 3. 给elasticsearch授予共享目录/data/es/Elastic-SnapShots权限 chown -R elasticsearch:elasticsearch /data/es/Elastic-SnapShots 4. ES集群所有节点配置文件设置 echo 'path.repo: [\"/data/es/Elastic-SnapShots\"]' >> /etc/elasticsearch/elasticsearch.yml ;\\ systemctl restart elasticsearch;\\ systemctl status elasticsearch 三、使用HDFS作为快照仓库后端存储 ES版本：5.6.8 HDFS版本：2.6.0 1、所有ES节点安装repository-hdfs插件 在线安装插件 /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-hdfs 离线安装插件，插件下载地址：https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip wget https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip ;\\ /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-hdfs-5.6.8.zip 2、重启ES集群所有节点 systemctl restart elasticsearch ;\\ systemctl status elasticsearch 3、后续创建HDFS类型仓库时遇到的问题 ES会以elasticsearch用户(即启动elasticsearch后台进程的用户)在HDFS的/user下创建文件时提示权限不足。所以修改HDFS上/user的权限 hdfs dfs -chmod -R 777 /user 如果HDFS集群在ES集群外面，ES中的Hadoop客户端向通过Hadoop NameNode节点返回的DataNode节点写数据时会找不到DataNode节点。因为创建仓库时只是指定NameNode节点的外网地址，而返回的DataNode节点IP地址是DataNode向NameNode节点注册的内网IP地址，ES集群根本无法访问到。所以打通两者之间的网络。 四、在 kibana 的 Dev Tools 上管理快照仓库 1、注册NFS类型的快照仓库 PUT /_snapshot/快照仓库名 { \"type\": \"fs\", \"settings\": { \"compress\": true, \"location\": \"/data/es/Elastic-SnapShots\" } } ## settings的其他参数： # chunk_size Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified in bytes or by using size value notation, i.e. 1g, 10m, 5k. Defaults to null (unlimited chunk size). #max_restore_bytes_per_sec Throttles per node restore rate. Defaults to 40mb per second. #max_snapshot_bytes_per_sec Throttles per node snapshot rate. Defaults to 40mb per second. #readonly Makes repository read-only. Defaults to false. 2、注册HDFS类型的快照仓库 PUT _snapshot/快照仓库名 { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://172.16.3.10:9000\", \"compress\": true, \"path\": \"elasticsearch/respositories\" } } ##settings的其他参数： ​ #uri The uri address for hdfs. ex: \"hdfs://:/\". (Required) #path The file path within the filesystem where data is stored/loaded. ex: \"path/to/file\". (Required) #load_defaults Whether to load the default Hadoop configuration or not. (Enabled by default) #conf. Inlined configuration parameter to be added to Hadoop configuration. (Optional) Only client oriented properties from the hadoop core and hdfs configuration files will be recognized by the plugin. #compress Whether to compress the metadata or not. (Disabled by default) #chunk_size Override the chunk size. (Disabled by default) #security.principal Kerberos principal to use when connecting to a secured HDFS cluster. If you are using a service principal for your elasticsearch node, you may use the _HOST pattern in the principal name and the plugin will replace the pattern with the hostname of the node at runtime (see Creating the Secure Repository). 3、删除快照仓库 DELETE /_snapshot/快照仓库名 4、查看所有的快照仓库 GET _snapshot/_all 五、快照管理 1、创建包含所有Index的全量快照 PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true 2、创建中包含指定索引的快照 PUT /_snapshot/快照仓库名/快照名?wait_for_completion=true { \"indices\": \"index-A,index-B\", \"ignore_unavailable\": true, \"include_global_state\": false } 3、查看仓库中所有的快照 GET _snapshot/快照仓库名/_all GET _cat/snapshots/快照仓库名 curl -XGET \"http://127.0.0.1:9200/_snapshot/快照仓库名/_all\" | jq -r '.snapshots[].snapshot' 4、删除快照 DELETE _snapshot/快照仓库名/快照名 5、查看多个快照的状态 GET /_snapshot/快照仓库名/快照1,快照2/_status 6、查看某个快照状态 GET _snapshot/快照仓库名/快照/_status GET _snapshot/快照仓库名/快照_1,快照名_2/_status 7、恢复一个快照 POST _snapshot/快照仓库名/快照名/_restore # 当恢复快照中的索引名已存在时，可重命名要恢复的索引名 POST _snapshot/快照仓库名/快照名/_restore { \"indices\": \"索引名\", \"rename_pattern\": \"索引名\", \"rename_replacement\": \"索引名-2\" } 六、使用 _cat API格式化查询快照仓库中的的快照 使用Snapshot API查出来的信息是JSON格式的，后续处理比较麻烦。可使用\"_cat\" API Endpoint格式化查询输出Snapshot仓库中的快照信息。关于\"_cat\" API的详细使用信息详见Elasticsearch的\"_cat\"API 1、查看_cat的snapshots API的所有参数 GET _cat/snapshots?help 或 curl -XGET \"http://localhost:9200/_cat/snapshots?help\" 名字 简称 描述 id id,snapshot unique snapshot status s,status snapshot name start_epoch ste,startEpoch start time in seconds since 1970-01-01 00:00:00 start_time sti,startTime start time in HH:MM:SS end_epoch ete,endEpoch end time in seconds since 1970-01-01 00:00:00 end_time eti,endTime end time in HH:MM:SS duration dur,duration duration indices i,indices number of indices successful_shards ss,successful_shards number of successful shards failed_shards fs,failed_shards number of failed shards total_shards ts,total_shards number of total shards reason r,reason reason for failures 2、示例 例如只查看快照仓库中的快照名并排序 GET _cat/snapshots/pvc-snap-repo?h=id&s=id 或 curl -XGET \"http://elasticsearch:9200/_cat/snapshots/pvc-snap-repo?h=id&s=id\" # 返回的结果格式是纯文本的 # apm-7.1.1-metric-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-onboarding-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-span-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-transaction-2019.07.16-snapshot-2019.07.22 # ansi-kpo-tek1269219-h136-2019.07.16-snapshot-2019.07.22 # curiouser-ocp-allinone-audit-2019.08.15-snapshot-2019.08.22 # kibana_sample_data_logs-snapshot-2019.07.22 # springboot2-demo-dev-2019.07.12-snapshot-2019.07.15 # springboot2-demo-dev-2019.07.13-snapshot-2019.07.17 七、常用脚本 1、按月份快照索引 #!/bin/bash index_name=test-app for i in {1..12} ;do month=2020-0$i index=`curl -s -u elastic:*** -XGET \"http://127.0.0.1:9200/_cat/indices/$index_name-$month*?h=i\" | tr '\\n' ','` curl -u elastic:*** \\ -XPUT \"http://127.0.0.1:9200/_snapshot/***/collection-$index_name-$month?wait_for_completion=true\" \\ -H \"Content-Type: application/json\" \\ -d '{\"indices\": \"'$index'\",\"ignore_unavailable\": true,\"include_global_state\": false}' curl -u elastic:*** -XDELETE \"http://127.0.0.1:9200/$index_name-$month*\" done 八、更新 Elasticsearch 7.2.0新版本有了管理Snapshot Repository的新功能 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-05 14:50:30 "},"origin/elasticsearch-插件管理.html":{"url":"origin/elasticsearch-插件管理.html","title":"插件管理","keywords":"","body":"ES自带的有插件管理脚本命令 以RPM方式安装的ES，插件管理脚本在/usr/share/elasticsearch/bin/elasticsearch-plugin。该脚本能安装，列出，移除插件 $> cd /usr/share/elasticsearch/bin/ $> ./elasticsearch-plugin list #列出所有插件 $> ./elasticsearch-plugin install plugin_name #安装插件 $> ./elasticsearch-plugin remove plugin_name #卸载插件 #该脚本的参数 #-v 输出详细信息 #-s 输出最简信息 The script may return the following exit codes: 0 : everything was OK 64 : unknown command or incorrect option parameter 74 : IO error 70 : any other error 设置代理来安装插件 $ sudo ES_JAVA_OPTS=\"-Dhttp.proxyHost=代理服务器IP地址 -Dhttp.proxyPort=代理服务器端口 -Dhttps.proxyHost=代理服务器IP地址 -Dhttps.proxyPort=代理服务器端口\" bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-index-clean-snapshots.html":{"url":"origin/elasticsearch-index-clean-snapshots.html","title":"索引快照清理策略","keywords":"","body":"ElasticSearch索引的快照、清理策略 一、简介 当使用ES存储接入的应用日志时，日志索引会日益增多。而真实的日志查询需求一般是要求半月可查，存储半年到一年。应用每天产生的日志普遍会存到对应ES中以当天日期命名的索引中。查询时根据需求，最多查询半月对应索引里的数据。至于超过半月以上的日志索引数据，可快照文件，存储到文件系统中。在有特殊场景需求时进行快照恢复进行查询。这样减小ES的索引压力，提高查询效率。 需求 将半月以上的日志索引快照成文件，存储到快照仓库中 删除已快照的日志索引 定时检测创建超过15天的索引并快照、清理 钉钉通知快照后删除的索引名称 脚本执行错误时告警 二、快照清理脚本 #!/bin/bash export SENTRY_DSN=http://*****@sentry.okd.curiouser.com/28 eval \"$(sentry-cli bash-hook)\" elasticsearch_host=${ELASTICSEARCH_HOST:192.168.1.2} elasticsearch_username=${ELASTICSEARCH_USERNAME:cronjob} elasticsearch_password=${ELASTICSEARCH_PASSWORD:******} elasticsearch_index_expiry_day=${ELASTICSEARCH_INDEX_EXPIRY_DAY:15} elasticsearch_exclude_index=${ELASTICSEARCH_EXCLUDE_INDEX:.*} elasticsearch_snapshots_repository=${ELASTICSEARCH_SNAPSHOTS_REPOSITORY:***} elasticsearch_index_expiry_sec=$((elasticsearch_index_expiry_day*86400)) elasticsearch_url=\"http://${elasticsearch_host}:9200\" allIndex=`curl -s -u ${elasticsearch_username}:${elasticsearch_password} -XGET \"${elasticsearch_url}/_cat/indices/_all?h=index\"` excludeIndex=`curl -s -u ${elasticsearch_username}:${elasticsearch_password} -XGET \"${elasticsearch_url}/_cat/indices/${elasticsearch_exclude_index}/?h=i\"` indices=`echo -e \"$allIndex\\n\"\"$excludeIndex\" |sort -n |uniq -u` for i in $indices ; do createdateincludemesc=`curl -s -u ${elasticsearch_username}:${elasticsearch_password} -XGET \"${elasticsearch_url}/_cat/indices/$i?h=cd\"` ; createdate=$((createdateincludemesc/1000)) currentdate=`date +%s` durationtime=$((currentdate-createdate)) ; if [ $durationtime -gt $elasticsearch_index_expiry_sec ] ;then snapshotsIndices=$i\"\\n\"${snapshotsIndices} fi done for i in `echo -e $snapshotsIndices` ; do if [ `curl -o /dev/null -w \"%{http_code}\\n\" -s -u ${elasticsearch_username}:${elasticsearch_password} -XPUT \"${elasticsearch_url}/_snapshot/${elasticsearch_snapshots_repository}/%3C$i-%7Bnow%2Fd%7D%3E?wait_for_completion=true\" -H 'Content-Type: application/json' -d'{\"indices\": \"'$i'\",\"ignore_unavailable\": true,\"include_global_state\": false}'` = 200 ] ;then if [ `curl -o /dev/null -w \"%{http_code}\\n\" -s -u ${elasticsearch_username}:${elasticsearch_password} -XDELETE \"${elasticsearch_url}/$i\"` = 200 ] ;then echo -e \"The Index $i \\t have been snapshoted to repository and deleted !\" ; else echo \"$i failed to delete \" ; fi else echo \"$i failed to snapshot \" ; fi done curl -s -o /dev/null 'https://oapi.dingtalk.com/robot/send?access_token=*****' \\ -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"text\", \"text\": {\"content\": \"已成功将以下'\"$elasticsearch_index_expiry_day\"'天之前的索引进行了快照：\\n'\"$snapshotsIndices\"'\"} }' 脚本功能 通过环境变量设置参数 脚本执行完成后发送钉钉通知，显示脚本涉及到的ES索引 集成Sentry告警，每当脚本执行出错时将时间发送至Sentry，再由Sentry进行邮件告警 可使用Linux cron工具或K8S上的cornjob定时每天早上1点执行该脚本 三、脚本部署执行 Linux的Cronjob echo \"0 1 * * * /opt/es-index-snapshots.sh\" > /etc/crontab Kubernetes的cronjob 1、构建Cronjob镜像 Dockerfile FROM centos:7 RUN curl -sL https://sentry.io/get-cli/ | bash ADD ./es-index-snapshots.sh /usr/sbin/es-index-snapshots.sh Entrypoint [\"/bin/sh\",\"-c\"] CMD [\"/usr/sbin/es-index-snapshots.sh\"] docker build -t es-index-snapshots:v1 . 2、k8s资源声明文件 es-index-snapshots-cronjob.yml apiVersion: batch/v1beta1 kind: CronJob metadata: name: es-index-snapshots-cronjob namespace: logging spec: concurrencyPolicy: Allow failedJobsHistoryLimit: 1 schedule: 0 1 * * * startingDeadlineSeconds: 200 successfulJobsHistoryLimit: 3 suspend: false jobTemplate: spec: template: spec: containers: - env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOST value: *.logging.svc - name: ELASTICSEARCH_USERNAME value: cronjob - name: ELASTICSEARCH_PASSWORD value: \"***\"\" - name: ELASTICSEARCH_INDEX_EXPIRY_DAY value: \"15\" - name: ELASTICSEARCH_EXCLUDE_INDEX value: .* - name: ELASTICSEARCH_SNAPSHOTS_REPOSITORY value: \"***\" image: es-index-snapshots:v1 imagePullPolicy: Always name: es-index-snapshots-cronjob resources: limits: cpu: 600m memory: 800Mi requests: cpu: 300m memory: 500Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: harbor-secrets restartPolicy: OnFailure schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 kubectl apply -f es-index-snapshots-cronjob.yml 四、基于ES SDK Python版本的清理脚本 1、Python脚本 #!/usr/bin/python3 # -*- coding: UTF-8 -*- import json,time,requests,sentry_sdk from elasticsearch import Elasticsearch es = Elasticsearch( [\"192.168.1.6\",\"192.168.170.7\"], http_auth=('elastic', '*****'), scheme=\"http\", port=9200, http_compress=True ) sentry_sdk.init(dsn='**********') dingding_webhook_token=\"*********\" # 获取所有索引 def getAllIndex(): return es.cat.indices('*', h='index,cd', format='json', s='index') # 将获取到的所有索引去除\".\"开头的、名字异常的或想排除的 def getExcludeSystemAndAberrantIndex(): return list(filter(lambda x: (not ( x['index'].startswith('.') or '%{[app]}' in x['index'] )), getAllIndex())) # 获取应用日志索引 def getAppIndex(): return list(filter(lambda x: ( not ('nginx' in x['index'] or 'mysql-slowlog' in x['index'] )), getExcludeSystemAndAberrantIndex())) # 获取Nginx日志索引 def getNginxIndex(): return list(filter(lambda x: ( 'nginx' in x['index'] ), getExcludeSystemAndAberrantIndex())) # 获取MySQL慢日志索引 def getMysqlSlowQueryLogIndex(): return list(filter(lambda x: ('mysql-slowlog' in x['index']), getAllIndex())) # Snapshots索引 def snapshotIndex(index): index_body = {\"indices\": index } return es.snapshot.create(body=index_body,repository='NFS-Snapshots-Repository', wait_for_completion='true', snapshot= index+'-snapshoted-'+ time.strftime('%m-%d') ) # 删除索引 def deleteIndex(index): es.indices.delete(index=index) # 钉钉通知 def dingdingNotification(token,msg,day): url = \"https://oapi.dingtalk.com/robot/send?access_token=\"+token headers = { \"Content-Type\": \"application/json\", \"Charset\": \"UTF-8\" } # 构建请求数据，post请求 data = { \"msgtype\": \"text\", \"text\": { \"content\": msg+\"\\n\" }, \"at\": { \"isAtAll\": 'true' } } if not requests.post(url, data=json.dumps(data), headers=headers) : print(\"发送钉钉通知失败！\") sentry_sdk.capture_exception(Exception(\"发送钉钉通知失败！\")) # 将创建日志超过指定天数的日志索引快照到存储仓库中，然后删除 def snapshotAndDeleteAppIndex(type,day): if type == 'app' : snapshoted_deleted_app_indices=[] for i in getAppIndex(): cts=time.time() if ( (cts - int(i[\"cd\"])/1000) ) > day*86400 : if 'SUCCESS' in snapshotIndex(i[\"index\"])['snapshot'][\"state\"]: deleteIndex(i['index']) print(i['index']+ \"已在ES中快照并删除！\") snapshoted_deleted_app_indices.append(i['index']) else: print(\"应用日志索引：\"+i['index']+\"快照失败\") sentry_sdk.capture_exception(Exception(\"应用日志索引：\"+i['index']+\"快照失败\")) continue if snapshoted_deleted_app_indices : Notification_Context=\"成功将以下\"+str(day)+\"天之前的应用日志索引进行了快照\\n\"+\"\\n\".join(str(i) for i in snapshoted_deleted_app_indices) dingdingNotification(dingding_webhook_token,Notification_Context,day) else: Notification_Context = \"没有超过\"+ str(day)+\"天的应用日志索引需要被快照删除！\" dingdingNotification(dingding_webhook_token, Notification_Context, day) elif type == 'nginx' : snapshoted_deleted_nginx_indices = [] for i in getNginxIndex(): cts=time.time() if ( (cts - int(i[\"cd\"])/1000) ) > day*86400 : if 'SUCCESS' in snapshotIndex(i[\"index\"])['snapshot'][\"state\"]: deleteIndex(i['index']) print(i['index'] + \"已在ES中快照并删除！\") snapshoted_deleted_nginx_indices.append(i['index']) else: print(\"Nginx日志索引：\" + i['index'] + \"快照失败\") sentry_sdk.capture_exception(Exception(\"Nginx日志索引：\" + i['index'] + \"快照失败\")) continue if snapshoted_deleted_nginx_indices: Notification_Context = \"成功将以下\" + str(day) + \"天之前的应用Nginx索引进行了快照\\n\" + \"\\n\".join(str(i) for i in snapshoted_deleted_nginx_indices) dingdingNotification(dingding_webhook_token, Notification_Context, day) else: Notification_Context = \"没有超过\" + str(day) + \"天的应用Nginx日志索引需要被快照删除！\" dingdingNotification(dingding_webhook_token, Notification_Context, day) elif type == 'mysqlslowlog' : snapshoted_deleted_mysqlslowlog_indices = [] for i in getMysqlSlowQueryLogIndex(): cts = time.time() if ((cts - int(i[\"cd\"]) / 1000)) > day * 86400: if 'SUCCESS' in snapshotIndex(i[\"index\"])['snapshot'][\"state\"]: deleteIndex(i['index']) print(i['index'] + \"已在ES中快照并删除！\") snapshoted_deleted_mysqlslowlog_indices.append(i['index']) sentry_sdk.capture_exception(Exception(\"MySQL慢查询日志索引：\" + i['index'] + \"快照失败\")) else: print(\"MySQL慢查询日志索引：\" + i['index'] + \"快照失败\") continue if snapshoted_deleted_mysqlslowlog_indices : Notification_Context = \"成功将以下\" + str(day) + \"天之前的MySQL慢查询日志索引进行了快照\\n\" + \"\\n\".join(str(i) for i in snapshoted_deleted_mysqlslowlog_indices) dingdingNotification(dingding_webhook_token, Notification_Context, day) else: Notification_Context = \"没有超过\" + str(day) + \"天的MySQL慢查询日志索引需要被快照删除！\" dingdingNotification(dingding_webhook_token, Notification_Context, day) def main(): # 快照删除9天之前的应用日志索引 snapshotAndDeleteAppIndex('app',15) # 快照删除7天之前的应用Nginx日志索引 snapshotAndDeleteAppIndex('nginx',10) # 快照删除15天之前的MySQL慢查询日志索引 # snapshotAndDeleteAppIndex('mysqlslowlog',30) if __name__ == \"__main__\" : main() 2、requirements.txt elasticsearch==7.0.0 pyyaml requests sentry_sdk 3、操作步骤 Python版本：3 默认清理策略 快照删除10天前的应用日记索引 快照删除7天前的应用Nginx日记索引 （索引名包含Nginx关键字的） 安装依赖 pip3 install -r requierements.txt 执行脚本 PYTHONIOENCODING=utf-8 python3 es-index-snapshots-clean.py Crontab定时执行脚本：每天凌晨1点执行 0 0 1 * * ? python3 es-index-snapshots-clean.py Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-28 14:15:03 "},"origin/elasticsearch-sample-data.html":{"url":"origin/elasticsearch-sample-data.html","title":"官方示例数据集","keywords":"","body":"一、示例数据集说明 示例数据集下载地址： curl -O https://download.elastic.co/demos/kibana/gettingstarted/8.x/shakespeare.json && \\ curl -O https://download.elastic.co/demos/kibana/gettingstarted/8.x/accounts.zip && \\ curl -O https://download.elastic.co/demos/kibana/gettingstarted/8.x/logs.jsonl.gz && \\ unzip accounts.zip && \\ gunzip logs.jsonl.gz ①shakespeare.json 莎士比亚所有的作品集。数据集的数据组织格式： { \"line_id\": INT, \"play_name\": \"String\", \"speech_number\": INT, \"line_number\": \"String\", \"speaker\": \"String\", \"text_entry\": \"String\", } ②accounts.json 随机生成的虚拟账号信息，数据集的数据组织格式 { \"account_number\": INT, \"balance\": INT, \"firstname\": \"String\", \"lastname\": \"String\", \"age\": INT, \"gender\": \"M or F\", \"address\": \"String\", \"employer\": \"String\", \"email\": \"String\", \"city\": \"String\", \"state\": \"String\" } ③logs.json 随机生成的日志数据，日志数据有几十个不同的字段，但是在教程中关注的字段如下： { \"memory\": INT, \"geo.coordinates\": \"geo_point\" \"@timestamp\": \"date\" } 二、映射数据集 ​ 在导入数据集之前，我们需要为各个字段建立一个映射。映射把索引里的文档划分成逻辑组，定义字段的特性，如字段是否可被搜索、是否被标记、是否能被拆分成多个文字等。 ①映射莎士比亚作品数据集 PUT /shakespeare { \"mappings\": { \"properties\": { \"speaker\": {\"type\": \"keyword\"}, \"play_name\": {\"type\": \"keyword\"}, \"line_id\": {\"type\": \"integer\"}, \"speech_number\": {\"type\": \"integer\"} } } } #因为speaker和play_name字段是“keyword”字段，所以他们不参与处理分析。字符类型的字段被当做做单一单元，即使字段值有多个字符 ②日志数据需要一个映射表明地理位置的经纬度，通过在那些字段使用一个geo_point类型。 PUT /logstash-2015.05.18 { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } PUT /logstash-2015.05.19 { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } PUT /logstash-2015.05.20 { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ③账号数据不需要任何映射，直接用ElasticSearch的bulk API导入数据 ④Curl命令导入 curl -X PUT \"localhost:9200/shakespeare?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"speaker\": {\"type\": \"keyword\"}, \"play_name\": {\"type\": \"keyword\"}, \"line_id\": {\"type\": \"integer\"}, \"speech_number\": {\"type\": \"integer\"} } } } ' curl -X PUT \"localhost:9200/logstash-2015.05.18?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ' curl -X PUT \"localhost:9200/logstash-2015.05.19?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ' curl -X PUT \"localhost:9200/logstash-2015.05.20?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ' 三、导入数据集 1、在数据集所在主机的shell利用curl命令导入数据集 ①导入莎士比亚作品数据集到shakespeare索引里 curl -u elastic:密码 -H 'Content-Type: application/x-ndjson' \\ -XPOST '127.0.0.1:9200/shakespeare/_bulk?pretty' \\ --data-binary @shakespeare.json ②导入日志数据集 curl -u elastic:密码 -H 'Content-Type: application/x-ndjson' \\ -XPOST '127.0.0.1:9200/_bulk?pretty' \\ --data-binary @logs.jsonl ③导入账号数据集到bank索引 curl -u elastic:密码 -H 'Content-Type: application/x-ndjson' \\ -XPOST '127.0.0.1:9200/bank/account/_bulk?pretty' \\ --data-binary @accounts.json 2、验证数据是否导入成功 在kibana的Dev Tools工具利用命令查看所有索引信息 GET _cat/indices/bank,shakespeare,logstash-2015*?v health status index pri rep docs.count docs.deleted store.size pri.store.size green open bank 5 1 1000 0 418.2kb 418.2kb green open shakespeare 5 1 111396 0 17.6mb 17.6mb green open logstash-2015.05.18 5 1 4631 0 15.6mb 15.6mb green open logstash-2015.05.19 5 1 4624 0 15.7mb 15.7mb green open logstash-2015.05.20 5 1 4750 0 16.4mb 16.4mb 四、数据查询 1、只显示某些字段 GET /bank/_search { \"query\": { \"match_all\": {} }, \"_source\": [\"account_number\", \"balance\"] } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match_all\": {} }, \"_source\": [\"account_number\", \"balance\"] }' 2、查询某字段值为20的Doc GET /bank/_search { \"query\": { \"match\": { \"account_number\": 20 } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"account_number\": 20 } } }' 3、查询某字段包含\"mill\"的Doc GET /bank/_search { \"query\": { \"match\": { \"address\": \"mill\" } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"address\": \"mill\" } } }' 4、查询某字段包含\"mill\"或\"lane\"的Doc GET /bank/_search { \"query\": { \"match\": { \"address\": \"mill lane\" } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"address\": \"mill lane\" } } }' GET /bank/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } } ' GET /bank/_search { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' 五、数据搜索及可视化 1、创建索引模式 2、Discover中搜索数据 3、可视化数据 饼图显示bank数据各个收入范围的年龄分布 参考 https://www.elastic.co/guide/en/kibana/7.9/tutorial-build-dashboard.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-10-12 18:33:22 "},"origin/elasticsearch-optimizing.html":{"url":"origin/elasticsearch-optimizing.html","title":"优化","keywords":"","body":"Elasticsearch性能优化 一、写入性能优化 在文档写入时，会根据_routing来计算（OperationRouting类）得出文档要写入哪个分片。这里的写入请求只会写主分片，当主分片写入成功后，会同时把写入请求发送给所有的副本分片，当副本分片写入成功后，会传回返回信息给主分片，主分片得到所有副本分片的返回信息后，再返回给客户端。 在写入时，我们可以在Request自己指定_routing，也可以在Mapping指定文档中的Field值作为_routing。如果没有指定_routing，则会把_id作为_routing进行计算。由于写入时，具有相同_routing的文档一定会分配在同一个分片上，所以如果是自定义的_routing，在查询时，一定要指定_routing进行查询，否则是查询不到文档的。这并不是局限性，恰恰相反，指定_routing的查询，性能上会好很多，因为指定_routing意味着直接去存储数据的shard上搜索，而不会搜索所有shard。 二、索引性能优化 段合并 关闭索引 三、搜索性能优化 优化索引速度 1. 使用bulk批量操作 批量请求将比单文档索引请求产生更好的性能。 为了知道批量请求的最佳大小，您应该在具有单个shard的单个节点上运行基准测试。首先尝试一次索引100个文档，然后索引200个，然后索引400个，等等，在每次基准测试运行时将批量请求中的文档数量增加一倍。当索引速度开始趋于稳定时，您就知道已经达到了数据批量请求的最佳大小。 2. 查询返回大小 尽量使用 Scroll滚动查询API。 3. 按照日期规划索引 4. 索引分片个数设置 5. 索引分片副本数设置 6. 禁止大文档 禁止单个Document的大小超过默认设置http.max_content_length(默认值100MB)（如果单个doc大小超过了设置值，elasticsearch会直接拒绝索引）。 虽然可修改http.max_content_length参数提高默认doc大小，但 Lucene引擎依旧会有2GB大小的限制 单个大doc会加重网络、内存和磁盘的消耗 7. 禁止节点开启Swapping 8. 节点给系统缓存预留内存 文件系统缓存将用于缓冲I / O操作 9. 文档ID尽量自动生成 10. 节点硬件尽量选性能好的 11. 提高索引缓存区大小 12. 使用多线程分散写入操作 使用单个线程发送批处理写入请求 13. 调整索引刷新间：refresh_interval 默认情况下索引的refresh_interval为1秒,这意味着数据写1秒后就可以被搜索到,每次索引的 refresh 会产生一个新的 lucene 段,这会导致频繁的 segment merge 行为,如果你不需要这么高的搜索实时性,应该降低索引refresh 周期,如:index.refresh_interval: 120s 二、优化查询速度 三、优化硬盘使用 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-问题总结.html":{"url":"origin/elasticsearch-问题总结.html","title":"问题总结","keywords":"","body":"一、xpack的monitoring功能“导致”failed to flush export bulks和 there are no ingest nodes in this cluster”报错 原因： xpack的monitoring功能需要定义exporter用于导出监控数据， 默认的exporter是local exporter，也就是直接写入本地的集群，并且要求节点开启了ingest选项。 解决方案: 将集群的结点配置里的ingest角色打开 或者在集群设置elasticsearch.yml里，将local exporter的use ingest关掉 xpack.monitoring.exporters.my_local: type: local use_ingest: false 但一般的，使用local cluster监控自己存在很大的问题，故障发生时，监控也没法看到了。 生产上最好是设置一个单独的监控集群，然后可以配置一个HTTP exporter，将监控数据送往这个监控集群 参考： https://www.elastic.co/guide/en/x-pack/5.5/monitoring-cluster.html#http-exporter-reference https://elasticsearch.cn/question/1915 二、监控日志索引Index的保存期限为7天 Elasticsearch的监控日志索引Index为\".monitoring-*\"开头的，保存期限为7天，7天之后会自动删除。 参考 https://discuss.elastic.co/t/how-system-index-like-monitoring-es-6-2018-02-06-are-being-deleted-automatically/119578 三、字段过大导致kibana搜索是分片失败 报错： The length of [response.keyword] field of [SwiBc3YBv0gFs9LK4P1_] doc of [docc-2020-12-18] index has exceeded [1000000] - maximum allowed to be analyzed for highlighting. This maximum can be set by changing the [index.highlight.max_analyzed_offset] index level setting. For large texts, indexing with offsets or term vectors is recommended! 原因：某个字段超出了字符偏移量上限 解决方案 PUT /分片失败的索引/_settings { \"index\" : { \"highlight.max_analyzed_offset\" : 60000000 } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 14:57:34 "},"origin/kubernete-prometheus.html":{"url":"origin/kubernete-prometheus.html","title":"Kubernetes的监控体系","keywords":"","body":"Kubernetes下的监控体系 一、监控对象 监控对象 示例 监控信息暴露方式 k8s主机 k8s依赖的主机 node exporter以daemonset形式每个节点部署一个 K8s上的中间件 redis、mysql、kafka、mongo 每种服务自带对应的exporter K8s上的系统服务 traefik traefik自带暴露metrics Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-07-24 16:42:18 "},"origin/kube-prometheus.html":{"url":"origin/kube-prometheus.html","title":"kube-prometheus","keywords":"","body":"kube Prometheus 一、简介 为了方便大家使用prometheus，Coreos出了提供了一个Operator，而为了方便大家一站式的监控方案就有了项目kube-prometheus是一个脚本项目，它主要使用jsonnet写成，其作用呢就是模板+参数然后渲染出yaml文件集，主要是作用是提供一个开箱即用的监控栈，用于kubernetes集群的监控和应用程序的监控。 这个项目主要包括以下软件栈 Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs Kube-state-metrics Grafana 说是开箱即用，确实也是我们只需要clone下来，然后kubectl apply ./manifests，manifests目录中生成的是预先生成的yaml描述文件，有诸多不方便的地方，比如说 镜像仓库的地址都在gcr和query.io，这两个地址国内拉起来都费劲 没有持久化存储promethus的数据 二、安装 1、默认安装 git clone https://github.com/coreos/kube-prometheus.git -b release-0.5 cd kube-prometheus kubectl apply -f manifests/setup kubectl apply -f manifests/ 之后手动修改Prometheus的k8s资源声明对象 2、定制化安装 ①安装jb、jsonnet、gojsontoyaml wget -c https://github.com/jsonnet-bundler/jsonnet-bundler/releases/download/v0.4.0/jb-darwin-amd64 -o /usr/local/bin/jb chmod +x /usr/local/bin/jb git clone https://github.com/brancz/gojsontoyaml.git cd gojsontoyaml go build chmod +x gojsontoyaml mv gojsontoyaml /usr/local/bin/ echo '{\"test\":\"test string with\\\\nmultiple lines\"}' | gojsontoyaml ②下载依赖 mkdir kube-prometheus-k8s118; cd kube-prometheus-k8s118 jb init # 初始化jb，创建依赖描述文件`jsonnetfile.json` jb install github.com/coreos/kube-prometheus/jsonnet/kube-prometheus@release-0.5 jb update ③编写编译命令的脚本build.sh #!/usr/bin/env bash # This script uses arg $1 (name of *.jsonnet file to use) to generate the manifests/*.yaml files. set -e set -x # only exit with zero if all commands of the pipeline exit successfully set -o pipefail # Make sure to use project tooling PATH=\"$(pwd)/tmp/bin:${PATH}\" # Make sure to start with a clean 'manifests' dir rm -rf manifests mkdir -p manifests/setup # Calling gojsontoyaml is optional, but we would like to generate yaml, not json jsonnet -J vendor -m manifests \"${1-example.jsonnet}\" | xargs -I{} sh -c 'cat {} | gojsontoyaml > {}.yaml' -- {} # Make sure to remove json files find manifests -type f ! -name '*.yaml' -delete rm -f kustomization ④编写定制化Prometheus的配置文件 在第三部jb init产生的k8s118.json中进行定制化Prometheus local k = import 'ksonnet/ksonnet.beta.3/k.libsonnet'; local ingress = k.extensions.v1beta1.ingress ; local ingressRule = ingress.mixin.spec.rulesType; local pvc = k.core.v1.persistentVolumeClaim; local registry = import 'registry.libsonnet'; local imagepullsecret = k.apps.v1beta2.deployment.mixin.spec.template.spec; local httpIngressPath = ingressRule.mixin.http.pathsType; local secret = k.core.v1.secret; local kp = (import 'kube-prometheus/kube-prometheus.libsonnet') + { _config+:: { namespace: 'monitoring', imageRepos+:: { prometheus: \"192.168.1.60/monitoring/prometheus\", alertmanager: \"192.168.1.60/monitoring/alertmanager\", kubeStateMetrics: \"192.168.1.60/monitoring/kube-state-metrics\", kubeRbacProxy: \"192.168.1.60/monitoring/kube-rbac-proxy\", nodeExporter: \"192.168.1.60/monitoring/node-exporter\", prometheusOperator: \"192.168.1.60/monitoring/prometheus-operator\", grafana: \"192.168.1.60/monitoring/grafana\", prometheusAdapter: \"192.168.1.60/monitoring/k8s-prometheus-adapter-amd64\", metricsServer: \"192.168.1.60/monitoring/metrics-server-amd64:v0.2.0\", }, grafana+:: { config+: { sections+: { server+: { root_url: 'http://grafana.apps.k8s118.curiouser.com/', }, smtp: { enabled: 'true', host: 'smtp.163.com:25', user: '****', password: '****', from_address: '****', from_name: 'Grafana', skip_verify: 'true', }, }, }, }, registry+:: { name: \"192.168.1.60\", secret_name: \"harbor-secret\", username: \"****\", password: \"****\", secret: local data = { 'auths': { [$._config.registry.name]: { auth: std.base64($._config.registry.username + \":\" + $._config.registry.password), }, }, }; local base = {'.dockerconfigjson': std.base64(std.toString(data))}; local name = $._config.registry.secret_name; secret.mixin.metadata.withNamespace($._config.namespace) + secret.new(name=name, data=base, type='kubernetes.io/dockerconfigjson') }, }, alertmanager+: { alertmanager+: { spec+: { imagePullSecrets: [{name: $._config.registry.secret_name}], } }, }, grafana+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, kubeStateMetrics+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, nodeExporter+: { daemonset+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, prometheusAdapter+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, prometheusOperator+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, prometheus+:: { prometheus+: { spec+: { retention: '7d', storage: { volumeClaimTemplate: pvc.new() + pvc.mixin.spec.withAccessModes('ReadWriteOnce') + pvc.mixin.spec.resources.withRequests({ storage: '50Gi' }), }, }, }, }, ingress+:: { 'grafana': ingress.new() + ingress.mixin.metadata.withName('grafana') + ingress.mixin.metadata.withNamespace($._config.namespace) + ingress.mixin.spec.withRules( ingressRule.new() + ingressRule.withHost('grafana.apps.k8s118.curiouser.com') + ingressRule.mixin.http.withPaths( httpIngressPath.new() + httpIngressPath.mixin.backend.withServiceName('grafana') + httpIngressPath.mixin.backend.withServicePort('http') ) ), 'alertmanager-main': ingress.new() + ingress.mixin.metadata.withName('alertmanager-main') + ingress.mixin.metadata.withNamespace($._config.namespace) + ingress.mixin.spec.withRules( ingressRule.new() + ingressRule.withHost('alertmanager.apps.k8s118.curiouser.com') + ingressRule.mixin.http.withPaths( httpIngressPath.new() + httpIngressPath.mixin.backend.withServiceName('alertmanager-main') + httpIngressPath.mixin.backend.withServicePort('web') ) ), 'prometheus-k8s': ingress.new() + ingress.mixin.metadata.withName('prometheus-k8s') + ingress.mixin.metadata.withNamespace($._config.namespace) + ingress.mixin.spec.withRules( ingressRule.new() + ingressRule.withHost('prometheus.apps.k8s118.curiouser.com') + ingressRule.mixin.http.withPaths( httpIngressPath.new() + httpIngressPath.mixin.backend.withServiceName('prometheus-k8s') + httpIngressPath.mixin.backend.withServicePort('web') ) ), } }; { ['setup/0namespace-' + name]: kp.kubePrometheus[name] for name in std.objectFields(kp.kubePrometheus) } + // { ['setup/0secret-' + name]: kp.secret[name] for name in std.objectFields(kp.secret)}+ { ['setup/prometheus-operator-' + name]: kp.prometheusOperator[name] for name in std.filter((function(name) name != 'serviceMonitor'), std.objectFields(kp.prometheusOperator)) } + // serviceMonitor is separated so that it can be created after the CRDs are ready { 'prometheus-operator-serviceMonitor': kp.prometheusOperator.serviceMonitor } + { ['node-exporter-' + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } + { ['kube-state-metrics-' + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } + { ['alertmanager-' + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } + { ['prometheus-' + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } + { ['prometheus-adapter-' + name]: kp.prometheusAdapter[name] for name in std.objectFields(kp.prometheusAdapter) } + { ['grafana-' + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) } + { [name + '-ingress']: kp.ingress[name] for name in std.objectFields(kp.ingress) } ⑤开始编译 ./build.sh jsonnetfile.json 编译完成后，会在manifests目录下生产K8s资源声明文件。 ⑥生产的K8s资源对象 manifests/setup/*.yaml namespace: monitoring customresourcedefinition.apiextensions.k8s.io: alertmanagers.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: podmonitors.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: prometheuses.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: prometheusrules.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: servicemonitors.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: thanosrulers.monitoring.coreos.com clusterrole.rbac.authorization.k8s.io: prometheus-operator clusterrolebinding.rbac.authorization.k8s.io: prometheus-operator deployment.apps: prometheus-operator service: prometheus-operator serviceaccount: prometheus-operator manifests/*.yaml alertmanager.monitoring.coreos.com: main secret: alertmanager-main service: alertmanager-main serviceaccount: alertmanager-main servicemonitor.monitoring.coreos.com: alertmanager secret: grafana-datasources configmap: grafana-dashboard-apiserver configmap: grafana-dashboard-cluster-total configmap: grafana-dashboard-controller-manager configmap: grafana-dashboard-k8s-resources-cluster configmap: grafana-dashboard-k8s-resources-namespace configmap: grafana-dashboard-k8s-resources-node configmap: grafana-dashboard-k8s-resources-pod configmap: grafana-dashboard-k8s-resources-workload configmap: grafana-dashboard-k8s-resources-workloads-namespace configmap: grafana-dashboard-kubelet configmap: grafana-dashboard-namespace-by-pod configmap: grafana-dashboard-namespace-by-workload configmap: grafana-dashboard-node-cluster-rsrc-use configmap: grafana-dashboard-node-rsrc-use configmap: grafana-dashboard-nodes configmap: grafana-dashboard-persistentvolumesusage configmap: grafana-dashboard-pod-total configmap: grafana-dashboard-prometheus-remote-write configmap: grafana-dashboard-prometheus configmap: grafana-dashboard-proxy configmap: grafana-dashboard-scheduler configmap: grafana-dashboard-statefulset configmap: grafana-dashboard-workload-total configmap: grafana-dashboards deployment.apps: grafana service: grafana serviceaccount: grafana servicemonitor.monitoring.coreos.com: grafana clusterrole.rbac.authorization.k8s.io: kube-state-metrics clusterrolebinding.rbac.authorization.k8s.io: kube-state-metrics deployment.apps: kube-state-metrics service: kube-state-metrics serviceaccount: kube-state-metrics servicemonitor.monitoring.coreos.com: kube-state-metrics clusterrole.rbac.authorization.k8s.io: node-exporter clusterrolebinding.rbac.authorization.k8s.io: node-exporter daemonset.apps: node-exporter service: node-exporter serviceaccount: node-exporter servicemonitor.monitoring.coreos.com: node-exporter apiservice.apiregistration.k8s.io: v1beta1.metrics.k8s.io configured clusterrole.rbac.authorization.k8s.io: prometheus-adapter clusterrole.rbac.authorization.k8s.io: system:aggregated-metrics-reader clusterrolebinding.rbac.authorization.k8s.io: prometheus-adapter clusterrolebinding.rbac.authorization.k8s.io: resource-metrics:system:auth-delegator clusterrole.rbac.authorization.k8s.io: resource-metrics-server-resources configmap: adapter-config deployment.apps: prometheus-adapter rolebinding.rbac.authorization.k8s.io: resource-metrics-auth-reader service: prometheus-adapter serviceaccount: prometheus-adapter clusterrole.rbac.authorization.k8s.io: prometheus-k8s clusterrolebinding.rbac.authorization.k8s.io: prometheus-k8s servicemonitor.monitoring.coreos.com: prometheus-operator prometheus.monitoring.coreos.com: k8s rolebinding.rbac.authorization.k8s.io: prometheus-k8s-config rolebinding.rbac.authorization.k8s.io: prometheus-k8s rolebinding.rbac.authorization.k8s.io: prometheus-k8s rolebinding.rbac.authorization.k8s.io: prometheus-k8s role.rbac.authorization.k8s.io: prometheus-k8s-config role.rbac.authorization.k8s.io: prometheus-k8s role.rbac.authorization.k8s.io: prometheus-k8s role.rbac.authorization.k8s.io: prometheus-k8s prometheusrule.monitoring.coreos.com: prometheus-k8s-rules service: prometheus-k8s serviceaccount: prometheus-k8s servicemonitor.monitoring.coreos.com: prometheus servicemonitor.monitoring.coreos.com: kube-apiserver servicemonitor.monitoring.coreos.com: coredns servicemonitor.monitoring.coreos.com: kube-controller-manager servicemonitor.monitoring.coreos.com: kube-scheduler servicemonitor.monitoring.coreos.com: kubelet ⑦预拉取镜像并推送到私有仓库中 sync-to-internal-registry.jsonnet local kp = import 'kube-prometheus/kube-prometheus.libsonnet'; local l = import 'kube-prometheus/lib/lib.libsonnet'; local config = kp._config; local makeImages(config) = [ { name: config.imageRepos[image], tag: config.versions[image], } for image in std.objectFields(config.imageRepos) ]; local upstreamImage(image) = '%s:%s' % [image.name, image.tag]; local downstreamImage(registry, image) = '%s/%s:%s' % [registry, l.imageName(image.name), image.tag]; local pullPush(image, newRegistry) = [ 'docker pull %s' % upstreamImage(image), 'docker tag %s %s' % [upstreamImage(image), downstreamImage(newRegistry, image)], 'docker push %s' % downstreamImage(newRegistry, image), ]; local images = makeImages(config); local output(repository) = std.flattenArrays([ pullPush(image, repository) for image in images ]); function(repository='my-registry.com/repository') std.join('\\n', output(repository)) 生成拉取镜像，并修改推送镜像的命令 $ jsonnet -J vendor -S --tla-str repository=192.168.1.60/monitoring sync-to-internal-registry.jsonnet # 会生成以下命令，复制执行 docker pull quay.io/prometheus/alertmanager:v0.20.0 docker tag quay.io/prometheus/alertmanager:v0.20.0 192.168.1.60/monitoring/alertmanager:v0.20.0 docker push 192.168.1.60/monitoring/alertmanager:v0.20.0 docker pull jimmidyson/configmap-reload:v0.3.0 docker tag jimmidyson/configmap-reload:v0.3.0 192.168.1.60/monitoring/configmap-reload:v0.3.0 docker push 192.168.1.60/monitoring/configmap-reload:v0.3.0 docker pull grafana/grafana:6.6.0 docker tag grafana/grafana:6.6.0 192.168.1.60/monitoring/grafana:6.6.0 docker push 192.168.1.60/monitoring/grafana:6.6.0 docker pull quay.io/coreos/kube-rbac-proxy:v0.4.1 docker tag quay.io/coreos/kube-rbac-proxy:v0.4.1 192.168.1.60/monitoring/kube-rbac-proxy:v0.4.1 docker push 192.168.1.60/monitoring/kube-rbac-proxy:v0.4.1 docker pull quay.io/coreos/kube-state-metrics:1.9.5 docker tag quay.io/coreos/kube-state-metrics:1.9.5 192.168.1.60/monitoring/kube-state-metrics:1.9.5 docker push 192.168.1.60/monitoring/kube-state-metrics:1.9.5 docker pull quay.io/prometheus/node-exporter:v0.18.1 docker tag quay.io/prometheus/node-exporter:v0.18.1 192.168.1.60/monitoring/node-exporter:v0.18.1 ⑧部署定制化的Prometheus到k8s集群 kubectl apply -f manifests/setup kubectl apply -f manifests/ # 创建harbor registry用户登录信息的secret kubectl create secret docker-registry harbor-secret --docker-server=192.168.1.60 --docker-username=k8s --docker-password=*** --docker-email=*** -n monitoring # 批量在monitoring命名空间下serviceaccount中添加imagePullSercret for i in `k get sa |awk '{print $1}'|grep -v NAME` ; do kubectl -n monitoring patch serviceaccount $i -p '{\"imagePullSecrets\": [{\"name\": \"harbor-secret\"}]}' ;done ⑨(可选)清理Prometheus资源 kubectl delete --ignore-not-found=true -f manifests/ -f manifests/setup 3、参考 https://qingmu.io/2019/08/30/Customize-your-kube-prometheus/ https://github.com/coreos/kube-prometheus#cluster-creation-tools https://github.com/coreos/kube-prometheus#customization-examples https://github.com/coreos/kube-prometheus/blob/master/docs/developing-prometheus-rules-and-grafana-dashboards.md https://github.com/coreos/kube-prometheus/issues/308 三、ServiceMonitor服务发现 1、监控k8s集群外的Ceph ①在cephMonitor节点部署Ceph exporter docker run -d --restart=always -v /etc/ceph:/etc/ceph -p=9128:9128 --name=ceph-export digitalocean/ceph_exporter --rgw.mode=1 ②创建对应的Endpoint、Service、ServiceMonitor apiVersion: v1 kind: Endpoints metadata: name: outcluster-exporter-ceph namespace: monitoring subsets: - addresses: - ip: 192.168.1.60 ports: - name: http port: 9128 protocol: TCP --- apiVersion: v1 kind: Service metadata: # 关键点：为了能被servicemonitor识别，添加对应的标签 labels: prometheus-target: outcluster-exporter-ceph name: outcluster-exporter-ceph namespace: monitoring spec: type: ClusterIP ports: - name: http port: 9128 protocol: TCP targetPort: http # 关键点：由于ceph exporter不在k8s集群中，不写常规service中的selector。 --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-apps: outcluster-exporter-ceph name: outcluster-exporter-ceph namespace: monitoring spec: endpoints: - interval: 15s port: http namespaceSelector: matchNames: - monitoring selector: matchLabels: # 指定prometheus去匹配有对应标签的service中监控数据 prometheus-target: outcluster-exporter-ceph 2、监控k8s集群外的主机 ①对应主机部署Node exporter 省略 ②创建对应的Endpoint、Service、ServiceMonitor apiVersion: v1 kind: Endpoints metadata: name: outcluster-exporter-node-tools namespace: monitoring labels: prometheus-target: outcluster-exporter-node subsets: - addresses: - ip: 192.168.1.60 ports: - name: http port: 9100 protocol: TCP --- apiVersion: v1 kind: Service metadata: # 关键点：为了能被servicemonitor识别，添加对应的标签 labels: prometheus-target: outcluster-exporter-node name: outcluster-exporter-node-tools namespace: monitoring spec: type: ClusterIP ports: - name: http port: 9100 protocol: TCP targetPort: http # 关键点：由于node exporter不在k8s集群中，不写常规service中的selector。 --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-apps: outcluster-exporter-node name: outcluster-exporter-node namespace: monitoring spec: endpoints: - interval: 15s port: http namespaceSelector: matchNames: - monitoring selector: matchLabels: # 指定prometheus去匹配有对应标签的service中监控数据 prometheus-target: outcluster-exporter-node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-20 11:28:11 "},"origin/prometheus-basic.html":{"url":"origin/prometheus-basic.html","title":"Prometheus基础概念","keywords":"","body":"Prometheus基础概念 一、简介 prometheus 是一个开源的系统监控和告警的工具包，其采用pull方式采集时间序列，通过http协议传输。 prometheus的优势在于，它是一个基于服务的告警系统，针对不同的服务，有不同的exporter，可以实现不一样的效果。 最初由SoundCloud发布。它通过HTTP协议从远程的机器收集数据并存储在本地的时序数据库上。它提供了一个简单的网页界面、一个功能强大的查询语言以及HTTP接口等等。 Prometheus可以通过安装在远程机器上的exporter来收集监控数据。 官网：https://prometheus.io/ Github：https://github.com/prometheus/prometheus 二、架构 prometheus的核心是一个时间序列数据库，我们可以通过它抓取并存储数据，并通过prometheus定义的一些查询语句来获取我们需要的数据 exporter的核心是一个静态web，通过不断更新的静态web暴露metric值 alertmanager是一个报警接口，接收prometheus推送的告警，并通过自己定义的一些规则去进行告警 Pushgateway 程序，主要是实现接收由Client push过来的指标数据，在指定的时间间隔，由主程序来抓取 三、数据模型 Prometheus 中存储的数据为时间序列，是由 metric 的名字和一系列的标签（键值对）唯一标识的，不同的标签则代表不同的时间序列 metric ：该名字应该具有语义，一般用于表示 metric 的功能，例如：http_requests_total, 表示 http 请求的总数。 metric 名字由 ASCII 字符，数字，下划线，以及冒号组成，且必须满足正则表达式 [a-zA-Z_:][a-zA-Z0-9_:]* 标 签：使同一个时间序列有了不同维度的识别。例如 http_requests_total{method=\"Get\"} 表示所有 http 请求中的 Get 请求。当 method=\"post\" 时，则为新的一个 metric。 标签中的键由 ASCII 字符，数字，以及下划线组成，且必须满足正则表达式 [a-zA-Z_:][a-zA-Z0-9_:]*。 样 本：实际的时间序列，每个序列包括一个 float64 的值和一个毫秒级的时间戳。 格式：{=, …}， 例如：http_requests_total{method=\"POST\",endpoint=\"/api/tracks\"}。 四、Metrics类型 Counter：只增不减的计数器 计数器可以用于记录只会增加不会减少的指标类型。比如记录应用请求的总量，cpu使用时间等 对于Counter类型的指标，只包含一个inc()方法，用于计数器+1 一般而言，Counter类型的metrics指标在命名中我们使用_total结束，如http_requests_total Gauge：可增可减的仪表盘 对于这类可增可减的指标，可以用于反应应用的当前状态。例如在监控主机时，主机当前空闲的内存大小，可用内存大小。或者容器当前的cpu使用率,内存使用率。 对于Gauge指标的对象则包含两个主要的方法inc()以及dec(),用户添加或者减少计数。 Histogram：自带buckets区间用于统计分布统计图 主要用于在指定分布范围内(Buckets)记录大小或者事件发生的次数。 Summary:：客户端定义的数据分布统计图 Summary和Histogram非常类型相似，都可以统计事件发生的次数或者大小，以及其分布情况。 Summary和Histogram都提供了对于事件的计数_count以及值的汇总_sum。 因此使用_count,和_sum时间序列可以计算出相同的内容，例如http每秒的平均响应时间：rate(basename_sum[5m]) /rate(basename_count[5m])。 同时Summary和Histogram都可以计算和统计样本的分布情况，比如中位数，9分位数等等。其中 0.0 不同在于Histogram可以通过histogram_quantile函数在服务器端计算分位数。 而Sumamry的分位数则是直接在客户端进行定义。 因此对于分位数的计算。 Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。相对的对于客户端而言Histogram消耗的资源更少。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-20 11:31:05 "},"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html":{"url":"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html","title":"Ceph Exporter","keywords":"","body":"一、Overview 由于在Openshift集群外使用了Ceph RBD和Ceph Filesystem作为PV的后端动态存储文件系统，所以ceph的集群监控也可使用Prometheus体系中的Ceph Exporter，接入到Openshift集群中的Prometheus。 二、以DaemonSet的形式部署Ceph Exporter --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: ceph-exporter name: ceph-exporter spec: selector: matchLabels: app: ceph-exporter template: metadata: labels: app: ceph-exporter spec: containers: - image: digitalocean/ceph_exporter imagePullPolicy: IfNotPresent name: ceph-exporter ports: - containerPort: 9128 hostPort: 9128 name: http protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/ceph name: ceph-confdir resources: limits: cpu: 200m memory: 400Mi requests: cpu: 100m memory: 200Mi dnsPolicy: ClusterFirst hostNetwork: true hostPID: true nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: node-exporter　#使用Node-Exporter创建的ServiceAccount serviceAccountName: node-exporter terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - hostPath: path: /etc/ceph #将ceph节点的配置文件路径暴露给exporter type: \"\" name: ceph-confdir templateGeneration: 1 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate --- apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: ceph-exporter name: ceph-exporter subsets: - addresses: - ip: 192.168.1.96 nodeName: allinone.okd311.curiouser.com targetRef: kind: Pod ports: - name: http port: 9128 protocol: TCP --- apiVersion: v1 kind: Service metadata: annotations: prometheus.io/port: '9128' prometheus.io/scrape: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: clusterIP: None ports: - name: http port: 9128 protocol: TCP targetPort: http selector: app: ceph-exporter sessionAffinity: None type: ClusterIP --- apiVersion: route.openshift.io/v1 kind: Route metadata: annotations: openshift.io/host.generated: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: port: targetPort: http to: kind: Service name: ceph-exporter weight: 100 wildcardPolicy: None 三、Ceph Exporter对接Prometheus 备份Prometheus原配置文件secret Prometheus原始配置secret文件 创建新的Prometheus配置secret 在原Prometheus配置文件中添加consul 服务发现和ceph-exporter相关的配置 ...省略... - job_name: consul-prometheus metrics_path: /monitor/prometheus scrape_interval: 20s scheme: http scrape_timeout: 5s consul_sd_configs: - server: consul-server.consul.svc:8500 services: [] scheme: http allow_stale: true refresh_interval: 20s - job_name: openshift-monitoring/ceph-exporter/0 honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - openshift-monitoring scrape_interval: 30s scheme: http relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: ceph-exporter - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_pod_name target_label: pod - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_service_name target_label: job replacement: ${1} - source_labels: - __meta_kubernetes_service_label_k8s_app target_label: job regex: (.+) replacement: ${1} - target_label: endpoint replacement: http ...省略... 替换Prometheus的POD secret 四、验证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/prometheus-blackbox-exporter.html":{"url":"origin/prometheus-blackbox-exporter.html","title":"Blackbox Exporter","keywords":"","body":"Blackbox exporter 一、简介 黑盒监控即以用户的身份测试服务的外部可见性，常见的黑盒监控包括HTTP 探针、TCP 探针 等用于检测站点或者服务的可访问性，以及访问效率等。 黑盒监控相较于白盒监控最大的不同在于黑盒监控是以故障为导向当故障发生时，黑盒监控能快速发现故障，而白盒监控则侧重于主动发现或者预测潜在的问题。一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题。 Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测 GitHub：https://github.com/prometheus/blackbox_exporter 二、安装 1、二进制 blackbox_exporter_version=0.18.0 && \\ wget https://github.com/prometheus/blackbox_exporter/releases/download/v$blackbox_exporter_version/blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz && \\ tar -zxvf blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz -C /opt && \\ rm -f blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz && \\ ln -s /opt/blackbox_exporter-0.18.0.linux-amd64 /opt/blackbox_exporter && \\ echo -e \"export BLACKBOX_EXPORTER_HOEM=/opt/blackbox_exporter\\nexport PATH=\\$PATH:\\$BLACKBOX_EXPORTER_HOEM\" >> /etc/profile && \\ source /etc/profile && \\ blackbox_exporter --help 命令参数 usage: blackbox_exporter [] Flags: -h, --help Show context-sensitive help (also try --help-long and --help-man). --config.file=\"blackbox.yml\" Blackbox exporter configuration file. --web.listen-address=\":9115\" The address to listen on for HTTP requests. --timeout-offset=0.5 Offset to subtract from timeout in seconds. --config.check If true validate the config file and then exit. --history.limit=100 The maximum amount of items to keep in the history. --web.external-url= The URL under which Blackbox exporter is externally reachable (for example, if Blackbox exporter is served via a reverse proxy). Used for generating relative and absolute links back to Blackbox exporter itself. If the URL has a path portion, it will be used to prefix all HTTP endpoints served by Blackbox exporter. If omitted, relevant URL components will be derived automatically. --web.route-prefix= Prefix for the internal routes of web endpoints. Defaults to path of --web.external-url. --log.level=info Only log messages with the given severity or above. One of: [debug, info, warn, error] --log.format=logfmt Output format of log messages. One of: [logfmt, json] --version Show application version. 启动 nohup blackbox_exporter --config.file=配置文件路径 --其他参数 > /var/log/blackbox_exporter.log 2>&1 & 2、Docker docker run --rm -d \\ -p 9115:9115 \\ --name blackbox_exporter \\ -v `配置文件路径`:/config \\ prom/blackbox-exporter:master --config.file=/config/blackbox.yml 3、Kubernetes apiVersion: apps/v1 kind: Deployment metadata: name: blackbox namespace: monitoring labels: app: blackbox-exporter spec: replicas: 2 revisionHistoryLimit: 3 selector: matchLabels: app: blackbox-exporter strategy: rollingUpdate: maxSurge: 30% maxUnavailable: 30% type: RollingUpdate template: metadata: labels: app: blackbox-exporter spec: containers: - image: prom/blackbox-exporter:master name: blackbox-exporter args: - --config.file=/etc/blackbox_exporter/blackbox.yml # ConfigMap 中的配置文件 - --log.level=info # 日志级别，可以把级别调到 error ports: - containerPort: 9115 name: http volumeMounts: - name: config mountPath: /etc/blackbox_exporter volumes: - name: config configMap: name: blackbox-config nodeSelector: role: monitoring --- apiVersion: v1 kind: ConfigMap metadata: name: blackbox-config namespace: monitoring data: blackbox.yml: |- modules: http_2xx: prober: http timeout: 10s http: valid_status_codes: [0,200] baidu-header: prober: http timeout: 10s http: valid_status_codes: [0,200] method: GET headers: Access-Token: *** --- apiVersion: v1 kind: Service metadata: labels: app: blackbox-exporter name: blackbox-exporter namespace: monitoring spec: ports: - name: http port: 9115 targetPort: http selector: app: blackbox-exporter 三、配置文件 配置文件详解：https://github.com/prometheus/blackbox_exporter/blob/master/CONFIGURATION.md 示例配置文件：https://github.com/prometheus/blackbox_exporter/blob/master/example.yml 1、配置文件结构 2、配置值类型 : 布尔值，可选 true | false : 整型值 : 与正则表达式[0-9] +（ms | [smhdwy]）匹配的持续时间 : 当前工作目录中的有效路径 : 字符串 : 包含密码的常规字符串，例如密码 : 正则表达式 3、HTTP探针配置 # 此探针接受的响应状态代码。默认为2xx。 [ valid_status_codes: , ... | default = 2xx ] # 此探针接受的HTTP版本。 [ valid_http_versions: , ... ] # The HTTP method the probe will use. [ method: | default = \"GET\" ] # 为探针设置的HTTP标头。 headers: [ : ... ] # 用于解压缩响应的压缩算法（gzip，br，deflate，identity）。如果指定了“ Accept-Encoding”标头，则必须使压缩算法，表示使用此选项是可接受的。例如可以使用`compression：gzip`和`Accept-Encoding：br，gzip`或`Accept-Encoding：br; q = 1.0，gzip; q = 0.9`。 gzip是 # 可接受的质量低于br的质量不会使配置无效，因为您可能会测试服务器即使请求也不会返回br编码的内容。在另一方面，“ compression：gzip”和“ Accept-Encoding：br，identity”不是有效的配置，因为您要求不返回gzip，并尝试解压缩服务器返回的任何内容都可能会失败。[压缩： |默认=“”] [ compression: | default = \"\" ] # 探针是否将遵循任何重定向 [ no_follow_redirects: | default = false ] # 如果存在SSL，则探测失败 [ fail_if_ssl: | default = false ] # 如果不存在SSL，则探测失败。 [ fail_if_not_ssl: | default = false ] # 如果响应内容与正则表达式匹配，则探测失败 fail_if_body_matches_regexp: [ - , ... ] # 如果响应内容与正则表达式不匹配，则探测失败 fail_if_body_not_matches_regexp: [ - , ... ] # 如果响应头与正则表达式匹配，则探测失败。对于具有多个值的标头，如果至少一个匹配，则失败。 fail_if_header_matches: [ - , ... ] # 如果响应头与正则表达式不匹配，则探测失败。对于具有多个值的标头，如果一个也不匹配，则失败。 fail_if_header_not_matches: [ - , ... ] # 为此探针配置TLS协议 tls_config: [ ] # 为此探针配置HTTP基本身份验证凭据。 basic_auth: [ username: ] [ password: ] [ password_file: ] # 为此探针配置访问目标的Bearer token [ bearer_token: ] # 为此探针配置访问目标的Bearer token文件 [ bearer_token_file: ] # 用于连接到目标的HTTP代理服务器。 [ proxy_url: ] # 为此探针配置IP协议（ip4，ip6） [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: | default = true ] # 为此探针配置访问目标的HTTP请求主体。 body: [ ] 匹配Header的正则表达式配置 header: , regexp: , [ allow_missing: | default = false ] 4、TCP探针配置 # The IP protocol of the TCP probe (ip4, ip6). [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: ] # The source IP address. [ source_ip_address: ] # The query sent in the TCP probe and the expected associated response. # starttls upgrades TCP connection to TLS. query_response: [ - [ [ expect: ], [ send: ], [ starttls: ] ], ... ] # Whether or not TLS is used when the connection is initiated. [ tls: ] # Configuration for TLS protocol of TCP probe. tls_config: [ ] 5、DNS探针配置 # The IP protocol of the DNS probe (ip4, ip6). [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: ] # The source IP address. [ source_ip_address: ] [ transport_protocol: | default = \"udp\" ] # udp, tcp # Whether to use DNS over TLS. This only works with TCP. [ dns_over_tls: ] # Configuration for TLS protocol of DNS over TLS probe. tls_config: [ ] query_name: [ query_type: | default = \"ANY\" ] [ query_class: | default = \"IN\" ] # List of valid response codes. valid_rcodes: [ - ... | default = \"NOERROR\" ] validate_answer_rrs: fail_if_matches_regexp: [ - , ... ] fail_if_all_match_regexp: [ - , ... ] fail_if_not_matches_regexp: [ - , ... ] fail_if_none_matches_regexp: [ - , ... ] validate_authority_rrs: fail_if_matches_regexp: [ - , ... ] fail_if_all_match_regexp: [ - , ... ] fail_if_not_matches_regexp: [ - , ... ] fail_if_none_matches_regexp: [ - , ... ] validate_additional_rrs: fail_if_matches_regexp: [ - , ... ] fail_if_all_match_regexp: [ - , ... ] fail_if_not_matches_regexp: [ - , ... ] fail_if_none_matches_regexp: [ - , ... ] 6、ICMP探针配置 # The IP protocol of the ICMP probe (ip4, ip6). [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: ] # The source IP address. [ source_ip_address: ] # Set the DF-bit in the IP-header. Only works with ip4, on *nix systems and # requires raw sockets (i.e. root or CAP_NET_RAW on Linux). [ dont_fragment: | default = false ] # The size of the payload. [ payload_size: ] 7、TLS相关配置 # Disable target certificate validation. [ insecure_skip_verify: | default = false ] # The CA cert to use for the targets. [ ca_file: ] # The client cert file for the targets. [ cert_file: ] # The client key file for the targets. [ key_file: ] # Used to verify the hostname for the targets. [ server_name: ] 参考： https://www.qikqiak.com/post/blackbox-exporter-on-prometheus/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-28 18:26:13 "},"origin/grafana-backup-restore.html":{"url":"origin/grafana-backup-restore.html","title":"Grafana的备份恢复","keywords":"","body":"Grafana的备份恢复与升级 一、备份恢复 官方文档：https://grafana.com/docs/grafana/latest/installation/upgrading/#backup 数据存储文件 MySQL 备份： mysqldump -u root -p[root_password] [grafana] > grafana_backup.sql 恢复：mysql -u root -p grafana Postgres 备份： pg_dump grafana > grafana_backup 恢复：psql grafana Sqlite(默认) 备份：直接备份DB文件grafana.db，默认路径：/var/lib/grafana/ 恢复： 直接将备份的grafana.db文件复制到/var/lib/grafana/下 修改权限：sudo chown nobody.nogroup grafana.db && sudo chmod 640 grafana.db 配置文件 备份：直接备份/etc/grafana/grafana.ini。（对于部署在k8s中的，配置文件是使用configmap挂载的可以不用备份） 恢复：直接恢复使用备份文件/etc/grafana/grafana.ini 已安装的插件 备份：直接备份插件目录 恢复 直接恢复备份的插件目录 升级插件：grafana-cli plugins update-all 二、问题总结 1、删除默认组织后使用SQLite备份文件grafana.db恢复时报错 报错信息： Datasource provisioning error: failed to provision \"prometheus\" data source: Organization not found 解决方案： ​ 使用Navicat连接SQLite备份文件grafana.db，在main.org表中添加一条记录 INSERT INTO \"main\".\"org\" (\"id\", \"version\", \"name\", \"address1\", \"address2\", \"city\", \"state\", \"zip_code\", \"country\", \"billing_email\", \"created\", \"updated\") VALUES (2, 3, 'Main Org.', '', '', '', '', '', '', NULL, '2020-11-26 03:39:06', '2020-11-26 03:39:06'); Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-17 13:12:20 "},"origin/sentry.html":{"url":"origin/sentry.html","title":"Sentry日志聚合告警平台","keywords":"","body":"一、简介 虽然我们有很多工具可以让开发工作变得更容易，但是发现和排查线上问题的过程仍然在很多时候让我们觉得很痛苦。当生产系统中产生了一个bug时，我们如何快速地得到报警？如何评估它的影响和紧迫性？如何快速地找到问题的根源？当hotfix完修复程序后，又如何知道它是否解决了问题？ Sentry在帮助我们与现有流程集成时回答了这些问题。例如，线上有一个bug，代码的某处逻辑的NullPointerException造成了这个问题，Sentry会立即发现错误，并通过邮件或其他基于通知规则的集成通知到相关责任人员，这个通知可以把我们引入到一个指示板，这个指示板为我们提供了快速分类问题所需的上下文，如：频率、用户影响、代码那一部分受到影响以及那个团队可能是问题的所有者。 Sentry是一个实时事件的日志聚合平台。 Sentry 的目的是为了让我们专注于系统与程序的异常信息，目的是提高排查问题的效率，日志事件的量到达一个限制时甚至丢弃一些内容。官方也提倡正确设置 Sentry 接收的日志 level 的同时，用户也能继续旧的日志备份 Sentry 是带有一定策略的问题分析工具，以样本的形式展示部分原始日志的信息。信息不全面的同时，使用过程中也可能出现 Sentry 聚合所带来的负面影响，特别是日志记录质量不够的情况下。 与传统的监控系统相比，Sentry 更依赖于发出的日志报告，而另外一些隐藏的逻辑问题或者业务问题很可能是不会得到反馈的。不能作为日志的替代品。 sentry主要是为让我们专注于系统和程序的异常信息，提高排查效率，日志事件的量达到一个限制值的时候可能还会丢弃一些内容。官方也提倡正确设置sentry接收的日志level的等级时，也能继续旧的日志备份。 不是排查的万能工具 sentry是带有问题聚合功能的分析工具，所以如果样本提供的内容不全面。日志记录的质量不高的情况，对于错误的快速排查，可能没有实质性的帮助。 不能作为传统监控的替代品 与传统监控系统相比，sentry更依赖发出的日志报告，而另外一些隐藏的逻辑问题或者业务问题可能不会得到反馈的。 那么Sentry是如何实现实时日志监控报警的呢？ 首先，Sentry是一个C/S架构，分为服务端和客户端 。SDK我们需要在自己应用中集成Sentry的SDK才能在应用发生错误是将错误信息发送给Sentry服务端。根据语言和框架的不同，我们可以选择自动或自定义设置特殊的错误类型报告给Sentry服务端。 而Sentry的服务端分为web、cron、worker这几个部分，应用（客户端）发生错误后将错误信息上报给web，web处理后放入消息队列或Redis内存队列，worker从队列中消费数据进行处理。 官方文档：https://docs.sentry.io/ GitHub：https://github.com/getsentry/sentry Sentry支持的客户端SDK： 二、部署 Sentry 本身是基于 Django 开发的，需要PostgreSQL、 Redis存储数据。 9.1.2之前可以使用一个镜像，通过不同的启动参数直接部署各个组件。之后10版本以上，官方推荐on-premise方式部署（添加了好多组件来优化性能，各个组件的镜像也分开了）参考：https://docs.sentry.io/server/installation 9.1.2版本的镜像：https://hub.docker.com/_/sentry/ Kubernetes Helm Charts Redis Charts: https://github.com/helm/charts/tree/master/stable/redis Postgresql Charts: https://github.com/helm/charts/tree/master/stable/postgresql Sentry Charts: https://github.com/helm/charts/tree/master/stable/sentry OpenShift 资源声明文件部署 资源声明文件 Docker部署9.1.2以下版本 1、部署Redis和PostgreSQL步骤省略 需要注意的是在PostgreSQL中创建EXTENSION表 postgres=# \\c sentry You are now connected to database \"sentry\" as user \"postgres\". sentry=# CREATE EXTENSION IF NOT EXISTS citext; 2、定制镜像、修改配置文件 定制镜像细节参考第四章第一节 创建配置文件sentry.conf.py from sentry.conf.server import * # NOQA from sentry.utils.types import Bool, Int import os import os.path import six CONF_ROOT = os.path.dirname(__file__) postgres = env('SENTRY_POSTGRES_HOST') or (env('POSTGRES_PORT_5432_TCP_ADDR') and 'postgres') if postgres: DATABASES = { 'default': { 'ENGINE': 'sentry.db.postgres', 'NAME': ( env('SENTRY_DB_NAME') or env('POSTGRES_ENV_POSTGRES_USER') or 'postgres' ), 'USER': ( env('SENTRY_DB_USER') or env('POSTGRES_ENV_POSTGRES_USER') or 'postgres' ), 'PASSWORD': ( env('SENTRY_DB_PASSWORD') or env('POSTGRES_ENV_POSTGRES_PASSWORD') or '' ), 'HOST': postgres, 'PORT': ( env('SENTRY_POSTGRES_PORT') or '' ), }, } # You should not change this setting after your database has been created unless you have altered all schemas first SENTRY_USE_BIG_INTS = True # If you're expecting any kind of real traffic on Sentry, we highly recommend # configuring the CACHES and Redis settings ########### # General # ########### # Instruct Sentry that this install intends to be run by a single organization # and thus various UI optimizations should be enabled. SENTRY_SINGLE_ORGANIZATION = env('SENTRY_SINGLE_ORGANIZATION', True) ######### # Redis # ######### # Generic Redis configuration used as defaults for various things including: # Buffers, Quotas, TSDB redis = env('SENTRY_REDIS_HOST') or (env('REDIS_PORT_6379_TCP_ADDR') and 'redis') if not redis: raise Exception('Error: REDIS_PORT_6379_TCP_ADDR (or SENTRY_REDIS_HOST) is undefined, did you forget to `--link` a redis container?') redis_password = env('SENTRY_REDIS_PASSWORD') or '' redis_port = env('SENTRY_REDIS_PORT') or '6379' redis_db = env('SENTRY_REDIS_DB') or '0' SENTRY_OPTIONS.update({ 'redis.clusters': { 'default': { 'hosts': { 0: { 'host': redis, 'password': redis_password, 'port': redis_port, 'db': redis_db, }, }, }, }, }) ######### # Cache # ######### # Sentry currently utilizes two separate mechanisms. While CACHES is not a # requirement, it will optimize several high throughput patterns. memcached = env('SENTRY_MEMCACHED_HOST') or (env('MEMCACHED_PORT_11211_TCP_ADDR') and 'memcached') if memcached: memcached_port = ( env('SENTRY_MEMCACHED_PORT') or '11211' ) CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': [memcached + ':' + memcached_port], 'TIMEOUT': 3600, } } # A primary cache is required for things such as processing events SENTRY_CACHE = 'sentry.cache.redis.RedisCache' ######### # Queue # ######### # See https://docs.getsentry.com/on-premise/server/queue/ for more # information on configuring your queue broker and workers. Sentry relies # on a Python framework called Celery to manage queues. rabbitmq = env('SENTRY_RABBITMQ_HOST') or (env('RABBITMQ_PORT_5672_TCP_ADDR') and 'rabbitmq') if rabbitmq: BROKER_URL = ( 'amqp://' + ( env('SENTRY_RABBITMQ_USERNAME') or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_USER') or 'guest' ) + ':' + ( env('SENTRY_RABBITMQ_PASSWORD') or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_PASS') or 'guest' ) + '@' + rabbitmq + '/' + ( env('SENTRY_RABBITMQ_VHOST') or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_VHOST') or '/' ) ) else: BROKER_URL = 'redis://:' + redis_password + '@' + redis + ':' + redis_port + '/' + redis_db ############### # Rate Limits # ############### # Rate limits apply to notification handlers and are enforced per-project # automatically. SENTRY_RATELIMITER = 'sentry.ratelimits.redis.RedisRateLimiter' ################## # Update Buffers # ################## # Buffers (combined with queueing) act as an intermediate layer between the # database and the storage API. They will greatly improve efficiency on large # numbers of the same events being sent to the API in a short amount of time. # (read: if you send any kind of real data to Sentry, you should enable buffers) SENTRY_BUFFER = 'sentry.buffer.redis.RedisBuffer' ########## # Quotas # ########## # Quotas allow you to rate limit individual projects or the Sentry install as # a whole. SENTRY_QUOTAS = 'sentry.quotas.redis.RedisQuota' ######## # TSDB # ######## # The TSDB is used for building charts as well as making things like per-rate # alerts possible. SENTRY_TSDB = 'sentry.tsdb.redis.RedisTSDB' ########### # Digests # ########### # The digest backend powers notification summaries. SENTRY_DIGESTS = 'sentry.digests.backends.redis.RedisBackend' ############## # Web Server # ############## # If you're using a reverse SSL proxy, you should enable the X-Forwarded-Proto # header and set `SENTRY_USE_SSL=1` if env('SENTRY_USE_SSL', False): SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https') SESSION_COOKIE_SECURE = True CSRF_COOKIE_SECURE = True SOCIAL_AUTH_REDIRECT_IS_HTTPS = True SENTRY_WEB_HOST = '0.0.0.0' SENTRY_WEB_PORT = 9000 SENTRY_WEB_OPTIONS = { 'http': '%s:%s' % (SENTRY_WEB_HOST, SENTRY_WEB_PORT), 'protocol': 'uwsgi', # This is need to prevent https://git.io/fj7Lw 'uwsgi-socket': None, 'http-keepalive': True, 'memory-report': False, # 'workers': 3, # the number of web workers } ############### # Mail Server # ############### email = env('SENTRY_EMAIL_HOST') or (env('SMTP_PORT_25_TCP_ADDR') and 'smtp') if email: SENTRY_OPTIONS['mail.backend'] = 'smtp' SENTRY_OPTIONS['mail.host'] = email SENTRY_OPTIONS['mail.from'] = env('SENTRY_SERVER_EMAIL') SENTRY_OPTIONS['mail.username'] = env('SENTRY_EMAIL_USER') or '' SENTRY_OPTIONS['mail.password'] = env('SENTRY_EMAIL_PASSWORD') or '' SENTRY_OPTIONS['mail.port'] = int(env('SENTRY_EMAIL_PORT') or 25) SENTRY_OPTIONS['mail.use-tls'] = env('SENTRY_EMAIL_USE_TLS', False) SENTRY_OPTIONS['mail.list-namespace'] = env('SENTRY_EMAIL_LIST_NAMESPACE') or 'localhost' else: SENTRY_OPTIONS['mail.backend'] = 'dummy' # The email address to send on behalf of SENTRY_OPTIONS['mail.from'] = env('SENTRY_SERVER_EMAIL') or 'root@localhost' # If you're using mailgun for inbound mail, set your API key and configure a # route to forward to /api/hooks/mailgun/inbound/ SENTRY_OPTIONS['mail.mailgun-api-key'] = env('SENTRY_MAILGUN_API_KEY') or '' # If you specify a MAILGUN_API_KEY, you definitely want EMAIL_REPLIES if SENTRY_OPTIONS['mail.mailgun-api-key']: SENTRY_OPTIONS['mail.enable-replies'] = True else: SENTRY_OPTIONS['mail.enable-replies'] = env('SENTRY_ENABLE_EMAIL_REPLIES', False) if SENTRY_OPTIONS['mail.enable-replies']: SENTRY_OPTIONS['mail.reply-hostname'] = env('SENTRY_SMTP_HOSTNAME') or '' ########## # Docker # ########## # Docker's environment configuration needs to happen # prior to anything that might rely on these values to # enable more \"smart\" configuration. ENV_CONFIG_MAPPING = { 'SENTRY_SECRET_KEY': 'system.secret-key', 'SENTRY_SLACK_CLIENT_ID': 'slack.client-id', 'SENTRY_SLACK_CLIENT_SECRET': 'slack.client-secret', 'SENTRY_SLACK_VERIFICATION_TOKEN': 'slack.verification-token', 'SENTRY_GITHUB_APP_ID': ('github-app.id', Int), 'SENTRY_GITHUB_APP_CLIENT_ID': 'github-app.client-id', 'SENTRY_GITHUB_APP_CLIENT_SECRET': 'github-app.client-secret', 'SENTRY_GITHUB_APP_WEBHOOK_SECRET': 'github-app.webhook-secret', 'SENTRY_GITHUB_APP_PRIVATE_KEY': 'github-app.private-key', 'SENTRY_VSTS_CLIENT_ID': 'vsts.client-id', 'SENTRY_VSTS_CLIENT_SECRET': 'vsts.client-secret', 'GOOGLE_CLIENT_ID': 'auth-google.client-id', 'GOOGLE_CLIENT_SECRET': 'auth-google.client-secret', } def bind_env_config(config=SENTRY_OPTIONS, mapping=ENV_CONFIG_MAPPING): \"\"\" Automatically bind SENTRY_OPTIONS from a set of environment variables. \"\"\" for env_var, item in six.iteritems(mapping): # HACK: we need to check both in `os.environ` and `env._cache`. # This is very much an implementation detail leaking out # due to assumptions about how `env` would be used previously. # `env` will pop values out of `os.environ` when they are seen, # so checking against `os.environ` only means it's likely # they won't exist if `env()` has been called on the variable # before at any point. So we're choosing to check both, but this # behavior is different since we're trying to only conditionally # apply variables, instead of setting them always. if env_var not in os.environ and env_var not in env._cache: continue if isinstance(item, tuple): opt_key, type_ = item else: opt_key, type_ = item, None config[opt_key] = env(env_var, type=type_) # If this value ever becomes compromised, it's important to regenerate your # SENTRY_SECRET_KEY. Changing this value will result in all current sessions # being invalidated. secret_key = env('SENTRY_SECRET_KEY') if not secret_key: raise Exception('Error: SENTRY_SECRET_KEY is undefined, run `generate-secret-key` and set to -e SENTRY_SECRET_KEY') if 'SENTRY_RUNNING_UWSGI' not in os.environ and len(secret_key) 3、初始化PostgreSQL数据库 docker run -it \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='*****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=***** \\ -e SENTRY_EMAIL_USER='*****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.67 \\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-db-init \\ sentry-9.1.2-ldap-dingtalk:v1 sentry upgrade --noinput 4、初始化用户信息 docker run -it \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='*****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=***** \\ -e SENTRY_EMAIL_USER='*****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.676\\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-user-init \\ sentry-9.1.2-ldap-dingtalk:v1 sentry createuser --no-input --email '*****@163.com' --superuser --password 18526 5、部署Worker docker run -d \\ -p 30276:9000 \\ --privileged \\ --restart=always \\ -v ./worker-data:/var/lib/sentry/files/ \\ -e SENTRY_USE_LDAP=True \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=**** \\ -e SENTRY_EMAIL_USER='****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.67 \\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-worker \\ sentry-9.1.2-ldap-dingtalk:v1 run worker 6、部署Web docker run -d \\ -p 30275:9000 \\ --privileged \\ --restart=always \\ -v ./web-data:/var/lib/sentry/files/ \\ -v ./sentry.conf.py:/etc/sentry/sentry.conf.py \\ -e LDAP_LOGLEVEL=INFO \\ -e LDAP_SENTRY_USERNAME_FIELD=cn \\ -e LDAP_FIND_GROUP_PERMS=False \\ -e LDAP_CACHE_GROUPS=True \\ -e LDAP_GROUP_CACHE_TIMEOUT=3600 \\ -e LDAP_GROUP_TYPE=groupOfUniqueNames \\ -e LDAP_MAP_FULL_NAME=gecos \\ -e LDAP_USER_FILTER='(&(memberOf=cn=sentry,cn=groups,dc=ldap,dc=synology,dc=curiouser,dc=com)(cn=%(user)s))' \\ -e LDAP_USER_DN='cn=users,dc=ldap,dc=synology,dc=curiouser,dc=com' \\ -e LDAP_BIND_PASSWORD=jL6u49t5A9P5 \\ -e LDAP_BIND_DN='uid=root,cn=users,dc=ldap,dc=synology,dc=curiouser,dc=com' \\ -e LDAP_SERVER='ldap://192.168.1.67:389' \\ -e SENTRY_USE_LDAP=True \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=**** \\ -e SENTRY_EMAIL_USER='****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.67 \\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-web \\ sentry-9.1.2-ldap-dingtalk:v1 三、配置 在Sentry完成一个项目的设置后，您将获得一个我们称之为DSN或数据源名称的值.它看起来很像一个标准的URL，但它实际上只是Sentry SDK所需的配置的标识.它由几个部分组成，包括协议，公共密钥和密钥，服务器地址和项目标识符。 '{PROTOCOL}://{PUBLIC_KEY}:{SECRET_KEY}@{HOST}/{PATH}{PROJECT_ID}' 四、Sentry集成LDAP认证登陆和钉钉群机器人 由于Sentry没有集成LDAP认证的官方插件，所以推荐了第三方插件sentry-ldap-auth来实现。 仅只是使用邮件的方式进行通知，这是不够的。Sentry默认的通知方式中是不支持钉钉的，它并没有直接集成钉钉。而是集成一个Webhook，这个Webhook发送的数据格式与钉钉群机器人回调地址接受的数据格式不一致，无法直接回调钉钉群机器人通知。所以只能集成第三方插件来支持。目前有两个 官方说明文档：https://docs.sentry.io/server/plugins/#rd-party-plugins Sentry-ldap-auth插件地址：https://github.com/Banno/getsentry-ldap-auth Sentry-Dingtalk插件地址：https://github.com/anshengme/sentry-dingding 第三方制作集成Sentry-ldap-auth插件Docker镜像的GitHub：https://github.com/locaweb/docker-sentry-ldap 相关文章：https://www.cnblogs.com/cjsblog/p/10585213.html 1、修改默认镜像，添加插件 Dockerfile FROM docker.io/sentry:9.1.2 RUN apt-get -qq update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q libxslt1-dev libxml2-dev libpq-dev libldap2-dev libsasl2-dev libssl-dev RUN echo \"sentry-ldap-auth\\npython-decouple==3.0\\ndjango-auth-ldap /tmp/req.txt && \\ pip install -r /tmp/req.txt && \\ apt-get remove -y -q libxslt1-dev libxml2-dev libpq-dev libldap2-dev libsasl2-dev libssl-dev && \\ rm -rf /var/lib/apt/lists/* /tmp/req.txt Makefile IMAGE_BASE = harbor.curiouser.cn IMAGE_NAME = tools/sentry-9.1.2-ldap-dingtalk IMAGE_VERSION = v1 all: build build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} 2、修改部署charts中的configmap，添加插件认证代码 Sentry Charts目录下templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: # .....上文省略...... data: # .....上文省略...... sentry.conf.py: |- # .....上文省略...... #################### # LDAP settings ## #################### SENTRY_USE_LDAP = env('SENTRY_USE_LDAP', False) if SENTRY_USE_LDAP: import ldap from django_auth_ldap.config import LDAPSearch, GroupOfUniqueNamesType AUTH_LDAP_SERVER_URI = env('LDAP_SERVER', 'ldap://localhost：389') AUTH_LDAP_BIND_DN = env('LDAP_BIND_DN', '') AUTH_LDAP_BIND_PASSWORD = env('LDAP_BIND_PASSWORD', '') # 设置搜索用户配置 AUTH_LDAP_USER_SEARCH = LDAPSearch( env('LDAP_USER_DN'), ldap.SCOPE_SUBTREE, env('LDAP_USER_FILTER', '(&(objectClass=inetOrgPerson)(cn=%(user)s))') ) # 设置搜索用户组配置 #AUTH_LDAP_GROUP_SEARCH = LDAPSearch( # env('LDAP_GROUP_DN', ''), # ldap.SCOPE_SUBTREE, # env('LDAP_GROUP_FILTER', '(objectClass=groupOfUniqueNames)') #) if env('LDAP_GROUP_TYPE', '') == 'groupOfUniqueNames': AUTH_LDAP_GROUP_TYPE = GroupOfUniqueNamesType() elif env('LDAP_GROUP_TYPE', '') == 'groupOfNames': AUTH_LDAP_GROUP_TYPE = GroupOfNamesType() elif env('LDAP_GROUP_TYPE', '') == 'posixGroup': AUTH_LDAP_GROUP_TYPE = PosixGroupType() elif env('LDAP_GROUP_TYPE', '') == 'nestedGroupOfNames': AUTH_LDAP_GROUP_TYPE = NestedGroupOfNamesType() AUTH_LDAP_REQUIRE_GROUP = None AUTH_LDAP_DENY_GROUP = None # 用户属性Mapping AUTH_LDAP_USER_ATTR_MAP = { 'username': env('LDAP_SENTRY_USER_FIELD', 'cn'), 'name': env('LDAP_MAP_FULL_NAME', 'sn'), 'email': env('LDAP_MAP_MAIL', 'mail') } # 用户搜索缓存 AUTH_LDAP_FIND_GROUP_PERMS = env('LDAP_FIND_GROUP_PERMS', False) AUTH_LDAP_CACHE_GROUPS = env('LDAP_CACHE_GROUPS', True) AUTH_LDAP_GROUP_CACHE_TIMEOUT = env('LDAP_GROUP_CACHE_TIMEOUT', 3600) # 用户在Sentry中的对应关系 AUTH_LDAP_DEFAULT_SENTRY_ORGANIZATION = env('LDAP_DEFAULT_SENTRY_ORGANIZATION','sentry') AUTH_LDAP_SENTRY_ORGANIZATION_ROLE_TYPE = 'manager' AUTH_LDAP_SENTRY_SUBSCRIBE_BY_DEFAULT = False AUTH_LDAP_SENTRY_ORGANIZATION_GLOBAL_ACCESS = True AUTH_LDAP_SENTRY_USERNAME_FIELD = env('LDAP_SENTRY_USERNAME_FIELD', 'cn') AUTHENTICATION_BACKENDS = AUTHENTICATION_BACKENDS + ('sentry_ldap_auth.backend.SentryLdapBackend',) ldap_is_active = env('LDAP_GROUP_ACTIVE', '') ldap_is_superuser = env('LDAP_GROUP_SUPERUSER', '') ldap_is_staff = env('LDAP_GROUP_STAFF', '') if ldap_is_active or ldap_is_superuser or ldap_is_staff: AUTH_LDAP_USER_FLAGS_BY_GROUP = { 'is_active': ldap_is_active, 'is_superuser': ldap_is_superuser, 'is_staff': ldap_is_staff, } # django_auth_ldap日志输出 import logging logger = logging.getLogger('django_auth_ldap') logger.addHandler(logging.StreamHandler()) ldap_loglevel = getattr(logging, env('LDAP_LOGLEVEL', 'DEBUG')) logger.setLevel(ldap_loglevel) LOGGING['overridable'] = ['sentry', 'django_auth_ldap'] LOGGING['loggers']['django_auth_ldap'] = {'handlers': ['console'],'level': env('LDAP_LOGLEVEL','DEBUG')} # .....下文省略...... 3、在Values文件中添加Web组件部署时的环境变量 values-dev.yaml ... web: ... env: ... - name: TZ value: Asia/Shanghai - name: SENTRY_USE_LDAP value: 'True' - name: LDAP_SERVER value: 'ldap://openldap.openldap.svc:389' - name: LDAP_BIND_DN value: 'cn=readonly,dc=curiouser,dc=com' - name: LDAP_BIND_PASSWORD value: readonly - name: LDAP_USER_DN value: 'ou=employee,dc=curiouser,dc=com' - name: LDAP_USER_FILTER value: >- (&(memberOf=cn=sentry,ou=apps,dc=curiouser,dc=com)(cn=%(user)s)) - name: LDAP_SENTRY_USER_FIELD value: 'cn' - name: LDAP_MAP_FULL_NAME value: 'sn' - name: LDAP_MAP_MAIL value: 'mail' - name: LDAP_GROUP_TYPE value: groupOfUniqueNames - name: LDAP_GROUP_CACHE_TIMEOUT value: '3600' - name: LDAP_CACHE_GROUPS value: 'True' - name: LDAP_DEFAULT_SENTRY_ORGANIZATION value: sentry - name: LDAP_FIND_GROUP_PERMS value: 'False' - name: LDAP_SENTRY_USERNAME_FIELD value: 'cn' - name: LDAP_LOGLEVEL value: INFO 五、Kubernetes event 客户端 Github 地址：https://github.com/getsentry/sentry-kubernetes helm repo add googleapis-incubator https://kubernetes-charts-incubator.storage.googleapis.com helm upgrade --install sentry-kubernetes-events googleapis-incubator/sentry-kubernetes \\ --namespace tools \\ --set sentry.dsn= \\ --set sentry.environment=test \\ --set sentry.release=test 六、Sentry-cli客户端 Sentry命令行客户端，作为为一个独立于代码SDK之外的客户端，通常可以用于发送一些自定义事件到服务端。例如在一些定时任务Bash脚本中集成，将一些自定义的事件同步到Sentry服务端，做到自定义事件监控。 官方文档：https://docs.sentry.io/cli/ GitHub地址：https://github.com/getsentry/sentry-cli 1. 下载安装 手动下载安装 github下载地址：https://github.com/getsentry/sentry-cli/releases/ 脚本下载安装 curl -sL https://sentry.io/get-cli/ | bash NPM下载安装 # 全局安装 npm install -g @sentry/cli --unsafe-perm Docker Image $ docker pull getsentry/sentry-cli $ docker run --rm -v $(pwd):/work getsentry/sentry-cli --help 2. 配置 全局配置文件：~/.sentryclirc INI语法格式 [auth] token=your-auth-token 环境变量 默认会读取当前.env 文件加载环境变量。可设置SENTRY_LOAD_DOTENV=0禁止 export SENTRY_AUTH_TOKEN=your-auth-token 命令行参数 sentry-cli --auth-token your-auth-token 项目配置文件 支持加载.properties，也可通过环境变量SENTRY_PROPERTIES指定项目配置文件路径 环境变量的形式 配置文件中的形式 描述 SENTRY_AUTH_TOKEN auth.token 与Sentry服务端通信用的认证Token SENTRY_API_KEY auth.api_key The legacy API key for authentication if you have one. SENTRY_URL defaults.url The URL to use to connect to sentry. This defaults to https://sentry.io/. SENTRY_ORG defaults.org The slug of the organization to use for a command. SENTRY_PROJECT defaults.project The slug of the project to use for a command. http.keepalive This ini only setting is used to control the behavior of the SDK with regards to HTTP keepalives. The default is true but it can be set to false to disable keepalive support. http_proxy http.proxy_url The URL that should be used for the HTTP proxy. The standard http_proxy environment variable is also honored. Note that it is lowercase. http.proxy_username This ini only setting sets the proxy username in case proxy authentication is required. http.proxy_password* This ini only setting sets the proxy password in case proxy authentication is required. http.verify_ssl This can be used to disable SSL verification when set to false. You should never do that unless you are working with a known self signed server locally. http.check_ssl_revoke If this is set to false then SSL revocation checks are disabled on Windows. This can be useful when working with a corporate SSL MITM proxy that does not properly implement revocation checks. Do not use this unless absolutely necessary. SENTRY_HTTP_MAX_RETRIES http.max_retries Sets the maximum number of retry attempts for upload operations (e.g., uploads of release files and debug symbols). The default is 5. ui.show_notifications If this is set to false some operating system notifications are disabled. This currently primarily affects xcode builds which will not show notifications for background builds. SENTRY_LOG_LEVEL log.level Configures the log level for the SDK. The default is warning. If you want to see what the library is doing you can set it to info which will spit out more information which might help to debug some issues with permissions. dsym.max_upload_size Sets the maximum upload size in bytes (before compression) of debug symbols into one batch. The default is 35MB or 100MB (depending on the version of sentry-cli) which is suitable for sentry.io but if you are using a different sentry server you might want to change this limit if necessary. SENTRY_NO_PROGRESS_BAR If set to 1, then sentry-cli will not display progress bars for any operations. SENTRY_DISABLE_UPDATE_CHECK update.disable_check If set to true, then the automatic update check in sentry-cli is disabled. This was introduced in 1.17. Versions before that did not include an update check. The update check is also not enabled for npm based installations of sentry-cli at the moment. DEVICE_FAMILY device.family Device family value reported to Sentry. DEVICE_MODEL device.model Device model value reported to Sentry. 3. 获取Auth Token 4. 获取并设置项目DSN 创建项目，获取DSN 设置DSN环境变量 export SENTRY_DSN=https://:@sentry.io/ 5. 验证配置文件 $ sentry-cli info Sentry Server: http://sentry-web-sentry.apps.okd311.curiouser.com Default Organization: Sentry Default Project: sentry-cli Authentication Info: Method: Auth Token User: *** Scopes: - event:admin - event:read - member:read - org:read - project:read - project:releases - team:read 6. Sentry-cli命令行参数 $ sentry-cli OPTIONS: --api-key 指定Sentry API key. --auth-token 指定Sentry auth token. -h, --help 打印帮助信息 --log-level 设置日志输出级别(日志级别:TRACE、DEBUG、INFO、WARN、ERROR) --url 指定Sentry服务端地址.[默认：https://sentry.io/] -V, --version 打印版本信息 子命令： bash-hook Prints out a bash script that does error handling. difutil Locate or analyze debug information files. help 显示帮助信息 info 打印Sentry服务端信息 issues Manage issues in Sentry. login Authenticate with the Sentry server. projects 管理sentry项目Manage projects on Sentry. react-native Upload build artifacts for react-native projects. releases Manage releases on Sentry. repos Manage repositories on Sentry. send-event Send a manual event to Sentry. uninstall Uninstall the sentry-cli executable. update Update the sentry-cli executable. upload-dif Upload debugging information files. upload-proguard Upload ProGuard mapping files to a project. 7. 手动发送事件 命令行参数 $ sentry-cli send-event [选项] 选项: -d, --dist Set the distribution. -E, --env 发送事件时一起发送指定的环境变量 -e, --extra ... 给事件添加额外信息 -f, --fingerprint ... 修改事件指纹. -h, --help 打印帮助信息 -l, --level 设置事件的严重程度或日志级别(默认error) --log-level 设置日志数据级别(TRACE, DEBUG, INFO, WARN, ERROR] --logfile 从日志文件中读取日志作为事件消息的补充内容(只读取文件中最近的100条数据) -m, --message ... 设置事件消息 -a, --message-arg ... 设置事件参数 --no-environ 设置发送事件时不一起发送系统环境变量 -p, --platform 设置事件所处平台。默认是'other' -r, --release 指定事件版本 -t, --tag ... 给事件添加标签 -u, --user ... 给事件添加用户信息 --with-categories Parses off a leading category for breadcrumbs from the logfile 示例 sentry-cli send-event \\ -m \"this is sentry-cli test event\" \\ -t sentry-cli:test \\ --no-environ \\ -l info \\ -p centos \\ -e a:b \\ -e c:d \\ -u root:curiouser \\ --log-level DEBUG\\ -a hahha \\ -E HOSTNAME:$HOSTNAME \\ --logfile output.log \\ -r v1 事件发送成功后，命令行会返回事件编号 8. 事件信息显示 9. Bash脚本执行异常监控 对于bash脚本，可以通过使用sentry-cli bash钩子自动发送错误异常到sentry服务端，并定位出异常所在行。 sentry-cli实际上只在启用set -e时才有效(在默认情况下，它将为您启用set -e)。 sentry-cli会注册EXIT和ERR Trap。 只需要在你的bash脚本开头添加 #!/bin/bash export SENTRY_DSN= eval \"$(sentry-cli bash-hook)\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-22 21:01:18 "},"origin/sentry-logstash对接Sentry.html":{"url":"origin/sentry-logstash对接Sentry.html","title":"Logstash与Sentry对接","keywords":"","body":"Logstah与Sentry的对接 一、简介 Sentry作为一个日志异常告警平台，对于异常日志聚合的告警，功能很强大。而基于ELK的日志系统，只能采集、聚合、存储应用日志，无法针对日志中的异常进行检测，聚合告警。所以可以在Logstash采集过程中输出一份日志数据到Sentry中进行聚合告警。 logstash-output-sentry插件：https://github.com/javiermatos/logstash-output-sentry 相关文章： https://medium.com/@yscaliskan/how-to-use-logstash-along-with-sentry-6c3d27790a38 https://clarkdave.net/2014/01/tracking-errors-with-logstash-and-sentry/ 整体对接思路 Filebeat file Input(多行采集+打标签) -------> Filebeat processor(添加字段) -------> Filebeat logatash output(输出到filebeat进行加工处理) Logstash beat input (监听) -------> Logstash dissect filter(判断符合标签的事件+从事件原始日志中映射提取字段) -------> Logstash sentry Output(输出到Sentry中) Filebeat 采集、输出要求: 可以多行采集(设置上下日志事件的标识),多行采集的日志信息到统一放到日志事件的“message”字段中 添加采集的日志类型字段，添加与Sentry相关信息(sentry上项目的ID、key、Secret)的字段 删除一些默认添加的字段信息 以日志中该日志产生的时间为事件的时间，而不是采集时的时间为事件时间 Logtash 接收、处理要求： 根据filebeat传送过来的事件中的类型字段判断是否进行过滤加工 二、上下文 以API网关Kong的Nginx的错误日志为例（该Nginx安装了LUA模块，错误日志里面有lua模块的错误日志）。日志文件中的一行代表着一个nginx出错的事件，示例如下： 2018/11/28 18:16:25 [warn] 2201#0: 9081632 [lua] cluster.lua:182: set_peer_down(): [lua-cassandra] setting host at 172.17.1.8 DOWN, context: ngx.timer 2018/11/28 18:16:25 [error] 2201#0: 9081632 [lua] init.lua:365: [cluster_events] failed to poll: failed to retrieve events from DB: [Unavailable exception] Cannot achieve consistency level LOCAL_ONE, context: ngx.timer 2019/11/28 18:16:26 [warn] 27201#0: 90815632 this is a warn log event 2019/11/28 18:16:26 [fatal] 27201#0: 90815632 this is a fatal log event 每一行日志可大致格式分为: 时间戳 日志级别 进程号 抛弃该处数据 具体错误日志 三、配置 1. Filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /root/Curiouser/test.log exclude_files: [\"_filebeat\", \".gz$\"] recursive_glob.enabled: true multiline.pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after tags: sentry-alert fields: service_name => \"kong\" sentry_project_id => \"7\" sentry_project_key => \"***\" sentry_project_secret => \"***\" processors: - drop_fields: fields: [\"agent\", \"tags\", \"input\", \"ecs\"] output.logstash: hosts: [\"127.0.0.1:5044\"] 2. Logstash安装sentry output插件 /usr/share/logstash/bin/logstash-plugin install logstash-output-sentry /usr/share/logstash/bin/logstash-plugin list 2. Logstash配置 input { beats { port => 5044 } } filter { # 判断\"tag\"包含\"log-alert\"标签的日志事件进行加工处理 if [tags] == \"log-alert\" { # 映射原始日志，从中提取数据赋予指定的字段（按行为单位） dissect { mapping => { \"message\" => \"%{timestamp} %{+timestamp} [%{level}] %{thread} %{} %{message}\" } } # 提取日志的产生时间作为事件的时间戳。 date { match => [ \"timestamp\", \"yyyy/MM/dd HH:mm:ss\" ] remove_field => \"timestamp\" } # 替换原始日志中的日志级别字段,sentry支持的日志级别为warning,而原始日志中的日志级别字段是warn，索引需要转换。 mutate { gsub => [ \"level\", \"warn\", \"warning\" ] } } } output { # 判断日志级别为\"warning\",\"error\",\"fatal\"的日志事件,发送到sentry if [level] == \"warning\" or [level] == \"error\" or [level] == \"fatal\" { sentry { message => \"message\" threads => 'thread' level => \"level\" tags => 'service:\"service_name\"' url => \"http://sentry.curiouser.com/api\" key => '%{sentry_project_key}' secret => '%{sentry_project_secret}' project_id => '%{sentry_project_id}' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker原理.html":{"url":"origin/docker原理.html","title":"Docker原理","keywords":"","body":"Docker原理 一、简介 二、Namespaces 命名空间（namespaces）是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。 CLONE_NEWCGROUP CLONE_NEWIPC CLONE_NEWNET CLONE_NEWNS CLONE_NEWPID CLONE_NEWUSER CLONE_NEWUTS 如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却有没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。 每一个使用 docker run 启动的容器其实都具有单独的网络命名空间，Docker 为我们提供了四种不同的网络模式，Host、Container、None 和 Bridge 模式。 三、Control Groups 四、Union Filesystem Linux 的命名空间和控制组分别解决了不同资源隔离的问题，前者解决了进程、网络以及文件系统的隔离，后者实现了 CPU、内存等资源的隔离，但是在 Docker 中还有另一个非常重要的问题需要解决 - 也就是镜像。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker-process-manager.html":{"url":"origin/docker-process-manager.html","title":"容器中的进程管理","keywords":"","body":"Docker容器中进程管理工具 一、简介 为了防止容器中直接使用ENTRYPOINT或CMD指令启动命令或应用程序产生PID为1的进程无法处理传递信号给子进程或者无法接管孤儿进程，进而导致产生大量的僵尸进程。对于没有能力处理以上两个进程问题的PID进程，建议使用dumb-int或tini这种第三方工具来充当1号进程。 Linux系统中，PID为1的进程需要担任两个重要的使命： 传递信号给子进程 如果pid为1的进程，无法向其子进程传递信号，可能导致容器发送SIGTERM信号之后，父进程等待子进程退出。此时，如果父进程不能将信号传递到子进程，则整个容器就将无法正常退出，除非向父进程发送SIGKILL信号，使其强行退出，这就会导致一些退出前的操作无法正常执行，例如关闭数据库连接、关闭输入输出流等。 接管孤儿进程，防止出现僵尸进程 如果一个进程中A运行了一个子进程B，而这个子进程B又创建了一个子进程C，若子进程B非正常退出（通过SIGKILL信号，并不会传递SIGKILL信号给进程C），那么子进程C就会由进程A接管，一般情况下，我们在进程A中并不会处理对进程C的托管操作（进程A不会传递SIGTERM和SIGKILL信号给进程C），结果就导致了进程B结束了，倒是并没有回收其子进程C，子进程C就变成了僵尸进程。 在docker中，docker stop命令会发送SIGTERM信号给容器的主进程来处理。如果主进程没有处理这个信号，docker会在等待一段优雅grace的时间后，发送SIGKILL信号来强制终止 二、容器中僵尸进程的危害 详情参考： 1、https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/ 2、https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86 三、dumb-int Github：https://github.com/Yelp/dumb-init dumb-int是一个用C写的轻量级进程管理工具。类似于一个初始化系统， 它充当PID 1，并立即以子进程的形式允许您的命令，注意在接收到信号时正确处理和转发它们 dumb-init 解决上述两个问题：向子进程代理发送信号和接管子进程。 默认情况下，dumb-init 会向子进程的进程组发送其收到的信号。原因也很简单，前面已经提到过，像 bash 这样的应用，自己接收到信号之后，不会向子进程发送信号。当然，dumb-init 也可以通过设置环境变量DUMB_INIT_SETSID=0来控制只向它的直接子进程发送信号。 另外 dumb-init 也会接管失去父进程的进程，确保其能正常退出。 安装 Alpine镜像的APK可以直接安装 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache dumb-init ENTRYPOINT [\"dumb-init\", \"--\"] CMD [\"/usr/local/bin/docker-entrypoint.sh\"] 二进制安装 RUN version=v1.2.2 && \\ wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/$version/dumb-init_$version_amd64 && \\ chmod +x /usr/local/bin/dumb-init DEB/RPM安装 RUN version=v1.2.2 && \\ wget https://github.com/Yelp/dumb-init/releases/download/$version/dumb-init_$version_amd64.deb | dpkg -i apt-get instal dumb-init pip安装 pip install dumb-init 三、tini Github：https://github.com/krallin/tini 安装 Alpine镜像的APK可以直接安装 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache tini ENTRYPOINT [\"tini\", \"--\"] CMD [\"/your/program\", \"-and\", \"-its\", \"arguments\"] 四、应用场景 1、php-fpm进程的接管 针对php应用，通常采用nginx+php-fpm的架构来处理请求。为了保证php-fpm进程出现意外故障能够自动恢复，通常使用supervisor进程管理工具进行守护。php-fpm的进程管理类也类似于nginx，由master，worker进程组成。master进程不处理请求，而是由worker进程处理！master进程只负责管理worker进程。 master进程负责监听子进程的状态，子进程挂掉之后，会发信号给master进程，然后master进程重新启一个新的worker进程。 进程号 父进程号 进程 21 10 master 22 21 |----worker1 23 21 |----worker2 使用Supervisor启动、守护php-fpm进程时的进程树 进程号 父进程号 进程 10 9 supervisor 21 10 |---master 22 21 |----worker1 23 21 |----worker2 # 使用supervisor启动、守护的是php-fpm的master进程，然后master进程再根据配置启动对应数量的worker进程。 当php-fpm的master进程意外退出后的进程树 进程号 父进程号 进程 10 9 supervisor 22 1 worker1 23 1 worker2 # 此时worker进程成为僵尸进程，被1号进程接管 此时supervisor检测到php-fpm master进程不存在就会在重新创建一个新的php-fpm master进程。但是会因为原先的php-fpm worker没有被杀掉，成为僵尸进程、依旧占用着端口而失败。本以为php-fpm会 参考 https://www.infoq.cn/article/2016/01/dumb-init-Docker https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/ https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-07-03 14:26:42 "},"origin/docker-user-process-manage.html":{"url":"origin/docker-user-process-manage.html","title":"容器中的用户管理","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html":{"url":"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html","title":"Dockerfile中CMD与ENTRYPOINT命令的区别","keywords":"","body":"CMD与ENTRYPOINT区别 CMD命令设置容器启动后默认执行的命令及其参数，但CMD设置的命令能够被docker run命令后面的命令行参数替换 ENTRYPOINT配置容器启动时的执行命令（不会被忽略，一定会被执行，即使运行 docker run时指定了其他命令） ENTRYPOINT 的 Exec 格式用于设置容器启动时要执行的命令及其参数，同时可通过CMD命令或者命令行参数提供额外的参数 ENTRYPOINT 中的参数始终会被使用，这是与CMD命令不同的一点 1. Shell格式和Exec格式命令 Shell格式：指令 CMD java -jar test.jar Exec格式：指令 [\"executable\", \"param1\", \"param2\", ...] ENTRYPOINT [\"java\", \"-jar\", \"test.jar\"] 2. Shell格式和Exec格式命令的区别 Shell格式中的命令会直接被Shell解析 Exec格式不会直接解析，需要加参数 3. CMD和ENTRYPOINT指令支持的命令格式 CMD 指令的命令支持以下三种格式: Exec格式: CMD [\"executable\",\"param1\",\"param2\"] Exec参数: CMD [\"param1\",\"param2\"] 用来为ENTRYPOINT 提供参数 Shell格式: CMD command param1 param2 ENTRYPOINT 指令的命令支持以下了两种格式: Exec格式：可用使用CMD的参数和可使用docker run [image] 参数后面追加的参数 Shell格式 ：不会使用 CMD参数，可使用docker run [image] 参数后面追加的参数 4. 示例 ENTRYPOINT的Exec格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的Exec格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的shell格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello /bin/sh -c Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的shell格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 + CMD的Shell格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 +CMD的Exec格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello 参考链接 https://blog.csdn.net/weixin_42971363/article/details/91506844 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker-使用Makefile操作Dockerfile.html":{"url":"origin/docker-使用Makefile操作Dockerfile.html","title":"使用Makefile操作Dockerfile.md","keywords":"","body":" IMAGE_BASE = docker-registry-default.apps.okd311.curiouser.com/openshift IMAGE_NAME = demo-springboot2 IMAGE_VERSION = latest IMAGE_TAGVERSION = $(GIT_COMMIT) all: build tag push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . tag: docker tag ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} makefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误：`Makefile:10: * multiple target patterns. Stop.`** Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-24 18:26:53 "},"origin/docker-multi-stage-build.html":{"url":"origin/docker-multi-stage-build.html","title":"多阶段构建","keywords":"","body":"Docker多阶段构建 一、简介 在编写Dockerfile构建docker镜像时，常遇到以下问题： RUN命令会让镜像新增layer，导致镜像变大，虽然通过&&连接多个命令能缓解此问题，但如果命令之间用到docker指令例如COPY、WORKDIR等，依然会导致多个layer； 有些工具在构建过程中会用到，但是最终的镜像是不需要的（例如用maven编译构建java工程），这要求Dockerfile的编写者花更多精力来清理这些工具，清理的过程又可能导致新的layer； 为了解决上述问题，从17.05版本开始Docker在构建镜像时增加了新特性：多阶段构建(multi-stage builds)，将构建过程分为多个阶段，每个阶段都可以指定一个基础镜像，这样在一个Dockerfile就能将多个镜像的特性同时用到 二、实践操作 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker-alpine.html":{"url":"origin/docker-alpine.html","title":"Alpine镜像","keywords":"","body":"Alpine镜像 一、简介 Alpine 操作系统是一个面向安全的轻型 Linux 发行版。它不同于通常 Linux 发行版，Alpine 采用了 musl libc 和 busybox 以减小系统的体积和运行时资源消耗，但功能上比 busybox 又完善的多，因此得到开源社区越来越多的青睐。在保持瘦身的同时，Alpine 还提供了自己的包管理工具 apk，可以通过 https://pkgs.alpinelinux.org/packages 网站上查询包信息，也可以直接通过 apk 命令直接查询和安装各种软件。 Alpine 由非商业组织维护的，支持广泛场景的 Linux发行版，它特别为资深/重度Linux用户而优化，关注安全，性能和资源效能。Alpine 镜像可以适用于更多常用场景，并且是一个优秀的可以适用于生产的基础系统/环境。 Alpine Docker 镜像也继承了 Alpine Linux 发行版的这些优势。相比于其他 Docker 镜像，它的容量非常小，仅仅只有 5 MB 左右（对比 Ubuntu 系列镜像接近 200 MB），且拥有非常友好的包管理机制。官方镜像来自 docker-alpine 项目。 目前 Docker 官方已开始推荐使用 Alpine 替代之前的 Ubuntu 做为基础镜像环境。这样会带来多个好处。包括镜像下载速度加快，镜像安全性提高，主机之间的切换更方便，占用更少磁盘空间等。 官方网站：https://alpinelinux.org/ Github：https://github.com/alpinelinux/docker-alpine Dockerhub：https://hub.docker.com/_/alpine 二、APK包管理器 可在包管理中心查看支持的包：https://pkgs.alpinelinux.org/packages 1、apk命令详解 命令格式 apk 子命令 参数项 全局参数项 -h, --help Show generic help or applet specific help -p, --root DIR Install packages to DIR -X, --repository REPO Use packages from REPO -q, --quiet Print less information -v, --verbose Print more information (can be doubled) -i, --interactive Ask confirmation for certain operations -V, --version Print program version and exit -f, --force Enable selected --force-* (deprecated) --force-binary-stdout Continue even if binary data is to be output --force-broken-world Continue even if 'world' cannot be satisfied --force-non-repository Continue even if packages may be lost on reboot --force-old-apk Continue even if packages use unsupported features --force-overwrite Overwrite files in other packages --force-refresh Do not use cached files (local or from proxy) -U, --update-cache Alias for --cache-max-age 1 --progress Show a progress bar --progress-fd FD Write progress to fd --no-progress Disable progress bar even for TTYs --purge Delete also modified configuration files (pkg removal) and uninstalled packages from cache (cache clean) --allow-untrusted Install packages with untrusted signature or no signature --wait TIME Wait for TIME seconds to get an exclusive repository lock before failing --keys-dir KEYSDIR Override directory of trusted keys --repositories-file REPOFILE Override repositories file --no-network Do not use network (cache is still used) --no-cache Do not use any local cache path --cache-dir CACHEDIR Override cache directory --cache-max-age AGE Maximum AGE (in minutes) for index in cache before refresh --arch ARCH Use architecture with --root --print-arch Print default arch and exit commit参数项 -s, --simulate Show what would be done without actually doing it --clean-protected Do not create .apk-new files in configuration dirs --overlay-from-stdin Read list of overlay files from stdin --no-scripts Do not execute any scripts --no-commit-hooks Skip pre/post hook scripts (but not other scripts) --initramfs-diskless-boot Enables options for diskless initramfs boot (e.g. skip hooks) 子命令 \u0016①安装与删除 add：安装包 --initdb Initialize database -u, --upgrade Prefer to upgrade package -l, --latest Select latest version of package (if it is not pinned), and print error if it cannot be installed due to other dependencies -t, --virtual NAME Instead of adding all the packages to 'world', create a new virtual package with the listed dependencies and add that to 'world'; the actions of the command are easily reverted by deleting the virtual package del：卸载并删除包 -r, --rdepends Recursively delete all top-level reverse dependencies too ②包的元信息管理 fix：在不改动主要的依赖的情况下进行包的修复或者升级 -d, --depends Fix all dependencies too -r, --reinstall Reinstall the package (default) -u, --upgrade Prefer to upgrade package -x, --xattr Fix packages with broken xattrs --directory-permissions Reset all directory permissions update：从远程仓库获取信息更新本地仓库索引 upgrade：令升级系统已安装的所以软件包（一般包括内核），当然也可指定仅升级部分软件包（通过-u或–upgrade选择指定 -a, --available Resets versioned world dependencies, and changes to prefer replacing or downgrading packages (instead of holding them) if the currently installed package is no longer available from any repository -l, --latest Select latest version of package (if it is not pinned), and print error if it cannot be installed due to other dependencies --no-self-upgrade Do not do early upgrade of 'apk-tools' package --self-upgrade-only Only do self-upgrade cache：对缓存进行操作，比如对缺失的包进行缓存或者对于不需要的包进行缓存删除 -u, --upgrade Prefer to upgrade package -l, --latest Select latest version of package (if it is not pinned), and print error if it cannot be installed due to other dependencies ③查询搜索包 info：列出所有已安装的软件包 -L, --contents List contents of the PACKAGE -e, --installed Check if PACKAGE is installed -W, --who-owns Print the package owning the specified file -R, --depends List packages that the PACKAGE depends on -P, --provides List virtual packages provided by PACKAGE -r, --rdepends List all packages depending on PACKAGE --replaces List packages whom files PACKAGE might replace -i, --install-if List the PACKAGE's install_if rule -I, --rinstall-if List all packages having install_if referencing PACKAGE -w, --webpage Show URL for more information about PACKAGE -s, --size Show installed size of PACKAGE -d, --description Print description for PACKAGE --license Print license for PACKAGE -t, --triggers Print active triggers of PACKAGE -a, --all Print all information about PACKAGE list：按照指定条件进行包的列表信息显示 -I, --installed List installed packages only -O, --orphaned List orphaned packages only -a, --available List available packages only -u, --upgradable List upgradable packages only -o, --origin List packages by origin -d, --depends List packages by dependency -P, --providers List packages by provider search：查询相关的包的详细信息，支持正则 -a, --all Show all package versions (instead of latest only) -d, --description Search package descriptions (implies -a) -x, --exact Require exact match (instead of substring match) -e Synonym for -x (deprecated) -o, --origin Print origin package name instead of the subpackage -r, --rdepends Print reverse dependencies of package --has-origin List packages that have the given origin dot：生成依赖之间的关联关系图（使用箭头描述） --errors Output only parts of the graph which are considered erroneous: e.g. cycles and missing packages --installed Consider only installed packages policy：显示包的仓库策略信息 ④源管理 stats：显示仓库和包的安装相关的统计信息 index：使用文件生成仓库索引文件 -o, --output FILE Write the generated index to FILE -x, --index INDEX Read INDEX to speed up new index creation by reusing the information from an old index -d, --description TEXT Embed TEXT as description and version information of the repository index --rewrite-arch ARCH Use ARCH as architecture for all packages fetch：从全局仓库下载包到本地目录 -L, --link Create hard links if possible -R, --recursive Fetch the PACKAGE and all its dependencies --simulate Show what would be done without actually doing it -s, --stdout Dump the .apk to stdout (incompatible with -o, -R, --progress) -o, --output DIR Directory to place the PACKAGEs to verify：验证包的完整性和签名信息 manifest：显示package各组成部分的checksum 2、操作 ①安装软件 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache git ②替换Alpine的软件源 常见国内Alpine软件源： 阿里云 FROM alpilne:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories 中科大 FROM alpilne:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories ③安装bash FROM alpine:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache bash bash-doc bash-completion ④安装telnet FROM alpine:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache busybox-extras ⑤安装Docker Client和Make FROM alpine:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache docker-cli make ⑥修改用户的所属用户组 FROM alpine:3.11.5 RUN sed -i 's/1001/0/g' /etc/passwd ⑦设置系统语言为“en_US.UTF-8”，以防中文乱码 FROM alpine:3.11.5 ENV LANG=en_US.UTF-8 \\ LANGUAGE=en_US.UTF-8 RUN apk --no-cache add ca-certificates \\ && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-bin-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-i18n-2.29-r0.apk \\ && apk add glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && rm -rf /usr/lib/jvm glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && /usr/glibc-compat/bin/localedef --force --inputfile POSIX --charmap UTF-8 \"$LANG\" || true \\ && echo \"export LANG=$LANG\" > /etc/profile.d/locale.sh \\ && apk del glibc-i18n 参考： https://github.com/gliderlabs/docker-alpine/issues/144 https://gist.github.com/alextanhongpin/aa55c082a47b9a1b0060a12d85ae7923 ⑧设置时区 FROM alpine:3.11.5 ENV TZ=Asia/Shanghai RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache tzdata \\ && cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ && echo \"Asia/Shanghai\" > /etc/timezone 参考 https://yeasy.gitbooks.io/docker_practice/cases/os/alpine.html https://blog.csdn.net/liumiaocn/article/details/87603628 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/dockerfile-hadolint.html":{"url":"origin/dockerfile-hadolint.html","title":"语法扫描工具Hadolint","keywords":"","body":"Dockerfile扫描工具Hadolint 一、简介 Hadolint是一个用Haskell 编写的开源Dockerfiles语法检查、优化工具。 Github地址：https://github.com/hadolint/hadolint 在线扫描：https://hadolint.github.io/hadolint/ 二、安装与配置 1、安装 OSX brew brew install hadolint Windows scoop scoop install hadolint docker docker pull hadolint/hadolint:latest-debian docker pull hadolint/hadolint:latest-alpine 2、配置 hadolint会按照以下顺序读取配置文件 $PWD/.hadolint.yaml $XDG_CONFIG_HOME/hadolint.yaml ~/.config/hadolint.yaml 扫描时指定配置文件 hadolint --config /path/to/config.yaml Dockerfile 配置忽略规则 echo \"export XDG_CONFIG_HOME=~/.hadolint\" >> /etc/profile && \\ mkdir ~/.hadolint && \\ source /etc/profile && \\ 创建并编辑配置vi ~/.hadolint/hadolint.yaml ignored: - DL3000 - SC1010 trustedRegistries: - docker.io - my-company.com:5000 3、命令参数 hadolint [-v|--version] [-c|--config FILENAME] [-f|--format ARG] [DOCKERFILE...] [--ignore RULECODE] [--trusted-registry REGISTRY (e.g. docker.io)] 可选参数: -h,--help Show this help text -v,--version Show version -c,--config FILENAME Path to the configuration file -f,--format ARG The output format for the results [tty | json | checkstyle | codeclimate | codacy] (default: tty) --ignore RULECODE A rule to ignore. If present, the ignore list in the config file is ignored --trusted-registry REGISTRY (e.g. docker.io) A docker registry to allow to appear in FROM instructions 三、扫描Dockerfile hadolint Dockerfile hadolint --ignore DL3003 --ignore DL3006 Dockerfile # 或者 docker run --rm -i hadolint/hadolint 输出扫描结果 Dockerfile:2 DL3020 Use COPY instead of ADD for files and folders Dockerfile:3 DL3025 Use arguments JSON notation for CMD and ENTRYPOINT arguments 四、与CI流程与编辑器的集成 官方集成示例文档：https://github.com/hadolint/hadolint/blob/master/docs/INTEGRATION.md 编辑器 VSCode Sublime Text 3 Vim and NeoVim Atom CI流程 1、Travis CI # Use container-based infrastructure for quicker build start-up sudo: false # Use generic image to cut start-up time language: generic env: # Path to 'hadolint' binary HADOLINT: \"${HOME}/hadolint\" install: # Download hadolint binary and set it as executable - curl -sL -o ${HADOLINT} \"https://github.com/hadolint/hadolint/releases/download/v1.17.5/hadolint-$(uname -s)-$(uname -m)\" && chmod 700 ${HADOLINT} script: # List files which name starts with 'Dockerfile' # eg. Dockerfile, Dockerfile.build, etc. - git ls-files --exclude='Dockerfile*' --ignored | xargs --max-lines=1 ${HADOLINT} 2、GitHub Actions name: Lint Dockerfile on: push jobs: linter: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Lint Dockerfile uses: brpaz/hadolint-action@master with: dockerfile: \"Dockerfile\" 3、Gitlab CI lint_dockerfile: image: hadolint/hadolint:latest-debian script: - hadolint Dockerfile 4、Jenkins declarative pipeline stage (\"lint dockerfile\") { agent { docker { image 'hadolint/hadolint:latest-debian' } } steps { sh 'hadolint dockerfiles/* | tee -a hadolint_lint.txt' } post { always { archiveArtifacts 'hadolint_lint.txt' } } } 5、Jenkins K8S plugin 声明hadolint pod - name: hadolint image: hadolint/hadolint:latest-debian imagePullPolicy: Always command: - cat tty: true stage('lint dockerfile') { steps { container('hadolint') { sh 'hadolint dockerfiles/* | tee -a hadolint_lint.txt' } } post { always { archiveArtifacts 'hadolint_lint.txt' } } } 6、Bitbucket Pipelines pipelines: default: - step: image: hadolint/hadolint:latest-debian script: - hadolint Dockerfile 五、扫描规则 DL开头的规则是hadolint的 SC开头的规则是ShellChecke的 Rule 描述 DL3000 Use absolute WORKDIR. DL3001 For some bash commands it makes no sense running them in a Docker container like ssh, vim, shutdown, service, ps, free, top, kill, mount, ifconfig. DL3002 Last user should not be root. DL3003 Use WORKDIR to switch to a directory. DL3004 Do not use sudo as it leads to unpredictable behavior. Use a tool like gosu to enforce root. DL3005 Do not use apt-get upgrade or dist-upgrade. DL3006 Always tag the version of an image explicitly. DL3007 Using latest is prone to errors if the image will ever update. Pin the version explicitly to a release tag. DL3008 Pin versions in apt-get install. DL3009 Delete the apt-get lists after installing something. DL3010 Use ADD for extracting archives into an image. DL3011 Valid UNIX ports range from 0 to 65535. DL3012 Provide an email address or URL as maintainer. DL3013 Pin versions in pip. DL3014 Use the -y switch. DL3015 Avoid additional packages by specifying --no-install-recommends. DL3016 Pin versions in npm. DL3017 Do not use apk upgrade. DL3018 Pin versions in apk add. Instead of apk add use apk add =. DL3019 Use the --no-cache switch to avoid the need to use --update and remove /var/cache/apk/* when done installing packages. DL3020 Use COPY instead of ADD for files and folders. DL3021 COPY with more than 2 arguments requires the last argument to end with / DL3022 COPY --from should reference a previously defined FROM alias DL3023 COPY --from cannot reference its own FROM alias DL3024 FROM aliases (stage names) must be unique DL3025 Use arguments JSON notation for CMD and ENTRYPOINT arguments DL3026 Use only an allowed registry in the FROM image DL3027 Do not use apt as it is meant to be a end-user tool, use apt-get or apt-cache instead DL3028 Pin versions in gem install. Instead of gem install use gem install : DL4000 MAINTAINER is deprecated. DL4001 Either use Wget or Curl but not both. DL4003 Multiple CMD instructions found. DL4004 Multiple ENTRYPOINT instructions found. DL4005 Use SHELL to change the default shell. DL4006 Set the SHELL option -o pipefail before RUN with a pipe in it SC1000 $ is not used specially and should therefore be escaped. SC1001 This \\c will be a regular 'c' in this context. SC1007 Remove space after = if trying to assign a value (or for empty string, use var='' ...). SC1010 Use semicolon or linefeed before done (or quote to make it literal). SC1018 This is a unicode non-breaking space. Delete it and retype as space. SC1035 You need a space here SC1045 It's not foo &; bar, just foo & bar. SC1065 Trying to declare parameters? Don't. Use () and refer to params as $1, $2 etc. SC1066 Don't use $ on the left side of assignments. SC1068 Don't put spaces around the = in assignments. SC1077 For command expansion, the tick should slant left (` vs ´). SC1078 Did you forget to close this double-quoted string? SC1079 This is actually an end quote, but due to next char, it looks suspect. SC1081 Scripts are case sensitive. Use if, not If. SC1083 This {/} is literal. Check expression (missing ;/\\n?) or quote it. SC1086 Don't use $ on the iterator name in for loops. SC1087 Braces are required when expanding arrays, as in ${array[idx]}. SC1095 You need a space or linefeed between the function name and body. SC1097 Unexpected ==. For assignment, use =. For comparison, use [ .. ] or [[ .. ]]. SC1098 Quote/escape special characters when using eval, e.g. eval \"a=(b)\". SC1099 You need a space before the #. SC2002 Useless cat. Consider `cmd ..orcmd file ..` instead. SC2015 Note that `A && B C` is not if-then-else. C may run when A is true. SC2026 This word is outside of quotes. Did you intend to 'nest '\"'single quotes'\"' instead'? SC2028 echo won't expand escape sequences. Consider printf. SC2035 Use ./*glob* or -- *glob* so names with dashes won't become options. SC2039 In POSIX sh, something is undefined. SC2046 Quote this to prevent word splitting SC2086 Double quote to prevent globbing and word splitting. SC2140 Word is in the form \"A\"B\"C\" (B indicated). Did you mean \"ABC\" or \"A\\\"B\\\"C\"? SC2154 var is referenced but not assigned. SC2155 Declare and assign separately to avoid masking return values. SC2164 Use `cd ... exitin casecd` fails. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/dockerfile-optimization.html":{"url":"origin/dockerfile-optimization.html","title":"Dockerfile优化","keywords":"","body":"Dockerfile优化 一、指令格式化 LABEL LABEL vendor=ACME\\ Incorporated \\ com.example.is-beta= \\ com.example.is-production=\"\" \\ com.example.version=\"0.0.1-beta\" \\ com.example.release-date=\"2015-02-12\" ENV Dockerfile中ENV指令像RUN指令一样，每一个都会创建一个临时层。 ENV JAVA_HOME=/opt/jdk1.8.0_241 \\ CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib ENV PATH=$PATH:$JAVA_HOME/bin RUN RUN apt-get update && \\ apt-get install -y --no-install-recommends \\ apt-transport-https \\ ca-certificates && \\ rm -rf /var/lib/apt/lists/* 二、指令优化 1、减少RUN指令，合并命令 RUN useradd -s /sbin/nologin -m -u 1001 curiouser && \\ mkdir -p /home/curiouser/{data,logs} && \\ rm -rf /etc/yum.repos.d/C* && \\ yum install -q -y git && \\ yum clean all && \\ curl -s http://192.168.1.7/repository/tools/jdk-8u241-linux-x64.tar.gz | tar -xC /opt/ 2、使用COPY来代替ADD 对于使用ADD指令下载远程服务器上的tar包并解压，建议使用以下方式代替 RUN curl -s http://192.168.1.7/repository/tools/jdk-8u241-linux-x64.tar.gz | tar -xC /opt/ 三、最小化基础镜像，减小镜像体积 1、尽量使用Alpine作为基础镜像 Alpine镜像大小最多才几MB。 使用APK命令装最小化需求的软件包 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache git 四、尽量不使用root用户 在做基础运行时镜像时，创建运行时普通用户和用户组，并做工作区与权限限制，启动服务时尽量使用普通用户。 gosu FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache gosu 参考： https://blog.csdn.net/boling_cavalry/article/details/93380447 五、使用进程管理工具来处理进程信号 为防止容器中的进程变成僵尸进程， dumb-init Github地址：https://github.com/Yelp/dumb-init FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache dumb-init # Runs \"/usr/bin/dumb-init -- /my/script --with --args\" ENTRYPOINT [\"dumb-init\", \"--\"] # or if you use --rewrite or other cli flags # ENTRYPOINT [\"dumb-init\", \"--rewrite\", \"2:3\", \"--\"] CMD [\"/my/script\", \"--with\", \"--args\"] 参考： https://www.infoq.cn/article/2016/01/dumb-init-Docker https://www.cnblogs.com/sunsky303/p/11046681.html 六、移除所有缓存等不必要信息 删除解压后的源压缩包（参考第二章第二节） 清理包管理器下载安装软件时的缓存 使用Alipine镜像中APK命令安装包时记得加上--no-cache 使用Ubuntu镜像中的APT命令安装软件后记得rm -rf /var/lib/apt/lists/* 七、使用合理的ENTRYPOINT脚本 示例： #!/bin/bash set -e if [ \"$1\" = 'postgres' ]; then chown -R postgres \"$PGDATA\" if [ -z \"$(ls -A \"$PGDATA\")\" ]; then gosu postgres initdb fi exec gosu postgres \"$@\" fi exec \"$@\" 八、其他建议 1、设置时区 FROM alpine:3.11.5 ENV TZ=Asia/Shanghai RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache tzdata \\ && cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ && echo \"Asia/Shanghai\" > /etc/timezone 2、设置系统语言 FROM alpine:3.11.5 ENV LANG=en_US.UTF-8 \\ LANGUAGE=en_US.UTF-8 RUN apk --no-cache add ca-certificates \\ && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-bin-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-i18n-2.29-r0.apk \\ && apk add glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && rm -rf /usr/lib/jvm glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && /usr/glibc-compat/bin/localedef --force --inputfile POSIX --charmap UTF-8 \"$LANG\" || true \\ && echo \"export LANG=$LANG\" > /etc/profile.d/locale.sh \\ && apk del glibc-i18n 3、使用Label标注作者、软件版本等元信息 FROM alpine:3.11.5 LABEL Author=Curiouser \\ Mail=****@163.com \\ PHP=7.3 \\ Tools=“git、vim、curl” \\ Update=\"添加用户组\" 4、指定工作区 WORKDIR /var/wwww 5、RUN指令显示优化 RUN set -eux ; \\ ls -al 九、镜像构建 1、命名 原则是见名知意。可使用三段式 镜像仓库地址/类型库/镜像名:版本号 registry/runtime/Java:8.1.2 registry/runtime/php-fpm-nginx:7.3-1.14 registry/cicd/kubctl-helm:1.17-3.0 registry/cicd/git-compose-docker:v1 registry/applications/demo:git_commit_id 2、使用Makefile IMAGE_BASE = registry/runtime IMAGE_NAME = php-fpm IMAGE_VERSION = 7.3 all: build push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} # 构建并推送 make # 仅构建 make build # 仅推送 make push 参考 https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ https://www.artindustrial-it.com/2017/09/20/10-best-practices-for-creating-good-docker-images/ https://gist.github.com/StevenACoffman/41fee08e8782b411a4a26b9700ad7af5 https://snyk.io/blog/10-docker-image-security-best-practices/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-08-31 10:07:13 "},"origin/shell-变量.html":{"url":"origin/shell-变量.html","title":"变量","keywords":"","body":"一. 变量的定义 1. 将命令的输出赋予变量 var=`shell命令` # `是反引号 var=$(shell命令) var=' line 1 line 2 line 3 ' 2. 变量的参数替换和扩展 表达式 含义 ${var_DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 * ${var=DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 * ${var:-DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * ${var:=DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * ${var+OTHER} 如果var声明了, 那么其值就是$OTHER, 否则就为null字符串 ${var:+OTHER} 如 果var被设置了, 那么其值就是$OTHER, 否则就为null字符串 ${var?ERR_MSG} 如果var没 被声明, 那么就打印$ERR_MSG* ${var:?ERR_MSG} 如果var没 被设置, 那么就打印$ERR_MSG* ${!varprefix*} 匹配之前所有以varprefix开头进行声明的变量 ${!varprefix@} 匹配之前所有以varprefix开头进行声明的变量 3. 读取标准输入输出赋值给变量 read -p \"请输入一个字符： \" key echo $key 二、变量的引用 1、基础使用 $var ${var} ${var:defaultvalue} 2、用变量的值作为新的变量名 name=test test_p=123 echo `eval echo '$'\"$name\"\"_p\"` 三、变量是否换行输出显示 如果想一个命令的多行输出赋值给一个变量 var='line 1 line 2 line 3 ' 使用echo输出时，在bash中 $ echo $var line 1 line 2 line 3 $ echo \"$var\" line 1 line 2 line 3 # 总共是有四行的输出，最后一个是空行 四、内置变量 内置变量 描述 $? 上一条命令执行状态 0 代表执行成功，1代表执行失败 $0~$9 位置参数1-9 ${10} 位置参数10 $# 位置参数个数 $$ 脚本进程的PID $- 传递到脚本中的标识 $! 运行在后台的最后一个作业的进程ID(PID) $_ 之前命令的最后一个参数 $@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 $* 稍有不同 $* 传递给脚本或函数的所有参数 $0 脚本的文件名 ${@: -1} 传递给脚本或函数的最后一个参数 ${@:1:$#-1} 传递给脚本或函数除最后一个参数以外的所有参数 $* 和 $@ 的区别 $ 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。但是当它们被双引号(\" \")包含时，\"$\" 会将所有的参数作为一个整体，以\"$1 $2 …$n\"的形式输出所有参数；\"$@\" 会将各个参数分开，以”$1” \"$2\" … \"$n\" 的形式输出所有参数。 四. 数值型变量的运算 1. 数值型变量的加减乘除 #样本数据 a=120 b=110 ((c=$a+$b)) #结果：230 ((d=$a-$b)) #结果：10 ((e=$a*$b)) #结果：13200 ((f=$a/$b)) #结果：1 c=$((a+b)) #结果：220 d=$((a-b)) #结果：20 e=$((a*b)) #结果：12000 f=$((a/b)) #结果：1 c=`expr a+b` #结果：220 d=`expr $a - $b` #结果：20 e=`expr $a \\* $b` #结果：12000 f=`expr $a / $b` #结果：1 2. 数值型变量的自增 a=1 #第一种整型变量自增方式 a=$(($a+1)) echo $a #第二种整型变量自增方式 a=$[$a+1] echo $a #第三种整型变量自增方式 a=`expr $a + 1` echo $a #第四种整型变量自增方式 let a++ echo $a #第五种整型变量自增方式 let a+=1 echo $a #第六种整型变量自增方式 ((a++)) echo $a 3. 数值类型变量的位数截取 a=1560418197875 # 截去后三位,要求只取\"1560418197\" # 方式1: 数值运算 b=$((a/1000)) # 方式2：字符截取（将数值变量当成字符串来处理） c=${a:0:-3} Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-07 11:42:47 "},"origin/shell-文件目录的判断.html":{"url":"origin/shell-文件目录的判断.html","title":"文件路径及变量的判断","keywords":"","body":"文件目录的判断 [ -a FILE ] 如果 FILE 存在则为真。 [ -b FILE ] 如果 FILE 存在且是一个块文件则返回为真。 [ -c FILE ] 如果 FILE 存在且是一个字符文件则返回为真。 [ -d FILE ] 如果 FILE 存在且是一个目录则返回为真。 [ -e FILE ] 如果 指定的文件或目录存在时返回为真。 [ -f FILE ] 如果 FILE 存在且是一个普通文件则返回为真。 [ -g FILE ] 如果 FILE 存在且设置了SGID则返回为真。 [ -h FILE ] 如果 FILE 存在且是一个符号符号链接文件则返回为真。（该选项在一些老系统上无效） [ -k FILE ] 如果 FILE 存在且已经设置了冒险位则返回为真。 [ -p FILE ] 如果 FILE 存并且是命令管道时返回为真。 [ -r FILE ] 如果 FILE 存在且是可读的则返回为真。 [ -s FILE ] 如果 FILE 存在且大小非0时为真则返回为真。 [ -u FILE ] 如果 FILE 存在且设置了SUID位时返回为真。 [ -w FILE ] 如果 FILE 存在且是可写的则返回为真。（一个目录为了它的内容被访问必然是可执行的） [ -x FILE ] 如果 FILE 存在且是可执行的则返回为真。 [ -O FILE ] 如果 FILE 存在且属有效用户ID则返回为真。 [ -G FILE ] 如果 FILE 存在且默认组为当前组则返回为真。（只检查系统默认组） [ -L FILE ] 如果 FILE 存在且是一个符号连接则返回为真。 [ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则返回为真。 [ -S FILE ] 如果 FILE 存在且是一个套接字则返回为真。 [ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 新, 或者 FILE1 存在但是 FILE2 不存在则返回为真。 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 老, 或者 FILE2 存在但是 FILE1 不存在则返回为真。 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则返回为真。 变量的判断 [ -z $变量 ] 判断变量是否存在 if [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi 参考： https://unix.stackexchange.com/questions/109625/shell-scripting-z-and-n-options-with-if https://stackoverflow.com/questions/3601515/how-to-check-if-a-variable-is-set-in-bash Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-10 15:33:59 "},"origin/shell-数值型的判断.html":{"url":"origin/shell-数值型的判断.html","title":"数值型的判断","keywords":"","body":"数值型的判断 # -gt 大于，如[ $a -gt $b ] # -lt 小于，如[ $a -lt $b ] # -eq 等于，如[ $a -eq $b ] # -ne 不等于，如[ $a -ne $b ] # -ge 大于等于，如[ $a -ge $b ] # le 小于等于 ，如 [ $a -le $b ] # 大于(需要双括号),如:(($a > $b)) # >= 大于等于(需要双括号),如:(($a >= $b)) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-02 18:20:44 "},"origin/shell-if判断.html":{"url":"origin/shell-if判断.html","title":"if判断","keywords":"","body":"if [ command ]; then 符合该条件执行的语句 fi if [ command ]; then command执行返回状态为0要执行的语句 else command执行返回状态为1要执行的语句 fi if [ command1 ]; then command1执行返回状态为0要执行的语句 elif [ command2 ]; then command2执行返回状态为0要执行的语句 else command1和command2执行返回状态都为1要执行的语句 fi PS: [ command ]，command前后要有空格 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/shell-for循环语句.html":{"url":"origin/shell-for循环语句.html","title":"for循环语句","keywords":"","body":"第一类：数字性循环 #!/bin/bash for((i=1;i #!/bin/bash for i in $(seq 1 10) do echo $(expr $i \\* 3 + 1); done #!/bin/bash for i in {1..10} do echo $(expr $i \\* 3 + 1); done #!/bin/bash awk 'BEGIN{for(i=1; i 第二类：字符性循环 #!/bin/bash for i in `ls`; do echo $i is file name\\! ; done #!/bin/bash for i in $* ; do echo $i is input chart\\! ; done #!/bin/bash for i in f1 f2 f3 ; do echo $i is appoint ; done #!/bin/bash list=\"rootfs usr data data2\" for i in $list; do echo $i is appoint ; done 第三类：路径查找 #!/bin/bash for file in /proc/*; do echo $file is file path \\! ; done #!/bin/bash for file in $(ls *.sh) do echo $file is file path \\! ; done Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/shell-while循环语句.html":{"url":"origin/shell-while循环语句.html","title":"while循环语句","keywords":"","body":"while condition ; do statements ... done Note: 和if一样，condition可以有一系列的statements组成，值是最后的statment的exit status Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/shell-until循环语句.html":{"url":"origin/shell-until循环语句.html","title":"until循环语句","keywords":"","body":"until [condition-is-true] ; do statements ... done Note: 执行statements，直至command正确运行。在循环的顶部判断条件,并且如果条件一直为false那就一直循环下去 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-08-18 16:01:41 "},"origin/shell-print-string.html":{"url":"origin/shell-print-string.html","title":"字符的输出","keywords":"","body":"字符的输出 1、多行变一行并追加分割符 原始：echo $a 1 2 3 效果：echo $a | tr '\\n' ',’ 1,2,3, Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-07-15 11:16:55 "},"origin/shell-字符串的截取拼接.html":{"url":"origin/shell-字符串的截取拼接.html","title":"字符串的截取拼接","keywords":"","body":"一. 字符串的截取 表达式 含义 ${#string} $string的字符个数 ${string:position} 在$string中, 从位置$position开始提取子串 ${string:position:length} 在$string中, 从位置$position开始提取长度为$length的子串 ${string#substring} 从 变量$string的开头, 删除最短匹配$substring的子串 ${string##substring} 从 变量$string的开头, 删除最长匹配$substring的子串 ${string%substring} 从 变量$string的结尾, 删除最短匹配$substring的子串 ${string%%substring} 从 变量$string的结尾, 删除最长匹配$substring的子串 ${string/substring/replacement} 使用$replacement, 来代替第一个匹配的$substring ${string//substring/replacement} 使用$replacement, 代替所有匹配的$substring ${string/#substring/replacement} 如果$string的前缀匹配$substring, 那么就用$replacement来代替匹配到的$substring ${string/%substring/replacement} 如果$string的后缀匹配$substring, 那么就用$replacement来代替匹配到的$substring expr match \"$string\" '$substring' 匹配$string开头的$substring* 的长度 expr \"$string\" : '$substring' 匹 配$string开头的$substring* 的长度 expr index \"$string\" $substring 在$string中匹配到的$substring的第一个字符出现的位置 expr substr $string $position $length 在$string中 从位置$position开始提取长度为$length的子串 expr match \"$string\" '\\($substring\\)' 从$string的 开头位置提取$substring* expr \"$string\" : '\\($substring\\)' 从$string的 开头位置提取$substring* expr match \"$string\" '.*\\($substring\\)' 从$string的 结尾提取$substring* expr \"$string\" : '.*\\($substring\\)' 从$string的 结尾提取$substring* 1. # 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a#*/};echo $b # 结果：openshift/origin-metrics-cassandra:v3.9 2. ## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a##*/};echo $b # 结果：origin-metrics-cassandra:v3.9 3. %号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%/*};echo $b # 结果：docker.io/openshift 4. %% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）vi ~? # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%%/*};echo $b # 结果：docker.io 5. 从左边第几个字符开始，及字符的个数 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0:5};echo $b # 结果：docke 6. 从左边第几个字符开始，一直到结束 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:7};echo $b # 结果：io/openshift/origin-metrics-cassandra:v3.9 7. 从右边第几个字符开始，向右截取 length 个字符。 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8:5};echo $b # 结果：dra:v 8. 从右边第几个字符开始，一直到结束 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8};echo $b # 结果：dra:v3.9 9. 截取字符串中的ip # 样本: a=\"当前 IP：123.456.789.172 来自于：中国 上海 上海 联通\" b=${a//[!0-9.]/};echo $b 或者 echo $a | grep -o -E \"[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]*\" # 结果：123.456.789.172 10.提取字符串中的数字 echo \"test-v1.1.0\" | tr -cd '[0-9.]' # 输出1.1.0 aa=\"test-v1.1.0\" | echo ${aa//[!0-9.]/} # 输出1.1.0 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 22:23:15 "},"origin/shell-字符串的包含判断关系.html":{"url":"origin/shell-字符串的包含判断关系.html","title":"字符串的包含判断关系","keywords":"","body":"样本数据 a=\"test\" b=\"curiouser\" c=\"test hahah devops\" 1. 通过grep来判断 if `echo $c |grep -q $a` ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 2. 字符串运算符 if [[ $c =~ $a ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 3. 用通配符*号代替str1中非str2的部分，如果结果相等说明包含，反之不包含 if [[ $c == *$a* ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 4. 利用替换 if [[ ${c/$a//} == $c ]] ;then echo \"$c\" \" ----不包含--- \" \"$a\" else echo \"$c\" \" ----包含--- \" \"$a\" fi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-10 15:34:03 "},"origin/shell-自定义函数.html":{"url":"origin/shell-自定义函数.html","title":"自定义函数","keywords":"","body":"自定义函数的格式 [ function ] 函数名 [()] { action; [return int;] } # 1.函数在被调用前先声明好 # 2.function关键字可有无 自定义函数的调用 # 函数名() # 函数名(参数1,参数2) # 函数名 参数1 参数2 # $(函数名 参数1 参数2) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/bash-scirpts.html":{"url":"origin/bash-scirpts.html","title":"常用bash脚本功能","keywords":"","body":"1、判断curl返回状态码 #!/bin/bash response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com) if [[ $response -ge 200 && $response -le 299 ]] ;then echo 'check point success' else echo 'check point fail' fi 2、读取文件中的配置到变量中 #!/bin/bash # 配置文件中的配置项格式为key1=value1，一行一个配置项 while read line;do eval \"$line\" done 3、根据console输入条件执行 #!/bin/bash echo \"选择以下功能:\" echo \" 0) 功能0\" echo \" 1) 功能1\" echo \" 2) 功能2\" echo \" 3) 功能3\" echo \" 4) 功能4\" read -p \"功能选项[4]: \" option until [[ -z \"$option\" || \"$option\" =~ ^[0-4]$ ]]; do read -p \"$option为无效的选项，请重新输入功能选项: \" option done case \"$option\" in 0) echo \"功能0已执行!\" ;; 1) echo \"功能1已执行!\" exit ;; 2) echo \"功能2已执行!\" exit ;; 3) echo \"功能3已执行!\" exit ;; # 默认选项 4|\"\") echo \"功能4已执行!\" exit ;; esac 4、将指定输出内容写入文件 { echo \"hahh\" echo \"lalal\" } > /tmp/test 5、判断变量是否存在或为空 if [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi 参考：https://stackoverflow.com/questions/3601515/how-to-check-if-a-variable-is-set-in-bash 6、换算秒为分钟、小时 #!/bin/bash a=60100 swap_seconds () { SEC=$1 (( SEC = 60 && SEC 3600 )) && echo -e \"持续时间: $(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\\c\" } b=`swap_seconds $a` echo $b 输出 持续时间: 16小时41分钟40秒 7、脚本命令行参数的传递与判断 #!/bin/bash main() { if [[ $# == 1 ]]; then case $1 in \"-h\") echo \"脚本使用方法: \" echo \" ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)\" exit ;; \"--help\") echo \"脚本使用方法: \" echo \" ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)\" exit ;; *) echo \"参数错误！\" exit ;; esac fi } main $* 8、检测docker 容器的启动状态 # 第一步：判断镜像是否存在 if [ `docker images --format {{.Repository}}:{{.Tag}} |grep -Fx 192.168.1.7:32772/applications/$CI_PROJECT_NAME:${CI_COMMIT_SHORT_SHA};echo $?` -eq 0 ];then docker pull 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; fi; # 第二步：判断是否已经有重名的容器在运行或者处在其他状态。重名的，先删掉，在启动；不重名的直接启动 if [ `docker ps -a --format {{.Names}} |grep -Fx $CI_PROJECT_NAME > /dev/null ;echo $?` -eq 0 ] ;then docker rm -f $CI_PROJECT_NAME ; docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; else docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; fi; # 第三步：循环五次判断容器的监控检测是否处于什么状态。健康状态就直接退出循环，不健康显示健康检查日志，正在启动的直接显示。处于其他状态的直接显示状态 n=0; while true ;do container_state=`docker inspect --format='{{json .State.Health.Status}}' $CI_PROJECT_NAME`; case $container_state in '\"starting\"' ) echo \"应用容器正在启动！\"; ;; '\"healthy\"' ) echo \"应用容器已启动，状态健康！\"; break; ;; '\"unhealthy\"' ) echo \"应用容器健康检测失败！\"; docker inspect --format='{{json .State.Health.Log}}' $CI_PROJECT_NAME; ;; * ) echo \"未知的状态:$container_state\"; ;; esac; sleep 1s; n=$(($n+1)); if [ $n -eq 5 ]; then break ; fi ; done 9、检查常见系统命令是否安装 check_command() { if ! command -v ifconfig >/dev/null 2>&1; then echo -e \"\\033[31mifconfig命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y net-tools >/dev/null 2>&1 elif os=\"centos\"; then yum install -y net-tools >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y net-tools >/dev/null 2>&1 fi elif ! command -v ip >/dev/null 2>&1; then echo -e \"\\033[31mip命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y iproute2 >/dev/null 2>&1 elif os=\"centos\"; then yum install -y iproute2 >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y iproute2 >/dev/null 2>&1 fi elif ! command -v curl >/dev/null 2>&1; then echo -e \"\\033[31mcurl命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y curl >/dev/null 2>&1 elif os=\"centos\"; then yum install -y curl >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y curl >/dev/null 2>&1 fi elif ! command -v wget >/dev/null 2>&1; then echo -e \"\\033[31mawk命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y wget >/dev/null 2>&1 elif os=\"centos\"; then yum install -y wget >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y wget >/dev/null 2>&1 fi elif ! command -v tail >/dev/null 2>&1; then echo -e \"\\033[31mcoreutils命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y coreutils >/dev/null 2>&1 elif os=\"centos\"; then yum install -y coreutils >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y coreutils >/dev/null 2>&1 fi elif ! command -v sed >/dev/null 2>&1; then echo -e \"\\033[31msed命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y sed >/dev/null 2>&1 elif os=\"centos\"; then yum install -y sed >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y sed >/dev/null 2>&1 fi elif ! command -v grep >/dev/null 2>&1; then echo -e \"\\033[31mgrep命令不存在，正在下载安装！\\033[0m\" if os=\"ubuntu\"; then apt install -y grep >/dev/null 2>&1 elif os=\"centos\"; then yum install -y grep >/dev/null 2>&1 elif os=\"fedora\"; then dnf install -y grep >/dev/null 2>&1 fi fi } 10、检查系统网络 check_network() { check_command ping -c 4 114.114.114.114 >/dev/null if [ ! $? -eq 0 ]; then echo -e \"\\033[31mIP地址无法ping通，请检查网络连接！！！\\033[0m\" exit fi ping -c 4 www.baidu.com >/dev/null if [ ! $? -eq 0 ]; then echo -e \"\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m\" exit fi curl -s --retry 2 --connect-timeout 2 www.baidu.com >/dev/null if [ ! $? -eq 0 ]; then echo -e \"\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m\" exit fi } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-28 00:00:00 "},"origin/maven-Settings配置文件详解.html":{"url":"origin/maven-Settings配置文件详解.html","title":"Mave Settings文件详解","keywords":"","body":"一、settings.xml文件作用 从settings.xml的文件名就可以看出，它是用来设置maven参数的配置文件。并且settings.xml是maven的全局配置文件。而pom.xml文件是所在项目的局部配置。Settings.xml中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 二、settings.xml文件位置 settings.xml文件一般存在于两个位置： 全局配置: ${M2_HOME}/conf/settings.xml 用户配置: user.home/.m2/settings.xml Note： 局部配置优先于全局配置。配置优先级从高到低：pom.xml> user settings > global settings 如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。 三、settings.xml元素详解 settings.xml中的顶级元素 LocalRepository 该值表示构建系统本地仓库的路径。其默认值：~/.m2/repository。 ${user.home}/.m2/repository InteractiveMode 表示maven是否需要和用户交互以获得输入。如果maven需要和用户交互以获得输入，则设置成true，反之则应为false。默认为true。 true UsePluginRegistry maven是否需要使用plugin-registry.xml文件来管理插件版本。如果需要让maven使用文件~/.m2/plugin-registry.xml来管理插件版本，则设为true。默认为false。 false Offline 表示maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。 当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 false PluginGroups 当插件的组织id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。 该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。 当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins和org.codehaus.mojo。 org.codehaus.mojo Servers 仓库的下载和部署是在pom.xml文件中的repositories和distributionManagement元素中定义的。然而，一般类似用户名、密码（有些仓库访问是需要安全认证的）等信息不应该在pom.xml文件中配置，这些信息可以配置在settings.xml中。 server001 my_login my_password ${usr.home}/.ssh/id_dsa some_passphrase 664 775 Mirrors 为仓库列表配置的下载镜像列表。 planetmirror.com PlanetMirror Australia http://downloads.planetmirror.com/pub/maven2 central Proxies 用来配置不同的代理 myproxy true http proxy.somewhere.com 8080 proxyuser somepassword *.google.com|ibiblio.org Profiles 根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 它包含了id、activation、repositories、pluginRepositories和 properties元素。这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。如果一个settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中带有相同id的profile。 test Activation 自动触发profile的条件逻辑。 如pom.xml中的profile一样，profile的作用在于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test） false 1.5 Windows XP Windows x86 5.1.2600 mavenVersion 2.0.3 ${basedir}/file2.properties ${basedir}/file1.properties 注：在maven工程的pom.xml所在目录下执行mvn help:active-profiles命令可以查看中央仓储的profile是否在工程中生效。 properties 对应profile的扩展属性列表。 maven属性和ant中的属性一样，可以用来存放一些值。这些值可以在pom.xml中的任何地方使用标记${X}来使用，这里X是指属性的名称。属性有五种不同的形式，并且都能在settings.xml文件中访问 1.0通过${project.version}获得version的值。 3. settings.x: 指代了settings.xml中对应元素的值。例如：false通过 ${settings.offline}获得offline的值。 4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问，例如 ${java.home}。 5. x: 在元素中，或者外部文件中设置，以$的形式使用。{someVar}的形式使用。 --> ${user.home}/our-project 注：如果该profile被激活，则可以在pom.xml中使用${user.install}。 Repositories 远程仓库列表，它是maven用来填充构建系统本地仓库所使用的一组远程仓库 codehausSnapshots Codehaus Snapshots false always warn http://snapshots.maven.codehaus.org/maven2 default pluginRepositories 发现插件的远程仓库列表。 和repository类似，只是repository是管理jar包依赖的仓库，pluginRepositories则是管理插件的仓库。 maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址 ActiveProfiles 手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。任何在activeProfile中定义的profile id，不论环境设置如何，其对应的 profile都会被激活。如果没有匹配的profile，则什么都不会发生。 例如，env-test是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。如果运行过程中找不到这样一个profile，Maven则会像往常一样运行。 env-test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/maven-生命周期阶段.html":{"url":"origin/maven-生命周期阶段.html","title":"Maven 生命周期阶段","keywords":"","body":"Maven的生命周期以及阶段插件 Maven拥有三个生命周期，每个生命周期包含一些阶段，这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段。 运行任何一个阶段的时候，它前面的所有阶段都会被运行 Maven三个生命周期只是定义了各个阶段要做的事情、但是不做任何实际工作、实际工作都是由插件的目标来完成的。插件以独立的形式存在、Maven会在需要的时候下载并使用插件 一个插件有可能有多个功能、每个功能就是一个目标。比如maven-dependency-plugin有十多个目标、每个目标对应了一个功能。插件的目标为dependency:analyze、dependency:tree和dependency:list。通用写法：插件前缀:插件目标。比如compiler:compile 一、Maven的生命周期阶段及插件绑定 上面三个生命周期中有很多原来的生命周期阶段没有默认绑定插件、也就意味着默认情况下他们没有任何意义。当然如果我们有自己特殊的处理、可以为他们绑定特殊的插件、比如下面会有提到的在打包的时候生成jar包的源码、可以在default生命周期的verify阶段绑定生成源码插件的生成源码的目标。 二、自定义插件绑定 自定义绑定允许我们自己掌控插件目标与生命周期的结合、下面以生成项目主代码的源码jar为例。使用到的插件和他的目标为：maven-source-plugin:jar-no-fork、将其绑定到default生命周期阶段verify上（可以任意指定三套生命周期的任意阶段）、在项目的POM配置中（也可以在父POM中配置、后面聚合与继承会有提到） org.apache.maven.plugins maven-source-plugin 2.1.1 attach-sources verify jar-no-fork # build元素下的plugins子元素中声明插件的使用。使用的maven-source-plugin插件，其groupId为org.apache.maven.plugins（官方插件的groupId），version版本为2.1.1.对于自定义绑定的插件，应应指定一个非快照的版本，避免插件版本变化造成构件不稳定。 execution元素用来配置执行的任务。上面配置了一个id为attach-source的任务，通过phrase将其绑定到了verify生命周期阶段上，再通过goals配置指定要执行的插件目标（及插件功能）。 需要注意的是： 即使不通过phrase来配置生命周期阶段，有的插件也定义了默认的生命周期阶段。可使用maven-help-plugin来查看插件的详细信息。例如： mvn help：describe-Dplugin=org.apache.maven.plugins:maven-source-plugin:2.1.1 上述配置有插件的坐标声明、还有excutions下面每个excution子元素配置的执行的一个个任务、通过phase指定与生命周期的那个阶段绑定、在通过goals指定执行绑定插件的哪些目标。 当插件的目标绑定到不同的生命周期阶段的时候、插件目标的执行顺序是有生命周期阶段的顺序决定的、当多个插件目标绑定到同一生命周期阶段的时候、顺序是按照插件声明的顺序来决定目标的执行顺序。 三、插件配置 有三种方式可配置插件功能目标执行任务时的参数 1、命令行 例如maven-surefire-Plugin提供了maven.test.skip参数，当值为true时就跳过单元测试。在命令行时，加上-D参数配置该插件的参数就能跳过单元测试 maven install -Dmaven.test.skip=true 参数-D是Java自带的，其功能就是通过命令行设置Java系统环境变量。 2、POM中设置插件的全局任务配置 例如在下面POM文件中配置了maven-source-plugin插件要编译什么java版本的源代码，生成什么java版本的字节码文件 org.apache.maven.plugins maven-source-plugin 2.1.1 1.5 1.5 这样，不管绑定到compile阶段的maven-source-plugin：compile任务，还是绑定到test-compile阶段的maven-source-plugin：testCompiler任务，就都能使用到该配置来基于Java 1.5版本来执行任务 3、POM中插件任务配置 用户还可以在POM文件中定义某插件的任务参数。例如以maven-antrun-plugin插件为例。他有一个run目标，可以用来在Maven中调用Ant任务。用户可以将maven-antrun-plugin的目标run绑定到多个生命周期阶段上，再加以不同的配置，就可以让Maven在不同的生命阶周期段执行不同的任务 org.apache.maven.plugins maven-antrun-plugin 1.3 ant-validate validate run HAHAHAHHAHAHAHHAHAHAHHAHAHA ant-verify verify run lalallalal 参考链接 https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference https://maven.apache.org/plugins/index.html https://blog.csdn.net/zhaojianting/article/details/80321488 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/maven-operations.html":{"url":"origin/maven-operations.html","title":"Maven常见操作","keywords":"","body":"Maven常见操作 1、设置代理 方式一：settings.xml中配置 ...省略... true http proxy.somewhere.com 8080 proxyuser somepassword www.google.com|*.somewhere.com ...省略... 方式二：设置环境变量 export MAVEN_OPTS=\"-DsocksProxyHost=127.0.0.1 -DsocksProxyPort=$PORT\" # 或者 -Dhttp.proxyHost= -Dhttp.proxyPort= -Dhttps.proxyHost= -Dhttps.proxyPort= Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 16:33:37 "},"origin/git-原理.html":{"url":"origin/git-原理.html","title":"git原理","keywords":"","body":"git原理 一、git是什么？ Git是一款免费、开源的分布式版本控制系统 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 当我们把代码从git hub档下来或者说初始化git项目后，便有了这三个分区的概念 二、git是怎么储存信息的 git object有三种类型： Blob Tree Commit 在Git仓库里面，HEAD、分支、普通的Tag可以简单的理解成是一个指针，指向对应commit的SHA1值。 四、git命令 分区转换命令 1. git add 数据从工作区转移至暂存区 2. git commit 数据从暂存区转移至版本库，也就是本地仓库 3. git push 数据从版本库中发送到远程仓库 分区对比命名 4. git diff 工作区与暂存区对比 5. git diff head 工作区与版本库对比 6. git diff -cached 暂存区与版本库对比 参考 https://zhuanlan.zhihu.com/p/96631135 https://www.zhihu.com/search?type=content&q=git Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/git-常用操作.html":{"url":"origin/git-常用操作.html","title":"git常用操作","keywords":"","body":"git常用操作 1、删除远程仓库分支 git push 远程仓库别名 :远程仓库中的分支 2、克隆远程仓库指定分支到本地指定路径 git clone -b 3、拉取远程仓库指定分支到本地仓库的特定分支 git fetch 远程仓库中分支名:本地分支名 #使用该方式会在本地新建分支，但是不会自动切换到该本地分支，需要手动checkout切换分支。 git remote update ;\\ git checkout -b local_branch / #使用该方式会在本地新建分支，并自动切换到该本地分支。采用此种方法建立的本地分支会和远程分支建立映射关系。 4、Git代理设置 # 设置代理 git config --global http.proxy 代理地址 git config --global https.proxy 代理地址 # 取消代理 git config --global --unset http.proxy git config --global --unset https.proxy # 查看代理设置 git config --global --get http.proxy git config --global --get https.proxy 5、查看单个文件修改历史 git log --pretty=oneline 文件名 5096d69f*** handle merge file conflict a7593501*** fix: 同步 master docker 文件夹所有文件 git show a7593501*** 6、修改已提交的Commint git commit --amend 7、HTTP方式设置记住用户名和密码 设置记住密码（默认15分钟） git config --global credential.helper cache 设置时间，例如设置一个小时之后失效 git config credential.helper 'cache --timeout=3600' 长期存储密码 git config --global credential.helper store 增加远程地址的时候设置密码 http://yourname:password@git.oschina.net/name/project.git 8、Git tag使用 # 查看tag git tag # 打某个分支的tag git tag -a 0.0.0_b1 -m \"test tag\" # 将本地tag推送到远程仓库中 git push origin --tags # 删除本地tag git tag -d 0.0.0_b3 # 删除远程仓库中的tag git push origin :tags/0.0.0_b1 9、获取最近一次提交的commit id # 获取完整commit id git rev-parse HEAD # 获取8位commit id git rev-parse --short HEAD 10、commit回退 # 进回退到最近一个的上一个commit git reset --hard HEAD^ # 回退到指定commit。commit的ID可残缺地写 git reset --hard # 查看已回退commit的历史，并回复回退的commit git reflog git reset --hard 11、git代理设置 # 设置使用HTTP类型的代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 # 取消代理的设置 git config --global --unset http.proxy git config --global --unset https.proxy # 设置使用socks5类型的代理 git config --global http.proxy 'socks5://127.0.0.1:1081' git config --global https.proxy 'socks5://127.0.0.1:1081' Git问题总结 fatal: I don't handle protocol 'http' 原因：.git/config中的远程仓库URL可能有乱码 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-04 15:05:40 "},"origin/git-gitignore文件.html":{"url":"origin/git-gitignore文件.html","title":"git .gitignore文件","keywords":"","body":"git的.gitignore文件 一、简介 一般来说每个Git项目中都需要一个“.gitignore”文件，这个文件的作用就是告诉Git哪些文件不需要添加到版本管理中。 二、用法 # 此为注释 – 将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 注意 这样没有扩展名的文件在Windows下不太好创建，方法：创建一个文件，文件名为：“.gitignore.”，注意前后都有一个点。保存之后系统会自动重命名为“.gitignore”。 假设我们只有过滤规则没有添加规则，那么我们就需要把/mtk/目录下除了one.txt以外的所有文件都写出来！ 如果你不慎在创建.gitignore文件之前就push了项目，那么即使你在.gitignore文件中写入新的过滤规则，这些规则也不会起作用，Git仍然会对所有文件进行版本管理。简单来说，出现这种问题的原因就是Git已经开始管理这些文件了，所以你无法再通过过滤规则过滤它们。所以一定要养成在项目开始就创建.gitignore文件的习惯，否则一旦push，处理起来会非常麻烦 三、样本 参考https://github.com/github/gitignore/blob/master/Maven.gitignore target/ pom.xml.tag pom.xml.releaseBackup pom.xml.versionsBackup pom.xml.next release.properties dependency-reduced-pom.xml buildNumber.properties .mvn/timing.properties .mvn/wrapper/maven-wrapper.jar ### IDES ### ### STS ### .apt_generated .classpath .factorypath .project .settings .springBeans .sts4-cache ### IntelliJ IDEA ### .idea *.iws *.iml *.ipr ### NetBeans ### /nbproject/private/ /nbbuild/ /dist/ /nbdist/ /.nb-gradle/ /build/ ### VS Code ### .vscode/ ### OS ### .DS_Store Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/origin-git-merge-git-rebase.html":{"url":"origin/origin-git-merge-git-rebase.html","title":"git merge与git rebase","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/git-submodule.html":{"url":"origin/git-submodule.html","title":"git submodule","keywords":"","body":"Git Submodule子模块 一、简介 经常碰到这种情况：当你在一个Git 项目上工作时，你需要在其中使用另外一个Git 项目。也许它是一个第三方开发的Git 库或者是你独立开发和并在多个父项目中使用的。这个情况下一个常见的问题产生了：你想将两个项目单独处理但是又需要在其中一个中使用另外一个。 在Git 中你可以用子模块submodule来管理这些项目，submodule允许你将一个Git 仓库当作另外一个Git 仓库的子目录。这允许你克隆另外一个仓库到你的项目中并且保持你的提交相对独立。 二、git submodule命令详解 Git submodule命令 git submodule 指令 指令 add add [-b ] [-f|--force] [--name ] [--reference ] [--depth ] [--] [] status status [--cached] [--recursive] [--] [...] init init [--] [...] deinit deinit [-f|--force] (--all|[--] ...) update update [--init] [--remote] [-N|--no-fetch] [--[no-]recommend-shallow] [-f|--force] [--checkout|--rebase|--merge] [--reference ] [--depth ] [--recursive] [--jobs ] [--] [...] summary summary [--cached|--files] [(-n|--summary-limit) ] [commit] [--] [...] foreach foreach [--recursive] sync sync [--recursive] [--] [...] absorbgitdirs 三、操作 添加子模块 git submodule add ssh://git@gitlab.apps.okd311.curiouser.com:30022/Demo/git-submodule-public.git public 查看子模块 git submodule 1b76c7ccb8e9a8460690433ffe5e14c3e9219890 public (heads/master) 初始化子模块 更新子模块 git submodule update 更新子模块 git submodule update --remote 参考 https://www.cnblogs.com/Again/articles/6686105.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/git-standard-commit-message.html":{"url":"origin/git-standard-commit-message.html","title":"git提交规范","keywords":"","body":"git提交规范 一、规范的好处 当我们提交代码的时候，需要编写提交信息（commit message）。而提交信息的主要用途是：告诉这个项目的人，这次代码提交里做了些什么。所以每次的提交信息大家应该按照某种规范进行提交，最好是能有规范和工具的约束。 易于阅读，在最短时间清楚每次提交的意义 良好的Git提交日志非常重要，最明显的一点是，它让整个Git提交历史的阅读变得非常轻松。大多数情况下，看提交历史的人跟提交代码的人都不是同一个人，当别人阅读你的提交历史时，他很可能是不知道具体代码细节的，你如何在最短的时间内让他一眼知道每次提交的意义 每次提交影响的具体范围？ 这个bug在哪次提交中被修复了？ 这个新功能是在哪次提交中增加的？ 修改是否向下兼容？ 是否回滚了代码？ 是否只是修改了文档、调整了代码格式？ 是否修改了测试、是否进行了重构？ 是否对代码进行了性能优化？ 提供更多的历史信息，方便快速浏览。 git log HEAD --pretty=format:%s 便于快速查找信息 可以过滤筛选某些commit git log HEAD --grep feature 生成CHANGELOG 规范的Git提交历史，还可以直接生成项目发版的CHANGELOG 二、开源社区的Angular提交规范 与我们日常工作稍有不同的是：工作中的 Release 计划一般都是事先安排好的，不需要一些 CHANGELOG 什么的。而开源应用、开源库需要有对应的 CHANELOG，则添加了什么功能、修改了什么等等。毕竟有很多东西是由社区来维护的。目前，社区有多种 Commit message 的写法规范。Angular 规范是目前使用最广的写法，比较合理和系统化，并且有配套的工具。 1. Angular规范的格式： 每个提交消息都由一个标题、一个正文和一个页脚组成。而标题又具有特殊格式，包括修改类型、影响范围和内容主题 (): [body正文] [footer注脚] =============中文版================== 注意： <>中为必填项 []中为可选项 提交消息的任何一行都不能超过100个字符（为了让消息在GitHub以及各种Git工具中都更容易阅读） type: 修改类型 每个类型值都表示了不同的含义，类型值必须是以下的其中一个： 主要type feat：提交了新功能 fix：修复了bug 次要type docs：只改动了文档相关的内容 style：调整代码格式，未修改代码逻辑（比如修改空格、格式化、缺少分号等） refactor：代码重构，既没修复bug也没有添加新功能 build：构造工具或者外部依赖的改动，例如webpack，npm，pom 特殊type perf：性能优化，提高性能的代码更改 test：添加或修改代码测试 ci：与CI（持续集成服务）有关的改动 chore：对构建流程或辅助工具和依赖库（如文档生成等）的更改 当一次改动包括主要type与次要type时，统一采用主要type。 scope: 影响范围 scope也为必填项，用于描述改动的范围，格式为项目名/模块名，例如：node-pc/common rrd-h5/activity，而we-sdk不需指定模块名。如果一次commit修改多个模块，建议拆分成多次commit，以便更好追踪和维护。 当修改影响多个范围时，也可以使用“*”。 它可以是你提交代码实际影响到的任何内容。例如$location、$browser、$compile、$rootScope、ngHref、ngClick、ngView等，唯一需要注意的是它必须足够简短。 subject: 标题 标题是对变更的简明描述： 使用祈使句，现在时态：是“change”不是“changed”也不是“changes” 不要大写首字母 结尾不要使用句号 body: 正文 正文是对标题的补充，但它不是必须的。和标题一样，它也要求使用祈使句且现在时态，正文应该包含更详细的信息，如代码修改的动机，与修改前的代码对比等。 footer: 注脚 当有以下两种情况需要写footer注脚： 不兼容的改变 如果当前代码有重大更改，应该以BREAKING CHANGE这个词开头，带一个空格或者两个换行符，然后是对变动的描述，变动理由以及如何迁移。 关闭issue 如果当前commit针对某个issue，可以以Closes为开头。 任何Breaking Changes（破坏性变更，不向下兼容）都应该在页脚中进行说明，它经常也用来引用本次提交解决的GitHub Issue。 Breaking Changes应该以“BREAKING CHANGE:”开头，然后紧跟一个空格或两个换行符，其他要求与前面一致。 Revert 还有一种特殊情况，如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。 revert: feat(pencil): add 'graphiteWidth' option This reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit .，其中的hash是被撤销 commit 的 SHA 标识符。 如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。 2、提交规范的辅助工具 IDEA插件Git Commit Template commitizen: 辅助撰写格式化的commit message 安装、配置 npm install -g commitizen commitizen工具是基于Node.js的，而非前端项目工程目录下是没有package.json文件，所以会报错： npm WARN saveError ENOENT: no such file or directory, open './package.json' npm WARN enoent ENOENT: no such file or directory, open './package.json' 对于此种错误，创建一个空的package.json文件，然后进入到项目目录，执行 npm init --yes 在项目目录里，使其支持 Angular 的 Commit message 格式。 commitizen init cz-conventional-changelog --save --save-exact 以后，凡是用到git commit命令，一律改为使用git cz。这时，就会出现选项，用来生成符合格式的 Commit message。 conventional-changelog: 生成CHANGELOG 如果你的所有 Commit 都符合 Angular 格式，那么发布新版本时， Change log 就可以用脚本自动生成，生成的文档包括以下三个部分。 New features Bug fixes Breaking changes 安装配置 npm install -g conventional-changelog # 生成CHANGELOG conventional-changelog -p angular -i CHANGELOG.md -w # 命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动 # 生成所有发布的 Change log conventional-changelog -p angular -i CHANGELOG.md -w -r 0 三、基于Jira的提交规范 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/regular-expression详解.html":{"url":"origin/regular-expression详解.html","title":"正则表达式","keywords":"","body":"正则表达式详解 一、什么是正则表达式？ 正则表达式（regular expression）就是用一个“字符串”来描述一个特征，然后去验证另一个“字符串”是否符合这个特征。比如 表达式“ab+” 描述的特征是“一个 'a' 和 任意个 'b' ”，那么 'ab', 'abb', 'abbbbbbbbbb' 都符合这个特征。 二、正则表达式能干什么？ 验证字符串是否符合指定特征，比如验证是否是合法的邮件地址 用来查找字符串，从一个长的文本中查找符合指定特征的字符串，比查找固定字符串更加灵活方便 用来替换，比普通的替换更强大 三、正则表达式规则 参考链接 http://www.regexlab.com/zh/regref.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ssl-tls.html":{"url":"origin/ssl-tls.html","title":"SSL/TLS","keywords":"","body":"SSL与TLS 一、简介 SSL(Secrue Socket Layer 安全套接层)： SSL(Secure Socket Layer 安全套接层)是基于HTTPS下的一个协议加密层，最初是由网景公司（Netscape）研发，后被IETF（The Internet Engineering Task Force - 互联网工程任务组）标准化后写入（RFCRequest For Comments 请求注释），RFC里包含了很多互联网技术的规范！ 起初是因为HTTP在传输数据时使用的是明文（虽然说POST提交的数据时放在报体里看不到的，但是还是可以通过抓包工具窃取到）是不安全的，为了解决这一隐患网景公司推出了SSL安全套接字协议层，SSL协议位于TCP/IP协议与各种应用层协议之间，为数据通讯提供安全支持。SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。 TLS(Transport Layer Security安全传输层协议) 由于HTTPS的推出受到了很多人的欢迎，在SSL更新到3.0时，IETF对SSL3.0进行了标准化，并添加了少数机制(但是几乎和SSL3.0无差异)，标准化后的IETF更名为TLS1.0(Transport Layer Security 安全传输层协议)，可以说TLS就是SSL的新版本3.1，并同时发布“RFC2246-TLS加密协议详解” Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ssl-tls-cfssl.html":{"url":"origin/ssl-tls-cfssl.html","title":"CFSSL","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ceph-rbd单节点安装.html":{"url":"origin/ceph-rbd单节点安装.html","title":"Ceph RBD单节点安装","keywords":"","body":"Prerequisite Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 关闭防火墙和SeLinuxsystemctl disable firewalld ; systemctl stop firewalld ; sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config ; setenforce 0 ; sestatus -v SSH免密码登录打通ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ; ssh-copy-id root@allinone.curiouser.com ; ssh allinone.curiouser.com Hosts绑定IP地址域名解析echo \"192.168.1.21 allinone.curiouser.com\" >> /etc/hosts 创建ceph用户并设置用户密码和为其添加root权限useradd ceph && echo ceph:ceph | chpasswd ; echo \"ceph ALL=(root) NOPASSWD:ALL\" > /etc/sudoers.d/ceph ; chmod 0440 /etc/sudoers.d/ceph 配置Ceph和Epel的yum源仓库 vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 可以修改ceph源（外国的源总是timeout） export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-jewel/el7 ; export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc (可选)手动安装下载ceph的rpm包（使用ceph-deploy install 安装ceph包网速太慢。）官方下载: http://download.ceph.com/rpm-jewel/el7/x86_64/ 。需要下载的包如下： ``` ceph-10.2.10-0.el7.x86_64.rpmceph-base-10.2.10-0.el7.x86_64.rpmceph-common-10.2.10-0.el7.x86_64.rpmceph-mds-10.2.10-0.el7.x86_64.rpmceph-mon-10.2.10-0.el7.x86_64.rpmceph-osd-10.2.10-0.el7.x86_64.rpmceph-radosgw-10.2.10-0.el7.x86_64.rpmceph-selinux-10.2.10-0.el7.x86_64.rpmrbd-mirror-10.2.10-0.el7.x86_64.rpm yum localinstall -y ./*.rpm ## 一、安装Ceph-Deploy 1. 安装Ceph-deploy ```bash yum install ceph-deploy -y 安装ceph相关的软件 ceph-deploy install $HOSTNAME 二、创建集群配置文件 创建ceph-deploy的集群配置文件夹，路径并切换过去 mkdir my-cluster ;cd my-cluster 用 ceph-deploy 创建集群，用 new 命令、并指定主机作为初始监视器。 ceph-deploy new $HOSTNAME # 该操作会在~/my-cluster下会生成三个文件 -rw-rw-r-- 1 ceph ceph 251 Jan 12 16:34 ceph.conf -rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log -rw------- 1 ceph ceph 73 Jan 12 16:30 ceph.mon.keyring ceph.conf中默认的osd pool为3，对应了三个node节点。如果只有两个node节点，则需要修改ceph.conf中的默认值 [global] fsid = 25c13add-967e-4912-bb33-ebbc2cb9376d mon_initial_members = allinone.curiouser.com mon_host = 172.16.2.3 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx # =========新增部分============ filestore_xattr_use_omap = true osd pool default size=1 osd max object name len = 256 osd max object namespace len = 256 mon_pg_warn_max_per_osd = 500 三、创建Monitor ceph-deploy mon create $HOSTNAME ; ceph-deploy gatherkeys $HOSTNAME ; ceph mds stat #查看mds节点状态 四、创建OSD 方式一： (可选)手动节点上挂载lvm存储到某个目录下，作为node节点上OSD的数据存储目录 yum install -y lvm2 ; disk=/dev/vdc ; pvcreate ${disk} ; vgcreate ${disk} ; vgcreate -s 16m ceph-osd ${disk} ; PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` ; lvcreate -l ${PE_Number} -n ceph-osd ceph-osd ; mkfs.xfs /dev/ceph-osd/ceph-osd ; mkdir -p /data/ceph/osd ; chown -R ceph:ceph /data/ceph/osd ; echo \"/dev/ceph-osd/ceph-osd /data/ceph/osd xfs defaults 0 0\" >> /etc/fstab ; mount -a ; df -mh #LV的文件系统格式注意要xfs,CentOS推荐使用xfs的文件系统.如果是ext4，需要在/etc/ceph/ceph.conf 中添加参数用来限制文件名的长度 osd max object name len = 256 osd max object namespace len = 64 # 之后重启osd服务 systemctl restart ceph-osd.target 准备并激活node节点上的OSD #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/data/ceph/osd #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/data/ceph/osd #查看OSD状态 ceph osd tree 方式二：(不是以目录为OSD数据存储设备，而是直接以硬盘。其实就是省去手动在硬盘上创建分区的操作) #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/dev/vdc #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/dev/vdc1 #查看OSD状态 ceph osd tree 五、安装验证 #集群健康状态检查 $> ceph health HEALTH_OK $> ceph -s $> systemctl is-enabled ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 六、其他信息 Ceph相关的SystemD Units ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 查看、 更新配置 ceph --show-config ceph-deploy --overwrite-conf config push $HOSTNAME 清除卸载Ceph ceph-deploy uninstall $HOSTNAME ceph-deploy purge $HOSTNAME rm -rf /var/lib/ceph/ /var/run/ceph rm -rf /data/ceph/osd/* Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ceph-filesystem单节点安装.html":{"url":"origin/ceph-filesystem单节点安装.html","title":"Ceph FileSystem单节点安装","keywords":"","body":"Context Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 一个cephfs至少要求两个librados存储池，一个为data，一个为metadata。当配置这两个存储池时，注意： 为metadata pool设置较高级别的副本级别，因为metadata的损坏可能导致整个文件系统不用 建议metadata pool使用低延时存储，比如SSD，因为metadata会直接影响客户端的响应速度 Preflight 一个 clean+active 的cluster（Ceph RBD单节点安装） cluster fb506b4e-43b8-4634-acb9-ea3ee5a97b91 health HEALTH_OK monmap e1: 1 mons at {allinone=192.168.1.96:6789/0} election epoch 29, quorum 0 allinone fsmap e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} osdmap e113: 1 osds: 1 up, 1 in flags sortbitwise,require_jewel_osds pgmap v61453: 192 pgs, 3 pools, 2639 MB data, 985 objects 2730 MB used, 94500 MB / 97231 MB avail 192 active+clean 一、操作 部署元数据服务器MDSceph-deploy mds create $HOSTNAME 创建cephfs需要的两个存储池：一个pool用来存储数据，一个pool用来存储元数据ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 创建CephFS ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls 二、验证 $ ceph mds stat e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} 三、客户端挂载 Kernel方式 #加载rbd内核模块 modprobe rbd lsmod | grep rbd # 获取client.admin用户的秘钥 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" mkdir /mnt/mycephfs mount -t ceph allinone.okd311.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== FUSE方式 yum -y install ceph-fuse ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m 192.168.197.154:6789 ~/mycephfs/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/wrk.html":{"url":"origin/wrk.html","title":"wrk","keywords":"","body":"命令行测试工具wrk 一、简介 GitHub地址：https://github.com/wg/wrk 常见的测试工具有：Apache ab, Apache JMeter (互联网公司用的较多)，LoadRunner 等 wrk 是一款针对 Http 协议的基准测试工具，它能够在单机多核 CPU 的条件下，使用系统自带的高性能 I/O 机制，如 epoll，kqueue 等，通过多线程和事件模式，对目标机器产生大量的负载。 PS: 其实，wrk 是复用了 redis 的 ae 异步事件驱动框架，准确来说 ae 事件驱动框架并不是 redis 发明的, 它来至于 Tcl 的解释器 jim, 这个小巧高效的框架, 因为被 redis 采用而被大家所熟知。 二、安装 安装 源码编译/Linux git clone https://github.com/wg/wrk.git wrk && \\ cd wrk && \\ make && \\ cp wrk /usr/local/bin MacOS brew install wrk Docker Dockerfile alpine镜像的软件源中有wrk包，版本为：4.1.0 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache wrk VOLUME [ \"/data\" ] WORKDIR /data ENTRYPOINT [\"wrk\"] Makefile IMAGE_BASE = tools IMAGE_NAME = wrk IMAGE_VERSION = v1 all: build build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . 使用命令 # 本地使用docker启动wrk容器 docker run -it tools/wrk:v1 -t10 -c400 -d30s --latency https://www.baidu.com docker run -it -v .:/data tools/wrk:v1 -s test.lua -c400 -d30s --latency https://www.baidu.com # 在K8S集群中临时启动一个pod kubectl run -n test --rm --generator=run-pod/v1 --image tools/wrk:v1 -i -- wrk -t10 -c400 -d30s --latency https://www.baidu.com 三、命令详解 wrk # 参数选项： # -c, --connections 跟服务器建立并保持的TCP连接数量 # -d, --duration 压测时间 # -t, --threads 使用多少个线程进行压测 # -s, --script 指定Lua脚本路径 # -H, --header 为每一个HTTP请求添加HTTP头 # --latency 在压测结束后，打印延迟统计信息 # --timeout 超时时间 # -v, --version 打印正在使用的wrk的详细版本信息 # 代表数字参数，支持国际单位 (1k, 1M, 1G) # 代表时间参数，支持时间单位 (2s, 2m, 2h) 四、压测报告解读 示例：对本地Nginx的Web服务进行10个线程、400个请求连接、长达30s的压力测试 wrk -t10 -c400 -d30s --latency http://localhost:8080/ 30 秒压测过后，生成如下压测报告： Running 30s test @ http://localhost:8080/ 10 threads and 400 connections（共12个测试线程，400个连接） （平均值） （标准差） （最大值）（正负一个标准差所占比例） Thread Stats Avg Stdev Max +/- Stdev （延迟） Latency 22.09ms 4.40ms 115.67ms 90.98% (每秒请求数) Req/Sec 1.09k 456.65 2.08k 59.60% Latency Distribution （延迟分布） 50% 21.14ms 75% 22.36ms 90% 24.83ms 99% 42.64ms 326894 requests in 30.03s, 269.96MB read (32.03s内处理了326894个请求，耗费流量269.96MB) Socket errors: connect 159, read 132, write 0, timeout 0 (发生错误数) Requests/sec: 10884.16 (QPS 10884.16,即平均每秒处理请求数为10884.16) Transfer/sec: 8.99MB (平均每秒流量8.99MB) 五、使用 Lua 脚本进行复杂测试 wrk可以通过编写 Lua 脚本的方式，在运行压测命令时，通过参数 --script 来指定 Lua 脚本，来进行Post请求 等复杂测试 wrk 支持在三个阶段对压测进行个性化，分别是启动阶段、运行阶段和结束阶段。每个测试线程，都拥有独立的Lua 运行环境。 启动阶段: function setup(thread) 在脚本文件中实现 setup 方法，wrk 就会在测试线程已经初始化，但还没有启动的时候调用该方法。wrk会为每一个测试线程调用一次 setup 方法，并传入代表测试线程的对象 thread 作为参数。setup 方法中可操作该 thread 对象，获取信息、存储信息、甚至关闭该线程。 thread.addr - get or set the thread's server address thread:get(name) - get the value of a global in the thread's env thread:set(name, value) - set the value of a global in the thread's env thread:stop() - stop the thread 运行阶段： function init(args) function delay() function request() function response(status, headers, body) init(args): 由测试线程调用，只会在进入运行阶段时，调用一次。支持从启动 wrk 的命令中，获取命令行参数； delay()： 在每次发送请求之前调用，如果需要定制延迟时间，可以在这个方法中设置； request(): 用来生成请求, 每一次请求都会调用该方法，所以注意不要在该方法中做耗时的操作； response(status, headers, body): 在每次收到一个响应时被调用，为提升性能，如果没有定义该方法，那么wrk不会解析 headers 和 body； 结束阶段： function done(summary, latency, requests) done() 方法在整个测试过程中只会被调用一次，我们可以从给定的参数中，获取压测结果，生成定制化的测试报告。 自定义 Lua 脚本中可访问的变量以及方法： 变量：wrk wrk = { scheme = \"http\", host = \"localhost\", port = 8080, method = \"GET\", path = \"/\", headers = {}, body = nil, thread = , } 以上定义了一个 table 类型的全局变量，修改该 wrk 变量，会影响所有请求。 方法： wrk.fomat wrk.lookup wrk.connect 上面三个方法解释如下： function wrk.format(method, path, headers, body) wrk.format returns a HTTP request string containing the passed parameters merged with values from the wrk table. # 根据参数和全局变量 wrk，生成一个 HTTP rquest 字符串。 function wrk.lookup(host, service) wrk.lookup returns a table containing all known addresses for the host and service pair. This corresponds to the POSIX getaddrinfo() function. # 给定 host 和 service（port/well known service name），返回所有可用的服务器地址信息。 function wrk.connect(addr) wrk.connect returns true if the address can be connected to, otherwise it returns false. The address must be one returned from wrk.lookup(). # 测试给定的服务器地址信息是否可以成功创建连接 六、通过 Lua 脚本压测示例 调用 POST 接口： wrk.method = \"POST\" wrk.body = \"foo=bar&baz=quux\" wrk.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\" 注意: wrk 是个全局变量，这里对其做了修改，使得所有请求都使用 POST 的方式，并指定了 body 和 Content-Type头。 自定义每次请求的参数： request = function() uid = math.random(1, 10000000) path = \"/test?uid=\" .. uid return wrk.format(nil, path) end 在 request 方法中，随机生成 1~10000000 之间的 uid，并动态生成请求 URL. 每次请求前，延迟 10ms: function delay() return 10 end 请求的接口需要先进行认证，获取 token 后，才能发起请求，咋办？ token = nil path = \"/auth\" request = function() return wrk.format(\"GET\", path) end response = function(status, headers, body) if not token and status == 200 then token = headers[\"X-Token\"] path = \"/test\" wrk.headers[\"X-Token\"] = token end end 上面的脚本表示，在 token 为空的情况下，先请求 /auth 接口来认证，获取 token, 拿到 token 以后，将 token 放置到请求头中，再请求真正需要压测的 /test 接口。 压测支持 HTTP pipeline 的服务： init = function(args) local r = {} r[1] = wrk.format(nil, \"/?foo\") r[2] = wrk.format(nil, \"/?bar\") r[3] = wrk.format(nil, \"/?baz\") req = table.concat(r) end request = function() return req end 通过在 init 方法中将三个 HTTP请求拼接在一起，实现每次发送三个请求，以使用 HTTP pipeline。 参考 https://www.cnblogs.com/quanxiaoha/p/10661650.html https://yeqown.github.io/2018/01/16/wrk%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/benchmark-tools-sysbench.html":{"url":"origin/benchmark-tools-sysbench.html","title":"sysbench","keywords":"","body":"基准测试工具SysBench 一、简介 SysBench是一个模块化的、跨平台、多线程基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况。它主要包括以下几种方式的测试： cpu性能 磁盘io性能 调度程序性能 内存分配及传输速度 POSIX线程性能 数据库性能(OLTP基准测试) 目前sysbench主要支持 MySQL,pgsql,oracle 这3种数据库。 GitHub：https://github.com/akopytov/sysbench 二、安装 1、包管理器安装 Linux Debian/Ubuntu curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash sudo apt -y install sysbench RHEL/CentOS: curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash sudo yum -y install sysbench Fedora: curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash sudo dnf -y install sysbench Arch Linux: sudo pacman -Suy sysbench MacOS # Add --with-postgresql if you need PostgreSQL support brew install sysbench 2、源码安装 ①安装依赖 Debian/Ubuntu apt -y install make automake libtool pkg-config libaio-dev # For MySQL support apt -y install libmysqlclient-dev libssl-dev # For PostgreSQL support apt -y install libpq-dev RHEL/CentOS yum -y install make automake libtool pkgconfig libaio-devel # For MySQL support, replace with mysql-devel on RHEL/CentOS 5 yum -y install mariadb-devel openssl-devel # For PostgreSQL support yum -y install postgresql-devel Fedora dnf -y install make automake libtool pkgconfig libaio-devel # For MySQL support dnf -y install mariadb-devel openssl-devel # For PostgreSQL support dnf -y install postgresql-devel macOS Assuming you have Xcode (or Xcode Command Line Tools) and Homebrew installed: brew install automake libtool openssl pkg-config # For MySQL support brew install mysql # For PostgreSQL support brew install postgresql # openssl is not linked by Homebrew, this is to avoid \"ld: library not found for -lssl\" export LDFLAGS=-L/usr/local/opt/openssl/lib ②编译安装 ./autogen.sh # Add --with-pgsql to build with PostgreSQL support ./configure make -j make install # The above will build sysbench with MySQL support by default. If you have MySQL headers and libraries in non-standard locations (and no `mysql_config` can be found in the `PATH`), you can specify them explicitly with `--with-mysql-includes` and `--with-mysql-libs` options to `./configure`. # To compile sysbench without MySQL support, use `--without-mysql`. If no database drivers are available database-related scripts will not work, but other benchmarks will be functional. 3、Docker Dockerfile FROM debian:latest RUN sed -i 's/deb.debian.org/mirrors.aliyun.com/g' /etc/apt/sources.list \\ && apt update \\ && apt -y install curl \\ && curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | bash \\ && apt update \\ && apt -y install sysbench redis-tools dumb-init \\ && rm -rf /var/lib/apt/lists/* ENTRYPOINT [\"dumb-init\", \"--\"] Makefile REGISTRY = 192.168.1.60 IMAGE_REPO = tools IMAGE_NAME = mid-bf-tools IMAGE_VERSION = $(v) all: build push build: docker build --rm -f Dockerfile -t ${REGISTRY}/${IMAGE_REPO}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${REGISTRY}/${IMAGE_REPO}/${IMAGE_NAME}:${IMAGE_VERSION} Kubernetes apiVersion: v1 kind: Pod metadata: labels: app: sysbench name: sysbench spec: containers: - image: 192.168.1.60/tools/mid-bf-tools:v1 name: sysbench command: - sysbench - --db-driver=mysql - --report-interval=2 - --mysql-table-engine=innodb - --oltp-table-size=100000 - --oltp-tables-count=24 - --threads=64 - --time=99999 - --mysql-host=galera - --mysql-port=3306 - --mysql-user=sbtest - --mysql-password=password - /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua - run restartPolicy: Never 第三方的docker镜像：https://hub.docker.com/r/severalnines/sysbench 三、使用方法 命令语法 sysbench [选项]... [testname] [命令] 通用选项: --threads=N # 线程的数量，默认是1 --events=N # 限制的最大事件数量，默认是0，不限制 --time=N # 整个测试执行的时间 [10] --forced-shutdown=STRING number of seconds to wait after the --time limit before forcing shutdown, or 'off' to disable [off] --thread-stack-size=SIZE size of stack per thread [64K] --rate=N average transactions rate. 0 for unlimited rate [0] --report-interval=N periodically report intermediate statistics with a specified interval in seconds. 0 disables intermediate reports [0] --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. [] --debug[=on|off] print more debugging info [off] --validate[=on|off] perform validation checks where possible [off] --help[=on|off] print help and exit [off] --version[=on|off] print version and exit [off] --config-file=FILENAME File containing command line options --tx-rate=N deprecated alias for --rate [0] --max-requests=N deprecated alias for --events [0] --max-time=N deprecated alias for --time [0] --num-threads=N deprecated alias for --threads [1] Pseudo-Random Numbers Generator options: --rand-type=STRING random numbers distribution {uniform,gaussian,special,pareto} [special] --rand-spec-iter=N number of iterations used for numbers generation [12] --rand-spec-pct=N percentage of values to be treated as 'special' (for special distribution) [1] --rand-spec-res=N percentage of 'special' values to use (for special distribution) [75] --rand-seed=N seed for random number generator. When 0, the current time is used as a RNG seed. [0] --rand-pareto-h=N parameter h for pareto distribution [0.2] Log options: --verbosity=N verbosity level {5 - debug, 0 - only critical messages} [3] --percentile=N percentile to calculate in latency statistics (1-100). Use the special value of 0 to disable percentile calculations [95] --histogram[=on|off] print latency histogram in report [off] General database options: --db-driver=STRING specifies database driver to use ('help' to get list of available drivers) [mysql] --db-ps-mode=STRING prepared statements usage mode {auto, disable} [auto] --db-debug[=on|off] print database-specific debug information [off] Compiled-in database drivers: mysql - MySQL driver mysql options: --mysql-host=[LIST,...] MySQL server host [localhost] --mysql-port=[LIST,...] MySQL server port [3306] --mysql-socket=[LIST,...] MySQL socket --mysql-user=STRING MySQL user [sbtest] --mysql-password=STRING MySQL password [] --mysql-db=STRING MySQL database name [sbtest] --mysql-ssl[=on|off] use SSL connections, if available in the client library [off] --mysql-ssl-cipher=STRING use specific cipher for SSL connections [] --mysql-compression[=on|off] use compression, if available in the client library [off] --mysql-debug[=on|off] trace all client library calls [off] --mysql-ignore-errors=[LIST,...] list of errors to ignore, or \"all\" [1213,1020,1205] --mysql-dry-run[=on|off] Dry run, pretend that all MySQL client API calls are successful without executing them [off] Compiled-in tests: fileio - File I/O test cpu - CPU performance test memory - Memory functions speed test threads - Threads subsystem performance test mutex - Mutex performance test command command是sysbench要执行的命令，包括以下三种： prepare 为测试提前准备数据 run 执行正式的测试 cleanup 在测试完成后对数据库进行清理 testname 可以是内置的fileio，memory等，也可以是lua脚本或者是lua脚本的路径 testname 指定了要进行的测试，在老版本的sysbench中，可以通过--test参数指定测试的脚本； 而在新版本中，--test参数已经声明为废弃，可以不使用--test，而是直接指定脚本。测试时使用的脚本为lua脚本，可以使用sysbench自带脚本，也可以自己开发。 测试fileio命令帮助 sysbench --test=fileio help fileio options: --file-num=N 代表生成测试文件的数量，默认为128。 --file-block-size=N 测试时所使用文件块的大小，如果想磁盘针对innodb存储引擎进行测试，可以将其设置为16384， 即innodb存储引擎页的大小。默认为16384。 --file-total-size=SIZE 创建测试文件的总大小，默认为2G大小。 --file-test-mode=STRING 文件测试模式，包含：seqwr(顺序写), seqrewr(顺序读写), seqrd(顺序读), rndrd(随机读), rndwr(随机写), rndrw(随机读写)。 --file-io-mode=STRING 文件操作的模式，sync（同步）,async（异步）,fastmmap（快速mmap）,slowmmap（慢速mmap）， 默认为sync同步模式。 --file-extra-flags=[LIST,...] list of additional flags to use to open files {sync,dsync,direct} [] --file-fsync-freq=N 执行fsync()函数的频率。fsync主要是同步磁盘文件，因为可能有系统和磁盘缓冲的关系。 0代表不使用fsync函数。默认值为100。 --file-fsync-all[=on|off] 每执行完一次写操作，就执行一次fsync。默认为off。 --file-fsync-end[=on|off] 在测试结束时执行fsync函数。默认为on。 --file-fsync-mode=STRING 文件同步函数的选择，同样是和API相关的参数，由于多个操作系统对于fdatasync支持不同， 因此不建议使用fdatasync。默认为fsync。 --file-merged-requests=N 大多情况下，合并可能的IO的请求数，默认为0不合并。 --file-rw-ratio=N 测试时的读写比例，默认时为1.5，即可3：2。 测试memory命令帮助 sysbench --test=memory help memory options: --memory-block-size=SIZE 测试内存块的大小，默认为1K --memory-total-size=SIZE 数据传输的总大小，默认为100G --memory-scope=STRING 内存访问的范围，包括全局和本地范围，默认为global --memory-oper=STRING 内存操作的类型，包括read, write, none，默认为write --memory-access-mode=STRING 内存访问模式，包括seq,rnd两种模式，默认为seq 测试threads命令帮助 $ sysbench --test=threads help threads options: --thread-yields=N 指定每个请求的压力，默认为1000 --thread-locks=N 指定每个线程的锁数量，默认为8 测试mutex命令帮助 sysbench --test=mutex help mutex options: --mutex-num=N 数组互斥的总大小。默认是4096 --mutex-locks=N 每个线程互斥锁的数量。默认是50000 --mutex-loops=N 内部互斥锁的空循环数量。默认是10000 注意 sysbench 1.0版本以上，默认不支持sysbench --test=oltp，使用sysbench oltp_read_write代替 参考：https://github.com/akopytov/sysbench/issues/281 四、示例及结果解读 示例1、测试MySQL读写性能 准备数据 手动在数据库里面创建对应的DB sysbench oltp_read_write \\ --db-driver=mysql \\ --mysql-host=localhost \\ --mysql-db=sysbench \\ --mysql-user=root \\ --mysql-password \\ --rand-type=uniform \\ --table_size=1000000 --tables=2 --threads=1 --events=0 \\ prepare 以上命令会在sysbench库中创建两张包含一百万条记录的表sbtest1和sbtest2 开始测试 sysbench oltp_read_write \\ --threads=16 \\ --histogram=on \\ --report-interval=10 \\ --mysql-host=localhost \\ --mysql-db=sysbench \\ --mysql-user=root \\ --mysql-password= \\ run 测试结果解读 Running the test with following options: Number of threads: 16 Initializing random number generator from current time Initializing worker threads... Threads started! [ 10s ] thds: 16 tps: 1690.78 qps: 33849.48 (r/w/o: 23700.00/6765.32/3384.16) lat (ms,95%): 29.72 err/s: 1.00 reconn/s: 0.00 Latency histogram (values are in milliseconds) value ------------- distribution ------------- count 3.020 | 1 3.130 | 1 3.187 |* 9 3.245 |** 20 3.304 |** 24 3.364 |**** 46 3.425 |****** 75 3.488 |*********** 141 3.551 |**************** 208 3.615 |******************** 258 3.681 |************************ 310 3.748 |*************************** 345 3.816 |******************************* 398 3.885 |******************************** 414 3.956 |******************************** 412 4.028 |************************************ 459 4.101 |**************************************** 512 4.176 |************************************** 488 4.252 |************************************ 457 4.329 |************************************** 486 4.407 |************************************* 470 4.487 |******************************** 407 4.569 |********************************* 422 4.652 |********************************* 417 4.737 |******************************** 411 4.823 |***************************** 365 4.910 |**************************** 360 4.999 |************************* 324 5.090 |************************* 322 5.183 |************************** 327 5.277 |********************* 270 5.373 |*********************** 289 5.470 |****************** 230 5.570 |******************* 240 5.671 |***************** 218 5.774 |**************** 206 5.879 |*************** 190 5.986 |************** 174 6.095 |************ 154 6.205 |************ 156 6.318 |************ 149 6.433 |************* 160 6.550 |************ 155 6.669 |********** 125 6.790 |********** 126 6.913 |********* 115 7.039 |********* 119 7.167 |********* 118 7.297 |******** 100 7.430 |********* 114 7.565 |******** 96 7.702 |******* 84 7.842 |******* 93 7.985 |******* 88 8.130 |******* 88 8.277 |******* 87 8.428 |****** 71 8.581 |****** 71 8.737 |***** 68 8.895 |****** 79 9.057 |***** 58 9.222 |****** 73 9.389 |***** 63 9.560 |**** 49 9.734 |***** 70 9.910 |**** 51 10.090 |*** 42 10.274 |*** 41 10.460 |***** 58 10.651 |***** 64 10.844 |**** 51 11.041 |**** 51 11.242 |**** 53 11.446 |**** 48 11.654 |*** 44 11.866 |**** 55 12.081 |**** 57 12.301 |**** 55 12.524 |**** 52 12.752 |***** 58 12.984 |**** 50 13.219 |**** 48 13.460 |**** 46 13.704 |*** 43 13.953 |**** 54 14.207 |****** 77 14.465 |*** 43 14.728 |**** 56 14.995 |***** 62 15.268 |**** 57 15.545 |**** 53 15.828 |**** 51 16.115 |**** 52 16.408 |**** 51 16.706 |**** 47 17.010 |**** 46 17.319 |*** 33 17.633 |*** 44 17.954 |***** 66 18.280 |*** 37 18.612 |*** 41 18.950 |*** 44 19.295 |**** 46 19.645 |**** 45 20.002 |*** 44 20.366 |** 30 20.736 |*** 37 21.112 |*** 38 21.496 |*** 42 21.886 |*** 33 22.284 |*** 35 22.689 |*** 35 23.101 |** 28 23.521 |*** 35 23.948 |*** 35 24.384 |** 26 24.827 |** 25 25.278 |** 31 25.737 |** 30 # ....省略.... 87.564 | 6 89.155 |* 9 90.775 | 5 92.424 | 6 94.104 | 6 95.814 | 6 97.555 | 4 99.327 | 5 101.132 |* 7 102.969 | 2 104.840 | 6 106.745 | 5 108.685 | 2 110.659 | 4 112.670 | 3 114.717 |* 8 116.802 | 5 118.924 | 3 121.085 | 1 123.285 | 6 125.525 |* 7 127.805 | 4 130.128 | 1 132.492 | 1 SQL statistics: queries performed: read: 237118 # 总select数量 write: 67723 # 总update、insert、delete语句数量 other: 33864 # commit、unlock tables以及其他mutex的数量 total: 338705 transactions: 16927 (1690.41 per sec.) # TPS queries: 338705 (33824.71 per sec.) # QPS ignored errors: 10 (1.00 per sec.) # 忽略的错误 reconnects: 0 (0.00 per sec.) # 重新连接 General statistics: total time: 10.0122s # 测试的总时间 total number of events: 16927 # 总的事件数，一般和TPS一样 Latency (ms): min: 2.99 # 最小响应时间 avg: 9.46 # 平均响应时间 max: 132.49 # 最大响应时间 95th percentile: 29.72 # 95%的响应时间是这个数据 sum: 160046.87 Threads fairness: events (avg/stddev): 1057.9375/8.09 execution time (avg/stddev): 10.0029/0.00 清理测试数据 sysbench oltp_read_write \\ --db-driver=mysql \\ --mysql-host=localhost \\ --mysql-db=sysbench \\ --mysql-user=root \\ --mysql-password \\ cleanup Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-08-27 14:36:23 "},"origin/sysbench-mysql.html":{"url":"origin/sysbench-mysql.html","title":"MySQL性能测试之sysbench","keywords":"","body":"MySQL性能测试 一、性能指标 TPS ：Transactions Per Second ，即数据库每秒执行的事务数，以 commit 成功次数为准。 QPS ：Queries Per Second ，即数据库每秒执行的 SQL 数（含 insert、select、update、delete 等）。 RT ：Response Time ，响应时间。包括平均响应时间、最小响应时间、最大响应时间、每个响应时间的查询占比。比较需要重点关注的是，前 95-99% 的最大响应时间。因为它决定了大多数情况下的短板。 Concurrency Threads ：并发量，每秒可处理的查询请求的数量。 总结来说，实际就是 2 个维度： 吞吐量 延迟 二、测试工具 mysqlslap mysqlslap可以模拟服务器的负载，并输出计时信息。在MySQL 4.1或者更新的版本中都可以使用。测试时可以执行并发连接数，并指定SQL 语句（可以在命令行上执行，也可以把SQL 语句写入到参数文件中）。如果没有指定SQL 语句，mysqlslap 会自动生成查询schema 的SELECT 语句。 sysbench sysbench是一款多线程系统压测工具。它可以根据影响数据库服务器性能的各种因素来评估系统的性能。例如，可以用来测试文件I/O、操作系统调度器、内存分配和传输速度、POSIX 线程，以及数据库服务器等。sysbench 支持Lua 脚本语言，Lua 对于各种测试场景的设置可以非常灵活。sysbench 是我们非常喜欢的一种全能测试工具，支持MySQL、操作系统和硬件的硬件测试。 MySQL Benchmark Suite （sql-bench） 在MySQL 的发行包中也提供了一款自己的基准测试套件，可以用于在不同数据库服务器上进行比较测试。它是单线程的，主要用于测试服务器执行查询的速度。结果会显示哪种类型的操作在服务器上执行得更快。 　　这个测试套件的主要好处是包含了大量预定义的测试，容易使用，所以可以很轻松地用于比较不同存储引擎或者不同配置的性能测试。其也可以用于高层次测试，比较两个服务器的总体性能。当然也可以只执行预定义测试的子集（例如只测试UPDATE 的性能）。这些测试大部分是CPU 密集型的，但也有些短时间的测试需要大量的磁盘I/O 操作。 　　这个套件的最大缺点主要有：它是单用户模式的，测试的数据集很小且用户无法使用指定的数据，并且同一个测试多次运行的结果可能会相差很大。因为是单线程且串行执行的，所以无法测试多CPU 的能力，只能用于比较单CPU 服务器的性能差别。使用这个套件测试数据库服务器还需要Perl 和BDB 的支持，相关文档请参考. Super Smack Super Smack是一款用于MySQL 和PostgreSQL的基准测试工具，可以提供压力测试和负载生成。这是一个复杂而强大的工具，可以模拟多用户访问，可以加载测试数据到数据库，并支持使用随机数据填充测试表。测试定义在\"smack\"文件中，smack 文件使用一种简单的语法定义测试的客户端、表、查询等测试要素。 等等 三、sysbench测试实例 参考：sysbench 中的测试示例 四、mysqlslap测试实例 mysqlslap是一个mysql官方提供的压力测试工具。以下是比较重要的参数： –defaults-file，配置文件存放位置 –concurrency，并发数 –engines，引擎 –iterations，迭代的实验次数 –socket，socket文件位置 自动测试： –auto-generate-sql，自动产生测试SQL –auto-generate-sql-load-type，测试SQL的类型。类型有mixed，update，write，key，read。 –number-of-queries，执行的SQL总数量 –number-int-cols，表内int列的数量 –number-char-cols，表内char列的数量 –create-schema，指定数据库名称 –query，指定SQL语句，可以定位到某个包含SQL的文件 1、测试同时不同的存储引擎的性能进行对比 mysqlslap -uroot -p123456 \\ -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --engine=myisam,innodb 2、执行一次测试，分别50和100个并发，执行1000次总查询 mysqlslap -uroot -p123456 \\ -a --concurrency=50,100 --number-of-queries 1000 6 3、50和100个并发分别得到一次测试结果(Benchmark)，并发数越多，执行完所有查询的时间越长。为了准确起见，可以多迭代测试几次: mysqlslap -uroot -p123456 \\ -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-08-24 20:24:24 "},"origin/redis-benchmark.html":{"url":"origin/redis-benchmark.html","title":"Redis性能测试之redis-benchmark","keywords":"","body":"Redis性能测试工具：redis-benchmark 一、简介 基本命令语法如下： redis-benchmark [option] [option value] 可选参数如下所示： 选项 描述 默认值 -h 指定服务器主机名 127.0.0.1 -p 指定服务器端口 6379 -s 指定服务器 socket -c 指定并发连接数 50 -n 指定请求数 10000 -d 以字节的形式指定 SET/GET 值的数据大小 2 -k 1=keep alive 0=reconnect 1 -r SET/GET/INCR 使用随机 key, SADD 使用随机值 -P 通过管道传输请求 1 -q 强制退出 redis。仅显示 query/sec 值 --csv 以 CSV 格式输出 -l 生成循环，永久执行测试 -t 仅运行以逗号分隔的测试命令列表。 -I Idle 模式。仅打开 N 个 idle 连接并等待。 二、示例 1、测试所有命令 redis-benchmark -n 100000 -q PING_INLINE: 106044.54 requests per second PING_BULK: 104058.27 requests per second SET: 106382.98 requests per second GET: 106496.27 requests per second INCR: 109289.62 requests per second LPUSH: 100806.45 requests per second RPUSH: 105042.02 requests per second LPOP: 104602.52 requests per second RPOP: 88809.95 requests per second SADD: 103842.16 requests per second HSET: 94250.71 requests per second SPOP: 94517.96 requests per second LPUSH (needed to benchmark LRANGE): 93283.58 requests per second LRANGE_100 (first 100 elements): 26795.28 requests per second LRANGE_300 (first 300 elements): 11272.69 requests per second LRANGE_500 (first 450 elements): 7966.22 requests per second LRANGE_600 (first 600 elements): 6308.35 requests per second MSET (10 keys): 77101.00 requests per second 2、测试设置 10 万随机 key 连续 SET 100 万次 redis-benchmark -t set -r 100000 -n 1000000 ====== SET ====== 1000000 requests completed in 10.21 seconds 50 parallel clients # 模拟 50 个客户端 3 bytes payload keep alive: 1 99.38% 3、使用 pipelining进行测试 Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。 这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。 服务端处理命令，并将结果返回给客户端。 默认情况下，每个客户端都是在一个请求完成之后才发送下一个请求 服务器几乎是按顺序读取每个客户端的命令。Redis 支持 pipelining，一次请求/响应服务器能实现处理新的请求即使旧的请求还未被响应。这样就可以将多个命令发送到服务器，而不用等待回复，最后在一个步骤中读取该答复。使得可以一次性执行多条命令成为可能。 Redis pipelining 可以提高服务器的 TPS。 # 设置16 条命令使用 pipelining redis-benchmark -r 1000000 -n 2000000 -t get,set,lpush,lpop -P 16 -q SET: 472255.00 requests per second GET: 559753.69 requests per second LPUSH: 569638.31 requests per second LPOP: 571428.56 requests per second # 不使用pipelining redis-benchmark -r 1000000 -n 2000000 -t get,set,lpush,lpop -q SET: 86422.95 requests per second GET: 88409.52 requests per second LPUSH: 87680.84 requests per second LPOP: 80994.61 requests per second 4、使用Lua脚本测试 redis-benchmark -n 100000 -q script load \"redis.call('set','foo','bar')\" script load redis.call('set','foo','bar'): 105042.02 requests per second 三、影响Redis 性能的因素 1、网络带宽和延迟 网络带宽和延迟通常是最大短板。建议在基准测试之前使用 ping 来检查服务端到客户端的延迟。根据带宽，可以计算出最大吞吐量。 比如将 4 KB 的字符串塞入 Redis，吞吐量是 100000 q/s，那么实际需要 3.2 Gbits/s 的带宽，所以需要 10 GBits/s 网络连接， 1 Gbits/s 是不够的。 在很多线上服务中，Redis 吞吐会先被网络带宽限制住，而不是 CPU。 为了达到高吞吐量突破 TCP/IP 限制，最后采用 10 Gbits/s 的网卡， 或者多个 1 Gbits/s 网卡。 2、CPU 由于Redis是单线程模型，Redis 更喜欢大缓存快速 CPU， 而不是多核。 3、连接方式 如果服务器和客户端都运行在同一个机器上面，那么 TCP/IP loopback 和 unix domain sockets 都可以使用。对 Linux 来说，使用 unix socket 可以比 TCP/IP loopback 快 50%。 默认 redis-benchmark 是使用 TCP/IP loopback。 当大量使用 pipelining 时候，unix domain sockets 的优势就不那么明显了。 4、部署环境 Redis 在 VM 上会变慢。虚拟化对普通操作会有额外的消耗，Redis 对系统调用和网络终端不会有太多的 overhead。建议把 Redis 运行在物理机器上， 特别是当你很在意延迟时候。在最先进的虚拟化设备（VMWare）上面，redis-benchmark 的测试结果比物理机器上慢了一倍，很多 CPU 时间被消费在系统调用和中断上面。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-08 10:58:50 "},"origin/kafka-perf.html":{"url":"origin/kafka-perf.html","title":"Kafka性能测试","keywords":"","body":"kafka性能测试 一、简介 性能测试工具是 Kafka 本身提供的用于生产者性能测试kafka-producer\u0002-perf-test.sh和用于消费者性能测试的kafka-consumer-perf-test.sh kafka-producer\u0002-perf-test.sh命令详解 usage: producer-performance \\ [-h] \\ --topic TOPIC \\ --num-records NUM-RECORDS \\ [--payload-delimiter PAYLOAD-DELIMITER] \\ --throughput THROUGHPUT \\ [--producer-props PROP-NAME=PROP-VALUE [PROP-NAME=PROP-VALUE ...]] \\ [--producer.config CONFIG-FILE] [--print-metrics] \\ [--transactional-id TRANSACTIONAL-ID] [--transaction-duration-ms TRANSACTION-DURATION] (--record-size RECORD-SIZE | --payload-file PAYLOAD-FILE) This tool is used to verify the producer performance. optional arguments: -h, --help show this help message and exit --topic TOPIC produce messages to this topic --num-records NUM-RECORDS number of messages to produce --payload-delimiter PAYLOAD-DELIMITER provides delimiter to be used when --payload-file is provided. Defaults to new line. Note that this parameter will be ignored if --payload-file is not provided. (default: \\n) --throughput THROUGHPUT throttle maximum message throughput to *approximately* THROUGHPUT messages/sec. Set this to -1 to disable throttling. --producer-props PROP-NAME=PROP-VALUE [PROP-NAME=PROP-VALUE ...] kafka producer related configuration properties like bootstrap.servers,client.id etc. These configs take precedence over those passed via --producer.config. --producer.config CONFIG-FILE producer config properties file. --print-metrics print out metrics at the end of the test. (default: false) --transactional-id TRANSACTIONAL-ID The transactionalId to use if transaction-duration-ms is > 0. Useful when testing the performance of concurrent transactions. (default: performance-producer-default-transactional-id) --transaction-duration-ms TRANSACTION-DURATION The max age of each transaction. The commitTransaction will be called after this time has elapsed. Transactions are only enabled if this value is positive. (default: 0) either --record-size or --payload-file must be specified but not both. --record-size RECORD-SIZE message size in bytes. Note that you must provide exactly one of --record-size or --payload-file. --payload-file PAYLOAD-FILE file to read the message payloads from. This works only for UTF-8 encoded text files. Payloads will be read from this file and a payload will be randomly selected when sending messages. Note that you must provide exactly one of --record-size or --payload-file. kafka-consumer-perf-test.sh命令详解 This tool helps in performance test for the full zookeeper consumer Option Description ------ ----------- --bootstrap-server (deprecated) is specified. The server (s) to connect to. --broker-list DEPRECATED, use --bootstrap-server instead; ignored if --bootstrap- server is specified. The broker list string in the form HOST1:PORT1, HOST2:PORT2. --consumer.config Consumer config properties file. --date-format The date format to use for formatting the time field. See java.text. SimpleDateFormat for options. (default: yyyy-MM-dd HH:mm:ss:SSS) --fetch-size The amount of data to fetch in a single request. (default: 1048576) --from-latest If the consumer does not already have an established offset to consume from, start with the latest message present in the log rather than the earliest message. --group The group id to consume on. (default: perf-consumer-50295) --help Print usage information. --hide-header If set, skips printing the header for the stats --messages REQUIRED: The number of messages to send or consume --num-fetch-threads Number of fetcher threads. (default: 1) --print-metrics Print out the metrics. --reporting-interval print progress info. (default: 5000) --show-detailed-stats If set, stats are reported for each reporting interval as configured by reporting-interval --socket-buffer-size The size of the tcp RECV size. (default: 2097152) --threads Number of processing threads. (default: 10) --timeout [Long: milliseconds] The maximum allowed time in milliseconds between returned records. (default: 10000) --topic REQUIRED: The topic to consume from. --version Display Kafka version. 二、生产者写入测试 向一个只有1个分区和1个副本的Topic主题perf-producer-test 中发送 100 万条消息，并且每条消息大小为 1024B 生产者，对应的 acks 参数为1 kafka-producer-perf-test.sh \\ --topic perf-producer-test \\ --num-records 1000000 \\ --record-size 1024 \\ --throughput -1 \\ --print-metrics \\ --producer-props bootstrap.servers=localhost:9092 acks=1 # --nurn-records：用来指定发送消息的总条数 # --record-size：用来设置每条消息的字节数 # --throughput：用来进行限流控制，当设定的值小于0不限流，当设定的值大于0时，当发送的吞吐量大于该值时就会被阻塞一段时间。 # --print-metrics：测试完成后的Metrics信息 # --producer-props：参数用来指定生产者的配置，可同时指定多组配置，各组配置之间以空格分隔，与producer-props 参数对应的还有一个producer-config参数，它用来指定生产者的配置文件 测试结果 67164 records sent, 13430.1 records/sec (13.12 MB/sec), 1461.5 ms avg latency, 2014.0 ms max latency. 92910 records sent, 18430.9 records/sec (18.00 MB/sec), 1718.4 ms avg latency, 2038.0 ms max latency. 120060 records sent, 23741.3 records/sec (23.18 MB/sec), 1304.2 ms avg latency, 1688.0 ms max latency. 110055 records sent, 21853.7 records/sec (21.34 MB/sec), 1428.9 ms avg latency, 1660.0 ms max latency. 119745 records sent, 23949.0 records/sec (23.39 MB/sec), 1281.9 ms avg latency, 1692.0 ms max latency. 110910 records sent, 22182.0 records/sec (21.66 MB/sec), 1383.2 ms avg latency, 1516.0 ms max latency. 135255 records sent, 27051.0 records/sec (26.42 MB/sec), 1191.1 ms avg latency, 1580.0 ms max latency. 122490 records sent, 24498.0 records/sec (23.92 MB/sec), 1214.0 ms avg latency, 1527.0 ms max latency. 1000000 records sent, 22281.639929 records/sec (21.76 MB/sec), 1332.82 ms avg latency, 2038.00 ms max latency, 1357 ms 50th, 1822 ms 95th, 1955 ms 99th, 2010 ms 99.9th. # records sent：表示测试时发送的消息总数 # records sec：表示以每秒发迭的消息数来统计吞吐量，括号中 MB/sec 表示以每秒发送的消息大小来统计吞吐量， # avg latency: 表示消息处理的平均耗时 # max latency：表示消息处理的最大耗时 # 50th,95th,99t,99.9th： 分别表示 50% 95 99% 99.9% 的消息处理耗时。 # 以下为测试完成后的Metrics信息 Metric Name Value app-info:commit-id:{client-id=producer-1} : 66563e712b0b9f84 app-info:start-time-ms:{client-id=producer-1} : 1608628521735 app-info:version:{client-id=producer-1} : 2.5.0 kafka-metrics-count:count:{client-id=producer-1} : 102.000 producer-metrics:batch-size-avg:{client-id=producer-1} : 15555.699 producer-metrics:batch-size-max:{client-id=producer-1} : 15570.000 producer-metrics:batch-split-rate:{client-id=producer-1} : 0.000 producer-metrics:batch-split-total:{client-id=producer-1} : 0.000 producer-metrics:buffer-available-bytes:{client-id=producer-1} : 33554432.000 producer-metrics:buffer-exhausted-rate:{client-id=producer-1} : 0.000 producer-metrics:buffer-exhausted-total:{client-id=producer-1} : 0.000 producer-metrics:buffer-total-bytes:{client-id=producer-1} : 33554432.000 producer-metrics:bufferpool-wait-ratio:{client-id=producer-1} : 0.861 producer-metrics:bufferpool-wait-time-total:{client-id=producer-1} : 37854962935.000 producer-metrics:compression-rate-avg:{client-id=producer-1} : 1.000 producer-metrics:connection-close-rate:{client-id=producer-1} : 0.000 producer-metrics:connection-close-total:{client-id=producer-1} : 0.000 producer-metrics:connection-count:{client-id=producer-1} : 2.000 producer-metrics:connection-creation-rate:{client-id=producer-1} : 0.044 producer-metrics:connection-creation-total:{client-id=producer-1} : 2.000 producer-metrics:failed-authentication-rate:{client-id=producer-1} : 0.000 producer-metrics:failed-authentication-total:{client-id=producer-1} : 0.000 producer-metrics:failed-reauthentication-rate:{client-id=producer-1} : 0.000 producer-metrics:failed-reauthentication-total:{client-id=producer-1} : 0.000 producer-metrics:incoming-byte-rate:{client-id=producer-1} : 114570.421 producer-metrics:incoming-byte-total:{client-id=producer-1} : 5134588.000 producer-metrics:io-ratio:{client-id=producer-1} : 0.224 producer-metrics:io-time-ns-avg:{client-id=producer-1} : 70608.839 producer-metrics:io-wait-ratio:{client-id=producer-1} : 0.561 producer-metrics:io-wait-time-ns-avg:{client-id=producer-1} : 177047.069 producer-metrics:io-waittime-total:{client-id=producer-1} : 25274885534.000 producer-metrics:iotime-total:{client-id=producer-1} : 10079976609.000 producer-metrics:metadata-age:{client-id=producer-1} : 44.621 producer-metrics:network-io-rate:{client-id=producer-1} : 2975.167 producer-metrics:network-io-total:{client-id=producer-1} : 133344.000 producer-metrics:outgoing-byte-rate:{client-id=producer-1} : 23246321.000 producer-metrics:outgoing-byte-total:{client-id=producer-1} : 1041667644.000 producer-metrics:produce-throttle-time-avg:{client-id=producer-1} : 0.000 producer-metrics:produce-throttle-time-max:{client-id=producer-1} : 0.000 producer-metrics:reauthentication-latency-avg:{client-id=producer-1} : NaN producer-metrics:reauthentication-latency-max:{client-id=producer-1} : NaN producer-metrics:record-error-rate:{client-id=producer-1} : 0.000 producer-metrics:record-error-total:{client-id=producer-1} : 0.000 producer-metrics:record-queue-time-avg:{client-id=producer-1} : 1329.508 producer-metrics:record-queue-time-max:{client-id=producer-1} : 2034.000 producer-metrics:record-retry-rate:{client-id=producer-1} : 0.000 producer-metrics:record-retry-total:{client-id=producer-1} : 0.000 producer-metrics:record-send-rate:{client-id=producer-1} : 22429.068 producer-metrics:record-send-total:{client-id=producer-1} : 1000000.000 producer-metrics:record-size-avg:{client-id=producer-1} : 1110.000 producer-metrics:record-size-max:{client-id=producer-1} : 1110.000 producer-metrics:records-per-request-avg:{client-id=producer-1} : 15.000 producer-metrics:request-latency-avg:{client-id=producer-1} : 3.289 producer-metrics:request-latency-max:{client-id=producer-1} : 305.000 producer-metrics:request-rate:{client-id=producer-1} : 1487.617 producer-metrics:request-size-avg:{client-id=producer-1} : 15623.765 producer-metrics:request-size-max:{client-id=producer-1} : 15639.000 producer-metrics:request-total:{client-id=producer-1} : 66672.000 producer-metrics:requests-in-flight:{client-id=producer-1} : 0.000 producer-metrics:response-rate:{client-id=producer-1} : 1487.650 producer-metrics:response-total:{client-id=producer-1} : 66672.000 producer-metrics:select-rate:{client-id=producer-1} : 3169.020 producer-metrics:select-total:{client-id=producer-1} : 142758.000 producer-metrics:successful-authentication-no-reauth-total:{client-id=producer-1} : 0.000 producer-metrics:successful-authentication-rate:{client-id=producer-1} : 0.000 producer-metrics:successful-authentication-total:{client-id=producer-1} : 0.000 producer-metrics:successful-reauthentication-rate:{client-id=producer-1} : 0.000 producer-metrics:successful-reauthentication-total:{client-id=producer-1} : 0.000 producer-metrics:waiting-threads:{client-id=producer-1} : 0.000 producer-node-metrics:incoming-byte-rate:{client-id=producer-1, node-id=node--1} : 12.562 producer-node-metrics:incoming-byte-rate:{client-id=producer-1, node-id=node-2} : 115053.336 producer-node-metrics:incoming-byte-total:{client-id=producer-1, node-id=node--1} : 563.000 producer-node-metrics:incoming-byte-total:{client-id=producer-1, node-id=node-2} : 5134025.000 producer-node-metrics:outgoing-byte-rate:{client-id=producer-1, node-id=node--1} : 2.254 producer-node-metrics:outgoing-byte-rate:{client-id=producer-1, node-id=node-2} : 23345828.974 producer-node-metrics:outgoing-byte-total:{client-id=producer-1, node-id=node--1} : 101.000 producer-node-metrics:outgoing-byte-total:{client-id=producer-1, node-id=node-2} : 1041667543.000 producer-node-metrics:request-latency-avg:{client-id=producer-1, node-id=node--1} : NaN producer-node-metrics:request-latency-avg:{client-id=producer-1, node-id=node-2} : 3.289 producer-node-metrics:request-latency-max:{client-id=producer-1, node-id=node--1} : NaN producer-node-metrics:request-latency-max:{client-id=producer-1, node-id=node-2} : 305.000 producer-node-metrics:request-rate:{client-id=producer-1, node-id=node--1} : 0.045 producer-node-metrics:request-rate:{client-id=producer-1, node-id=node-2} : 1494.039 producer-node-metrics:request-size-avg:{client-id=producer-1, node-id=node--1} : 50.500 producer-node-metrics:request-size-avg:{client-id=producer-1, node-id=node-2} : 15624.232 producer-node-metrics:request-size-max:{client-id=producer-1, node-id=node--1} : 51.000 producer-node-metrics:request-size-max:{client-id=producer-1, node-id=node-2} : 15639.000 producer-node-metrics:request-total:{client-id=producer-1, node-id=node--1} : 2.000 producer-node-metrics:request-total:{client-id=producer-1, node-id=node-2} : 66670.000 producer-node-metrics:response-rate:{client-id=producer-1, node-id=node--1} : 0.045 producer-node-metrics:response-rate:{client-id=producer-1, node-id=node-2} : 1494.073 producer-node-metrics:response-total:{client-id=producer-1, node-id=node--1} : 2.000 producer-node-metrics:response-total:{client-id=producer-1, node-id=node-2} : 66670.000 producer-topic-metrics:byte-rate:{client-id=producer-1, topic=perf-producer-test2} : 23259932.490 producer-topic-metrics:byte-total:{client-id=producer-1, topic=perf-producer-test2} : 1037067350.000 producer-topic-metrics:compression-rate:{client-id=producer-1, topic=perf-producer-test2} : 1.000 producer-topic-metrics:record-error-rate:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-error-total:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-retry-rate:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-retry-total:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-send-rate:{client-id=producer-1, topic=perf-producer-test2} : 22428.565 producer-topic-metrics:record-send-total:{client-id=producer-1, topic=perf-producer-test2} : 1000000.000 三、消费者性能测试 消费主题perf-producer-test中的 100 万条消息 kafka-consumer-perf-test.sh \\ --topic perf-producer-test \\ --messages 1000000 \\ --print-metrics \\ --broker-list localhost:9092 测试结果 | start.time | end.time | data.consumed.in.MB | MB.sec | data.consumed.in.nMsg | nMsg.sec | rebalance.time.ms | fetch.time.ms | fetch.MB.sec | fetch.nMsg.sec ================================================================================================== | 2020-12-22 09:28:31:384 | 2020-12-22 09:28:47:769 | 976.5625 | 59.6010 | 1000000 | 61031.4312 | 1608629311854 | -1608629295469 | -0.0000 | -0.0006 # start.time: 起始运行时间 # end.time: 结束运行时 # data.consumed.in.MB: 消息总量（ 单位为 MB ） # MB.sec: 按字节大小计算的消费吞吐量（ MB单位为 MB ）、 # data.consumed.in.nMsg :消费的消息总数 # nMsg.sec: 按消息个数计算的吞吐量 # rebalance.time.ms：再平衡的时间,单位为ms # fetch.time.ms：拉取消息的持续时间，单位为ms # fetch.MB.sec: 每秒拉取消息的字节大小，单位MB # fetch.nMsg.sec: 每秒拉取消息的个数 ,其中 fetch.time. s= end.time - start.time - rebalance.time.ms a Metric Name Value consumer-coordinator-metrics:assigned-partitions:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:commit-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 11.750 consumer-coordinator-metrics:commit-latency-max:{client-id=consumer-perf-consumer-89119-1} : 30.000 consumer-coordinator-metrics:commit-rate:{client-id=consumer-perf-consumer-89119-1} : 0.097 consumer-coordinator-metrics:commit-total:{client-id=consumer-perf-consumer-89119-1} : 4.000 consumer-coordinator-metrics:failed-rebalance-rate-per-hour:{client-id=consumer-perf-consumer-89119-1} : 77.983 consumer-coordinator-metrics:failed-rebalance-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:heartbeat-rate:{client-id=consumer-perf-consumer-89119-1} : 0.116 consumer-coordinator-metrics:heartbeat-response-time-max:{client-id=consumer-perf-consumer-89119-1} : 9.000 consumer-coordinator-metrics:heartbeat-total:{client-id=consumer-perf-consumer-89119-1} : 5.000 consumer-coordinator-metrics:join-rate:{client-id=consumer-perf-consumer-89119-1} : 0.022 consumer-coordinator-metrics:join-time-avg:{client-id=consumer-perf-consumer-89119-1} : 32.000 consumer-coordinator-metrics:join-time-max:{client-id=consumer-perf-consumer-89119-1} : 32.000 consumer-coordinator-metrics:join-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:last-heartbeat-seconds-ago:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:last-rebalance-seconds-ago:{client-id=consumer-perf-consumer-89119-1} : 16.000 consumer-coordinator-metrics:partition-assigned-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:partition-assigned-latency-max:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:partition-lost-latency-avg:{client-id=consumer-perf-consumer-89119-1} : NaN consumer-coordinator-metrics:partition-lost-latency-max:{client-id=consumer-perf-consumer-89119-1} : NaN consumer-coordinator-metrics:partition-revoked-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:partition-revoked-latency-max:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:rebalance-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 101.000 consumer-coordinator-metrics:rebalance-latency-max:{client-id=consumer-perf-consumer-89119-1} : 101.000 consumer-coordinator-metrics:rebalance-latency-total:{client-id=consumer-perf-consumer-89119-1} : 101.000 consumer-coordinator-metrics:rebalance-rate-per-hour:{client-id=consumer-perf-consumer-89119-1} : 78.086 consumer-coordinator-metrics:rebalance-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:sync-rate:{client-id=consumer-perf-consumer-89119-1} : 0.022 consumer-coordinator-metrics:sync-time-avg:{client-id=consumer-perf-consumer-89119-1} : 24.000 consumer-coordinator-metrics:sync-time-max:{client-id=consumer-perf-consumer-89119-1} : 24.000 consumer-coordinator-metrics:sync-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-fetch-manager-metrics:bytes-consumed-rate:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2} : 22454094.6 consumer-fetch-manager-metrics:bytes-consumed-rate:{client-id=consumer-perf-consumer-89119-1} : 22453606.095 consumer-fetch-manager-metrics:bytes-consumed-total:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2}：100602.000 consumer-fetch-manager-metrics:bytes-consumed-total:{client-id=consumer-perf-consumer-89119-1} : 1033000602.000 consumer-fetch-manager-metrics:fetch-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 13.353 consumer-fetch-manager-metrics:fetch-latency-max:{client-id=consumer-perf-consumer-89119-1} : 288.000 consumer-fetch-manager-metrics:fetch-rate:{client-id=consumer-perf-consumer-89119-1} : 21.633 consumer-fetch-manager-metrics:fetch-size-avg:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2} : 1037149.199 consumer-fetch-manager-metrics:fetch-size-avg:{client-id=consumer-perf-consumer-89119-1} : 1037149.199 consumer-fetch-manager-metrics:fetch-size-max:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2} : 1038179.000 consumer-fetch-manager-metrics:fetch-size-max:{client-id=consumer-perf-consumer-89119-1} : 1038179.000 consumer-fetch-manager-metrics:fetch-throttle-time-avg:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-fetch-manager-metrics:fetch-throttle-time-max:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-fetch-manager-metrics:fetch-total:{client-id=consumer-perf-consumer-89119-1} : 996.000 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 20:12:49 "},"origin/pxe-kickstart无人值守部署OS.html":{"url":"origin/pxe-kickstart无人值守部署OS.html","title":"PXE-Kickstart无人值守部署OS","keywords":"","body":"一、PXE Kickstart网络引导无人值守部署主机OS 1. PXE简介 PXE(Pre-boot Execution Environment，预启动执行环境)是由Intel公司开发的最新技术，工作于Client/Server的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持通过网络启动作系统，在启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。 严格来说，PXE 并不是一种安装方式，而是一种引导方式。进行 PXE 安装的必要条件是在要安装的计算机中必须包含一个 PXE 支持的网卡（NIC），即网卡中必须要有 PXE Client。PXE 协议可以使算机通过网络启动。此协议分为 Client端和 Server 端，而PXE Client则在网卡的 ROM 中。当计算机引导时，BIOS 把 PXE Client 调入内存中执行，然后由 PXE Client 将放置在远端的文件通过网络下载到本地运行。运行 PXE 协议需要设置 DHCP 服务器和 TFTP 服务器。DHCP 服务器会给 PXE Client（将要安装系统的主机）分配一个 IP 地址，由于是给 PXE Client 分配 IP 地址，所以在配置 DHCP 服务器时需要增加相应的 PXE 设置。此外，在 PXE Client 的 ROM 中，已经存在了 TFTP Client，那么它就可以通过 TFTP 协议到 TFTP Server 上下载所需的文件了。 2. PXE工作流程 ① PXE Client 从自己的PXE网卡启动，向本网络中的DHCP服务器索取IP ② DHCP 服务器返回分配给客户机的IP 以及PXE文件的放置位置(该文件一般是放在一台TFTP服务器上) ③ PXE Client 向本网络中的TFTP服务器索取pxelinux.0 文件 ④ PXE Client 取得pxelinux.0 文件后之执行该文件 ⑤ 根据pxelinux.0 的执行结果，通过TFTP服务器加载内核和文件系统 ⑥ 进入安装画面, 此时可以通过选择HTTP、FTP、NFS 方式之一进行安装 详细工作流程，请参考下面这幅图： 3. Kickstart简介 Kickstart是一种无人值守的安装方式。它的工作原理是在安装过程中记录典型的需要人工干预填写的各种参数，并生成一个名为ks.cfg的文件。如果在安装过程中（不只局限于生成Kickstart安装文件的机器）出现要填写参数的情况，安装程序首先会去查找Kickstart生成的文件，如果找到合适的参数，就采用所找到的参数；如果没有找到合适的参数，便需要安装者手工干预了。所以，如果Kickstart文件涵盖了安装过程中可能出现的所有需要填写的参数，那么安装者完全可以只告诉安装程序从何处取ks.cfg文件，然后就去忙自己的事情。等安装完毕，安装程序会根据ks.cfg中的设置重启系统，并结束安装。 二、PXE+Kickstart无人值守安装OS的工作流程 三、PXE服务端配置 Prerequisite PXE主机： 主机名 IP地址 OS 路由器 pk.tools.curiouser.com 192.168.1.80 CentOS 7.5.1804 192.168.1.1 1、基础准备 上传ISO文件并挂载，关闭Firewall和SELinux yum install -y wget && \\ mkdir -p /mnt/{cdrom,iso/CentOS7.5.1804} && \\ wget http://vault.centos.org/7.5.1804/isos/x86_64/CentOS-7-x86_64-Minimal-1804.iso /mnt/iso && \\ echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom/CentOS7.5.1804 iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld 2、配置HTTD服务 安装服务 yum install -y httpd 配置服务 ln -s /mnt/cdrom/CentOS7.5.1804 /var/www/html/CentOS7.5.1804 启动验证服务，服务端口tcp:80 systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 3、配置DHCP服务 安装服务 yum install -y dhcp 配置服务/etc/dhcp/dhcpd.conf default-lease-time 600; max-lease-time 7200; log-facility local7; subnet 192.168.1.0 netmask 255.255.255.0 { option routers 192.168.1.1; # 给 client 的默认网关 option subnet-mask 255.255.255.0; # 给 client 的子网掩码 option domain-name \"curiouser.com\"; # 给 client 的搜索域 option domain-name-servers 192.168.1.1; # 给 client 的域名服务器 range dynamic-bootp 192.168.1.100 192.168.1.120; # 可供分配的IP范围 default-lease-time 21600; max-lease-time 43200; next-server 192.168.1.80; # TFTP Server 的IP地址 filename \"pxelinux.0\"; # pxelinux启动文件位置; } 启动验证服务,服务端口为67 systemctl start dhcpd && \\ systemctl enable dhcpd && \\ systemctl status dhcpd && \\ ss -nulp | grep dhcpd 4、配置TFTP服务 安装服务 yum install -y tftp-server tftp xinetd net-tools 配置服务/etc/xinetd.d/tftp service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /var/lib/tftpboot #默认disable是yes的，把它改为no即可 disable = no per_source = 11 cps = 100 2 flags = IPv4 } 启动验证服务,服务端口为UDP:69 systemctl start xinetd && \\ systemctl enable xinetd && \\ systemctl status xinetd && \\ ss -unlp | grep 69 && \\ netstat -a | grep tftp && \\ netstat -tunap | grep :69 5、准备相关文件 安装服务 yum install -y syslinux tree 拷贝文件 cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ && \\ cp /mnt/cdrom/CentOS7.5.1804/images/pxeboot/vmlinuz /var/lib/tftpboot/vmlinuz-7.5 && \\ cp /mnt/cdrom/CentOS7.5.1804/images/pxeboot/initrd.img /var/lib/tftpboot/initrd-7.5.img && \\ cp /mnt/cdrom/CentOS7.5.1804/isolinux/{vesamenu.c32,boot.msg,splash.png} /var/lib/tftpboot/ && \\ cp /usr/share/syslinux/{chain.c32,mboot.c32,menu.c32,memdisk} /var/lib/tftpboot/ && \\ mkdir /var/lib/tftpboot/pxelinux.cfg /var/lib/tftpboot/目录结构 tree -phL 2 /var/lib/tftpboot/ ├── [-rw-r--r-- 84] boot.msg # 窗口提示信息文件,提示信息在菜单出现前出现，显示时间较短，可以添加些艺术字之类的信息。 ├── [-rw-r--r-- 20K] chain.c32 ├── [-rw-r--r-- 50M] initrd-7.5.img # 这是一个初始化文件，一个最小的系统镜像 ├── [-rw-r--r-- 33K] mboot.c32 ├── [-rw-r--r-- 26K] memdisk ├── [-rw-r--r-- 54K] menu.c32 # 系统自带的两种图形模块之一，不能自定义背景图片 ├── [-rw-r--r-- 26K] pxelinux.0 ├── [drwxr-xr-x 21] pxelinux.cfg # 启动菜单目录 ├── [-rw-r--r-- 186] splash.png # 背景图片 ├── [-rw-r--r-- 149K] vesamenu.c32 # 系统自带的两种图形模块之一 └── [-rwxr-xr-x 5.9M] vmlinuz-7.5 # CentOS 7.5.1804的内核文件 创建/var/lib/tftpboot/pxelinux.cfg/default （default文件参数详见：PXE引导配置文件参数详解） bash -c 'cat >/var/lib/tftpboot/pxelinux.cfg/default Install CentOS 7.5.1804 x86_64 kernel vmlinuz-7.5 append initrd=initrd-7.5.img text ks=http://192.168.1.80/CentOS7.5.1804.cfg label CentOS7.7.1908 menu label ^2> Install CentOS 7.7.1908 x86_64 menu default kernel vmlinuz-7.7 append initrd=initrd-7.7.img text ks=http://192.168.1.80/CentOS7.7.1908.cfg EOF' 6、创建KS文件 /var/www/html/CentOS7.5.1804.cfg 方式一：手动编写(KS文件具体参数详情见笔记：Kickstart文件参数详解) install text lang en_US.UTF-8 keyboard us auth --useshadow --passalgo=sha512 url --url=\"http://192.168.1.80/CentOS7.5.1804\" rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ selinux --disabled firewall --disabled network --bootproto=dhcp --device=ens192 --ipv6=auto --activate network --hostname=test reboot timezone Asia/Shanghai --isUtc --nontp bootloader --location=mbr --boot-drive=sda clearpart --all --drives=sda services --enabled=NetworkManager,sshd firstboot --enable ignoredisk --only-use=sda #(可选)autopart --type=lvm --fstype=xfs part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 %packages @^minimal @core %end %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') echo $ipaddr $HOSTNAME >> /etc/hosts echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 方式二、使用system-config-kickstart图形化界面配置 安装：system-config-kickstart yum install -y system-config-kickstart 7、验证KS文件的语法正确性 yum install -y pykickstart ksvalidator /var/www/html/CentOS7.5.1804.cfg 8、(可选)自动安装配置脚本 前提： CentOS-7-x86_64-Everything-1804.iso已经放置在/mnt/iso文件夹下 pxe-kickstart-CentOS7.cfg mkdir /mnt/cdrom && \\ echo \"/mnt/iso/CentOS-7-x86_64-Everything-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld && \\ yum install -y httpd dhcp tftp-server tftp xinetd net-tools syslinux tree && \\ ln -s /mnt/cdrom/ /var/www/html/CentOS7 && \\ systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 && \\ bash -c 'cat >> /etc/dhcp/dhcpd.conf /etc/xinetd.d/tftp /var/lib/tftpboot/pxelinux.cfg/default 参考 https://blog.csdn.net/yanghua1012/article/details/80426659 https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/installation_guide/ch-boot-x86#sn-boot-menu-x86 http://www.178linux.com/99307 https://blog.51cto.com/lzhnb/2117618 https://marclop.svbtle.com/creating-an-automated-centos-7-install-via-kickstart-file https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-file-create https://www.cnblogs.com/cloudos/p/8143929.html http://bbs.51cto.com/thread-621450-1.html https://wiki.centos.org/zh/HowTos/PXE/PXE_Setup/Menus Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/pxe-kickstart文件参数详解.html":{"url":"origin/pxe-kickstart文件参数详解.html","title":"Kickstart文件参数详解","keywords":"","body":"一、简介 kickstart文件中各部分(section)要遵循一定的顺序。每个部分中的项(Item)并不需要按照一定的顺序排列，除非有其他要求。各部分的顺序如下： 命令部分 %packages部分 %pre, %post, 以及%traceback部分 -- 这些部分的顺序可以任意排列 %packages, %pre, %post以及%traceback部分需要以%end结束。 不要求的项(Item)可以被省略。 省略任何一个被要求的项将会导致安装程序向用户询问相关的问题，就像典型安装过程向用户询问那样。一旦用户给出了答案，安装过程将会继续自动进行，除非又遇到缺失的项。 以(#)开头的行作为注释行被忽略。 如果在kickstart安装中使用了不推荐的命令、选项或者语法，警告日志将会被记录到anaconda日志中。因为在一个或者两个发行版之间这些不推荐的项经常会被删掉，所以检查安装日志以确保没有使用这些项非常必要。当使用ksvalidator的时候，这些不推荐的项会导致错误。 如果选项后接等号（=），则必须指定一个值 引用磁盘的特殊说明： Kickstart一直通过设备节点名(例如 sda)来引用磁盘。Linux内核采用了更加动态的方法，设备名并不会在重启时保持不变。因此，这会使得在Kickstart脚本中引用磁盘变得复杂。为了满足稳定的设备命名，你可以在项(Item)中使用/dev/disk代替设备名。例如，你可以使用： part / --fstype=ext4 --onpart=/dev/disk/by-path/pci-0000:00:05.0-scsi-0:0:0:0-part1 part / --fstype=ext4 --onpart=/dev/disk/by-id/ata-ST3160815AS_6RA0C882-part1 来代替： part / --fstype=ext4 --onpart=sda1 这种方式提供了对磁盘的持久引用，因而比仅仅使用sda更加有意义。 这在大的存储环境中特别有意义。你也可以使用类似于shell的入口来应用磁盘。这种方式主要用来简化大的存储环境中clearpart以及ignoredisk命令的使用。例如，为了替代： ignoredisk --drives=sdaa,sdab,sdac 你可以使用如下的入口： ignoredisk --drives=/dev/disk/by-path/pci-0000:00:05.0-scsi-* 最后，如果想要在任何地方引用已经存在的分区或者文件系统（例如，在part --ondisk=中），你可以通过文件系统标签(label)或者UUID来进行。例如： part /data --ondisk=LABEL=data part /misc --ondisk=UUID=819ff6de-0bd6-4bf4-8b72-dbe41033a85b 二、必需选项 bootloader ：指明引导程序(bootloader)如何被安装。引申Bootloader相关概念：CentOS系统启动流程 # 建议给Bootloader设置密码以防止黑客修改系统启动项或者不授权登录系统 --append= 指定内核参数，要指定多个参数，使用空格分隔。引导程序默认的参数是\"rhgb quiet\"。举例: \"bootloader --location=mbr --append=\"hdd=ide-scsi ide=nodma\" --boot-drive= 指定安装bootloader到哪个磁盘上 --leavebootorder 防止安装程序更改 EFI 或者 ISeries/PSeries 系统中的现有可引导映像。 --driveorder= 指定在 BIOS 引导顺序中的首选驱动器。例如: bootloader --driveorder=sda,hda --location= 指定引导记录的写入位置（在大多数情况下不需要指定这个选项），有效值如下 1.mbr 默认选项。具体要看该驱动器是使用主引导记录（MBR）还是 GUID 分区表（GPT）方案： a.在使用 GPT 格式化的磁盘中，这个选项会在 BIOS 引导分区中安装 stage 1.5 引导装载程序。 b.在使用 MBR 格式化的磁盘中，会在 MBR 与第一个分区之间的空白空间中安装 stage 1.5。 2.partition 在包含内核的分区的第一个扇区中安装引导装载程序 3.none -不安装引导装载程序。 --password= 如果使用GRUB2,则会将使用这个选项指定的密码设定为引导装载程序密码.这应用来限制对GRUB2 shell的访问,并可以跳过任意内核选项.如果指定密码,GRUB2还会询问用户名。该 用户名总是root --iscrypted 通常当使用 --password= 选项指定引导装载程序密码时，会将其以明文方式保存在 Kickstart 文件中。如果要加密此密码，可使用这个选项和一个加密的密码。 请使用 grub2-mkpasswd-pbkdf2 命令生成加密的密码，输入要使用的密码，并将该命令的输出结果（以 grub.pbkdf2 开头的哈希符号）复制到 Kickstart 文件中 --timeout= 指定引导装载程序引导默认选项前等待的时间（以秒为单位）。 --default= 设定引导装载程序配置中的默认引导映像。 --extlinux 使用 extlinux 引导装载程序而不是 GRUB2。这个选项只能用于支持 extlinux 的系统。 --disabled 这个选项是 --location=none 的加强版。--location=none 只是简单地禁用 bootloader 安装，而 --disabled 则不仅禁用 bootloader 安装，也会禁用 bootloader 软件 包的安装，从而节省了空间。 keyboard：设置系统键盘类型 --vckeymap= # 指定VConsole应该使用的字符映射表。是字符映射表的文件名，和/usr/lib/kbd/keymaps目录下的文件名除去\".map.gz\"后相同。 --xlayouts=,,..., #指定 X 布局列表，该列表可使用逗号分开，无空格。接受与 setxkbmap(1) 相同格式的值，可以是 layout 格式（比如 cz），也可 #以是 layout (variant) 格式（比如 cz (qwerty)）。 --switch=,,..., #指定布局切换选项（在多个键盘布局间切换的快捷方式）列表。必须使用逗号分开多个选项，无空格。接受值与 setxkbmap(1) 格式相同。 ​ #示例使用 --xlayouts= 选项设置两个键盘布局（English (US) 和 Czech (qwerty)），并允许使用 Alt+Shift 在二者之间进行切换： # keyboard --xlayouts=us,'cz (qwerty)' --switch=grp:alt_shift_toggle lang：设置在安装过程中使用的语言以及系统的缺省语言 示例：lang en_US 文本模式的安装过程不支持某些语言(主要是中文,日语,韩文和印度的语言).如果用lang命令指定这些语言中的一种,安装过程仍然会使用英语,但是系统会缺省使用指定的语言. part 或者 partition：在系统上创建一个分区。（install模式必须） part |swap|pv.id|rdid.id options 1.mntpoint:挂载点，是在创建普通分区时指定新分区挂载位置的项；挂载点需要格式正. 例如 /, /usr, /home 2.swap 分区将被用作交换分区。为了自动决定交换分区的大小，可以使用--recommended选项。 3.raid. 表示创建的分区类型为raid型；必须用id号进行唯一区别； 4.pv. 表示所创建的分区类型为LVM型；必须用唯一id号进行区别； --size= 最小分区大小(MB)。这里可以指定一个整数值如500.不要在后面加MB。 --grow 告诉分区增长以填满可用空间(如果有的话)，或者填满设置的最大值。注意，--grow并不支持RAID卷在上的分区。 --maxsize= 分区被设置为grow时的最大分区大小(MB)。指定一个整数值，不要在后面加上MB。 --noformat 告诉安装程序不格式化分区，和--onpart一起使用。 --onpart= or --usepart= 把分区放在已经存在的设备上。使用\"--onpart=LABEL=name\"或者\"--onpart=UUID=name\" 来通过各自的标签(label)或uuid来指定一个分区。Stop (medium size).png Anaconda也许会以特殊的顺序创建分区，所以使用标签比只用分区名要安全些 --ondisk= or --ondrive= 强制在特定的磁盘上创建。 --asprimary 强制分区作为主分区，否则会导致分区失败。 --fsprofile= 为在该分区上创建文件系统的程序指定使用类型。使用类型定义了创建文件系统时各种各样的调整参数。为了让该选项能起作用，文件系统必须支持使用类型的概念，而且必须有 一个配置文件列出所有可用的类型。对于ext2/3/4，配置文件位于/etc/mke2fs.conf。 --fstype= 为分区设置文件系统类型。有效值包括ext4,ext3,ext2,btrfs,swap以及vfat。其它文件系统是否有效取决于传递给anaconda使能其它文件系统的命令行参数。 --fsoptions= 为挂载文件系统指定自由格式的字符串选项。该字符串将会被拷贝到安装系统的/etc/fstab文件中并且应该被引号括起来。 --label= 指定分区上创建的文件系统标签。如果所指定的标签已经被其它文件系统使用，新的标签将会被创建。 --recommended 自动决定分区大小。 --onbiosdisk= 强制在BIOS发现的特定磁盘上创建分区。 --encrypted 说明该分区应该被加密 --passphrase= 指定加密分区时指定的密码短语。如果没有上述--encrypted选项，该选项不起任何作用。如果没有密码短语被指定，将会使用系统范围内的默认密码短语，如果没有默认的， 安装器会停下来提醒。 --escrowcert= 从加载X.509认证。存储用证书加密过的数据加密密钥。只在--encrypted指定时有效。 --backuppassphrase 只在--escrowcert指定时相关。除了存储数据加密密钥之外，产生一个随机密码短语并将其添加到该分区。然后使用--escrowcert指定的证书加密并存储于/root。如果 不止一个LUKS卷使用--backuppassphrase，它们将共享该密码短语。 例： part /boot --fstype=“ext3” --size=100 part swap --fstype=“swap” –size=512 part / --bytes-pre-inode=4096 --fstype=“ext4”--size=10000 part /data --onpart=/dev/sdb1 --noformat part raid.100 --size=2000 part pv.100 --size=1000 auth/authconfig：设置系统的授权验证选项。它只是authconfig程序的封装，因而所有被authconfig程序识别的选项都可以应用于auth命令。想要获取完整的列表，请参考authconfig手册。默认情况下，密码一般会被加密但并不会放在shadow文件中。 --enablemd5,每个用户口令都使用md5加密. --enablenis,启用NIS支持.在缺省情况下,--enablenis使用在网络上找到的域.域应该总是用--nisdomain=选项手工设置. --nisdomain=,用在NIS服务的NIS域名. --nisserver=,用来提供NIS服务的服务器(默认通过广播). --useshadow或--enableshadow,使用屏蔽口令. --enableldap,在/etc/nsswitch.conf启用LDAP支持,允许系统从LDAP目录获取用户的信息(UIDs,主目录,shell 等等).要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN(distinguished name). --enableldapauth,把LDAP作为一个验证方法使用.这启用了用于验证和更改密码的使用LDAP目录的pam_ldap模块.要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN. --ldapserver=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定所使用的LDAP服务器的名字.这个选项在/etc/ldap.conf文件里设定. --ldapbasedn=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定用户信息存放的LDAP目录树里的DN.这个选项在/etc/ldap.conf文件里设置. --enableldaptls,使用TLS(传输层安全)查寻.该选项允许LDAP在验证前向LDAP服务器发送加密的用户名和口令. --enablekrb5,使用Kerberos 5验证用户.Kerberos自己不知道主目录,UID或shell.如果启用了Kerberos,必须启用LDAP,NIS,Hesiod或者使用/usr/sbin/useradd命令来使这个工作站获知用户的帐号.如果使用这个选项,必须安装pam_krb5软件包. --krb5realm=,工作站所属的Kerberos 5领域. --krb5kdc=,为领域请求提供服务的KDC.如果的领域内有多个KDC,使用逗号(,)来分隔它们. --krb5adminserver=,领域内还运行kadmind的KDC.该服务器处理改变口令以及其它管理请求.如果有不止一个KDC,该服务器必须是主KDC. --enablehesiod,启用Hesiod支持来查找用户主目录,UID 和 shell.在网络中设置和使用 Hesiod 的更多信息,可以在 glibc 软件包里包括的/usr/share/doc/glibc-2.x.x/README.hesiod里找到.Hesiod是使用DNS记录来存储用户,组和其他信息的 DNS 的扩展. --hesiodlhs,Hesiod LHS(\"left-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对 base DN的使用. --hesiodrhs,Hesiod RHS(\"right-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对base DN的使用. --enablesmbauth,启用对SMB服务器(典型的是Samba或Windows服务器)的用户验证.SMB验证支持不知道主目录,UID 或 shell.如果启用SMB,必须通过启用LDAP,NIS,Hesiod或者用/usr/sbin/useradd命令来使用户帐号为工作站所知.要使用这个选项,必须安装pam_smb软件包. --smbservers=,用来做SMB验证的服务器名称.要指定不止一个服务器,用逗号(,)来分隔它们. --smbworkgroup=,SMB服务器的工作组名称. --enablecache,启用nscd服务.nscd服务缓存用户,组和其他类型的信息.如果选择在网络上用NIS,LDAP或hesiod分发用户和组的信息,缓存就尤其有用. rootpw：设置系统root账号的密码 rootpw [--iscrypted|--plaintext] [--lock] password --iscrypted #如果该选项存在,口令就会假定已被加密.可使用以下命令生成用随机盐值进行sha512加密后的密码 # python -c 'import crypt,getpass;pw=getpass.getpass();print(crypt.crypt(pw) if (pw==getpass.getpass(\"Confirm: \")) else exit())' --plaintext # 使用不加密的密码 --lock #锁定root用户，root用户将无法登陆Console 三、可选选项 install/upgrade install：默认安装方法。必须从 cdrom、harddrive、nfs、liveimg 或者 url（用于 FTP、HTTP、或者 HTTPS 安装）中指定安装类型 upgrade: 升级现有系统. text/graphical：kickstart安装模式 text：在文本模式下执行kickstart安装. graphical： 在图形模式下执行kickstart安装.kickstart安装默认在图形模式下安装. cdrom/harddrive/url/nfs/liveimg：指定安装类型 cdrom: 从系统上的第一个光盘驱动器CD-ROM/DVD中安装. harddrive [--biospart= | --partition=] [--dir=]: # 从本地驱动器上包含ISO镜像的目录安装，该驱动器必须是vfat或者ext2文件系统。除了改目录之外，还需要以后面的方式提供install.img。 # 一种方式是由boot.iso启动，另一种是在ISO镜像相同的目录中创建一个images/目录，然后将install.img放在那里。 --biospart= 安装用到的BIOS分区（例如82p2）。 --partition= 安装用到的硬盘分区 --dir= 包含ISO镜像和images/install.img的目录 # 例如:harddrive --partition=hdb2 --dir=/tmp/install-tree nfs --server= --dir= [--opts=] # 从指定的NFS服务器安装. --server=,指定NFS服务器（主机名或者IP）。 --dir=,包含安装树的variant目录的目录. --opts=,用于挂载NFS输出的Mount选项(可选). # 例如:nfs --server=nfsserver.example.com --dir=/tmp/install-tree url --url=|--mirrorlist= [--proxy=] [--noverifyssl] # 通过FTP或HTTP从远程服务器上的安装树中安装. --url= 安装用到的URL。支持的协议有HTTP, HTTPS, FTP, file等。 --mirrorlist= 安装用到的镜像URL。在该URL中完成$releasever和$basearch的变量替换 --proxy= 指定安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名 # 例如:url --url http:///或url --url ftp://:@/ liveimg --url= [--proxy=] [--checksum=] [--noverifyssl] #使用磁盘映像而不是软件包安装。映像文件可以是取自实时 ISO 映像的 squashfs.img 文件，压缩 tar 文件（.tar、.tbz、.tgz、.txz、.tar.bz2、.tar.gz 或者 .tar.xz） #或者安装介质可以挂载的任意文件系统。支持的文件系统为 ext2、ext3、ext4、vfat 和 xfs。 # Anaconda预期该镜像包含完成系统安装所需的实用程序。因此，创建磁盘镜像最好的方法是使用livemedia-creator。 # 如果该镜像包含/LiveOS/*.img（这是squashfs.img的构成），LiveOS中的第一个*.img将会被挂载，并用来安装目标系统。 --url= 执行安装的位置。支持的协议为 HTTP、HTTPS、FTP 和 file。 --proxy=[protocol://][username[:password]@]host[:port] 指定在安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替。 --checksum= 可选，镜像文件的sha256校验和。 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名。 reboot/poweroff/shutdown/halt：安装完成后做什么操作 reboot：重启（缺省选项） poweroff：关闭系统并断电.通常,在手工安装过程中,anaconda会显示一条信息并等待用户按任意键来重新启动系统. shutdown：关闭系统. halt：halt选项基本和shutdown -h命令相同. services：设置禁用或允许列出的服务 --disabled 设置服务为禁用 --enabled 启动服务 例：services --disabled autid,cups,smartd,nfslock 服务之间用逗号隔开，不能有空格 selinux： 在系统里设置SELinux状态.在anaconda里,SELinux缺省为enforcing. selinux [--disabled|--enforcing|--permissive] --enforcing,启用SELinux,实施缺省的targeted policy. 注:如果kickstart文件里没有selinux选项,SELinux将被启用并缺省设置为--enforcing. --permissive,输出基于SELinux策略的警告,但实际上不执行这个策略. --disabled,在系统里完全地禁用 SELinux. user：在系统上创建新用户 user --name= [--groups=] [--homedir=] [--password=] [--iscrypted] [--shell=] [--uid=] --name=,提供用户的名字.这个选项是必需的. --groups=,除了缺省的组以外,用户应该属于的用逗号隔开的组的列表. --homedir=,用户的主目录.如果没有指定,缺省为/home/. --password=,新用户的密码.如果没有指定,这个帐号将缺省被锁住. --iscrypted=,所提供的密码是否已经加密？ --shell=,用户的登录shell.如果不提供,缺省为系统的缺省设置. --uid=,用户的UID.如果未提供,缺省为下一个可用的非系统 UID. network：配置系统的网卡信息 --device= 指定要使用network命令配置或者激活的设备。能够以和 ksdevice启动选项相同的方式指定。例如：network --bootproto=dhcp --device=eth0 对于第一个network命令，如果选项没有被指定，它默认是 1)ksdevice启动选项, 2)为了获得kickstart而激活的设备,或者 3)UI上的选择框。对于如下的network命令，需要--device选项。 --ip= 网络接口的IP地址。 --ipv6= 网络接口的IPv6地址。可以是[/]形式的静态地址，例如，3ffe:ffff:0:1::1/128(如果前缀被省略，会被假定为64)，\"auto\"地址分配基于动态的邻居发现协议，而\"dhcp\"会使用DHCPv6协议。 --gateway= 默认网关，是一个IPv4或者IPv6地址。 --nodefroute 组织设备抓取默认路由。在安装器中使用--activate选项激活其它设备时非常有用，从F16起。 --nameserver= 主域名服务器，是一个IP地址。多个域名服务器必须由逗号分隔。 --nodns 并不配置DNS服务器。 --netmask= 安装系统的网络掩码。 --hostname= 安装系统的主机名。 --ethtool= 指定将要传递给ethtool程序的设备附加的低级别设置。 --essid= 无线网网络ID。 --wepkey= 无线网WEP加密密钥。 --wpakey= 无线网WPA加密密钥(从F16起)。 --onboot= 是否在启动时使能该设备。 --dhcpclass= DHCP类别。 --mtu= 设备的MTU。 --noipv4 在该设备上禁用IPv4。 --noipv6 在该设备上禁用IPv6。 --bondslaves 使用该选项指定的网卡作为多网卡绑定的从网卡，虚拟出的网卡的名字由--device指定。例如--bondslaves=eth0,eth1。自Fedora 19开始。 --bondopts 为--bondslaves和--device选项指定的绑定接口指定一个逗号分隔的参数列表。例如：--bondopts=mode=active-backup,primary=eth1。如果一个选项本身以逗号作为分隔符，那么使用分号作为选项之间的分隔符。 --vlanid 使用--device指定的设备作为父设备来创建的vlan设备的Id(802.1q标签)。例如，network --device=eth0 --vlanid=171将会创建vlan设备eth0.171。从Fedora 19起。 logging：该命令控制anaconda安装过程中的错误日志，并不影响安装系统。 --host=,发送日志信息到给定的远程主机,这个主机必须运行配置为可接受远程日志的syslogd进程. --port=,如果远程的syslogd进程没有使用缺省端口,这个选项必须被指定. --level=,debug,info,warning,error或critical中的一个.指定tty3上显示的信息的最小级别.然而,无论这个级别怎么设置,所有的信息仍将发送到日志文件. zerombr ：清除mbr信息，会同时清空系统用原有分区表 clearpart：在建立新分区前清空系统上原有的分区表，默认不删除分区 --all 擦除系统上原有所有分区； --drives 删除指定驱动器上的分区 --initlabel 初始化磁盘卷标为系统架构的默认卷标 --linux 擦除所有的linux分区 --none（default）不移除任何分区 ​ 例：clearpart --drives=hda,hdb --all --initlabel eula：使用这个选项以非用户互动方式接受终端用户许可证协议（End User License Agreement，EULA）。指定这个选项可防止 Initial Setup 在完成安装并第一次重启系统时提示您接受该许可证。 --agreed（强制） - 接受 EULA。必须总是使用这个选项，否则 eula 命令就毫无意义。 ignoredisk:指定安装程序忽略指定的磁盘。如果您使用自动分区并希望忽略某些磁盘的话，这就很有用 --only-use - 指定安装程序要使用的磁盘列表。忽略其他所有磁盘。 例如1：要在安装过程使用磁盘 sda，并忽略所有其他磁盘：ignoredisk --only-use=sda 要包括不使用 LVM 的多路经设备： ignoredisk --only-use=disk/by-id/dm-uuid-mpath-2416CD96995134CA5D787F00A5AA11017 要包括使用 LVM 的多路径设备： ignoredisk --only-use=disk/by-id/scsi-58095BEC5510947BE8C0360F604351918 --interactive - 允许手动导航高级存储页面。 repo：配置用于软件包安装来源的额外的yum库.可以指定多个repo行. repo --name=repoid [--baseurl=|--mirrorlist=url] [options] --name= - 该库的 id。这个选项是必选项。如果库名称与另一个之前添加的库冲突，则会忽略它。因为这个安装程序使用预先配置的库列表，就是说您无法添加名称与预先配置的库相同的库。 --baseurl= - 程序库的 URL。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --mirrorlist，但不能同时使用这两个选项。 --mirrorlist= - URL 指向该程序库的一组镜像。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --baseurl，但不能同时使用这两个选项。 --install - 将所安装系统提供的存储库配置保存在/etc/yum.repos.d/目录中。不使用这个选项，在 Kickstart 文件中配置的程序库只能在安装过程中使用，而无法在安装的系统中使用。 --cost= - 为这个库分配的 cost 整数值。如果多个库提供同样的软件包，这个数字就是用来规定那个库优先使用，cost 较低的库比 cost 较高的库优先。 --excludepkgs= - 逗号分开的软件包名称列表，同时一定不能从这个存储库中提取该软件包名称。如果多个库提供同样的软件包，且要保证其来自某个特定存储库。 可接受完整软件包名称（比如 publican）和 globs（比如 gnome-*）。 --includepkgs= - 逗号分开的软件包名称列表，同时一定要从这个存储库中提取 glob。如果多个存储库提供同样的软件包，且要保证其来自某个特定存储库，这个选项就很有用了。 --proxy=[protocol://][username[:password]@]host[:port] - 指定只有这个存储库使用的 HTTP/HTTPS/FTP 代理服务器。 这个设置不会影响其他存储库，也不会影响将 install.img 附加到 HTTP 安装的方法。 --ignoregroups=true - 组成安装树时使用这个选项，且对安装过程本身没有影响。它告诉组合工具在镜像树时不要查看软件包组信息，这样就不会镜像大量无用数据。 --noverifyssl - 连接到 HTTPS 服务器时禁止 SSL 验证。 interactive：在安装过程中使用kickstart文件里提供的信息,但允许检查和修改给定的值.将遇到安装程序的每个屏幕以及kickstart文件里给出的值. autostep：通常 Kickstart 安装会跳过不必要的页面。这个选项可让安装程序浏览所有页面，并摘要显示每个页面。部署系统时不应使用这个选项，因为它会干扰软件包安装 --autoscreenshot 在安装的每一步均截屏。这些截屏将在安装过程中保存在 /tmp/anaconda-screenshots 中，并在安装完成后保存在 /root/anaconda-screenshots 中。 sshpw：安装过程中是否开启SSH与安装进程进行交互与监控 sshpw --username=name password [--iscrypted|--plaintext] [--lock] --username --iscrypted --plaintext --lock autopart：自动生成分区（autopart 选项不能与 part/partition, raid、logvol 或者 volgroup 在同样的 Kickstart 文件中一同使用。） --type= - 选择您要使用的预先定义的自动分区方案之一。可接受以下值： lvm: LVM 分区方案。 btrfs: Btrfs 分区方案。 plain: 不附带 LVM 或者 Btrfs 的常规分区。 thinp: LVM 精简分区方案。 --fstype= - 选择可用文件系统类型之一。可用值为 ext2、ext3、ext4、xfs 和 vfat。默认系统为 xfs。有关使用这些文件系统的详情，请查看 第 6.14.4.1.1 节 “文件系统类型”。 --nolvm - 不使用 LVM 或者 Btrfs 进行自动分区。这个选项等同于 --type=plain。 --encrypted - 加密所有分区。这等同于在手动图形安装过程的起始分区页面中选中 加密分区 复选框。 # 注意：加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。 # 如果要在虚拟机中安装系统，则可添加 virtio-rng 设备（虚拟随机数生成器）， --passphrase= - 为所有加密设备提供默认的系统范围内的密码短语。 --escrowcert=URL_of_X.509_certificate - 将所有加密卷数据加密密码保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷的密码都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时这个选项才有意义。 --cipher= - 如果指定 Anaconda 默认 aes-xts-plain64 无法满足需要，则可以指定要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 《Red Hat Enterprise Linux 7 安全指南》中有可用加密类型列表，但 Red Hat 强烈推荐您使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 firewall：配置系统防火墙选项 firewall –enable|--disable [ --trust ] [ --port= ] --enable 拒绝回应输出要求的进入连接，比如 DNS 答复或 DHCP 请求。如果需要访问在这台机器中运行的服务，可以选择通过防火墙允许具体的服务。 --disable 不配置任何iptables防御规则； --trust 在这里列出设备,比如em1,允许所有流量通过该防火墙进出那个设备.要列出一个以上的设备,请使用--trust em1 --trust em2。不要使用逗号分开的格式，比如 --trust em1, em2。 --port 可以用端口:协议（port:protocal）格式指定允许通过防火墙的端口。例如，如果想允许 IMAP 通过您的防火墙，可以指定 imap:tcp。还可以具体指定端口号码，要允许 UDP 分组 在端口 1234 通过防火墙，输入 1234:udp。要指定多个端口，用逗号将它们隔开。 --service= 这个选项提供允许服务通过防火墙的高级方法。有些服务（比如 cups、avahi 等等）需要开放多个端口，或者另外有特殊配置方可工作。 您应该使用 --port 选项指定每个具体端口，或者指定 --service= 并同时打开它们 incoming - 使用以下服务中的一个或多个来替换，从而允许指定的服务通过防火墙。 --ssh --smtp --http --ftp ​ 示例：firewall --enable --trust eth0 --trust eth1 --port=80:tcp group:在系统中生成新组。如果某个使用给定名称或者 GID 的组已存在，这个命令就会失败。另外，该 user 命令可用来为新生成的用户生成新组 group --name=name [--gid=gid] --name= - 提供组名称。 --gid= - 组的 UID。如果未提供，则默认使用下一个可用的非系统 GID。 volgroup：创建逻辑卷组vg volgroup name partition [options] ​ --noformat - 使用现有卷组，且不进行格式化。 --useexisting - 使用现有卷组并重新格式化。如果使用这个选项，请勿指定 partition。例如：volgroup rhel00 --useexisting --noformat --pesize= - 以 KiB 为单位设定卷组物理扩展大小。默认值为 4096 (4 MiB)，最小值为 1024 (1 MiB)。 --reserved-space= - 以 MB 为单位指定在卷组中预留的未使用空间量。只适用于新生成的卷组。 --reserved-percent= - 指定卷组中预留未使用空间的比例。只适用于新生成的卷组。 ​ 注意1 ：不要在逻辑卷和卷组名称中使用小横线（-）。如果使用这个字符，会完成安装，但 /dev/mapper/ 目录列出这些卷和卷组时，小横线会加倍。例如：某个卷组名为 volgrp-01，包含名 为 logvol-01 逻辑卷，该逻辑卷会以 /dev/mapper/volgrp--01-logvol--01 列出。这个限制只适用于新创建的逻辑卷和卷组名。如果您使用 --noformat 选项重复使用现有名称， 它们的名称就不会更改。 注意2: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷。例如： part pv.01 --size 10000 volgroup volgrp pv.01 logvol / --vgname=volgrp --size=2000 --name=root logvol：创建逻辑卷lv logvol mntpoint --vgname=name --name=name [options] mntpoint — 是该分区挂载的位置，且必须是以下格式之一： 1. /path 例如：/ 或者 /home 2.swap 该分区被用作交换空间。要自动决定 swap 分区的大小，使用 --recommended 选项：swap --recommended 使用 --hibernation 选项自动决定 swap 分区的大小，同时还允许 您的系统有附加空间以便可以休眠：swap --hibernation分配的分区大小将与 --recommended 加上系统 RAM 量相等。 这些选项如下所示： --noformat- 使用现有逻辑卷且不要对其进行格式化。 --useexisting 使用现有逻辑卷并重新格式化它。 --fstype= 为逻辑卷设置文件系统类型。有效值有：xfs、ext2、ext3、ext4、swap 和 vfat。 --fsoptions= 指定在挂载文件系统时所用选项的自由格式字符串。将这个字符串复制到安装的系统的 /etc/fstab 中，并使用括号括起来。 --mkfsoptions= 指定要提供的附加参数，以便在这个分区中建立文件系统。没有对任何参数列表执行任何操作，因此必须使用可直接为 mkfs 程序提供的格式。 就是说可使用逗号分开或双引号分开的多个选项，要看具体文件系统。 --label= 为逻辑卷设置标签。 --grow 会让逻辑卷使用所有可用空间（若有），或使用设置的最大值（如果指定了最大值）。必须给出最小值，可使用 --percent= 选项或 --size= 选项。 --size= 以 MB 单位的逻辑卷大小。这个选项不能与 --percent= 选项一同使用。 --percent= 考虑任何静态大小逻辑卷时的逻辑卷大小，作为卷组中剩余空间的百分比。这个选项不能与 --size= 选项一同使用。 #重要:创建新逻辑卷时，必须使用 --size= 选项静态指定其大小，或使用 --percent= 选项指定剩余可用空间的百分比。不能再同一逻辑卷中同时使用这些选项。 --maxsize= - 当将逻辑卷被设置为可扩充时以 MB 为单位的最大值。在这里指定一个整数值，如500（不要在数字后添加单位）。 --recommended - 创建 swap 逻辑卷时可采用这个选项，以根据您的系统硬件自动决定这个卷的大小。 --resize - 重新定义逻辑卷大小。如果使用这个选项，则必须还指定 --useexisting 和 --size。 --encrypted - 指定该逻辑卷应该用 --passphrase= 选项提供的密码进行加密。如果没有指定密码短语，安装程序将使用 autopart --passphrase 命令指定默认系统级密码， 如果没有设定默认密码则会停止安装，并提示输入密码短语。 # 注意:加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。如果要在虚拟机中安装系统，则可 # 添加 virtio-rng 设备（虚拟随机数生成器） --passphrase= - 指定在加密这个逻辑卷时要使用的密码短语。必须与 --encrypted 选项一同使用，单独使用这个选项无效。 --cipher= - 指定如果对 Anaconda 默认 aes-xts-plain64 不满意时要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 推荐使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 --escrowcert=URL_of_X.509_certificate 将所有加密卷数据加密密钥作为文件保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷 的密钥都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时 这个选项才有意义。 --thinpool - 创建精简逻辑卷。（使用 none 挂载点）。 --metadatasize=size - 为新的精简池设备指定元数据大小（单位 MiB）。 --chunksize=size - 为新的精简池设备指定块大小（单位 KiB）。 --thin - 创建精简逻辑卷。（要求使用 --poolname） --poolname=name - 指定在其中创建精简逻辑卷的精简池名称。需要 --thin 选项。 --profile=name 指定与精简逻辑卷配合使用的配置文件名称。如果使用此选项，还要用于给定逻辑的卷元数据中包含该名称。默认情况下，可使用的配置文件为在 /etc/lvm/profile 目录中定 义的 default 和 thin-performance。详情请查看 lvm(8) 手册页。 --cachepvs= - 用逗号分开的物理卷列表，应作为这个卷的缓存使用。 --cachemode= - 指定应使用哪种模式缓存这个逻辑卷 - 可以是 writeback，也可以是 writethrough。 #注意:有关缓存的逻辑卷及其模式的详情，请查看 lvmcache(7) 手册页。 --cachesize= - 附加到该逻辑卷的缓存大小，单位为 MiB。这个选项需要 --cachepvs= 选项。 ​ 注意1: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷以占据逻辑组里剩余的 90% 空间。例如：： part pv.01 --size 1 --grow volgroup myvg pv.01 logvol / --vgname=myvg --name=rootvol --percent=90 timezone：设置系统的时区 timezone [ --utc ] 例：timezone --utc Asia/Shanghai 软件包的选择 在 Kickstart 文件中使用 %packages 命令列出要安装的软件包。 可以根据环境、组或者其软件包名称指定软件包。 安装程序定义包含相关软件包的几个环境和组。有关环境和组列表请查看安装光盘中的 repodata/*-comps-variant.architecture.xml 文件。 *-comps-variant.architecture.xml 文件包含描述可用环境（使用 标签标记）和组（ 标记）的结构。每个组都有一个 ID、用户可见性数值、名称、描述和软件包列表。如果未安装选择该组，那么就会安装该软件包列表中标记为 mandatory 的软件包；如果未明确指定，也会安装标记为 default 的软件包，而标记为 optional 的软件包必须在明确指定后方可安装。 您可以使用 ID（ 标签）或者名称（ 标签）指定软件包组或者环境。 %packages 部分必须以 %end 命令结尾。 除组外，您还要指定要安装的整体环境 %packages @^Infrastructure Server %end 指定组，每个条目一行，以 @ 符号开始，接着是空格，然后是完整的组名或 *-comps-variant.architecture.xml 中指定的组 id。 %packages @X Window System @Desktop @Sound and Video %end 根据名称指定独立软件包，每行一个条目。您可以在软件包名称中使用星号（*）作为通配符。 %packages sqlite curl aspell docbook* %end 使用小横线（-）开头指定安装中不使用的软件包或组。 %packages -@Graphical Internet -autofs -ipa*fonts %end 常用软件包选择选项 以下选项可用于 %packages。要使用这个选项，请将其添加到软件包选择部分的开始。例如： %packages --multilib --ignoremissing ​ --default 安装默认软件包组。这与在互动安装过程中的 软件包选择 页面中没有其他选择时要安装的软件包组对应。 --excludedocs 不要安装软件包中的任何文档。大多数情况下，这样会排除一般安装在 /usr/share/doc* 目录中的所有文件，但要排除的具体文件取决于各个软件包。 --ignoremissing 忽略所有在这个安装源中缺少的软件包、组及环境，而不是暂停安装询问是应该放弃还是继续安装。 --instLangs= 指定要安装的语言列表。注：这与软件包组等级选择不同。这个选项不会告诉您应该安装哪些软件包组，而是通过设置 RPM 宏控制应该安装独立软件包中的哪些事务文件。 --multilib 为 multilib 软件包配置已安装的系统（即允许在 64 位系统中安装 32 位软件包），并安装在这一部分指定的软件包。通常在 AMD64 和 Intel 64 系统中，只安装用于整个架构 （标记为 x86_64）的软件包以及用于所有架构（标记为 noarch）软件包。使用这个选项时，将自动安装用于 32 位 AMD 系统 Intel（标记为 i686）的软件包。这只适用于在 %packages 部分明确指定的软件包。对于那些仅作为相依性安装而没有在 Kickstart 文件中指定的软件包，将只安装其所需架构版本，即使有更多可用架构也是如此。 --nocore 禁用默认总被安装的 @Core 软件包组。禁用 @Core 软件包组应只用于创建轻量级的容器；用 --nocore 安装桌面或服务器系统将导致系统不可用。 具体软件包组参数项 这个列表中的选项只用于单一软件包组。不是在 Kickstart 文件的 %packages 命令中使用，而是在组名称中添加条目。例如： %packages @Graphical Internet --optional %end ​ --nodefaults 只安装该组的强制软件包，不是默认选择。 --optional 除安装默认选择外，还要安装在 *-comps-variant.architecture.xml 文件组定义中标记为自选的软件包。 注：有些软件包组，比如 Scientific Support，没有指定任何强制或默认软件包 - 只有自选软件包。在这种情况下必须使用 --optional 选项，否则不会安装这个组中的任何软件包。 安装前脚本 ks.cfg文件被解析后马上加入要运行的命令.这个部分必须处于kickstart文件的最后(在命令部分之后)而且 必须用%pre命令开头，%end结尾. 可以在%pre部分访问网络；然而,此时命名服务还未被配置,所以只能使用IP地址. 预安装脚本不会在 chroot 环境中运行 --interpreter= 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例 %pre #!/bin/sh hds=\"\" mymedia=\"\" for file in /proc/ide/h* do mymedia=`cat $file/media` if [ $mymedia == \"disk\" ] ; then hds=\"$hds `basename $file`\" fi done set $hds numhd=`echo $#` drive1=`echo $hds | cut -d' ' -f1` drive2=`echo $hds | cut -d' ' -f2` ​ #Write out partition scheme based on whether there are 1 or 2 hard drives if [ $numhd == \"2\" ] ; then #2 drives echo \"#partitioning scheme generated in %pre for 2 drives\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75 --ondisk hda\" >> /tmp/part-include echo \"part / --fstype xfs --size 1 --grow --ondisk hda\" >> /tmp/part-include echo \"part swap --recommended --ondisk $drive1\" >> /tmp/part-include echo \"part /home --fstype xfs --size 1 --grow --ondisk hdb\" >> /tmp/part-include else #1 drive echo \"#partitioning scheme generated in %pre for 1 drive\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75\" >> /tmp/part-include echo \"part swap --recommended\" >> /tmp/part-include echo \"part / --fstype xfs --size 2048\" >> /tmp/part-include echo \"part /home --fstype xfs --size 2048 --grow\" >> /tmp/part-include fi %end 安装后脚本 post-install 脚本是在 chroot 环境里运行的.因此,某些任务如从安装介质复制脚本或RPM将无法执行. --nochroot 允许指定想在chroot环境之外运行的命令 --interpreter 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例 %post --nochroot cp /etc/resolv.conf /mnt/sysimage/etc/resolv.conf %end 四、示例 **全新安装系统** install #在文本模式下安装 text #指定安装过程中语言为英语 lang en_US.UTF-8 #指定键盘类型为US布局 keyboard us ​ auth --useshadow --passalgo=sha512 # url --url=\"http://192.168.1.80/CentOS7\" #设置root用户密码 rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ #开启SELinux selinux --enforcing #关闭防火墙 firewall --disabled #设置网络信息。网卡1设置手动获取IP，不初始化IPV6,不设置为默认路由。网卡2设置DHCP获取IP，不初始化IPV6,设置为默认路由。 network --bootproto=static --ip=192.168.10.6 --device=enp0s3 --activate --nodefroute --noipv6 --nameserver=114.114.114.114 --netmask=24 --gateway=192.168.10.1 network --bootproto=dhcp --device=enp0s8 --activate --nameserver=114.114.114.114 --noipv6 #设置主机名 network --hostname=test #安装完成后重启 reboot #设置时区为上海的时区 timezone Asia/Shanghai --isUtc --nontp #将BootLoader安装在sda磁盘上 bootloader --location=mbr --boot-drive=sda #安装时清理sda磁盘上所有的分区 clearpart --all --drives=sda #将SSH和NetworkManager设置为开机自启动 services --enabled=NetworkManager,sshd firstboot --enable #只使用sda磁盘 ignoredisk --only-use=sda #对sda磁盘进行分区。单位是MB part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 ​ %packages @^minimal @core %end #安装后要执行的脚本 %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 五、验证KS文件的语法正确性 yum install -y pykickstart ksvalidator ks文件 六、比较OS不同版本间的KS语法差异 ksverdiff -f RHEL6 -t RHEL7 # -f 指定要比较的第一个发行本，-t 指定要比较的最后一个发行本 参考连接 ★★★★★： https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/installation_guide/sect-kickstart-syntax ★★★★★：https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-commands ★★★★★ https://blog.csdn.net/yanghua1012/article/details/80426659 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/pxe-引导配置文件参数详解.html":{"url":"origin/pxe-引导配置文件参数详解.html","title":"PXE引导配置文件参数详解","keywords":"","body":"default ks 　　　　#默认启动的是 'label ks' 中标记的启动内核 prompt 1 #显示 'boot: ' 提示符。为 '0' 时则不提示，将会直接启动 'default' 参数中指定的内容。 timeout 6 　　　　 #在用户输入之前的超时时间，单位为 1/10 秒。 display boot.msg #显示某个文件的内容，注意文件的路径。默认是在/var/lib/tftpboot/ 目录下。也可以指定位类似 '/install/boot.msg'这样的，路径+文件名。 F1 boot.msg 　　　 #按下 'F1' 这样的键后显示的文件。 F2 options.msg F3 general.msg F4 param.msg F5 rescue.msg label linux #'label' 指定你在 'boot:' 提示符下输入的关键字，比如boot: linux[ENTER]，这个会启动'label linux' 下标记的kernel 和initrd.img 文件。 kernel vmlinuz #kernel 参数指定要启动的内核。 append initrd=initrd.img #append 指定追加给内核的参数，能够在grub 里使用的追加给内核的参数，在这里也都可以使用。 label text kernel vmlinuz append initrd=initrd.img text label ks kernel vmlinuz append ks=http://192.168.111.130/ks.cfg initrd=initrd.img #告诉系统，从哪里获取ks.cfg文件 label local localboot 1 label memtest86 kernel memtest append - default vesamenu.c32 prompt 0 timeout 60 display boot.msg menu background splash.jpg menu title ########## Curiouser PXE Boot Menu ########## label CentOS7.5.1804 menu label ^1> Install CentOS 7.5.1804 x86_64 kernel vmlinuz-7.5 append initrd=initrd-7.5.img text ks=http://192.168.1.80/CentOS7.5.1804.cfg label CentOS7.7.1908 menu label ^2> Install CentOS 7.7.1908 x86_64 menu default kernel vmlinuz-7.7 append initrd=initrd-7.7.img text ks=http://192.168.1.80/CentOS7.7.1908.cfg 参考 https://wiki.centos.org/zh/HowTos/PXE/PXE_Setup/Menus https://blog.51cto.com/4690837/2333165 https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/installation_guide/ch-boot-x86#sn-boot-menu-x86 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/tool-SublimeText.html":{"url":"origin/tool-SublimeText.html","title":"Sublime Text 3","keywords":"","body":"Sublime Text使用总结 一、简介 Sublime Text是一款具有代码高亮、语法提示、自动完成且反应快速的编辑器软件 Sublime Text具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。 二、安装 Sublime Text官网：http://www.sublimetext.com/3 三、插件管理器Package Control Package Control：https://packagecontrol.io/installation 使用Ctrl + `打开Sublime Text控制台 将下面的代码粘贴到控制台里 Sublime Text 3 import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by) Sublime Text 2 import urllib2,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), 'wb' ).write(by) if dh == h else None; print('Error validating download (got %s instead of %s), please try manual install' % (dh, h) if dh != h else 'Please restart Sublime Text to finish installation') 给Package Controller设置代理 Sublime Text > Preferences > Package Settings > Package Control > Settings - User 编辑 Package Control.sublime-settings，添加两行: \"http_proxy\": \"http://代理IP地址:3128\", \"https_proxy\": \"http://代理IP地址:3128\", 解决无法安装插件问题 使用第三方Channel，见附件 Preference-->Package Settings-->Package Control-->Settings User { \"bootstrapped\": true, \"channels\": [ \"D:/Sublime Text 3/data/channel_v3.json\" ] } 四、常用快捷键 快捷键 功能 Ctrl+H 查找替换 Ctrl+F 查找内容 Ctrl+Shift+F 在文件夹内查找内容，可进行替换 Ctrl+L 选择一行 Ctrl+Shift+D 复制当前行到下行 Ctrl+K+U 大写光标所在词 Ctrl+K+L 小写光标所在词 Ctrl+Shift+D 复制光标所在整行到下一行 Ctrl+Shift+L 鼠标选中多行（按下快捷键），即可同时编辑这些行 Ctrl+Shift+↑/↓ 可以移动此行代码，与上/下行互换 Ctrl+Shift+←/→ 向右/向右单位性地选中文本 Alt+Shift+1 窗口分屏，恢复默认1屏（非小键盘的数字） Alt+Shift+2 左右分屏-2列 Alt+Shift+3 左右分屏-3列 Alt+Shift+4 左右分屏-3列 Alt+Shift+5 等分4屏 Alt+Shift+8 垂直分屏-2屏 Alt+Shift+9 垂直分屏-3屏 Ctrl+K+B 开启/关闭侧边栏 Ctrl+/ 注释单行 Ctrl+Shift+/ 注释多行 Ctrl+K+K 从光标处开始删除代码至行尾 Ctrl+Shift+K 删除整行 Tab 向右缩进 Shift+Tab 向左缩进 Ctrl+J 合并选中的多行代码为一行 Shift+↑/↓/←/→ 向上/下/左/右选中文本 Ctrl+K+0 展开所有折叠代码 Ctrl+M 光标移动至括号内结束或开始的位置 Ctrl+Shift+M 选择括号内的内容 Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑 Ctrl+G 跳转到第几行 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+V 粘贴并格式化 Ctrl+X 删除当前行 Ctrl+Z 撤销 Ctrl+Y 恢复撤销 Ctrl+U 软撤销 Ctrl+T 左右字母互换 Ctrl+Tab 按文件浏览过的顺序，切换当前窗口的标签页 Ctrl+PageDown 向左切换当前窗口的标签页 Ctrl+PageUp 向右切换当前窗口的标签页 Ctrl+W 关闭当前打开文件 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+P 打开命令面板 Ctrl+： 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 Ctrl+R 打开搜索框，自动带@，输入关键字，查找文件中的函数名 Ctrl+P 打开搜索框。1、输入当前项目中的文件名，快速搜索文件2、@和关键字，查找文件中函数名3、：和数字，跳转到文件中该行代码4、#和关键字，查找变量名 五、常用插件 插件名 功能 描述 DeleteBlankLines 去除文本中的空白行 Windows: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank LinesLinux: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank Lines ChineseLocalizations 汉化Sublime Text 请使用主菜单的 帮助/Language 子菜单来切换语言。 目前支持 简体中文 繁体中文 日本語。 要换回英语不需要卸载本插件，请直接从菜单切换英文。 HTML-CSS-JS Prettify HTML/CSS/JS代码格式化 需要安装NodeJS并设置node执行文件的路径右键-->HTML-CSS-JS Prettify-->Set ‘node’ Path GBK Encoding Support 支持gbk编码 Alignment 代码格式的自动对齐 默认快捷键Ctrl+Alt+A Clipboard History 粘贴板历史记录，方便使用复制/剪切的内容 Ctrl+alt+v：显示历史记录Ctrl+alt+d：清空历史记录Ctrl+shift+v：粘贴上一条记录（最旧）Ctrl+shift+alt+v：粘贴下一条记录（最新） ConvertToUTF8 编辑并保存目前编码不被 Sublime Text 支持的文件 IMESupport 支持中文输入法跟随光标 AutoFileName 自动完成文件名的输入，如图片选取 Trailing spaces 检测并一键去除代码中多余的空格 一键删除多余空格：CTRL+SHITF+T（需配置），更多配置请点击标题。快捷键配置：在Preferences / Key Bindings – User加上{ \"keys\": [\"ctrl+shift+t\"], \"command\": \"delete_trailing_spaces\" } FileDiffs 比较当前文件与选中的代码、剪切板中代码、另一文件、未保存文件之间的差别。可配置为显示差别在外部比较工具，精确到行。 右键标签页，出现FileDiffs Menu或者Diff with Tab…选择对应文件比较即可 DocBlockr 生成优美注释 标准的注释，包括函数名、参数、返回值等，并以多行显示，手动写比较麻烦。输入/、/*然后回车，还有很多用法，请参照https://sublime.wbond.net/packages/DocBlockr SideBarEnhancements 增强型侧边栏 Terminal 接使用终端打开你的项目文件夹，并支持使用快捷键。 默认调用系统自带的PowerShell ctrl+shift+t 打开文件所在文件夹，ctrl+shift+alt+t 打开文件所在项目的根目录文件夹 SFTP 快速编辑远程服务器上的文件 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/windows-cmd发送SMTP邮件.html":{"url":"origin/windows-cmd发送SMTP邮件.html","title":"CMD发送SMTP邮件","keywords":"","body":"Preflight 邮箱开启POP3/SMTP和IMAP/SMTP服务 一、操作 1. windows开启telnet服务 打开控制面板，找到“打开或关闭windows功能”（在“程序”里面），选中对话框中的Telnet客户端，然后确定，等待完成。这时就开启了telnet功能。 2. 在命令刚窗口输入 telnet smtp.163.com 25 3. 向服务器表明身份 helo 163.com # 如果成功，服务器返回 250 OK 4. 登录认证 auth login # 用户名的Base64加密字符。如果成功，服务器返回一串字符，类似于：334 UGFzc3dvcmQ6（334 是不变的，后面的字母可能会变） ***** # 密码的Base64加密字符，如果登录成功，服务器返回一串字符：235 Authentication successful表示登录成功，如果不能成功登录，请检查账号密码是否正确。 ***** # 对于字符串的Base64加密可使用CMD中的“certutil -encode 包含想要加密字符串的文本文件 Base64加密后输出文本文件” 5. 填写发件人和收件人邮箱地址 mail from: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 rcpt to: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 6. 编写邮件 data # 服务器返回 354 End data with . To:******@163.com From:******@163.com Subject:test mail From:******@163.com test body 123 . # 服务器返回 250 Ok: queues as ... 表示邮件已经发送 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/windows-小技巧.html":{"url":"origin/windows-小技巧.html","title":"Windows小技巧","keywords":"","body":"1. CMD下的换行符 在CMD下,可以用^作为换行符,类似于Linux下的\\ 2. CMD下查看端口使用情况 netstat -ano |findstr 8080 3. CMD下杀掉进程 taskkill /pid 8080 -t -f 4. CMD下校验文件的MD5、SHA1、SHA256值 certutil -hashfile yourfilename.ext MD5 certutil -hashfile yourfilename.ext SHA1 certutil -hashfile yourfilename.ext SHA256 5. CMD下激活windows系统 以管理员身份运行CMD 卸载之前的激活密钥 slmgr -upk 设置KMS服务器 slmgr -skms KMS服务器 常用的KMS服务器 kms.03k.org kms.chinancce.com kms.lotro.cc cy2617.jios.org kms.shuax.com kms.luody.info kms.cangshui.net zh.us.to 122.226.152.230 kms.digiboy.ir kms.library.hk kms.bluskai.com 输入新的密钥 slmgr -ipk 激活密钥 密钥 win10专业版密钥 W269N-WFGWX-YVC9B-4J6C9-T83GX 激活 slmgr -ato 6. PowerShell下载文件 $client = new-object System.Net.WebClient $client.DownloadFile('#1', '#2') # #1为下载链接 #2为文件保存的路径 Note： 一定要在路径中写上保存的新文件的全名（包括后缀） 建议保存的文件格式与下载的文件格式一致 7. 离线安装.NET Framework 3.5 Preflight windows 10 的系统ISO镜像 以管理员身份运行的CMD 将ISO镜像中source/sxs目录拷贝到某个路径下（以桌面为例） 在以管理员身份运行的CMD执行以下命令 dism.exe /online /enable-feature /featurename:netfx3 /Source:C:\\Users\\user\\Desktop\\sxs 8. 添加开机自启动bat脚本 方法一：（推荐） 将脚本放置“C:\\Users\\Curiouser\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup”路径下 方法二： 9. 修改远程桌面的默认端口3389 Windows+R,输入regedit，打开注册表，修改一下注册表的值(十进制)，然后重启远程桌面 HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\\PortNumber 防火墙放行新指定的远程桌面端口 10. 防火墙放行指定端口 11. CMD下的用户管理 net user：查看目前系统存在的用户 net user username：查看用户的详细信息 whoami：查看计算机当前登陆的用户 query user：查看已登陆用户的详细信息 logoff+空格+ID号：注销用户 net user 用户名 密码 /add：新增本地用户 net localgroup administrators 用户名 /add：将本地用户加入管理员用户组 net user 用户名 /del：删除用户 runas /user:用户 cmd：以某个用户运行命令 12. Windows软件授权管理工具slmgr命令 13、Window下类Unix终端Cygwin 官网下载地址：https://cygwin.com/install.html 注意：在安装时，会让选择预下载的软件，记得预下载lynx、wget、curl、zsh ①安装apt-cyg包管理器 apt-cyg是Cygwin下类似于apt的包管理器，可安装Github 地址：https://github.com/transcode-open/apt-cyg git clone https://github.com/transcode-open/apt-cyg.git cd apt-cyg install apt-cyg /bin # 或者 lynx -source rawgit.com/transcode-open/apt-cyg/master/apt-cyg > apt-cyg # 先为lynx命令设置代理，不然下载很慢 install apt-cyg /bin # 配置apt-cyg的镜像源 apt-cyg mirror http://mirrors.163.com/cygwin # 更新源 apt-cyg update # 安装软件 apt-cyg install jq vim 参考： https://zhuanlan.zhihu.com/p/66930502 ②安装配置zsh及oh-my-zsh 参考ZSH ③设置默认终端shell $ mkpasswd > /etc/passwd # 然后在/etc/passwd文件中设置当前用户为/bin/zsh ④为lynx命令设置代理 echo -e \"http_proxy:http://localhost:80\\nhttps_proxy:http://localhost:80\" >> /etc/lynx.cfg Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-27 23:35:18 "},"origin/windows-server.html":{"url":"origin/windows-server.html","title":"Windows Server管理","keywords":"","body":"Windows Server 管理 一、Windows Server 2012 R2 找回管理员密码 使用系统ISO光盘启动，在安装界面打开CMD（Shift+F10打开），替换放大镜程序 C:\\WINDOWS\\system32\\magnify.exe为C:\\WINDOWS\\system32\\cmd.exe。重启，在锁屏桌面打开放大镜(实际打开的是CMD)，使用net命令重制管理员密码。 进BIOS设置开机从系统ISO光盘启动，启动进入系统安装程序后按Shift+F10进入cmd 在cmd中替换放大镜程序，然后重启系统(从原系统盘启动) X是光盘路径，C盘是系统保留的100M的那个分区，D盘才是我们真正意义上的系统盘（如果有保留的话） # 备份原始放大镜程序 copy D:\\WINDOWS\\system32\\magnify.exe D:\\WINDOWS\\system32\\magnify.exe.bak # 替换放大镜程序 copy D:\\WINDOWS\\system32\\cmd.exe D:\\WINDOWS\\system32\\magnify.exe 在登录页面打开放大镜，然后就在弹出的CMD中使用net命令重置管理员用户 net user Administrator newpassword 然后就可以使用新密码进行登录啦 重制密码登录后再进行1～2操作，恢复原始放大镜程序。 参考 https://blog.csdn.net/njuptxiao/article/details/85098666 https://blog.51cto.com/ilyncsteven/2107216 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/windows-nssm.html":{"url":"origin/windows-nssm.html","title":"Windows进程守护工具NSSM","keywords":"","body":"Windows下的进程守护工具NSSM 一、简介 NSSM(the Non-Sucking Service Manager)是Windows环境下一款免安装的服务管理软件，它可以将应用封装成服务，使之像windows服务可以设置自动启动等。并且可以监控程序运行状态，程序异常中断后自动启动，实现守护进程的功能。不仅支持图形界面操作，也完全支持命令行设置。 同类型的工具还有微软自己的srvany，不过nssm更加简单易用，并且功能强大。它的特点如下： 支持普通exe程序（控制台程序或者带界面的Windows程序都可以） 安装简单，修改方便 可以重定向输出（并且支持Rotation） 可以自动守护封装了的服务，程序挂掉了后可以自动重启 可以自定义环境变量 这里面的每一个功能都非常实用，使用NSSM来封装服务可以大大简化我们的开发流程了。 开发的时候是一个普通程序，降低了开发难度，调试起来非常方便 安装简单，并且可以随时修改服务属性，更新也更加方便 可以利用控制台输出直接实现一个简单的日志系统 不用考虑再加一个服务实现服务守护功能 官方网站：https://nssm.cc/ 下载地址：https://nssm.cc/download （下载解压到windows系统环境Path下，在CMD中可使用即可） 二、配置 使用文档：https://nssm.cc/usage 1、服务的配置 ①安装服务的命令格式 ​ nssm install 服务名 参数项 [...] ②设置服务参数配置的命令格式 nssm set [subparameter] value ③重置服务参数配置的命令格式 nssm reset [subparameter] 三、服务的生命周期管理 1、注册一个服务 # 打开GUI界面配置一个服务 nssm install # 命令行配置一个服务 nssm install 服务名 \"C:\\Program Files\\Java\\jdk1.8.0_251\\bin\\java.exe\" 2、配置一个服务 nssm set 服务名 Application “C:\\Program Files\\Java\\jdk1.8.0_251\\bin\\java.exe” nssm set 服务名 AppDirectory “C:\\Application” nssm set 服务名 AppParameters “-jar test.jar ” nssm set 服务名 DisplayName \"Test\" nssm set 服务名 Description \"测试\" nssm set 服务名 Start SERVICE_AUTO_START 3、列出所有服务 nssm list 4、查看一个服务的配置 nssm get 5、启动一个服务 nssm start 服务名 6、查看服务的状态 nssm status 服务名 7、停止一个服务 nssm stop 服务名 8、重启一个服务 nssm restart 服务名 9、删除一个服务 nssm remove 服务名 10、暂停/继续服务 nssm pause 服务名 nssm continue 服务名 11、手动轮转日志文件 nssm rotate 服务名 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/windows-deployment-service-aik.html":{"url":"origin/windows-deployment-service-aik.html","title":"Windows 无人值守部署服务","keywords":"","body":"Windows 部署服务 一、简介 Windows Server 提供的WDS（Windows Deploy Service）很方便的帮助用户去批量部署windows 操作系统，而且可以和MDT（Microsoft Deployment Toolkit）去结合使用，达到无人值守快速部署Windows系统（Linux下使用的是Kickstart）。 二、Windows Server安装基础服务 DHCP服务 Windows部署服务 三、配置DHCP服务 四、配置Windows部署服务 五、配置Windows部署服务 1、上传Windows ISO镜像文件到服务器中并挂载 2、将ESD转换WIM 参看ESD文件中包括的WIM dism /Get-WimInfo /WimFile:install.esd 转换WIM文件 dism /export-image /SourceImageFile:.\\install.esd /SourceIndex:4 /DestinationImageFile:wndows10-1903.wim /Compress:none /CheckInt egrity /Compress参数项： - fast - recovery - none 参考：https://www.wintips.org/how-to-extract-install-esd-to-install-wim-windows-10-8/ 参考 https://www.jianshu.com/p/3eabf3ae0c27 https://www.dianshouit.com/?thread-63.htm= https://blog.51cto.com/gaowenlong/824781 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/windows-wakeuponlan.html":{"url":"origin/windows-wakeuponlan.html","title":"主机的网络唤醒WOL服务","keywords":"","body":"主机的网络唤醒服务Wake On Lan 一、简介 为了实现远程控制主机的开关机，现在大部分主板及网卡支持网络接收信号唤醒关机状态的主机的功能。 二、BIOS及网卡配置 1、BIOS设置 确保在 BIOS 中的电源管理设置下启用 WOL。 确保在 BIOS 中禁用深度睡眠（不适用于所有系统）。此节能设置会关闭 NIC。 确认系统电源关闭时链路指示灯保持亮起状态。如果没有链路指示灯，则 NIC 无法接收到用于唤醒系统的魔术包。 2、网卡配置允许接收魔幻数据包 注意无线网卡可能无法接收魔幻数据包，所以不支持网络唤醒 Windows 有的网卡需要更新最新驱动才能看到电源管理配置 Linux # 查看网卡是否支持网络唤醒 ethtool eth0 Supports Wake-on: g Wake-on: g # 如果显示g,表示wol功能已经开启 # 使用ethtool设置网卡接受什么形式的网络唤醒 ethtool -s eth0 wol g a -- wake on ARP b -- wake on broadcast messages d -- disable (wake on nothing) f -- wake on filter(s) g -- wake on MagicPacket(tm) m -- wake on multicast messages p -- wake on phy activity s -- enable SecureOn(tm) password for MagicPacket(tm) u -- wake on unicast messages 三、唤醒工具 确认发送魔术包的系统可以对客户端系统执行 ping 命令。 确认魔术包中使用的 MAC 地址与客户端系统上用于以太网 1 的 MAC 地址相匹配。 如果您在魔术包中指定了 IP 地址，则网络交换机可能无法将其正确广播到整个网络。您可能需要更改地址，以将该包广播到整个网络。例如，如果客户端地址为 192.168.1.12，则该包中使用的广播地址将是 192.168.1.255。 命令行 MAOS：brew install wakeonlan Linux：apt/yum install -y wakeonlan 由于主机在关机状态，其他主机无法通过arp协议主动探测其IP地址与MAC地址的映射关系，所以先手动添加ARP记录，再使用唤醒工具发送魔幻数据包。 arp -s IP地址 MAC地址 wakeonlan命令有个zsh插件wakeonlan，创建~/.wakeonlan/Test文件，写入MAC地址 IP地址，即可使用wake Test快捷命令唤醒主机。或者手动使用以下命令 wakeonlan -i 192.168.1.9 -p 9 AA:BB:CC:DD:EE:FF 手机APP IOS：RemoteBoot Python脚本 #!/usr/bin/env python import socket import sys if len(sys.argv) (example: 192.168.1.255 00:11:22:33:44:55)\" sys.exit(1) mac = sys.argv[2] data = ''.join(['FF' * 6, mac.replace(':', '') * 16]) sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1) sock.sendto(data.decode(\"hex\"), (sys.argv[1], 9)) python wake.py 192.168.1.255 00:11:22:33:44:55 Synology NAS 使用IPKGui搜索下载wakelan，默认安装在/opt/bin/路径下 四、魔幻数据包 下图是使用Wireshark抓到的魔幻数据包 参考 https://apple.stackexchange.com/questions/95246/wake-other-computers-from-mac-osx Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-25 14:03:41 "},"origin/macos-tips.html":{"url":"origin/macos-tips.html","title":"MacOS小技巧","keywords":"","body":"MacOS小技巧 1、DMG 格式文件制作以及 ISO 转换互转 DMG 格式是 Mac OS X 中常用的打包格式 创建 DMG 格式的文件 $ hdiutil create -size 100M -stdinpass -format UDZO -srcfolder folder_to_compress archive_name.dmg UDZO（压缩格式，默认） UDRO（只读格式） UDBZ（Better compressed image） UDRW（可读写格式） UDTO（DVD 格式） 修改 DMG 文件的大小 $ hdiutil resize 150M /path/to/the/diskimage 修改 DMG 格式中的加密口令 $ hdiutil chpass /path/to/the/diskimage 挂载 DMG 格式的文件 $ hdiutil attach archive_name.dmg 它的挂载点在 /Volumes 目录的同名目录下 $ ls -lah /Volumes/archive_name/ 卸载 DMG 文件 $ hdiutil eject /Volumes/archive_name/ 将 ISO 格式的文件转为 DMG 格式的文件 $ hdiutil convert /path/imagefile.iso -format UDRW -o /path/convertedimage.dmg 将 DMG 格式的文件转为 ISO 格式的文件 $ hdiutil convert /path/imagefile.dmg -format UDTO -o /path/convertedimage.cdr $ hdiutil makehybrid /path/convertedimage.cdr -iso -joliet -o /path/convertedimage.iso 2、删除虚拟网络设备 sudo ifconfig utun3 delete 3、路由修改 # 删除路由 ip route delete 172.16.1.2/32 # 添加路由 sudo route add 172.16.1.2/32 -interface utun2 4、HomeBrew安装使用 ①简介 brew 是从下载源码解压然后 ./configure && make install formula：定义了一个软件包。包括了这个软件的，依赖、源码位置及编译方法等 tap：一个包含 formula 的 git 仓库 cask：homebrew 的一个扩展仓库，用来安装 一些带界面的应用软件，下载好后会自动安装 bottle：homebrew 提供的已经编译好的 formula。这些 bottle 可以在这里看到。在大部分的情况下，执行 Homebrew路径 cd \"$(brew --repo)\" Cask仓库路径 cd \"$(brew --repo)\"/Library/Taps/homebrew/homebrew-cask Core仓库路径 cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" ②安装 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" ③使用 # 安装一个包 brew install # 搜索一个包 brew search brew search /正则表达式/ # 标准格式 brew search /^vi/ #规定了只能是vi开头 brew search /^vi\\\\w$/ # 查看这个包的信息 brew info # 安装图形化的软件 brew cask install # 卸载对应包名字 brew uninstall # 列出过时的包 brew outdated # 更新过时的包，不带包名就跟新所有包 brew upgrade [ package_name ] # 更新HomeBrew自身 brew update # 清除缓存 brew cleanup [包名] # 列出已经安装的包 brew list # 查看homebrew 的配置 brew config # 添加或者删除仓库 brew [un]tap ④使用国内的镜像源 中科大 # Homebrew 源代码仓库 cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git # Homebrew 核心软件仓库 cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git # Homebrew cask 软件仓库，提供 macOS 应用和大型二进制文件 cd \"$(brew --repo)\"/Library/Taps/homebrew/homebrew-cask git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git # Homebrew cask 其他版本 (alternative versions) 软件仓库，提供使用人数多的、需要的版本不在 cask 仓库中的应用。 cd \"$(brew --repo)\"/Library/Taps/homebrew/homebrew-cask-versions git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask-versions.git # Homebrew 预编译二进制软件包 echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' >> ~/.zshrc source ~/.zshrc 5、HomeBrew的备份恢复 ①简介 homebrew-bundle - https://github.com/Homebrew/homebrew-bundle Mac 上非常常用的包管理器 Homebrew, 我们经常用它来安装其他的软件包 还有 Homebrew-cask, 可以用来安装图形界面的 App homebrew-bundle 类似 node 中的 package.json 或者 Cocoapods 中的 Podfile 我们将需要的包和 App, 声明在一个 Brewfile 中, 然后执行 brew bundle 即可安装所有包 ②备份内容 brew tap 中的软件库 brew 安装的命令行工具 brew cask 安装的 App Mac App Store 安装的 App ③备份命令 # 执行 brew bundle dump brew bundle dump --describe --force --file=\"~/Desktop/Brewfile\" # 参数说明 --describe：为列表中的命令行工具加上说明性文字。 --force：直接覆盖之前生成的 Brewfile 文件。如果没有该参数，则询问你是否覆盖。 --file=\"~/Desktop/Brewfile\"：在指定位置生成文件。如果没有该参数，则在当前目录生成 Brewfile 文件。 生成的Brewfile 文件内容 ## 该部分是 brew 中的 tap，相当于一个个软件库 tap \"homebrew/bundle\" tap \"homebrew/cask\" ## 该部分是 brew 安装的命令行工具 # Mac App Store command-line interface brew \"mas\" # UNIX shell (command interpreter) brew \"zsh\" # Fish shell like syntax highlighting for zsh brew \"zsh-syntax-highlighting\" ## 该部分是 brew cask 安装的 app cask \"mounty\" cask \"dteoh/sqa/slowquitapps\" ## 该部分是 Mac App Store 安装的 app mas \"ting_en\", id: 734383760 mas \"Xcode\", id: 497799835 ④恢复命令 brew install mas # 批量安装软件 brew bundle --file=\"~/Desktop/Brewfile\" 参考： https://wsgzao.github.io/post/homebrew-bundle/ 6、使用Brew安装的软件信息 ①MySQL 日志和底层DB数据文件: /usr/local/var/mysql bin文件路径：/usr/local/Cellar/mysql@mysql版本/mysql版本 brew 启动命令：brew services restart mysql@mysql版本 ②Nginx 主配置文件路径：/usr/local/etc/nginx/nginx.conf bin文件路径: /usr/local/Cellar/nginx/nginx版本号/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-14 17:38:07 "},"origin/linux-小技巧.html":{"url":"origin/linux-小技巧.html","title":"Linux小技巧","keywords":"","body":"1、Linux SSH安全设置 只允许某用户从指定IP地址登陆 sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\ systemctl restart sshd 修改会话保持时间 #ClientAliveInterval 0 #ClientAliveCountMax 3 修改成 ClientAliveInterval 30 #（每30秒往客户端发送会话请求，保持连接） ClientAliveCountMax 3 #（去掉注释即可，3表示重连3次失败后，重启SSH会话） 增加ssh登陆的验证次数 MaxAuthTries 20 允许root用户登录 sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config ;\\ systemctl restart sshd 设置登录方式 #AuthorizedKeysFile .ssh/authorized_keys //公钥公钥认证文件 #PubkeyAuthentication yes //可以使用公钥登录 #PasswordAuthentication no //不允许使用密码登录 2、bash不显示路径 命令行会变成-bash-3.2$主要原因可能是用户主目录下的配置文件丢失 # 方式一 cp -a /etc/skel/. ~ # 方式二 echo \"export PS1='[\\u@\\h \\W]\\$'\" >> ~/.bash_profile ;\\ source ~/.bash_profile 3、同时监控多个文件 tail -f file1 file2 4、查看网卡 # 方式一 ifconfig -a # 方式二 cat /proc/net/dev 5、cp目录下的带隐藏文件的子目录 cp -R /home/test/* /tmp/test /home/test下的隐藏文件都不会被拷贝，子目录下的隐藏文件倒是会的 cp -R /home/test/. /tmp/test cp的时候有重复的文件需要覆盖时会让不停的输入yes来确认，可以使用yes| yes|cp -r /home/test/. /tmp/test 6、获取出口IP地址 curl http://members.3322.org/dyndns/getip curl https://ip.cn curl cip.cc curl myip.ipip.net curl ifconfig.me 7、ISO自动挂载 echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh 8、查看系统版本号和内核信息 cat /proc/version uname -a lsb_release -a cat /etc/redhat-release cat /etc/issue rpm -q redhat-release 9、查看物理CPU个数、核数、逻辑CPU个数 CPU总核数 = 物理CPU个数 每颗物理CPU的核数 总逻辑CPU数 = 物理CPU个数 每颗物理CPU的核数 * 超线程数 # 查看CPU信息（型号） cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c # 查看物理CPU个数 cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l # 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep \"cpu cores\"| uniq # 查看逻辑CPU的个数 cat /proc/cpuinfo| grep \"processor\"| wc -l 10、Linux缓存 cached是cpu与内存间的，buffer是内存与磁盘间的，都是为了解决速度不对等的问题。buffer是即将要被写入磁盘的，而cache是被从磁盘中读出来的 buff：作为buffer cache的内存，是块设备的读写缓冲区 cache：作为page cache的内存，文件系统的cache。Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中。 pagecache：页面缓存（pagecache）可以包含磁盘块的任何内存映射。这可以是缓冲I/O，内存映射文件，可执行文件的分页区域——操作系统可以从文件保存在内存中的任何内容。Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。 dentries：表示目录的数据结构 inodes：表示文件的数据结构 #内核配置接口 /proc/sys/vm/drop_caches 可以允许用户手动清理cache来达到释放内存的作用，这个文件有三个值：1、2、3（默认值为0） #释放pagecache echo 1 > /proc/sys/vm/drop_caches #释放dentries、inodes echo 2 > /proc/sys/vm/drop_caches #释放pagecache、dentries、inodes echo 3 > /proc/sys/vm/drop_caches 11、设置代理 $> bash -c 'cat >> /etc/profile 注意： 当使用“export http_proxy”和“export https_proxy”设置代理时，curl默认所有的请求都是走的代理，请求域名不通过/etc/hosts解析。 所以当有需求curl命令不走代理，通过/etc/hosts解析时，代理设置要通过“export HTTP_PROXY”和“export HTTPS_PROXY”设置。（原因是url.c（版本7.39中的第4337行）处看先检查小写版本，如果找不到，则检查大写。链接：https://stackoverflow.com/questions/9445489/performing-http-requests-with-curl-using-proxy） no_proxy不支持模糊匹配。不支持*.a.com，支持.a.com 12、查看网卡UUID nmcli con | sed -n '1,2p' 13、时间戳与日期 日期与时间戳的相互转换 #将日期转换为Unix时间戳 date +%s #将Unix时间戳转换为指定格式化的日期时间 date -d @1361542596 +\"%Y-%m-%d %H:%M:%S\" date日期操作 date +%Y%m%d #显示前天年月日 date -d \"+1 day\" +%Y%m%d #显示前一天的日期 date -d \"-1 day\" +%Y%m%d #显示后一天的日期 date -d \"-1 month\" +%Y%m%d #显示上一月的日期 date -d \"+1 month\" +%Y%m%d #显示下一月的日期 date -d \"-1 year\" +%Y%m%d #显示前一年的日期 date -d \"+1 year\" +%Y%m%d #显示下一年的日期 获得毫秒级的时间戳 在linux Shell中并没有毫秒级的时间单位，只有秒和纳秒。其实这样就足够了，因为纳秒的单位范围是（000000000..999999999），所以从纳秒也是可以的到毫秒的 current=`date \"+%Y-%m-%d %H:%M:%S\"` #获取当前时间，例：2015-03-11 12:33:41 timeStamp=`date -d \"$current\" +%s` #将current转换为时间戳，精确到秒 currentTimeStamp=$((timeStamp*1000+`date \"+%N\"`/1000000)) #将current转换为时间戳，精确到毫秒 echo $currentTimeStamp 14、nohup手动后台运行进程并记录进程号 nohup jar -jar jar包 /data/app/logs/app.log 2>&1 && echo $! > /data/app/run.pid # 2>&1是把标准错误2重定向到标准输出1中，而标准输出又导入文件里面，所以标准错误和标准输出都会输出到文件。 # 同时把启动的进程号pid输出到文件 注意： 如果运行时的shell为zsh，将任务放置后台的命令由”&“变为”&!“。例如：nohup jar -jar jar包 /data/app/logs/app.log 2>&1 &! && echo $! > /data/app/run.pid 参考：https://stackoverflow.com/questions/19302913/exit-zsh-but-leave-running-jobs-open 15、生成文件的MD值 在网络传输、设备之间转存、复制大文件等时，可能会出现传输前后数据不一致的情况。这种情况在网络这种相对更不稳定的环境中，容易出现。那么校验文件的完整性，也是势在必行的。 在网络传输时，我们校验源文件获得其md5sum，传输完毕后，校验其目标文件，并对比如果源文件和目标文件md5 一致的话，则表示文件传输无异常。否则说明文件在传输过程中未正确传输。 md5值是一个128位的二进制数据，转换成16进制则是32（128/4）位的进制值。 md5校验，有很小的概率不同的文件生成的md5可能相同。比md5更安全的校验算法还有SHA*系列的。 Linux的md5sum命令 md5sum命令用于生成和校验文件的md5值。它会逐位对文件的内容进行校验。是文件的内容，与文件名无关，也就是文件内容相同，其md5值相同。 #md5sum命令的详解 $> md5sum --h Usage: md5sum [OPTION]... [FILE] With no FILE, or when FILE is -, read standard input. -b, --binary 二进制模式读取文件 -c, --check 从文件中读取、校验MD5值 --tag 创建一个BSD-style风格的校验值 -t, --text 文本模式读取文件（默认） #校验文件MD5值使用的参数 The following four options are useful only when verifying checksums: --quiet don't print OK for each successfully verified file --status don't output anything, status code shows success --strict exit non-zero for improperly formatted checksum lines -w, --warn warn about improperly formatted checksum lines --help display this help and exit --version output version information and exit #生成的MD5值重定向到文件中 $>md5sum filename > filename.md5 #生成的MD5值重定向追加到文件中 $> md5sum filename >>filename.md5 #多个文件输出到一个md5文件中，这要使用通配符* $> md5sum *.iso > iso.md5 #同时计算多个文件的MD5值 $> md5sum filetohashA.txt filetohashB.txt filetohashC.txt > hash.md5 #校验MD5:把下载的文件file和该文件的file.md5报文摘要文件放在同一个目录下 $> md5sum -c file.md5 #创建一个BSD风格的校验值 $> md5sum --tag file.md5 MD5 (file.md5) = 9192e127b087ed0ae24bb12070f3051a Python生成MD5值 # 方式一：使用md5包 import md5 src = 'this is a md5 test.' m1 = md5.new() m1.update(src) print m1.hexdigest() # 方式二：使用hashlib（推荐） import hashlib m2 = hashlib.md5() m2.update(src) print m2.hexdigest() # 加密常见的问题： 1：Unicode-objects must be encoded before hashing 　　解决方案：import hashlib 　　　　　　　m2 = hashlib.md5() 　　　　　　　m2.update(src．encode('utf-8')) 　　　　　　　print m2.hexdigest() Java生成MD5值 import java.security.MessageDigest; public static void main(String[] args) { String password = \"123456\"; try { MessageDigest instance = MessageDigest.getInstance(\"MD5\");// 获取MD5算法对象 byte[] digest = instance.digest(password.getBytes());// 对字符串加密,返回字节数组 StringBuffer sb = new StringBuffer(); for (byte b : digest) { int i = b & 0xff;// 获取字节的低八位有效值 String hexString = Integer.toHexString(i);// 将整数转为16进制 // System.out.println(hexString); if (hexString.length() 16、添加用户 useradd (选项) （参数） #选项 －c：加上备注文字，备注文字保存在passwd的备注栏中 －d：指定用户登入时的启始目录 －D：变更预设值 －e：指定账号的有效期限，缺省表示永久有效 －f：指定在密码过期后多少天即关闭该账号 －g：指定用户所属的起始群组 －G：指定用户所属的附加群组 －m：自动建立用户的登入目录 －M：不要自动建立用户的登入目录 －n：取消建立以用户名称为名的群组 －r：建立系统账号 －s：指定用户登入后所使用的shell －u：指定用户ID号 17、su 与 sudo su : switch to another user 切换用户 sudo : superuser do 允许用户使用superuser的身份执行命令 su username ：切换为username，需要输入username密码 su : 切换为root用户，需要输入root密码 su - : 切换为root用户，需要输入root密码，且环境变量也改变 su - -c \"command\" ：使用root身份执行命令，完成后即退出root身份 sudo command : 与su -c相似，需要输入当前用户（superuser，/etc/sudoers中指定）密码 sudo su -：使用当前用户密码实现root身份的切换 su - hdfs -c command 切换用户并以某用户的身份去执行一条命令 su - hdfs test.sh 切换用户并以某用户的身份去执行一个shell文件 18、重新开启SELinux 如果在使用setenforce命令设置selinux状态的时候出现这个提示：setenforce: SELinux is disabled。那么说明selinux已经被彻底的关闭了,如果需要重新开启selinux vi /etc/selinux/config 更改为：SELINUX=1 必须重启linux，不重启是没办法立刻开启selinux的 重启完以后，使用getenforce,setenforce等命令就不会报“setenforce: SELinux is disabled”了。这时，我们就可以用setenforce命令来动态的调整当前是否开启selinux。 19、检查软件是否已安装，没有就自动安装 rpm -qa |grep \"jq\" if [ $? -eq 0 ] ;then echo \"jq hava been installed \" else yum -y install epel-release && yum -y install jq fi 20、使用privoxy代理http，https流量使用socket连接ShadowSocks服务器 echo \"安装ShadowSocks\" && \\ yum -y install epel-release && yum -y install python-pip && pip install shadowsocks && \\ bash -c 'cat > /etc/shadowsocks.json /etc/systemd/system/shadowsocks.service > /etc/profile && \\ echo \"export https_proxy=http://127.0.0.1:8118\" >> /etc/profile && \\ source /etc/profile && \\ curl www.google.com 21、批量打通指定主机SSH免密钥登录脚本 CentOS $> bash -c 'cat > ./HitthroughSSH.sh ./hosts.txt 22、硬盘自动分区，格式化，开机自动挂载到/data disk=/dev/sdc;\\ bash -c \"fdisk ${disk}>/etc/fstab ;\\ sed -i '$ s/$/ \\/data ext4 defaults 0 0/' /etc/fstab ;\\ mkdir /data ;\\ mount -a ;\\ df -h 23、在hosts文件中添加IP地址与主机名的域名映射 ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 24、禁用透明大页 Redhat sed -i '$a echo nerver > /sys/kernel/mm/redhat_transparent_hugepage/defrag\\necho nerver > /sys/kernel/mm/redhat_transparent_hugepage/enabled' CentOS echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled ;\\ sed -i '/GRUB_CMDLINE_LINUX/ s/\"$/ transparent_hugepage=never\"/' /etc/default/grub ;\\ grub2-mkconfig -o /boot/grub2/grub.cfg 25、安装JDK环境 Prerequisite： JDK安装包已下载在内网HTTP服务器中 curl -# http://192.168.1.7:32770/repository/public-resources/jdk-8u241-linux-x64.tar.gz | tar -zxC /opt/ && \\ ln -s `ls /opt |grep jdk1.8.0_241*| sed \"s:^:/opt: \"` /opt/jdk && \\ sed -i '$a export JAVA_HOME=/opt/jdk\\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\\nexport PATH=$PATH:$JAVA_HOME/bin' /etc/profile && \\ source /etc/profile && \\ ln -s /opt/jdk/bin/java /usr/bin/java && \\ java -version && \\ javac -version 26、安装Tomcat，并由systemctl托管 Prerequisite： 已安装JDK Tomcat安装包已下载在内网HTTP服务器中 wget http://192.168.1.2/tomcat/apache-tomcat-8.5.20.tar.gz;\\ tar -zxvf apache-tomcat-8.5.20.tar.gz -C /opt;\\ rm -rf apache-tomcat-8.5.20.tar.gz;\\ ln -s /opt/apache-tomcat-8.5.20 /opt/tomcat;\\ bash -c 'cat > /lib/systemd/system/tomcat.service 27、安装Nginx bash -c 'cat > /etc/yum.repos.d/nginx.repo 28、安装最新stable单机Zookeeper Prerequisite： 已安装JDK download_url=`echo https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/``curl -s -L https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/ |grep apache-zookeeper | awk -F \">\" '{print $2}'|awk -F \"\\\"\" '{print $2}' | head -n 1` && \\ curl -# $download_url | tar -zxC /opt/ && \\ ln -s `ls /opt/ |grep apache-zookeeper-* | sed \"s:^:/opt/: \"` /opt/zookeeper && \\ sed -i '$a export ZOOKEEPER_HOME=/opt/zookeeper\\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin' /etc/profile && \\ source /etc/profile && \\ cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg && \\ sed -i -e '/dataDir/d' -e '/dataLogDir/d' /opt/zookeeper/conf/zoo.cfg && \\ sed -i -e '$a dataDir=/data/zookeeper/data\\ndataLogDir=/data/zookeeper/logs\\nserver.1=127.0.0.1:2888:3888\\nautopurge.purgeInterval=24\\nautopurge.purgeInterval=5\\nadmin.enableServer=true\\nadmin.enableServer=true admin.serverPort=9990' /opt/zookeeper/conf/zoo.cfg && \\ mkdir -p /data/zookeeper/{data,logs} && \\ echo \"1\" > /data/zookeeper/data/myid && \\ zkServer.sh start && \\ zkServer.sh status && \\ jps -l # admin server 访问地址：http://主机IP地址:9990/commands 29、安装最新stable单机的Kafka Prerequisite： 已安装Zookeeper download_d=`echo https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/``curl -sL https://mirrors.tuna.tsinghua.edu.cn/apache/kafka |grep \\`date +%Y\\` |grep \"folder.gif\" | tac | head -n 1 |awk -F \">\" '{print $3}' |awk -F \"/\" '{print $1}'` && \\ download_url=`echo $download_d/``curl -sL $download_d |grep kafka_ | tac | head -n 1 | awk -F \">\" '{print $2}' | awk -F \"\\\"\" '{print $2}'` && \\ curl -# $download_url | tar -zxC /opt/ && \\ ln -s `ls /opt |grep kafka_*| sed \"s:^:/opt/: \"` /opt/kafka && \\ sed -i '$a export KAFKA_HOME=/opt/kafka\\nexport PATH=$PATH:$KAFKA_HOME/bin' /etc/profile && \\ source /etc/profile && \\ cp /opt/kafka/config/server.properties /opt/kafka/config/server_bak.properties && \\ sed -i '/\\#\\ Log\\ directory\\ to\\ use/iLOG_DIR=\\/data\\/kafka\\/logs' /opt/kafka/bin/kafka-run-class.sh && \\ sed -i -e 's/log.dirs=\\/tmp\\/kafka-logs/log.dirs=\\/data\\/kafka\\/data/g' -e 's/log.retention.hours=168/log.retention.hours=12/g' -e '$a auto.create.topics.enable=true\\ndelete.topic.enable=true' /opt/kafka/config/server.properties && \\ mkdir -p /data/kafka/{logs,data} && \\ kafka-server-start.sh -daemon /opt/kafka/config/server.properties && \\ jps -l 30、安装Hadoop客户端 以hadoop 2.8.3版本为例 wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz ;\\ tar -xvf hadoop-2.8.3.tar.gz -C /opt ;\\ rm -rf hadoop-2.8.3.tar.gz ;\\ ln -s /opt/hadoop-2.8.3 /opt/hadoop ;\\ sed -i '$a export HADOOP_HOME=/opt/hadoop\\nexport PATH=$PATH:$HADOOP_HOME/bin' /etc/profile ;\\ source /etc/profile #然后在/opt/hadoop-2.8.3/etc/hadoop/core-site.xml配置文件标签中填写HDFS NameNode节点的IP地址及端口号 fs.default.name hdfs://172.16.3.10:9000 hdfs dfs -ls / 31、安装Maven环境 curl https://mirrors.tuna.tsinghua.edu.cn/apache/maven/binaries/apache-maven-3.2.2-bin.tar.gz -o /opt/apache-maven-3.2.2-bin.tar.gz && \\ tar -zxvf /opt/apache-maven-*.tar.gz -C /opt/ && \\ rm -rf /opt/apache-maven-*.tar.gz && \\ ln -s /opt/apache-maven-3.2.2 /opt/maven && \\ sed -i '$a export M2_HOME=/opt/maven\\nexport PATH=$PATH:$M2_HOME/bin' /etc/profile && \\ source /etc/profile && \\ mvn version 32、安装NodeJS环境 wget https://nodejs.org/dist/v8.9.4/node-v8.9.4-linux-x64.tar.xz ;\\ tar -xvf node-v8.9.4-linux-x64.tar.xz -C /opt/ ;\\ rm -rf node-v8.9.4-linux-x64.tar.xz ;\\ ln -s /opt/node-v8.9.4-linux-x64 /opt/nodejs ;\\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile;\\ source /etc/profile;\\ yum install gcc-c++ make -y;\\ npm config set registry https://registry.npm.taobao.org ;\\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ ;\\ npm version 33、安装docker/docker-compose 脚本自动安装 sudo curl -sSL https://get.docker.com | sh CentOS/Redhat 设置新硬盘LVM成docker的数据目录 yum install -y yum-utils epel-rease lvm2 && \\ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo && \\ yum list docker-ce --showduplicates | sort -r && \\ yum install -y docker-ce docker-compose && \\ mkdir /etc/docker && \\ bash -c 'cat > /etc/docker/daemon.json > /etc/fstab && \\ df -mh && \\ systemctl daemon-reload && \\ systemctl enable docker && \\ systemctl start docker && \\ docker info &&\\ docker info |grep \"Insecure Registries:\" -A 4 && \\ ls /var/lib/docker/ Ubuntu apt-get remove docker docker-engine docker.io containerd runc && \\ apt install -y software-properties-common && \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - # X86_64 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" # arm64 sudo add-apt-repository \\ \"deb [arch=arm64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" apt-get update && \\ apt-cache docker-ce && \\ apt-get install -y docker-ce && \\ touch /etc/docker/daemon.json && \\ bash -c ' tee /etc/docker/daemon.json 34、字符转换命令expand/unexpand 用于将文件的制表符（Tab）转换为空格符（Space），默认一个Tab对应8个空格符，并将结果输出到标准输出。若不指定任何文件名或所给文件名为”-“，则expand会从标准输入读取数据。 功能与之相反的命令是unexpand，是将空格符转成Tab符。 vi/vim在命令模式下通过设置\":set list\"可显示文件中的制表符“^I” expand命令参数 -i, --initial do not convert tabs after non blanks -t, --tabs=NUMBER have tabs NUMBER characters apart, not 8 -t, --tabs=LIST use comma separated list of explicit tab positions --help display this help and exit --version output version information and exit unexpand命令参数 -a, --all convert all blanks, instead of just initial blanks --first-only convert only leading sequences of blanks (overrides -a) -t, --tabs=N have tabs N characters apart instead of 8 (enables -a) -t, --tabs=LIST use comma separated LIST of tab positions (enables -a) --help display this help and exit --version output version information and exit 实例 将文件中每行第一个Tab符替换为4个空格符，非空白符后的制表符不作转换 #使用\"----\"或\"--\"代表一个制表符，使用\":\"代表一个空格 ----abcd--e $ expand -i -t 4 old-file > new-file ::::abcd--e 注意 不是所有的Tab都会转换为默认或指定数量的空格符，expand会以对齐为原则将Tab符替换为适当数量的空格符，替换的原则是使后面非Tab符处在一个物理Tab边界（即Tab size的整数倍。例如： #使用\"----\"或\"--\"代表一个制表符，使用\":\"代表一个空格 abcd----efg--hi $ expand -t 4 file abcd::::efg::hi 35、修改时区 Docker容器中 添加环境变量：TZ = Asia/Shanghai Linux主机 timedatectl set-timezone \"Asia/Shanghai\" # 设置时区 timedatectl status # 查看当前的时区状态 date -R # 查看时区 或者 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 36、shell脚本的调试 在脚本运行时添加-x参数 在脚本中开头添加set -x 37、删除“-”开头的文件或文件夹 当直接使用rm -f删除以-开头的文件与文件夹时，rm或其他命令报参数错误，会误认为-后面的内容是命令的参数 rm -rf -- -XGET cd -- -XGET 38、硬盘快速分区 方式一：使用parted命令 parted命令详解：https://www.cnblogs.com/Cherry-Linux/p/10103172.html disk=/dev/vdb && \\ parted -s -a optimal $disk mklabel gpt -- mkpart primary ext4 1 -1 方式二：使用fdisk disk=/dev/vdb && \\ bash -c \"fdisk ${disk} 39、别名传参 别名并不能直接传参，但是可以使用以下方式代替： 方式一：使用functions替代 $ test () { num=${1:-5} dmesg |grep -iw usb|tail -$num } $ test 5 方式二：使用read读取输入，然后使用变量替换命令中的参数 $ alias taila='{ IFS= read -r line_num && tail -n $line_num /var/logs/message ;} 参考： https://askubuntu.com/questions/626458/can-i-pass-arguments-to-an-alias-command https://www.kutu66.com//ubuntu/article_158110 40、Ubuntu/Debian的镜像源URL字段 Nexus设置apt proxy仓库，代理http://archive.ubuntu.com/ubuntu/ deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-security main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-updates main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-proposed main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-backports main restricted 第一字段，指示包类型。 deb：二进制包 deb-src：源码包 第二字段，指示镜像站点，即「源」！ URL 定位到某个目录，该目录下必有「dists」「pool」两个子目录。如： http://ftp.cn.debian.org/debian/ http://ftp.sjtu.edu.cn/ubuntu/ 第三字段，指示包的「版本类型」，姑且称为「仓库」。 打开某源，进入「dists」子目录可见该源中有哪些仓库，即其下诸子目录。命名形式为「系统发行版名-仓库名」，如 Debian 的「jessie-backports」「stretch-updates」，Ubuntu 的「vivid-updates」「wily-proposed」。无仓库名的即为主仓库。 Debian 的 stable、testing 为链接，指向具体系统发行版，会随时间而变。比如，当前 stable 为 jessie，所以 stable-backports 与 jessie-backports 等效。但本人不建义使用 stable、testing，因为下一个 stable 发布后，你的源便自动指向了一个新版本，然而你并未阅读新版本的发行说明，并未做好升级的准备。 Debian 的仓库自 squeeze 起与 Ubuntu 基本相同。除主仓库外，有： security：Ubuntu 用于指安全性更新。即影响系统安全的 bug 修补。Debian 特殊一些，见下文。 updates：非安全性更新。即不影响到系统安全的 bug 修补。 proposed-updates：预更新。小 beta 版。过后会进入「updates」或「security」。Ubuntu 仅用proposed」，无后缀「updates」。 backports：后备。Debian stable 发布后，Ubuntu 某版本正式发布后，其所有软件版本号便已被冻结，所有软件只修 bug，不增加任何特性。但有人可能需要新特性，甚至某些较新的软件原来根本就没有。该仓库正因此而设，但欠官方维护，且可能在系统正式发布之后过一段时间才有内容。此仓库处于第二优先顺序，而上述几个仓库处于第一优先顺序。安装第二优先顺序的包必须特别指明，见 apt-get(8) aptitude(8) 的 --target-release 选项。 提示：并非所有版本都设有上述全部仓库，请打开源中 dists 目录查看。 后续字段，指示包许可类型。 后续字段排名不分先后，最终结果取其并集。按包本身的许可及所直接依赖的包的许可划分。打开某仓库，可见几个子目录。 Debian 最多有三种 main：本身是自由软件，且所有依赖的包也都是自由软件，此类可称纯自由软件，见 https://www.debian.org/distrib/packages《Debian自由软件指导方针》。 contrib：本身是自由软件，但依赖不纯，即依赖中至少有一例 contrib 或 non-free 者。 non-free：本身并非自由软件，无论依赖如何。当然，该软件是可免费使用或试用的。免费一例 https://packages.debian.org/jessie/unrar，试用xx天一例 https://packages.debian.org/jessie/rar。 Ubuntu 最多有四种 main：官方维护的自由软件。 universe：社区维护的自由软件。 restricted：设备专有驱动。 multiverse：同 Debian 的「non-free」。 某些另类的第三方源，未必遵循上述惯例。总之，打开仓库目录自己看。 特别之处： Debian 安全性更新 不像 Ubuntu 放在「security」仓库，而是放在单独一个源中。各大镜像站通常都把一般的包放在根下来一级的「debian」目录中，而安全性更新则会放在「debian-security」目录中，如果有的话，如 http://ftp.cn.debian.org/debian-security/。 Debian 官方建议，所有安全性更新，只从官方主站更新，勿使用其它镜像站，除非你对镜像站非常有信心，见 https://www.debian.org/security/index.en.html。所以，很多镜像站并不提供安全更新源。 安全性更新的第三字段形式固定为「版本名/updates」，如「wheezy/updates」「jessie/updates」。 Debian 多媒体源 一些多媒体软件因牵涉到版权问题，包括硬件解码器，Debian 官方并未收录，有一网站专门填补该空缺，见 http://www.deb-multimedia.org。 最后忠告： 不要同时启用多个源，同一仓库的源启用一个即可，否则容易引起混乱。以下实例便是列有多套而仅启用一套。 参考 https://forum.ubuntu.org.cn/viewtopic.php?t=366506 41、裸磁盘分区扩容 ①停掉向挂载路径写文件的服务或进程 ② 卸载挂载 umount /data 如果提示umount:/data:target is bus,使用fuser找出正在往挂载路径写文件的进程并kill掉，再次卸载挂载 yum install psmisc -y fuser -mv /data USER PID ACCESS COMMAND /data: root kernel mount /data root 13830 ..c.. bash ③修复分区表 磁盘扩大容量后，分区表中记录的柱头等信息需要更新，否则创建新分区时会报GPT PMBR size mismatch parted -l 在弹出Fix/Ignore?的提示时输入Fix后回车即可。 ④删掉旧分区再重建新分区 fdisk /dev/sdb d # 删除原来的分区/dev/sdb1 n # 创建新的分区 1 # 分区号与旧的保持一致 w # 写入分区表并生效 ⑤调整分区 e2fsck -f /dev/sdb1 检查分区信息 resize2fs /dev/sdb1 调整分区大小 ⑥重新挂载并验证数据是否丢失？容量是否扩容？ 42、MacOS下tar归档文件时,排错._*文件 MacOS下的tar命令，在归档压缩文件或文件夹时，会产生._*的隐藏文件()也一并归档到压缩包中，增加压缩包体积。可以在归档时不包含这些文件 COPYFILE_DISABLE=1 tar czf test.tar /your/files # 去除旧压缩包中的“._*”文件 tar -cf newTar --include='some/path/*' oldTar 参考： https://stackoverflow.com/questions/30962501/how-do-i-delete-a-single-file-from-a-tar-gz-archive https://superuser.com/questions/259703/get-mac-tar-to-stop-putting-filenames-in-tar-archives 43、echo 换行 echo -e \"test\\ndasdasd\" > test 44、dd命令 dd 可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。 参数说明: if=文件名：输入文件名，默认为标准输入。即指定源文件。 of=文件名：输出文件名，默认为标准输出。即指定目的文件。 ibs=bytes：一次读入bytes个字节，即指定一个块大小为bytes个字节。 obs=bytes：一次输出bytes个字节，即指定一个块大小为bytes个字节。 bs=bytes：同时设置读入/输出的块大小为bytes个字节。 cbs=bytes：一次转换bytes个字节，即指定转换缓冲区大小。 skip=blocks：从输入文件开头跳过blocks个块后再开始复制。 seek=blocks：从输出文件开头跳过blocks个块后再开始复制。 count=blocks：仅拷贝blocks个块，块大小等于ibs指定的字节数。 conv=，关键字可以有以下11种： conversion：用指定的参数转换文件。 ascii：转换ebcdic为ascii ebcdic：转换ascii为ebcdic ibm：转换ascii为alternate ebcdic block：把每一行转换为长度为cbs，不足部分用空格填充 unblock：使每一行的长度都为cbs，不足部分用空格填充 lcase：把大写字符转换为小写字符 ucase：把小写字符转换为大写字符 swap：交换输入的每对字节 noerror：出错时不停止 notrunc：不截短输出文件 sync：将每个输入块填充到ibs个字节，不足部分用空（NUL）字符补齐。 --help：显示帮助信息 --version：显示版本信息 示例： 刻录ISO镜像到硬盘(u盘) sudo dd if=CentOS-7-x86_64-Minimal-2009.iso of=/dev/disk2 bs=1m # 观察刻录进度 sudo watch kill -USR1 $(pgrep ^dd) # -USR1是dd专用的信号，它接收到该信号，就会显示刻录的进度 # 检查刻录是否结束后 sync # 弹出磁盘 umount /dev/disk2 修复无法格式化的U盘 dd if=/dev/zero of=/dev/sdc bs=512 count=1 文件中英文大小写转换 dd if=testfile_2 of=testfile_1 conv=ucase 将本地的/dev/hdb整盘备份到/dev/hdd dd if=/dev/hdb of=/dev/hdd 将备份文件恢复到指定盘 dd if=/root/image of=/dev/hdb 将备份文件恢复到指定盘 dd if=/root/image of=/dev/hdb 备份/dev/hdb全盘数据，并利用gzip工具进行压缩，保存到指定路径 dd if=/dev/hdb | gzip > /root/image.gz 将压缩的备份文件恢复到指定盘 gzip -dc /root/image.gz | dd of=/dev/hdb 备份磁盘开始的512个字节大小的MBR信息到指定文件 dd if=/dev/hda of=/root/image count=1 bs=512 # count=1指仅拷贝一个块；bs=512指块大小为512个字节。 # 恢复 dd if=/root/image of=/dev/hda 备份软盘 dd if=/dev/fd0 of=disk.img count=1 bs=1440k (即块大小为1.44M) 拷贝内存内容到硬盘 dd if=/dev/mem of=/root/mem.bin bs=1024 (指定块大小为1k) 拷贝光盘内容到指定文件夹，并保存为cd.iso文件 dd if=/dev/cdrom(hdc) of=/root/cd.iso 将/dev/hdb全盘数据备份到指定路径的image文件 dd if=/dev/hdb of=/root/image 销毁磁盘数据 利用随机数据填充硬盘来销毁数据 dd if=/dev/urandom of=/dev/hda1 修复硬盘 当硬盘较长时间(一年以上)放置不使用后，磁盘上会产生magnetic flux point，当磁头读到这些区域时会遇到困难，并可能导致I/O错误。当这种情况影响到硬盘的第一个扇区时，可能导致硬盘报废。 dd if=/dev/sda of=/dev/sda 或dd if=/dev/hda of=/dev/hda 45 、生成随机字符串 # 根据时间戳加随机数计算md5值并取前10位 echo $(date +%s)$RANDOM | md5sum | base64 | head -c 10 head -c 16 /dev/random | base64 openssl rand -hex 10 cat /proc/sys/kernel/random/uuid| cksum |cut -f1 -d\" \" | base64 head -n 5 /dev/urandom |sed 's/[^a-Z0-9]//g'|strings -n 4 tr -dc '_A-Z#\\-+=a-z(0-9%^>)]{ 46、ssh目录的权限问题 .ssh目录的权限应为700 .ssh目录下文件的权限应为600 47、常见包管理器的阿里云镜像源设置 npm npm config set registry https://registry.npm.taobao.org --global npm config set disturl https://npm.taobao.org/dist --global npm config get registry Python mkdir ~/.pip echo -e \"[global]\\nindex-url = https://mirrors.aliyun.com/pypi/simple/\\n[install]\\ntrusted-host=mirrors.aliyun.com\\n\" > ~/.pip/pip.conf 48、使用curl命令发送邮件 curl -s --ssl-reqd --write-out %{http_code} --output /dev/null \\ --url \"smtp://发件人SMTP服务器地址:发件人SMTP服务器端口\" \\ --user \"发件人SMTP服务器用户名:发件人SMTP服务器密码\" \\ --mail-from 发件人邮箱地址 \\ --mail-rcpt 收件人邮箱地址 \\ --upload-file /tmp/emai-data.txt # /tmp/emai-data.txt的内容 FROM: 发件人邮箱地址 To: 收件人邮箱地址 CC: 抄送人邮箱地址 Subject: 主题 Content-Type: multipart/mixed; boundary=MixedBoundary --MixedBoundary Content-Type: multipart/related; boundary=AlternativeBoundary --AlternativeBoundary Content-Type: multipart/related; boundary=RelatedBoundary --RelatedBoundary Content-Type: text/html; charset=utf-8 测试 --RelatedBoundary-- --AlternativeBoundary-- --MixedBoundary Content-Type: text/plain; name=test.txt Content-Transfer-Encoding: base64 Content-Disposition: attachment; filename=test.txt [base64编码的附件内容] --MixedBoundary-- 参考： https://skeletonkey.com/filemaker-18-smtp-curl/ https://www.soliantconsulting.com/blog/html-email-filemaker/ https://stackoverflow.com/questions/44728855/curl-send-html-email-with-embedded-image-and-attachment 49、split按行或大小切割大文件 split命令 可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。 选项 -a, --suffix-length=N 指定后缀长度(默认为2) --additional-suffix=SUFFIX append an additional SUFFIX to file names -b, --bytes=SIZE put SIZE bytes per output file -C, --line-bytes=SIZE put at most SIZE bytes of lines per output file -d, --numeric-suffixes[=FROM] 使用数字作为后缀(默认起始值为0) -e, --elide-empty-files do not generate empty output files with '-n' --filter=COMMAND write to shell COMMAND; file name is $FILE -l, --lines=NUMBER 值为每一输出档的行数大小。 -n, --number=CHUNKS generate CHUNKS output files; see explanation below -u, --unbuffered immediately copy input to output with '-n r/...' --verbose 在每个输出文件打开前输出文件特征 --help 显示此帮助信息并退出 --version 显示版本信息并退出 SIZE is an integer and optional unit (example: 10M is 10*1024*1024). Units are K, M, G, T, P, E, Z, Y (powers of 1024) or KB, MB, ... (powers of 1000). CHUNKS may be: N split into N files based on size of input K/N output Kth of N to stdout l/N split into N files without splitting lines l/K/N output Kth of N to stdout without splitting lines r/N like 'l' but use round robin distribution r/K/N likewise but only output Kth of N to stdout 实例 使用split命令将date.file文件分割成大小为10KB的小文件： # split -b 10k date.file date.file xaa xab xac xad xae xaf xag xah xai xaj 文件被分割成多个带有字母的后缀文件，如果想用数字后缀可使用-d参数，同时可以使用-a length来指定后缀的长度： # split -b 10k date.file -d -a 3 date.file x000 x001 x002 x003 x004 x005 x006 x007 x008 x009 为分割后的文件指定文件名的前缀： # split -b 10k date.file -d -a 3 split_file date.file split_file000 split_file001 split_file002 split_file003 split_file004 split_file005 split_file006 split_file007 split_file008 split_file009 使用-l选项根据文件的行数来分割文件，例如把文件分割成每个包含10行的小文件： split -l 10 date.file 50、journalctl查看内核/应用日志 Systemd统一管理所有Unit的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。该工具是从message这个文件里读取信息。 ①查看所有日志 journalctl ②查看内核日志（不显示应用日志） journalctl -k ③查看系统本次启动的日志 # 查看系统本次启动的日志 journalctl -b journalctl -b -0 # 查看上一次启动的日志 需更改设置,如上次系统崩溃，需要查看日志时，就要看上一次的启动日志。 journalctl -b -1 ④查看指定时间的日志 journalctl --since=\"2018-10-3018:17:16\" journalctl --since \"20 minago\" journalctl --since yesterday journalctl --since \"2020-09-23 22:50:00\" --until \"2020-09-23 23:20:00\" journalctl --since 09:00 --until\"1 hour ago\" journalctl --since\"15:15\" --until now ⑤显示尾部的最新10行日志 journalctl -n ⑥显示尾部指定行数的日志 journalctl -n 20 ⑦实时滚动显示最新日志 journalctl -f ⑧查看某个Unit的日志 journalctl -u nginx.service # 只显示今天的 journalctl -u nginx.service --since today # 实时滚动显示 journalctl -u nginx.service -f # 合并显示多个Unit的日志 journalctl -u nginx.service -u php-fpm.service --since today ⑨指定用户的日志 journalctl _UID=33 --since today ⑩设置/显示日志文件配置 # 显示日志占据的硬盘空间 journalctl --disk-usage # 指定日志文件占据的最大空间 journalctl --vacuum-size=1G # 指定日志文件保存多久 journalctl --vacuum-time=1years 51、read提示字符中的换行 read -p $'第一行内容\\n第二行内容:' 变量 52、nmcli命令行/numtui字符界面管理网络 参考：https://www.cnblogs.com/liuhedong/p/10695969.html 通常用 con 关键字替换 connection，并用 mod 关键字替换 modify nmtui 是一个基于文本用户界面的，用于控制网络的管理器，当我们执行 nmtui 时，它将打开一个基于文本的用户界面，通过它我们可以添加、修改和删除连接。除此之外，nmtui 还可以用来设置系统的主机名。 安装命令 yum install NetworkManager NetworkManager-tui # 或者 apt install network-manager ①显示网络管理器的整体状态 nmcli general status ②查看网卡设备 $ nmcli dev DEVICE TYPE STATE CONNECTION wlan0 wifi connected **** eth0 ethernet unmanaged -- lo loopback unmanaged -- tun0 tun unmanaged -- p2p-dev-wlan0 wifi-p2p unmanaged -- ③查看附近的WIFI网络 $ nmcli d wifi list IN-USE BSSID SSID MODE CHAN RATE SIGNAL BARS SECURITY * CC:2D:21:4B:53:81 Stark-Industries Infra 4 270 Mbit/s 100 ▂▄▆█ WPA1 WPA2 E8:3F:67:FF:2A:42 HUAWEI-忆 Infra 6 130 Mbit/s 60 ▂▄▆_ WPA2 90:47:3C:3E:32:D1 CMCC-VzjQ Infra 7 130 Mbit/s 60 ▂▄▆_ WPA1 WPA2 E8:3F:67:FF:2A:47 -- Infra 6 130 Mbit/s 57 ▂▄▆_ WPA2 E8:3F:67:FF:2A:44 666666 Infra 6 130 Mbit/s 57 ▂▄▆_ WPA2 8C:FD:18:4A:79:74 CMCC-GNTn Infra 9 130 Mbit/s 55 ▂▄__ WPA1 WPA2 8C:FD:18:4A:79:78 CMCC-GNTn-5G Infra 36 270 Mbit/s 52 ▂▄__ WPA1 WPA2 ④连接WIFI $ nmcli d wifi connect password ⑤连接隐藏WIFI $ nmcli c add type wifi con-name ifname wlan0 ssid $ nmcli c modify wifi-sec.key-mgmt wpa-psk wifi-sec.psk $ nmcli c up ⑥查看网络设备连接状态 nmcli connection show nmcli connection show --active # 以活动的连接进行排序 nmcli connection show --order +active # 将所有连接以名称排序 nmcli connection show --order +name # 将所有连接以类型排序(倒序) nmcli connection show --order -type ⑦固定IP地址 # 列出当前活动的连接 nmcli connection # 固定IP地址 nmcli con mod ipv4.addresses 192.168.1.4/24 # 设置网关 nmcli con mod ipv4.gateway 192.168.1.1 # 设置手动获取IP，不使用DHCP nmcli con mod ipv4.method manual # 设置DNS nmcli con mod ipv4.dns \"8.8.8.8\" # 生效配置 nmcli con up 53、对bash执行curl的脚本进行传参 curl http://test.com/test/test.sh | bash -s arg1 arg2 bash 54、windows下编写的脚本文件，放到Linux中无法识别格式 在Linux中执行.sh脚本，异常/bin/sh^M: bad interpreter: No such file or directory。windows下编写的脚本文件，放到Linux中无法识别格式，在vi的时候,会在下面显示此文件的格式,比如 \"dos.txt\" [dos] 120L, 2532C 字样,表示是一个[dos]格式文件,如果是MAC系统的,会显示[MAC]。dos格式文件传输到unix系统时,会在每行的结尾多一个^M 用vi打开脚本文件，在命令模式下输入set ff=unix 用命令:set ff?可以看到dos或unix的字样 其他工具去除参考：文本处理的第七章节 55、文件编码格式 查看 brew/yum/apt install -y enca enca 文件名 file 文件名 vim中:set fileencoding 转换 # 将GBK编码的文件转换成UTF-8编码 enconv -L zh_CN -x UTF-8 filename # 将UTF-8 编码的文件转换成GBK编码 iconv -f UTF-8 -t GBK file1 -o file2 vim中:set fileencoding=utf-8 56、Linux安装使用SQLServer客户端sqlcmd 安装 # CentOS/RHEL curl https://packages.microsoft.com/config/rhel/8/prod.repo > /etc/yum.repos.d/msprod.repo sudo yum remove mssql-tools unixODBC-utf16-devel sudo yum install mssql-tools unixODBC-devel # Ubuntu/Debian curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add - curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list apt-get update apt-get install mssql-tools unixodbc-dev 使用 sqlcmd -S SERVERNAME,49399 -U User -P pwd -d DatabaseName -Q \"SELECT * FROM Test;\" # 如果执行出现“-bash: !”: event not found\",终端shell设置set +H 参考： https://serverfault.com/questions/208265/what-is-bash-event-not-found https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-tools?view=sql-server-ver15#ubuntu 57、NTP同步时间 Windows系统上自带的两个：time.windows.com 和 time.nist.gov MacOS上自带的两个：time.apple.com 和 time.asia.apple.com NTP授时快速域名服务：cn.ntp.org.cn http://www.ntp.org.cn/ yum install ntp -y && \\ cp /etc/ntp.conf /etc/ntp.conf.bak && \\ ntpdate -u NTP服务器 && \\ sed -i '/^server/d' /etc/ntp.conf && \\ echo -e \"server 内网NTP服务器IP地址\\nserver 外网NTP服务器IP地址\" >> /etc/ntp.conf && \\ systemctl enable ntpd && \\ systemctl start ntpd && \\ systemctl status ntpd && \\ ntpstat NTP服务端配置 yum install ntp -y && \\ mv /etc/ntp.conf /etc/ntp.conf.bak && \\ bash -c 'cat > /etc/ntp.conf NTP客户端配置 yum install ntp -y && \\ mv /etc/ntp.conf /etc/ntp.conf.bak && \\ bash -c 'cat > /etc/ntp.conf NTP常用命令 # 从时间服务器更新系统时间 ntpdate -u NTP服务器 # 查询不更新 ntpdate -q NTP服务器 #查看时间同步状态 ntpstat #列出所有作为时钟源校正过本地NTP服务器时钟的上层NTP服务器的列表 ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== *172.16.0.2 LOCAL(0) 11 u 43 64 377 0.623 26.067 8.290 # remote： 远程NTP服务器的IP地址或域名，带 “*” 的表示本地NTP服务器与该服务器同步。 # refid： 远程NTP服务器的上层服务器的IP地址或域名。 # st： 远程NTP服务器所在的层数。 # t： 本地NTP服务器与远程NTP服务器的通信方式，u: 单播； b: 广播； l: 本地。 # when： 上一次校正时间与现在时间的差值。 # poll： 本地NTP服务器查询远程NTP服务器的时间间隔。 # reach： 是一种衡量前8次查询是否成功的位掩码值，377表示都成功，0表示不成功。 # delay： 网络延时，单位是10的-6次方秒。 # offset： 本地NTP服务器与远程NTP服务器的时间偏移。 # jitter： 查询偏差的分布值，用于表示远程NTP服务器的网络延时是否稳定，单位为10的-6次方秒。 ntpdate -d NTP服务器 常见NTP时间服务器 pool.ntp.org # 中国 cn.ntp.org.cn # 中国香港 hk.ntp.org.cn # 美国 us.ntp.org.cn # 阿里云NTP服务器 ntp.aliyun.com ntp1.aliyun.com ntp2.aliyun.com ntp3.aliyun.com ntp4.aliyun.com ntp5.aliyun.com ntp6.aliyun.com ntp7.aliyun.com # 阿里云Time服务器 time1.aliyun.com time2.aliyun.com time3.aliyun.com time4.aliyun.com time5.aliyun.com time6.aliyun.com time7.aliyun.com # 北京大学 s1c.time.edu.cn s2m.time.edu.cn # 清华大学 s1b.time.edu.cn s1e.time.edu.cn s2a.time.edu.cn s2b.time.edu.cn #苹果提供的授时服务器 time1.apple.com time2.apple.com time3.apple.com time4.apple.com time5.apple.com time6.apple.com time7.apple.com #Google提供的授时服务器 time1.google.com time2.google.com time3.google.com time4.google.com Windows下NTP客户端服务配置 运行对话框输入gpedit.msc进入组策略 依次进入 计算机配置 > 管理模板 > 系统 > Windows时间服务 > 时间提供程序 然后进入 控制面板 > 时钟、语言和区域 > 设置时间和日期 > Internet时间 > 更改设置 58、Yum升级内核 内核下载地址：https://elrepo.org/linux/kernel/ kernel-lt（lt=long-term）长期有效 kernel-ml（ml=mainline）主流版本 安装最新内核 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org && \\ rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm && \\ yum --enablerepo=elrepo-kernel install -y kernel-ml 配置默认内核 # 查看grube启动时当前默认设置的内核 grub2-editenv list # 查看grub2当前支持可启动的内核 awk -F \\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 0 : CentOS Linux (5.11.8-1.el7.elrepo.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.el7.x86_64) 7 (Core) 2 : CentOS Linux (0-rescue-7acaacd4599a461f9540eece4c227d87) 7 (Core) # 设置grube启动时使用最新的内核 grub2-set-default 'CentOS Linux (5.11.8-1.el7.elrepo.x86_64) 7 (Core)' # 再次查看grube启动时当前默认设置的内核 grub2-editenv list # 重启生效 reboot now 更新基础软件 # 更新kernel-ml-devel、kernel-ml-headers、kernel-ml-doc、kernel-tools、perf、kernel-ml-headers yum --enablerepo=elrepo-kernel install -y kernel-ml-devel kernel-ml-headers kernel-ml-doc kernel-tools perf python-perf 59、新增磁盘分区不显示设备 对一个磁盘创建了新分区后，fdisk -l 可以显示，但是不显示在/dev/分区号。使用partprobe重新扫描磁盘分区 partprobe 60、APT（Advanced Packaging Tools） 使用APT的操作系统： Ubuntu Debian 包查询 Debian：https://www.debian.org/distrib/packages APT软件包的类型 Main：自由软件及其源代码 Contrib：本身是自由软件，但是需要依赖一些非自由软件运行 Non-Free：收到许可条例限制的软件 说明文档： Debian：https://www.debian.org/doc/manuals/debian-reference/ch02.zh-cn.html Ubuntu: http://manpages.ubuntu.com/manpages/xenial/man8/apt.8.html 常用软件镜像源 中科大 Ubuntu：https://mirrors.ustc.edu.cn/ubuntu/ Debian：https://mirrors.ustc.edu.cn/debian 清华 Ubuntu：https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ Debian：https://mirrors.tuna.tsinghua.edu.cn/debian/ ①查询软件版本 apt-cache madison 包 ②列出软件的所有来源 apt-cache policy 包 # 或者 apt-cache showpkg 包 ③模拟安装软件 apt-get install -s 包 ④安装testing类型仓库里的软件 echo \"deb https://mirrors.tuna.tsinghua.edu.cn/debian testing main contrib non-free \" >> /etc/apt/sources.list apt-get update Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 16:33:06 "},"origin/linux-diff.html":{"url":"origin/linux-diff.html","title":"比较两个文件的不同","keywords":"","body":"Linux 比较文件的不同 一、diff比较两个文件的不同 语法 diff [-abBcdefHilnNpPqrstTuvwy][-][-C ][-D ][-I ][-S ][-W ][-x ][-X ][--help][--left-column][--suppress-common-line][文件或目录1][文件或目录2] 参数 - 　指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。 -a或--text 　diff预设只会逐行比较文本文件。 -b或--ignore-space-change 　不检查空格字符的不同。 -B或--ignore-blank-lines 　不检查空白行。 -c 　显示全部内文，并标出不同之处。 -C或--context 　与执行\"-c-\"指令相同。 -d或--minimal 　使用不同的演算法，以较小的单位来做比较。 -D或ifdef 　此参数的输出格式可用于前置处理器巨集。 -e或--ed 　此参数的输出格式可用于ed的script文件。 -f或-forward-ed 　输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。 -H或--speed-large-files 　比较大文件时，可加快速度。 -l或--ignore-matching-lines 　若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。 -i或--ignore-case 　不检查大小写的不同。 -l或--paginate 　将结果交由pr程序来分页。 -n或--rcs 　将比较结果以RCS的格式来显示。 -N或--new-file 　在比较目录时，若文件A仅出现在某个目录中，预设会显示： Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。 -p 　若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。 -P或--unidirectional-new-file 　与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。 -q或--brief 　仅显示有无差异，不显示详细的信息。 -r或--recursive 　比较子目录中的文件。 -s或--report-identical-files 　若没有发现任何差异，仍然显示信息。 -S或--starting-file 　在比较目录时，从指定的文件开始比较。 -t或--expand-tabs 　在输出时，将tab字符展开。 -T或--initial-tab 　在每行前面加上tab字符以便对齐。 -u,-U或--unified= 　以合并的方式来显示文件内容的不同。 -v或--version 　显示版本信息。 -w或--ignore-all-space 　忽略全部的空格字符。 -W或--width 　在使用-y参数时，指定栏宽。 -x或--exclude 　不比较选项中所指定的文件或目录。 -X或--exclude-from 　您可以将文件或目录类型存成文本文件，然后在=中指定此文本文件。 -y或--side-by-side 　以并列的方式显示文件的异同之处。 --help 　显示帮助。 --left-column 　在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。 --suppress-common-lines 　在使用-y参数时，仅显示不同之处。 实例 样本数据 test-a |------a asd sdf qaz1 adasd |------c 21312 test-b |------a asd sdf qaz1 adad |------b 12312 23121 3432432 1231 |------c 21312 1、比较两个文件下所有文件(包括子文件)的不同 # diff -yr --suppress-common-lines test-a test-b diff -yr --suppress-common-lines test-a/a test-b/a qaz1 adasd | qaz1 adad Only in test-b: b Files test-a/c and test-b/c are identical # test-a/a文件与test-b/a文件存在差异 # 表示b文件只在test-b文件夹中存在 # test-a/c文件与test-b/c文件完全一致 2、比较两个文件下除指定指定文件外其他文件(包括子文件)的不同 # diff -yr --suppress-common-lines -x .git -x .idea test-a test-b Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-09-27 16:34:29 "},"origin/linux-baseserver.html":{"url":"origin/linux-baseserver.html","title":"基础服务安装","keywords":"","body":"一、NFS 1、安装 yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind 2、配置 vi /etc/exports 要共享的目录 指定客户端IP地址1(权限) 指定客户端IP地址2(权限) /NFS 172.16.2.0/24(ro,rw) /NFS 172.16.2.0/24(ro,rw) 192.168.0.2(ro,rw) 客户端IP地址的指定方式： 指定ip地址的主机：192.168.0.100 指定子网中的所有主机：192.168.0.0/24 或 192.168.0.0/255.255.255.0 指定域名的主机：nfs.test.com 指定域中的所有主机：*.test.com 所有主机：* 权限 ro：共享目录只读； rw：共享目录可读可写； all_squash：所有访问用户都映射为匿名用户或用户组； no_all_squash（默认）：访问用户先与本机用户匹配，匹配失败后再映射为匿名用户或用户组； root_squash（默认）：将来访的root用户映射为匿名用户或用户组； no_root_squash：来访的root用户保持root帐号权限； anonuid=：指定匿名访问用户的本地用户UID，默认为nfsnobody（65534）； anongid=：指定匿名访问用户的本地用户组GID，默认为nfsnobody（65534）； secure（默认）：限制客户端只能从小于1024的tcp/ip端口连接服务器； insecure：允许客户端从大于1024的tcp/ip端口连接服务器； sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性； async：将数据先保存在内存缓冲区中，必要时才写入磁盘； wdelay（默认）：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率； no_wdelay：若有写操作则立即执行，应与sync配合使用； subtree_check（默认） ：若输出目录是一个子目录，则nfs服务器将检查其父目录的权限； no_subtree_check ：即使输出目录是一个子目录，nfs服务器也不检查其父目录的权限，这样可以提高效率； 3、生效配置 exportfs -a 4、客户端配置 ①客户端安装 yum install nfs-utils rpcbind ②手动挂载 mount -t nfs Server_IP:Share_Dir Mount_Dir ③自动挂载 echo \"Server_IP:Share_Dir Mount_dir nfs 权限 0 1\" >> /etc/fstab mount -a ④查看服务端共享的目录 showmount -e Server_IP 二、FTP的虚拟用户 vsftpd服务器同时支持匿名用户、本地用户和虚拟用户三类用户账号，使用虚拟用户账号可以提供集中管理的FTP根目录，方便了管理员的管理，同时将用于FTP登录的用户名、密码与系统用户账号区别开，进一步增强了FTP服务器的安全性。 设置不使用ftp所在主机的系统用户及匿名用户登录 限制登录的虚拟用户只在指定的目录下操作 方便添加虚拟用户 1、安装 yum install -y vsftpd vsftpd -version 2、配置 创建系统用户，所有的虚拟用户都对应到这个用户 useradd -d /data/ftp/ -s /sbin/nologin vftpuser 创建虚拟用户的账号密码文本 sehll >vi /etc/vsftpd/vuser.txt testuser 1234test #奇数行是用户名，偶数是密码 #生成虚拟数据库文件（*如果db_load没有安装，yum install db4-utils db4-devel db4-4.3安装才能使用。） db_load –T –t hash –f /etc/vsftpd/vuser.txt /etc/vsftpd/vuser.db #修改虚拟数据库文件vuser.db的权限为 700 chmod 700 /etc/vsftpd/vuser.db #配置PAM文件，目的是对客户端进行验证.编辑/etc/pam.d/vsftpd文件，注释所有内容，后添加： vi /etc/pam.d/vsftpd auth required pam_userdb.so db=/etc/vsftpd/vuser account required pam_userdb.so db=/etc/vsftpd/vuser #不能写成db=/etc/vsftpd/vuser.db 配置vsftp的配置文件/etc/vsftpd/vsftpd.conf anonymous_enable=NO #是否允许匿名用户登录 local_enable=YES #是否允许vsftp所在主机的本地用户登录 listen=YES listen_ipv6=NO guest_enable=YES #激活虚拟账户 guest_username=vftpuser #把虚拟账户绑定为系统账户vftpuser pam_service_name=vsftpd #使用PAM验证，指定PAM配置文件，文件已经在/etc/pam.d/存在（第二步配置的） user_config_dir=/etc/vsftpd/vsftpd_user_conf #设置虚拟用户的配置文件目录，配置文件名与虚拟用户名同名 write_enable=NO 配置虚拟用户的配置文件/etc/vsftpd/vsftpd_user_conf/testuser anon_world_readable_only=NO #浏览FTP目录 anon_upload_enable=YES #允许上传 anon_mkdir_write_enable=YES #建立和删除目录 anon_other_write_enable=YES #改名和删除文件 local_root=/data/ftp/test #指定虚拟用户在系统用户下面的路径，限制虚拟用户家目录，虚拟用户登录后主目录 write_enable=YES #启用/禁止用户的写权限 allow_writeable_chroot=YES download_enable=NO #设置只能上传不能下载 cmds_denied=DELE #禁用掉删除DELE命令 #每行配置项结尾不能有空格 创建配置文件中设置的目录并设置相关权限 mkdir -p /data/ftp/test ;\\ chown -R vftpuser:vftpuser /data/ftp/test ;\\ chmod -R 770 /data/ftp/test 3、启动验证 systemctl start vsftpd systemctl status vsftpd -l ftp 127.0.0.1 > ls > put local_file_path ftp_file_path 4、配置中出现的错误 ①refusing to run with writable root inside chroot () 限定了用户不能跳出其主目录之后，使用该用户登录FTP时往往会遇到这个错误 500 OOPS: vsftpd: refusing to run with writable root inside chroot () 从2.3.5之后，vsftpd增强了安全检查，如果用户被限定在了其主目录下，则该用户的主目录不能再具有写权限了！如果检查发现还有写权限，就会报该错误。 要修复这个错误，可以用命令chmod a-w /home/user去除用户主目录的写权限（注意把目录替换成你自己的）。或者你可以在vsftpd_user_conf下的虚拟用户配置文件中增加下列一项：allow_writeable_chroot=YES ②530 login incrrect无法登陆。 查看日志tail -f /var/log/secure或者systemctl status vsftpd -l PAM unable to dlopen(/lib/security/pam_userdb.so): /lib/security/pam_userdb.so: cannot open shared object file: No such file or directory 原来pam_userdb.so在/lib64/security/pam_userdb.so,修改/etc/pam.d/vsftpd #将下列内容 auth required /lib/security/pam_userdb.so db=/etc/vsftpd/vuser account required /lib/security/pam_userdb.so db=/etc/vsftpd/vuser #替换成 auth required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser account required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser #或者 auth required pam_userdb.so db=/etc/vsftpd/vuser account required pam_userdb.so db=/etc/vsftpd/vuser 保存重启vsftpd服务。 ③425 Security: Bad IP connecting 当使用非21端口进行端口转发连接的话，会出现上述情况。 解决方案： 1.#vim /etc/vsftpd/vsftpd.conf 2.添加：pasv_promiscuous=YES 3.保存后退出 4.重启vsftpd #service vsftpd restart #pasv_promiscuous选项参数说明： 此选项激活时，将关闭PASV模式的安全检查。该检查确保数据连接和控制连接是来自同一个IP地址。小心打开此选项。此选项唯一合理的用法是存在于由安全隧道方案构成的组织中。默认值为NO。 合理的用法是：在一些安全隧道配置环境下，或者更好地支持FXP时(才启用它)。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-17 20:12:21 "},"origin/linux-daemon.html":{"url":"origin/linux-daemon.html","title":"后台启动进程","keywords":"","body":"Linux如何守护进程 一、问题的由来 Web应用写好后，下一件事就是启动，让它一直在后台运行。 这并不容易。举例来说，下面是一个最简单的Node应用server.js，只有6行。 var http = require('http'); http.createServer(function(req, res) { res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello World'); }).listen(5000); 你在命令行下启动它。 $ node server.js 看上去一切正常，所有人都能快乐地访问 5000 端口了。但是，一旦你退出命令行窗口，这个应用就一起退出了，无法访问了。怎么才能让它变成系统的守护进程（daemon），成为一种服务（service），一直在那里运行呢？ 二、前台任务与后台任务 上面这样启动的脚本，称为\"前台任务\"（foreground job）。它会独占命令行窗口，只有运行完了或者手动中止，才能执行其他命令。 变成守护进程的第一步，就是把它改成\"后台任务\"（background job）。 $ node server.js & 只要在命令的尾部加上符号&，启动的进程就会成为\"后台任务\"。如果要让正在运行的\"前台任务\"变为\"后台任务\"，可以先按ctrl + z，然后执行bg命令（让最近一个暂停的\"后台任务\"继续执行）。 \"后台任务\"有两个特点。 （1）继承当前 session （对话）的标准输出（stdout）和标准错误（stderr）。因此，后台任务的所有输出依然会同步地在命令行下显示。 （2）不再继承当前 session 的标准输入（stdin）。你无法向这个任务输入指令了。如果它试图读取标准输入，就会暂停执行（halt）。 可以看到，\"后台任务\"与\"前台任务\"的本质区别只有一个：是否继承标准输入。所以，执行后台任务的同时，用户还可以输入其他命令。 三、SIGHUP信号 变为\"后台任务\"后，一个进程是否就成为了守护进程呢？或者说，用户退出 session 以后，\"后台任务\"是否还会继续执行？ Linux系统是这样设计的。 用户准备退出 session 系统向该 session 发出SIGHUP信号 session 将SIGHUP信号发给所有子进程 子进程收到SIGHUP信号后，自动退出 上面的流程解释了，为什么\"前台任务\"会随着 session 的退出而退出：因为它收到了SIGHUP信号。 那么，\"后台任务\"是否也会收到SIGHUP信号？ 这由 Shell 的huponexit参数决定的。 $ shopt | grep huponexit 执行上面的命令，就会看到huponexit参数的值。 大多数Linux系统，这个参数默认关闭（off）。因此，session 退出的时候，不会把SIGHUP信号发给\"后台任务\"。所以，一般来说，\"后台任务\"不会随着 session 一起退出。 四、disown 命令 通过\"后台任务\"启动\"守护进程\"并不保险，因为有的系统的huponexit参数可能是打开的（on）。 更保险的方法是使用disown命令。它可以将指定任务从\"后台任务\"列表（jobs命令的返回结果）之中移除。一个\"后台任务\"只要不在这个列表之中，session 就肯定不会向它发出SIGHUP信号。 $ node server.js & $ disown 执行上面的命令以后，server.js进程就被移出了\"后台任务\"列表。你可以执行jobs命令验证，输出结果里面，不会有这个进程。 disown的用法如下。 # 移出最近一个正在执行的后台任务 $ disown # 移出所有正在执行的后台任务 $ disown -r # 移出所有后台任务 $ disown -a # 不移出后台任务，但是让它们不会收到SIGHUP信号 $ disown -h # 根据jobId，移出指定的后台任务 $ disown %2 $ disown -h %2 五、标准 I/O 使用disown命令之后，还有一个问题。那就是，退出 session 以后，如果后台进程与标准I/O有交互，它还是会挂掉。还是以上面的脚本为例，现在加入一行。 var http = require('http'); http.createServer(function(req, res) { console.log('server starts...'); // 加入此行 res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello World'); }).listen(5000); 启动上面的脚本，然后再执行disown命令。 $ node server.js & $ disown 接着，你退出 session，访问5000端口，就会发现连不上。这是因为\"后台任务\"的标准 I/O 继承自当前 session，disown命令并没有改变这一点。一旦\"后台任务\"读写标准 I/O，就会发现它已经不存在了，所以就报错终止执行。 为了解决这个问题，需要对\"后台任务\"的标准 I/O 进行重定向。 $ node server.js > stdout.txt 2> stderr.txt 六、nohup 命令 还有比disown更方便的命令，就是nohup。 $ nohup node server.js & nohup命令对server.js进程做了三件事。 阻止SIGHUP信号发到这个进程。 关闭标准输入。该进程不再能够接收任何输入，即使运行在前台。 重定向标准输出和标准错误到文件nohup.out。 也就是说，nohup命令实际上将子进程与它所在的 session 分离了。 注意，nohup命令不会自动把进程变为\"后台任务\"，所以必须加上&符号。 七、Screen 命令与 Tmux 命令 另一种思路是使用 terminal multiplexer （终端复用器：在同一个终端里面，管理多个session），典型的就是 Screen 命令和 Tmux 命令。 它们可以在当前 session 里面，新建另一个 session。这样的话，当前 session 一旦结束，不影响其他 session。而且，以后重新登录，还可以再连上早先新建的 session。 Screen 的用法如下。 # 新建一个 session $ screen $ node server.js 然后，按下ctrl + A和ctrl + D，回到原来的 session，从那里退出登录。下次登录时，再切回去。 $ screen -r 如果新建多个后台 session，就需要为它们指定名字。 $ screen -S name # 切回指定 session $ screen -r name $ screen -r pid_number # 列出所有 session $ screen -ls 如果要停掉某个 session，可以先切回它，然后按下ctrl + c和ctrl + d。 Tmux 比 Screen 功能更多、更强大，它的基本用法如下。 $ tmux $ node server.js # 返回原来的session $ tmux detach 除了tmux detach，另一种方法是按下Ctrl + B和d ，也可以回到原来的 session。 # 下次登录时，返回后台正在运行服务session $ tmux attach 如果新建多个 session，就需要为每个 session 指定名字。 # 新建 session $ tmux new -s session_name # 切换到指定 session $ tmux attach -t session_name # 列出所有 session $ tmux list-sessions # 退出当前 session，返回前一个 session $ tmux detach # 杀死指定 session $ tmux kill-session -t session-name 八、Node 工具 对于 Node 应用来说，可以不用上面的方法，有一些专门用来启动的工具：forever，nodemon 和 pm2。 forever 的功能很简单，就是保证进程退出时，应用会自动重启。 # 作为前台任务启动 $ forever server.js # 作为服务进程启动 $ forever start app.js # 停止服务进程 $ forever stop Id # 重启服务进程 $ forever restart Id # 监视当前目录的文件变动，一有变动就重启 $ forever -w server.js # -m 参数指定最多重启次数 $ forever -m 5 server.js # 列出所有进程 $ forever list nodemon一般只在开发时使用，它最大的长处在于 watch 功能，一旦文件发生变化，就自动重启进程。 # 默认监视当前目录的文件变化 $ nodemon server.js ＃ 监视指定文件的变化 $ nodemon --watch app --watch libs server.js pm2 的功能最强大，除了重启进程以外，还能实时收集日志和监控。 # 启动应用 $ pm2 start app.js # 指定同时起多少个进程（由CPU核心数决定），组成一个集群 $ pm2 start app.js -i max # 列出所有任务 $ pm2 list # 停止指定任务 $ pm2 stop 0 ＃ 重启指定任务 $ pm2 restart 0 # 删除指定任务 $ pm2 delete 0 # 保存当前的所有任务，以后可以恢复 $ pm2 save # 列出每个进程的统计数据 $ pm2 monit # 查看所有日志 $ pm2 logs # 导出数据 $ pm2 dump # 重启所有进程 $ pm2 kill $ pm2 resurect # 启动web界面 http://localhost:9615 $ pm2 web 十、Systemd 除了专用工具以外，Linux系统有自己的守护进程管理工具 Systemd 。它是操作系统的一部分，直接与内核交互，性能出色，功能极其强大。我们完全可以将程序交给 Systemd ，让系统统一管理，成为真正意义上的系统服务。 参考文章 http://www.ruanyifeng.com/blog/2016/02/linux-daemon.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-文本处理.html":{"url":"origin/linux-文本处理.html","title":"文本处理","keywords":"","body":"一、awk 1、获取匹配关键字后的内容 awk '{ if(match($0,\"关键字\")) {print substr($0,RSTART+RLENGTH) }}'文件 #示例 # 原始文本 2018-07-31T09:33:08.160102Z 1 [Note] A temporary password isgenerated for root@localhost: oco4Pr&a!o;v # 命令 awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTAR+RLENGTH) }}' test.log # 结果 oco4Pr&a!o;v 2、去除文本中的空行 awk NF test.txt # NF代表当前行的字段数，空行的话字段数为0,被awk解释为假，因此不进行输出。 3、获取匹配关键字后多少的位字符串 # 样本 a=\"Location: https://allinone.okd311.curiouser.com:8443/oauth/token/implicit#access_token=FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ&expires_in=86400&scope=user%3Afull&token_type=Bearer\" # 获取\"access_token=\"的值 # 方式一 echo $a | grep \"access_token=\" |awk -F\"access_token=\" '/access_token=/{printf substr($2,0,43)}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ # 方式二 echo $a | grep \"access_token=\" |awk '{ if(match($0,\"access_token=\")) {print substr($0,RSTART+RLENGTH) }}'| awk -F '&' '{print $1}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ 4、使用多个分隔符分割字符串 # @, 空格和Tab都是字段分隔符 awk -F”[@ /t]\" '{print $1 $2}' 5、为每行增加行号 awk '$0=NR\":\"$0' 文件名 > 新文件名 # $0表示原来每行的内容， # NR表示行号， # 双引号之间表示行号与原来内容之间的delimiter 二、sed sed [-hnV][-e][-f][文本文件] 参数说明： -e 或 --expression= 以选项中指定的script来处理输入的文本文件。 -f 或 --file= 以选项中指定的script文件来处理输入的文本文件。 -h 或 --help 显示帮助。 -n 或 --quiet或--silent 仅显示script处理后的结果。 -V 或 --version 显示版本信息。 动作说明 a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行) c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行) p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 0、替换环境变量的值到文件中 如果想替换字符串中的值为环境变量的值，可以使用将匹配规则使用\"\"括起来，在\"\"中直接引用环境变量 echo \"test asdasdas \\$TEST\" > test.txt export aaaaa=1234 sed -i -e \"s/\\$TEST/$aaaaa/g\" test.txt 如果变量的值中包含特殊字符，例如/，与sed匹配模式的关键字符造成歧义。可使用？代替/ echo \"test asdasdas \\$TEST\" > test.txt export aaaaa=/1234 sed -i \"s?\\$TEST?$aaaaa?g\" test.txt 1、新增内容到末尾行的末尾 sed '$ s/$/新增内容/' file_path 2、去除文本中空行和开头\"##\"的行 sed '/^$/d;/^##/d' file_path 3、去除文本中的空行 sed '/^\\s*$/d' test.txt 4、多个匹配规则 sed -i -e '/hah/a lala\\nhehe' -e '/lala/d' test 5、在查找到匹配行后的操作 sed -i '/hah/a lallalla' test #在查找到匹配行后添加一行 sed -i '/hah/a lala\\nhehe' test #在查找到匹配行后添加多行 sed -i '/hah/d' test #删除查找到匹配行 6、在查找匹配行的末首或末尾添加内容 sed -i '/ha/ s/^/la' test #在查找包含\"ha\"的行首追加\"la\",\"laha\" sed -i '/ha/ s/$/la' test #在查找包含\"ha\"的行末追加\"la\"，\"hala\" 7、去掉文本中开头带#号注释的行 sed -i -c -e '/^$/d;/^#/d' file 8、去除文本中的换行符^M Windows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误 sed 's/^M//' 原文件>新文件 # 注意，^M = Ctrl v + Ctrl m，而不是手动输入^M 9、每行前后添加空行 1.每行后面添加一行空行： sed G tmp 每行前面添加一行空行： sed ‘{x;p;x;}’ tmp 每行后面添加两行空行： sed ‘G;G’ tmp 每行前面添加两行空行： sed '{x;p;x;x;p;x;}' tmp 每行后面添加三行空行： sed ‘G;G;G’ tmp 每行前面添加三行空行： sed '{x;p;x;x;p;x;x;p;x}' tmp 依次类推，添加几行空行，就有几个G或者x;p;x 10、如果行后有空行，则删除，然后每行后面添加空行 sed '/^$/d;G' tmp 11、在匹配行前后添加空行 如果一行里面有shui这个单词，那么在他后面会添加一个空行 sed '/shui/G' tmp 如果一行里面有shui这个单词，那么在他前后各添加一个空行 sed '/shui/{x;p;x;G}' tmp 如果一行里面有shui这个单词，那么在他前面添加一个空行 sed '/shui/{x;p;x;}' tmp 在第一行前面添加空行，想在第几行，命令中的1就改成几 sed ‘1{x;p;x;}’ tmp 在第一行后面添加空行，想在第几行，命令中的1就改成几 sed ‘1G’ tmp 12、每几行后面添加一个空行 每两行后面增加一个空行 sed 'N;/^$/d;G' file.txt 每两行前面添加一个空行 sed 'N;/^$/d;{x;p;x;}' tmp 每三行后面增加一个空行 sed 'N;N;/^$/d;G' file.txt 每三行前面增加一个空行 sed 'N;N;/^$/d;{x;p;x;}' tmp 13、以x为开头或以x为结尾的行前后添加空行 以xi为开头的行后面添加空行 sed '/^xi/G;' tmp 以xi为结尾的行前面添加空行 sed '/^xi/{x;p;x;}' tmp 以xi为结尾的行后面添加空行 sed '/xi$/G;' tmp 以xi为结尾的行后面添加空行 sed '/xi$/{x;p;x;}' tmp 三、grep grep [OPTION]... PATTERN [FILE]... -r 是递归查找 -n 是显示行号 -R 查找所有文件包含子目录 -i 忽略大小写 -l 只列出匹配的文件名 -L 列出不匹配的文件名 -w 只匹配整个单词，而不是字符串的一部分 -C 匹配的上下文分别显示[number]行 1、统计某文件夹下文件的个数 ls -l /data|grep \"^-\"|wc -l #不包含子目录 ls -lR /data|grep \"^-\"|wc -l #不含子目录 2、统计某文件夹下目录的个数 ls -l /data |grep \"^ｄ\"|wc -l #不包含子目录 ls -lR /data|grep \"^ｄ\"|wc -l #包含子目录 3、统计某目录(包含子目录)下的所有某种类型的文件 ls -lR /data|grep txt|wc -l 4、去除文本中的空行 grep -v '^\\s*$' test.txt 5、多个匹配条件 grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的 grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。 grep -E '关键词1|关键词2' 6、xargs配合grep查找 find -type f -name '*.php'|xargs grep 'GroupRecord' 7、查找路径下含有某字符串的所有文件 grep -rn \"hello,world!\" * 8、完全匹配关键词 grep -Fx 关键词 9、精准匹配 grep -w 关键词 # 不加-w是默认情况 四、egrep 1、只显示文本中的非空行和非注释行 egrep -v '^$|#' file_path 五、cut 1、获取硬盘某个分区的UUID号追加到fstab blkid | grep /dev/sdb5 | cut -d ' ' -f 2 >>/etc/fstab;sed -i '$ s/$/ data ext4 defaults 0 0/' /etc/fstab 六、wc 1、统计某个目录下某种文件内总共多少行 find mapred/ -name \"*.java\" -print | xargs cat | wc -l 七、dos2unix dos2unix是将Windows格式文件转换为Unix、Linux格式的实用命令。Windows格式文件的换行符为\\r\\n ,而Unix&Linux文件的换行符为\\n. dos2unix命令其实就是将文件中的\\r\\n 转换为\\n。而unix2dos则是和dos2unix互为孪生的一个命令，它是将Linux&Unix格式文件转换为Windows格式文件的命令。 安装 yum install dos2unix -y #会安装dos2Unix、unix2dos、unix2mac这三条命令 用法 dos2unix [options] [-c convmode] [-o file ...] [-n infile outfile ...] -h 显示命令dos2unix联机帮助信息。 -k 保持文件时间戳不变 -q 静默模式，不输出转换结果信息等 -V 显示命令版本信息 -c 转换模式 -o 在源文件转换，默认参数 -n 保留原本的旧档，将转换后的内容输出到新档案.默认都会直接在原来的文件上修改， 1、一次转换多个文件 $> dos2unix filename1 filename2 filename3 2、默认情况下会在源文件上进行转换，如果需要保留源文件，那么可以使用参数-n dos2unix -n oldfilename newfilename 3、保持文件时间戳不变 $ ls -lrt dosfile -rw-r--r-- 1 root root 67 Dec 26 11:46 dosfile $ dos2unix dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile $ dos2unix -k dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile 4、静默模式格式化文件 unix2dos -q dosfile 八、vscode 1、搜索空行并进行替换 按下ctrl+h键进行正则匹配：^\\s*(?=\\r?$)\\n，然后进行替换操作 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-18 17:15:13 "},"origin/ssh-agent.html":{"url":"origin/ssh-agent.html","title":"SSH私钥代理ssh-agent","keywords":"","body":"SSH私钥管理程序 SSH-Agent 一、ssh-agent能干什么？ 多SSH私钥管理 使用不同的密钥连接到不同的主机时，需要手动指定对应的密钥，ssh-agent可以帮助我们选择对应的密钥进行认证，不用手动指定密钥即可进行连接。 避免重复输入加密私钥的密码 当私钥设置了密码，我们又需要频繁的使用私钥进行认证时，ssh-agent可以帮助我们免去重复的输入密码的操作 二、ssh-agent管理 1、安装 yum install -y openssh-clients 2、启动 ssh-agent启动有两种方式 ssh-agent #SHELL 在OS默认shell中再创建一个子shell，在子shell中运行ssh-agent进程，退出子shell自动结束代理。 eval '$(ssh-agent -s)' 单独启动一个代理进程，退出当前shell时最好使用ssh-agent -k关闭对应代理 3、关闭 在当前bash中，使用ssh-agent -k命令可以关闭对应的ssh-agent进程 如果在退出了当前bash以后再使用'ssh-agent -k'命令，是无法关闭对应的ssh-agent进程的，此时使用kill命令 使用 ssh-agent $SHELL 命令启动的ssh-agent也可以使用'ssh-agent -k'命令关闭ssh代理。 4、锁定ssh-agent # 加锁，需要输入加锁密码 ssh-add -x # 解锁，需要输入加锁密码 ssh-add -X 三、私钥管理 ①添加私钥到ssh-agent ssh-add ~/.ssh/id_rsa ②删除ssh-agent管理的私钥 ssh-add -d ～/.ssh/id_rsa # 删除ssh-agent中所有的私钥 ssh-add -D ③查看ssh-agent管理的私钥 ssh-add -l ④查看ssh-agent管理的私钥对应的公钥内容 ssh-add -L Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-htpasswd.html":{"url":"origin/linux-htpasswd.html","title":"htpasswd","keywords":"","body":"一、Overviews htpasswd命令是Apache的Web服务器内置工具，用于创建和更新储存用户名、域和用户基本认证的密码文件,主要用于对基于http用户的认证。 二、安装 yum install -y httpd-tools 三、语法 htpasswd(选项)(参数) 选项 -c：创建一个加密文件 -n：不更新加密文件，只将加密后的用户名密码显示在屏幕上 -m：默认采用MD5算法对密码进行加密 -d：采用CRYPT算法对密码进行加密 -p：不对密码进行进行加密，即明文密码 -s：采用SHA算法对密码进行加密 -b：在命令行中一并输入用户名和密码而不是根据提示输入密码 -D：删除指定的用户 参数 用户：要创建或者更新密码的用户名 密码：用户的新密码 四、常见操作 1、利用htpasswd命令添加用户 htpasswd .passwd -bc www.linuxde.net php # 在bin目录下生成一个.passwd文件，用户名www.linuxde.net，密码：php，默认采用MD5加密方式 2、在原有密码文件中增加下一个用户 htpasswd .passwd -b Jack 123456 #去掉-c选项，即可在第一个用户之后添加第二个用户，依此类推。 3、不更新密码文件，只显示加密后的用户名和密码 htpasswd -nb Jack 123456 # 不更新.passwd文件，只在屏幕上输出用户名和经过加密后的密码 4、利用htpasswd命令删除用户名和密码 htpasswd .passwd -D Jack 5、利用htpasswd命令修改密码 htpasswd .passwd -D Jack htpasswd .passwd -b Jack 123456 # 即先使用htpasswd删除命令删除指定用户，再利用htpasswd添加用户命令创建用户即可实现修改密码的功能。 参考链接 http://man.linuxde.net/htpasswd Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-shyaml.html":{"url":"origin/linux-shyaml.html","title":"YAML文本处理工具shyaml","keywords":"","body":"Linux下YAML文本处理工具shyaml 一、Overviews 通过 shyaml，可以直接获取键、值、键值对或对应的类型 二、安装 pip install shyaml 三、语法 cat | shyaml ACTION KEY [DEFAULT] ACTION get-type：获取相应的类型 get-value：获取值 get-values{,-0}：对序列类型来说，获取值列表 keys{,-0}：返回键列表 values{,-0}：返回值列表 key-values,{,-0}：返回键值对 Note： 结果默认是加\\n换行符，若用-0形式则以NUL字符填充 KEY为要查询的键，如不提供，则使用DEFAULT 四、示例 --- idc_group: name: bx bx: news_bx: news_bx web3_bx: web3_php-fpm_bx 如果要获取idc_group.name的值则可以执行 cat file.yaml | shyaml get-value idc_group.name 想获取idc_group.bx的键值对可执行 cat file.yaml | shyaml key-values idc_group.bx 参考链接 https://www.linuxidc.com/Linux/2016-04/130403.htm https://dev.to/vikcodes/yq-a-command-line-tool-that-will-help-you-handle-your-yaml-resources-better-8j9 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-09 17:18:59 "},"origin/linux-jq.html":{"url":"origin/linux-jq.html","title":"JSON文本处理工具jq","keywords":"","body":"Linux下JSON文本处理工具jq 一、Overviews jq 是一款命令行下处理 JSON 数据的工具。其可以接收标准输入，命令管道或者文件中的 JSON 数据，经过一系列的过滤器(filters)和表达式的转后形成我们需要的数据结构并将结果输出到标准输出中。jq 的这种特性使我们可以很容易地在 Shell 脚本中调用它。 二、安装 yum install -y epel-release ;\\ yum install -y jq 三、jq命令参数 jq [options] [file...] options: -c 使输出紧凑，而不是把每一个JSON对象输出在一行。; -n 不读取任何输入，过滤器运行使用null作为输入。一般用作从头构建JSON数据。; -e set the exit status code based on the output; -s 读入整个输入流到一个数组(支持过滤); -r 如果过滤的结果是一个字符串，那么直接写到标准输出（去掉字符串的引号）; -R read raw strings, not JSON texts; -C 打开颜色显示; -M 关闭颜色显示; -S sort keys of objects on output; --tab use tabs for indentation; --arg a v jq 通过该选项提供了和宿主脚本语言交互的能力。该选项将值(v)绑定到一个变量(a)上。在后面的 filter 中可以直接通过变量引用这个值。例如，filter '.$a'表示查询属性名称等于变量 a 的值的属性。; --argjson a v set variable $a to JSON value ; --slurpfile a f set variable $a to an array of JSON texts read from ; 四、示例 样本数据 {\"snapshots\": [{\"snapshot\": \"AAA-api-frame-2019-02-08-2019.02.24\",\"uuid\": \"aMhlAmhrqng\",\"version_id\": 7050100,\"version\": \"7.5.1\",\"indices\": [\"AAA-api-frame-2019-02-08\"],\"include_global_state\": false,\"state\": \"SUCCESS\",\"start_time\": \"2019-02-24T08:53:01.193Z\",\"end_time\": \"2019-02-24T08:53:01.593Z\",\"duration_in_millis\": 402,\"failures\": [],\"shards\": {\"total\": 1,\"failed\": 0,\"successful\": 1}},{\"snapshot\": \"BBB-api-frame-2019-02-09-2019.02.24\",\"uuid\": \"Wp5MBOFWJA\",\"version_id\": 7050199,\"version\": \"7.5.1\",\"indices\": [\"BBB-api-frame-2019-0209\"],\"include_global_state\": false,\"state\": \"SUCCESS\",\"start_time\": \"2019-02-24T11:04:56.063Z\",\"end_time\": \"2020-02-24T11:04:56.463Z\",\"duration_in_millis\": 399,\"failures\": [],\"test\": \"hah\",\"shards\": {\"total\": 1,\"failed\": 0,\"successful\": 1}}]} { \"snapshots\": [ { \"snapshot\": \"AAA-api-frame-2019-02-08-2019.02.24\", \"uuid\": \"aMhlAmhrqng\", \"version_id\": 7050100, \"version\": \"7.5.1\", \"indices\": [ \"AAA-api-frame-2019-02-08\" ], \"include_global_state\": false, \"state\": \"SUCCESS\", \"start_time\": \"2019-02-24T08:53:01.193Z\", \"end_time\": \"2019-02-24T08:53:01.593Z\", \"duration_in_millis\": 402, \"failures\": [], \"shards\": { \"total\": 1, \"failed\": 0, \"successful\": 1 } }, { \"snapshot\": \"BBB-api-frame-2019-02-09-2019.02.24\", \"uuid\": \"Wp5MBOFWJA\", \"version_id\": 7050199, \"version\": \"7.5.1\", \"indices\": [ \"BBB-api-frame-2019-0209\" ], \"include_global_state\": false, \"state\": \"SUCCESS\", \"start_time\": \"2019-02-24T11:04:56.063Z\", \"end_time\": \"2020-02-24T11:04:56.463Z\", \"duration_in_millis\": 399, \"failures\": [], \"test\": \"hah\", \"shards\": { \"total\": 1, \"failed\": 0, \"successful\": 1 } } ] } 1、输出控制 ①美化输出 $ jq -r '.' test.json ②换行与不换行输出 示例数据 [{\"id\": 16176,\"iid\": 7},{\"id\": 16173,\"iid\": 4}] 默认遍历数组中一个对象属性时会换行显示 $ cat test.json | jq -r '.[] | .id , .iid' 16176 7 16173 4 jq 加-j参数，可不换行输出 $ cat test.json | jq -jr '.[] | .id , .iid' 161767161734 或者 $ cat test.json | jq -r '.[] | \"\\(.id) , \\(.iid)\"' 16176 , 7 16173 , 4 ③输出额外信息 示例数据还使用上一个 $ cat test.json | jq -r '.[] | \" id: \\(.id) , iid: \\(.iid)\"' id: 16176 , iid: 7 id: 16173 , iid: 4 $ cat test.json | jq -jr '.[] | \" \\\"\" , \"IID: \" , .iid , \" ID: \" , .id ,\"\\\"\" ' \"IID: 7 ID: 16176\" \"IID: 4 ID: 16173\" ④格式化输出 $ cat test.json | jq -r '[.id,.iid] as [$id,$iid] | \"\\($id) -|- \\($iid)\"' 1 -|- 11 22 -|- 21 ⑤以Key=value的形式输出 jq -r '.snapshots[].shards|to_entries[]|\"\\(.key | ascii_upcase)=\\(.value)\"' test.json TOTAL=1 FAILED=0 SUCCESSFUL=1 ⑥压缩输出 jq -c '.' test.json 2、访问属性值 ①输出属性的值 $ jq -r '.snapshots[].snapshot' test.json $ jq -r '.snapshots[].snapshot,.snapshots[].end_time' test.json # 如果属性名中有空格，需要加双引号 $ jq -r '.\"with space\"' test.json $ jq -r '\"\\(.\"@timestamp\") \\(.end_time)\"' test.json ②批量访问属性值 $ jq -r '.snapshots[] | [.snapshot,.end_time] test.json 3、操作属性值 ①取值赋予变量 $ cat test.json | jq -r '[.id,.iid] as [$id,$iid] | \"\\($id)|\\($iid)\"' 4、JSON数组的操作 ①遍历访问数组 $ jq -r '.snapshots[]' test.json $ jq -r '.snapshots[] | .snapshot' test.json $ jq -r '.snapshots[].snapshot' test.json ②按索引访问数组 获取snapshot的index $ jq -r '.snapshots[0]' test.json $ jq -r '.snapshots[1].indices[0]' test.json ③数组切片 只取数组指定位置的值 # 从0开始到第一个 $ jq -r '.snapshots[0:1]' test.json # 从头开始到第一个 $ jq -r '.snapshots[:1]' test.json # 倒数一个到最后一个 $ jq -r '.snapshots[-1:]' test.json 5、函数操作 ①keys：获取有哪些属性 $ jq -r '.snapshots[] | keys' test.json ②length：获取属性的个数 $ jq -r '.snapshots[] | length' test.json ③min/max：大小的比较 $ jq -r '[.snapshots[].indices[0]] | min' test.json $ jq -r '[.snapshots[].indices[0]] | max' test.json ④ select：筛选过滤 $ jq -r '.snapshots[] | select(.duration_in_millis ⑤select：正则表达式筛选过滤 $ jq -r '.snapshots[] | select(.snapshot|test(\"^BBB.*\") ) | .version_id' test.json ⑥del：删除属性 $ jq -r 'del(.snapshots[].version)' test.json # 删除匹配到的属性 $ jq -r 'del( .snapshots[] | select(.uuid == \"Wp5MBOFWJA\"))' test.json > test-deled.json ⑦map：map属性值进行操作 判断属性值是否存在 $ jq -r '.snapshots | map(has(\"snapshot\"))' test.json 操作数值类型的属性值 $ jq -r '.snapshots | map(.duration_in_millis+2)' test.json ⑧unique：去重属性值 $ jq -r '.snapshots | map(.state) | unique' test.json ⑨重组json结构 $ jq -r '.snapshots | map(.) | .[] | {\"快照名\": .snapshot,\"快照的索引\": .indices}' test.json { \"快照名\": \"AAA-api-frame-2019-02-08-2019.02.24\", \"快照的索引\": [ \"AAA-api-frame-2019-02-08\" ] } { \"快照名\": \"BBB-api-frame-2019-02-09-2019.02.24\", \"快照的索引\": [ \"BBB-api-frame-2019-0209\" ] } 参考链接 https://www.baeldung.com/linux/jq-command-json https://www.ibm.com/developerworks/cn/linux/1612_chengg_jq/index.html?ca=drs-&utm_source=tuicool&utm_medium=referral Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-01 17:08:53 "},"origin/yaml-yq.html":{"url":"origin/yaml-yq.html","title":"yaml文本处理工具yq","keywords":"","body":"YAML文本处理工具yq 一、简介 对于k8s工程师，整天接触最多的是k8s资源对象的声明yaml文件。有的时候需要在脚本里面处理它。yq是一个go写的命令行处理工具。 Github：https://github.com/mikefarah/yq 二、安装 MacOS brew install yq Windows choco install yq Ubuntu and other Linux distros supporting snap packages: snap install yq Go GET GO111MODULE=on go get github.com/mikefarah/yq/v3 Run with Docker Oneshot use: docker run --rm -v \"${PWD}\":/workdir mikefarah/yq yq [flags] FILE... Run commands interactively: docker run --rm -it -v \"${PWD}\":/workdir mikefarah/yq sh It can be useful to have a bash function to avoid typing the whole docker command: yq() { docker run --rm -i -v \"${PWD}\":/workdir mikefarah/yq yq \"$@\" } 三、使用 命令详解 Usage: yq [flags] yq [command] Available Commands: compare yq x [--prettyPrint/-P] dataA.yaml dataB.yaml 'b.e(name==fr*).value' delete yq d [--inplace/-i] [--doc/-d index] sample.yaml 'b.e(name==fred)' help Help about any command merge yq m [--inplace/-i] [--doc/-d index] [--overwrite/-x] [--append/-a] sample.yaml sample2.yaml new yq n [--script/-s script_file] a.b.c newValue prefix yq p [--inplace/-i] [--doc/-d index] sample.yaml a.b.c read yq r [--printMode/-p pv] sample.yaml 'b.e(name==fr*).value' shell-completion Generates shell completion scripts validate yq v sample.yaml write yq w [--inplace/-i] [--script/-s script_file] [--doc/-d index] sample.yaml 'b.e(name==fr*).value' newValue Flags: -C, --colors print with colors -h, --help help for yq -I, --indent int sets indent level for output (default 2) -P, --prettyPrint pretty print -j, --tojson output as json. By default it prints a json document in one line, use the prettyPrint flag to print a formatted doc. -v, --verbose verbose mode -V, --version Print version information and quit Use \"yq [command] --help\" for more information about a command. 原始yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: creationTimestamp: \"2019-06-02T09:30:01Z\" finalizers: - kubernetes.io/pvc-protection name: test namespace: app-test resourceVersion: \"54534390\" selfLink: /api/v1/namespaces/test/persistentvolumeclaims/test uid: c35e6108-46c4-4f71-809d spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: nfs-client volumeMode: Filesystem volumeName: pvc-c35e6108-46c4-4f71-809d status: accessModes: - ReadWriteOnce capacity: storage: 10Gi phase: Bound 示例1 cat test.yaml | yq r - status accessModes: - ReadWriteOnce capacity: storage: 10Gi phase: Bound 示例2 cat test.yaml | yq r - spec.resources requests: storage: 10Gi 示例3 cat test.yaml | yq r - metadata | grep -E 'name:|namespace:' name: test namespace: app-test 原始yaml apiVersion: v1 kind: Secret metadata: name: test namespace: test type: Opaque data: prometheus-additional.yaml: dGVzdAo= 示例1 cat a.yaml | yq r - data.'\"prometheus-additional.yaml\"' | base64 --decode 参考 https://github.com/mikefarah/yq Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-24 22:11:49 "},"origin/linux-curl.html":{"url":"origin/linux-curl.html","title":"Curl命令详解","keywords":"","body":"一、Curl命令详解 帮助文档：https://curl.se/docs/manpage.html 语法：curl [options] [URL...] 参数： Options: (H) means HTTP/HTTPS only, (F) means FTP only --anyauth 可以使用“任何”身份验证方法 -a, --append FTP/SFTP上传文件时，curl将追加到目标文件，而非覆盖 --basic 使用HTTP基本验证 --cacert FILE 指定CA证书文件(SSL) --capath DIR 指定CA目录 (SSL) -E, --cert CERT[:PASSWD] Client certificate file and password (SSL) --cert-type 指定证书文件类型 (DER/PEM/ENG) (SSL) --ciphers LIST 指定SSL密码 --compressed 响应压缩格式 (deflate/gzip) -K, --config FILE 后接参数文件，参数文件中可以定义HTTP请求的相关的内容（URL、HEAD、DATA） --connect-timeout SECONDS 设置最大请求时间 -C, --continue-at OFFSET 断点续转 -b, --cookie STRING/FILE 设置cookies -c, --cookie-jar FILE 操作结束后把cookie写入到文件中 --create-dirs 建立本地目录层次结构 --crlf 上传时把LF转变成CRLF --crlfile FILE Get a CRL list in PEM format from the given file -d, --data DATA HTTP POST data (H) --data-ascii DATA 以ascii的方式post数据 --data-binary DATA 以二进制的方式post数据 --data-urlencode DATA HTTP POST data url encoded (H) --delegation STRING GSS-API delegation permission --digest 使用HTTP数字身份验证 --disable-eprt 禁止使用EPRT或LPRT --disable-epsv 禁止使用EPSV -D, --dump-header FILE 把header信息写入到文件中 --egd-file FILE 为随机数据(SSL)设置EGD socket路径 --engine ENGINGE 指定加密引擎(SSL). \"--engine list\" for list -f, --fail 连接失败时不显示http错误 -F, --form CONTENT form表单提交 --form-string STRING 模拟http表单提交数据 --ftp-account DATA Account data string (F) --ftp-alternative-to-user COMMAND String to replace \"USER [name]\" (F) --ftp-create-dirs 如果远程目录不存在，创建远程目录 --ftp-method [MULTICWD/NOCWD/SINGLECWD] 控制CWD的使用 --ftp-pasv 使用 PASV/EPSV 代替端口 -P, --ftp-port ADR Use PORT with given address instead of PASV (F) --ftp-skip-pasv-ip Skip the IP address for PASV (F) --ftp-pret Send PRET before PASV (for drftpd) (F) --ftp-ssl-ccc Send CCC after authenticating (F) --ftp-ssl-ccc-mode ACTIVE/PASSIVE Set CCC mode (F) --ftp-ssl-control Require SSL/TLS for ftp login, clear for transfer (F) -G, --get 使用get请求发送 -d参数指定的数据 -g, --globoff 禁用网址序列和范围使用{}和[] -H, --header LINE 增加Head头 -I, --head 只显示文档信息 -h, --help 显示帮助信息 --hostpubmd5 MD5 Hex encoded MD5 string of the host public key. (SSH) -0, --http1.0 强制使用HTTP 1.0协议 --ignore-content-length 忽略的HTTP头信息的长度 -i, --include 输出响应Head头 -k, --insecure 允许curl使用非安全的ssl连接并且传输数据（证书不受信） --interface INTERFACE 使用指定网络接口/地址 -4, --ipv4 解析域名为ipv4地址(域名有多个ip时) -6, --ipv6 解析域名为ipv6地址(域名有多个ip时) -j, --junk-session-cookies 读取文件时忽略session cookie (H) --keepalive-time SECONDS 设置连接的保活时间 --key KEY 私钥文件名(SSL/SSH) --key-type TYPE 私钥文件类型 (DER/PEM/ENG) (SSL) --krb LEVEL 使用指定安全级别的krb (F) --libcurl FILE Dump libcurl equivalent code of this command line --limit-rate RATE 指定最大的传输速率 -l, --list-only 列出ftp目录下的文件名称(F) --local-port RANGE 强制使用本地端口号 -L, --location curl自动重定向（3xx） --location-trusted like --location and send auth to other hosts (H) -M, --manual 显示全手动 --mail-from FROM 指定发信人邮箱(SMTP) --mail-rcpt TO 指定收信人邮箱(SMTP) --mail-auth AUTH Originator address of the original email --max-filesize BYTES 允许下载文件的最大大小 --max-redirs NUM Maximum number of redirects allowed (H) -m, --max-time SECONDS 设置整个操作的允许消耗的最大时间，对于在延时网络下的批量操作有利 --metalink Process given URLs as metalink XML file --negotiate 使用HTTP Negotiate身份验证(H) -n, --netrc 从netrc文件中读取用户名和密码 --netrc-optional 使用 .netrc 或者 URL来覆盖-n --netrc-file FILE 指定.netrc文件 -N, --no-buffer 禁用输出流缓冲区 --no-keepalive 连接不保活 --no-sessionid Disable SSL session-ID reusing (SSL) --noproxy List of hosts which do not use proxy --ntlm 使用 HTTP NTLM 身份验证 -o, --output FILE 将响应数据输出到指定文件，后接文件参数 --pass PASS 私钥密码 (SSL/SSH) --post301 301重定向后不切换至GET请求 (H) --post302 302重定向后不切换至GET请求 (H) --post303 303重定向后不切换至GET请求 (H) -#, --progress-bar 对发送和接收进行简单的进度条展示 --proto PROTOCOLS Enable/disable specified protocols --proto-redir PROTOCOLS Enable/disable specified protocols on redirect -x, --proxy [PROTOCOL://]HOST[:PORT] 设置代理 --proxy-anyauth 选择任一代理身份验证方法 (H) --proxy-basic 在代理上使用基本身份验证 (H) --proxy-digest 在代理上使用数字身份验证 (H) --proxy-negotiate 在代理上使用Negotiate身份验证 (H) --proxy-ntlm 在代理上使用ntlm身份验证 (H) -U, --proxy-user USER[:PASSWORD] 设置代理用户名和密码 --proxy1.0 HOST[:PORT] 使用HTTP/1.0的代理 -p, --proxytunnel Operate through a HTTP proxy tunnel (using CONNECT) --pubkey KEY 公钥文件 (SSH) -Q, --quote CMD 文件传输前，发送命令到服务器 (F/SFTP) --random-file FILE File for reading random data from (SSL) -r, --range RANGE 检索来自HTTP/1.1或FTP服务器字节范围 --raw Do HTTP \"raw\", without any transfer decoding (H) -e, --referer 发送\"Referer Page\"到服务器 -J, --remote-header-name Use the header-provided filename (H) -O, --remote-name 把输出写到文件中，保留远程文件的文件名 --remote-name-all Use the remote file name for all URLs -R, --remote-time 在本地生成文件时，保留远程文件时间 -X, --request COMMAND 指定HTTP请求方法 --resolve HOST:PORT:ADDRESS 强制解析HOST:PORT到某个ADDRESS --retry NUM 传输出现问题时，重试的次数 --retry-delay SECONDS 传输出现问题时，设置重试间隔时间 --retry-max-time SECONDS 传输出现问题时，设置最大重试时间 -S, --show-error 显示错误信息 -s, --silent 静默模式。不输出任何东西 --socks4 HOST[:PORT] 用socks4代理给定主机和端口 --socks4a HOST[:PORT] 用socks4a代理给定主机和端口 --socks5 HOST[:PORT] 用socks5代理给定主机和端口 --socks5-basic socks5代理开启username/password认证 --socks5-gssapi socks5代理开启GSS-API认证 --socks5-hostname HOST[:PORT] SOCKS5 proxy, pass host name to proxy --socks5-gssapi-service NAME SOCKS5 proxy service name for gssapi --socks5-gssapi-nec Compatibility with NEC SOCKS5 server -Y, --speed-limit RATE 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止 -y, --speed-time SECONDS 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止。默认30s --ssl Try SSL/TLS (FTP, IMAP, POP3, SMTP) --ssl-reqd Require SSL/TLS (FTP, IMAP, POP3, SMTP) -2, --sslv2 使用SSLv2的（SSL） -3, --sslv3 使用SSLv3的（SSL） --ssl-allow-beast Allow security flaw to improve interop (SSL) --stderr FILE 指定错误信息输出文件 --tcp-nodelay 使用TCP_NODELAY选项 -t, --telnet-option OPT=VAL Telnet选项设置 --tftp-blksize VALUE 设置TFTP BLKSIZE(必须大于512) -z, --time-cond TIME 传送时间设置 -1, --tlsv1 强制使用TLS version 1.x --tlsv1.0 使用TLSv1.0 (SSL) --tlsv1.1 使用TLSv1.1 (SSL) --tlsv1.2 使用TLSv1.2 (SSL) --trace FILE dump出输入输出数据至文件 --trace-ascii FILE 跟'--trace'一样，但是没有hex输出 --trace-time 跟踪/详细输出时，添加时间戳 --tr-encoding Request compressed transfer encoding (H) -T, --upload-file FILE 上传文件 --url URL URL to work with -B, --use-ascii 使用ASCII文本传输 -u, --user USER[:PASSWORD] 设置服务端用户和密码 --tlsuser USER TLS用户名 --tlspassword STRING TLS密码 --tlsauthtype STRING TLS认证类型(default SRP) --unix-socket FILE Connect through this UNIX domain socket -A, --user-agent STRING 发送用户代理给服务器 (H) -v, --verbose 获取更多输入输出相关的内容，对于debug非常有用 -V, --version 显示当前的curl版本 -w, --write-out FORMAT 指定完成请求以后输出什么信息 --xattr Store metadata in extended file attributes -q If used as the first parameter disables .curlrc 二、实例详解 1、通过-o/-O选项保存下载的文件到指定的文件中 -o：将文件保存为命令行中指定的文件名的文件中 -O：使用URL中默认的文件名保存文件到本地 # 将文件下载到本地并命名为mygettext.html curl -o mygettext.html http://www.gnu.org/software/gettext/manual/gettext.html # 将文件保存到本地并命名为gettext.html curl -O http://www.gnu.org/software/gettext/manual/gettext.html 2、显示response中的Headers或Body -i：显示response header 和 body -I：只显示response header curl -i https://www.baidu.com curl -I https://www.baidu.com 3、同时获取多个文件 curl -O URL1 -O URL2 4、代理的设置 -x：为CURL设置代理 curl -x 192.168.1.2:3128 http://google.com/ 如果curl命令请求不想走系统代理 curl --noproxy http://www.baidu.com 5、允许重定向 -L：允许重定向 curl -L -x 192.168.1.2:3128 http://google.com/ 6、限速 --limit-rate： 对CURL的最大网络使用进行限制 curl --limit-rate 1000B -O http://www.gnu.org/software/gettext/manual/gettext.html 7、添加认证信息 -u: 在访问需要认证的页面时，可通过-u选项提供用户名和密码进行授权 curl -u username:password URL # 通常的做法是在命令行只输入用户名，之后会提示输入密码，这样可以保证在查看历史记录时不会将密码泄露 curl -u username URL 8、获取更多信息 -v 和 -trace：获取更多信息 curl -v -L -x 192.168.1.2:3128 http://google.com/ 9、自定义HTTP请求 -X: 可以指定curl发送HTTP请求的方法，例如GET(默认),PUT,POST,DELETE等 -H：添加请求的Header信息 -d/--data: 添加请求的Body curl -XPUT \"http://127.0.0.1:9200/test/test/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"id\": \"191\", \"prd_id\": \"4\", \"mer_id\": \"1000005\", \"data_status\": \"0\", \"datachange_createtime\": \"1543915326\", \"datachange_lasttime\": \"1543915368\" }' 10、断点续传 -C: 可对大文件使用断点续传功能 curl -C -O http://www.gnu.org/software/gettext/manual/gettext.html 11、模仿浏览器 -A：指定浏览器去访问网站(有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本) curl -A \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)\" http://google.com/ 12、显示下载进度条 -# ：显示下载进度条 curl -# -O http://www.linux.com/dodo1.JPG 13、伪造referer（盗链） 很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了 -e: 设定referer curl -e \"www.linux.com\" http://mail.linux.com # 这样就会让服务器其以为你是从www.linux.com点击某个链接过来的 14、保存与使用Cookie -D: 保存Cookie -b: 使用Cookie # 将网站的cookies信息保存到sugarcookies文件中 curl -D sugarcookies http://localhost/sugarcrm/index.php # 使用上次保存的cookie信息 curl -b sugarcookies http://localhost/sugarcrm/index.php 15、忽略证书不受信问题 -k: 忽略HTTPS证书不受信问题 curl -k https://allinone.okd311.curiouser.com:8443 16、引用环境变量 Shell脚本中使用Curl命令，经常要通过变量替换变量中的值。使用'\"$var\"'进行应用 curl 'https://oapi.dingtalk.com/robot/send?access_token=******' \\ -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"text\", \"text\": {\"content\": \"消息是: '\"$message\"'\"} }' 17、强制域名解析至指定IP地址 curl --resolve test.test.com:80:127.0.0.1 \"http://test.test.com/\" 18、显示请求的耗时情况 curl 命令提供了 -w 参数，能够帮助分析请求的哪一步耗时比较长，好进一步找到问题的原因。 -w后面指定要显示的内容，后面可通过curl的变量显示某一项耗时。以下为内置 time_namelookup：DNS 域名解析耗时 time_connect：TCP 连接建立的时间，就是三次握手的时间 time_appconnect：SSL/SSH 等上层协议建立连接的时间，比如 connect/handshake 的时间 time_redirect：从开始到最后一个请求事务的时间 time_pretransfer：从请求开始到响应开始传输的时间 time_starttransfer：从请求开始到第一个字节将要传输的时间，这包括time_pretransfer以及服务器计算结果所需的时间。 time_total：这次请求花费的全部时间 url_effective: 最终获取的url地址，尤其是当你指定给curl的地址存在301跳转，且通过-L继续追踪的情形。 time_redirect: 重定向时间，包括到最后一次传输前的几次重定向的DNS解析，连接，预传输，传输时间 num_redirects: 在请求中跳转的次数 ssl_verify_result: ssl认证结果，返回0表示认证成功。 size_request: 请求的大小 $ curl -kls \\ -w \"\\n请求响应状态码 : %{http_code}\\n----------\\n请求信息：\\n 客户端信息: %{local_ip}:%{local_port} \\n 服务器信息: %{remote_ip}:%{remote_port}\\n 发送请求个数: %{num_connects}\\n 请求大小: %{size_request} bytes\\n 重定向URL: %{redirect_url}\\n 响应Header大小: %{size_header} bytes\\n请求耗时统计:\\n DNS解析完成时间: 第%{time_namelookup}秒\\n TCP握手完成时间: 第%{time_connect}秒\\n SSL握手完成时间: 第%{time_appconnect}秒\\n 客户端发送请求开始时间: 第%{time_pretransfer}秒\\n 请求收到第一个字节时间: 第%{time_starttransfer}秒\\n 请求结束时间: 第%{time_total}秒\\n----------\\nTCP和SSL连接耗时: %{time_pretransfer} - %{time_namelookup}\\n服务器处理耗时: %{time_starttransfer} - %{time_pretransfer}\\n响应数据传输耗时: %{time_total} - %{time_starttransfer}\\n共计耗时: %{time_total}秒\\n\" \\ https://google.com 301 Moved 301 Moved The document has moved here. 请求响应状态码 : 301 ---------- 请求信息： 客户端信息: 127.0.0.1:64073 服务器信息: 127.0.0.1:8001 发送请求个数: 1 请求大小: 184 bytes 重定向URL: https://www.google.com/ 响应Header大小: 508 bytes 请求耗时统计: DNS解析完成时间: 第0.000114秒 TCP握手完成时间: 第0.000358秒 SSL握手完成时间: 第0.460882秒 客户端发送请求开始时间: 第0.460956秒 请求收到第一个字节时间: 第0.603045秒 请求结束时间: 第0.603188秒 ---------- TCP和SSL连接耗时: 0.460956 - 0.000114 服务器处理耗时: 0.603045 - 0.460956 响应数据传输耗时: 0.603188 - 0.603045 共计耗时: 0.603188秒 可将-w的输出格式配置写在curl的默认配置文件~/.curlrc中 -kls -w \"\\n请求响应状态码 : %{http_code}\\n----------\\n请求信息：\\n 客户端信息: %{local_ip}:%{local_port} \\n 服务器信息: %{remote_ip}:%{remote_port}\\n 发送请求个数: %{num_connects}\\n 请求大小: %{size_request} bytes\\n 重定向URL: %{redirect_url}\\n 响应Header大小: %{size_header} bytes\\n请求耗时统计:\\n DNS解析完成时间: 第%{time_namelookup}秒\\n TCP握手完成时间: 第%{time_connect}秒\\n SSL握手完成时间: 第%{time_appconnect}秒\\n 客户端发送请求开始时间: 第%{time_pretransfer}秒\\n 请求收到第一个字节时间: 第%{time_starttransfer}秒\\n 请求结束时间: 第%{time_total}秒\\n----------\\nTCP和SSL连接耗时: %{time_pretransfer} - %{time_namelookup}\\n服务器处理耗时: %{time_starttransfer} - %{time_pretransfer}\\n响应数据传输耗时: %{time_total} - %{time_starttransfer}\\n共计耗时: %{time_total}秒\\n\" Chrome控制台时间显示的耗时对应 参考： https://curl.se/docs/manpage.html https://blog.cloudflare.com/a-question-of-timing/ https://cizixs.com/2017/04/11/use-curl-to-analyze-request/ https://blog.csdn.net/weifangan/article/details/80741981 19、新版本Curl不支持旧的TLS版本 创建~/.openssl_allow_tls1.0.cnf openssl_conf = openssl_init [openssl_init] ssl_conf = ssl_sect [ssl_sect] system_default = system_default_sect [system_default_sect] CipherString = DEFAULT@SECLEVEL=1 生效 OPENSSL_CONF=~/.openssl_allow_tls1.0.cnf curl -v https://***** # 或者 export OPENSSL_CONF=~/.openssl_allow_tls1.0.cnf curl -v https://***** unset OPENSSL_CONF 或者直接设置全局的OpenSSL配置文件 /etc/ssl/openssl.cnf openssl_conf = openssl_init [openssl_init] ssl_conf = ssl_sect [ssl_sect] system_default = system_default_sect [system_default_sect] CipherString = DEFAULT@SECLEVEL=1 参考： https://askubuntu.com/questions/1250787/when-i-try-to-curl-a-website-i-get-ssl-error Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-04 16:59:12 "},"origin/linux-rsync.html":{"url":"origin/linux-rsync.html","title":"rsync命令详解","keywords":"","body":"rsync命令详解 一、rsync命令详解 二、实例详解 1.使用SCP模式同步时 https://superuser.com/questions/138893/scp-to-remote-server-with-sudo Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-lvm.html":{"url":"origin/linux-lvm.html","title":"LVM原理及使用","keywords":"","body":"LVM原理及使用 一、原理简介 LVM是 Logical Volume Manager(逻辑卷管理)的简写，它由Heinz Mauelshagen在Linux 2.4内核上实现。 LVM将一个或多个硬盘的分区在逻辑上集合，相当于一个大硬盘来使用，当硬盘的空间不够使用的时候，可以继续将其它的硬盘的分区加入其中，这样可以实现磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的磁盘存储。它使系统管理员可以更方便的为应用与用户分配存储空间。在LVM管理下的存储卷可以按需要随时改变大小与移除(可能需对文件系统工具进行升级)。LVM也允许按用户组对存储卷进行管理，允许管理员用更直观的名称(如\"sales'、'development')代替物理磁盘名(如'sda'、'sdb')来标识存储卷 LVM功能实际是通过内核中的dm模块（device mapper）实现，它将一个或多个底层块设备组织成一个逻辑设备的模块，在/dev/目录下以dm-#形式展现 只要是块设备都可以用于创建LVM2。注意分区时ID号要是8e 物理存储介质（The physical media）：指系统的存储设备--硬盘，如：/dev/hda1、/dev/sda等等，是存储系统最低层的存储单元 物理卷PV（physical volume）：物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数 卷组VG（volume group）：在较低的逻辑层从多个PV中抽象出来的卷组，由一个或多个物理卷组成 PE（physical extend）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB 逻辑卷LV（logical volume）：由多个LV“块”组成可供挂载使用的设备文件 二、使用步骤 1、安装相关软件包 yum install -y lvm2 2、创建PV pvcreate /dev/sdc pvdisplay \"/dev/sdc\" is a new physical volume of \"100.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdc VG Name PV Size 100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID KiXHSv-PbKj-kOiM-yXhN-ntiw-ULpt-JhvgnB 3、创建VG # vgcreate命令用法 vgcreate -s [N[mgt]] VG名称 PV名称 # -s 指定VG中的PE大小，单位：MB,GB,TB vgcreate -s 16M docker /dev/sdc 4、查看VG vgdisplay --- Volume group --- VG Name docker System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 199.99 GiB PE Size 4.00 MiB Total PE 51198 Alloc PE / Size 38399 / 5、创建LV #lvcreate命令参数 lvcreate -l PE个数 -n LV名称 VG名称 ​ lvcreate -l 6399 -n docker-lib docker 6、查看LV容量 lvdisplay --- Logical volume --- LV Path /dev/docker/docker LV Name docker VG Name docker LV UUID hlbSQl-RfGK-PpUZ-u7Vx-5t3X-WOX7-dxLomX LV Write Access read/write LV Creation host, time node7.test.openshift.com, 2018-09-07 16:11:37 +0800 LV Status available # open 1 LV Size 7、格式化LV mkfs.ext3 LV_Name mkfs.ext4 LV_Name mkfs.xfs LV_Name 8、挂载LV echo \"LV_Name 挂载目录点 文件系统格式 defaults 0 0\" >> /etc/fstab mount -a 三、扩容VG和LV VG已无PE可用 新增硬盘 在线扩容（不卸载,不重启主机） 1、创建PV pvcreate /dev/sdd 2、将PV添加到VG中。之后可看PE数量增加 vgextend VG_Name /dev/sdd 3、扩容LV(之后可看LV容量增加) lvresize -l +6399 LV_Path 或者 lvresize -L +50G LV_Path 4、检查并修复文件系统 e2fsck -f LV_Name 5、将扩容后的LV完整地扩充到文件系统中 LV文件系统是ext4时 resize2fs LV_Path LV文件系统是xfs时 xfs_growfs LV_Path 四、挂载已创建的LVM磁盘 # 查看已有硬盘 fdisk -l # 查看lvm磁盘的lv lvdisplay # 查看lv是否激活 lvscan ACTIVE '/dev/data/data' [ 参考 https://wiki.archlinux.org/index.php/LVM_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 16:33:04 "},"origin/linux-交换分区.html":{"url":"origin/linux-交换分区.html","title":"Linux交换分区","keywords":"","body":"交换分区概念及管理 一、什么是交换分区呢？ Linux divides its physical RAM (random access memory) into chucks of memory called pages. Swapping is the process whereby a page of memory is copied to the preconfigured space on the hard disk, called swap space, to free up that page of memory. The combined sizes of the physical memory and the swap space is the amount of virtual memory available. Swap space in Linux is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space. While swap space can help machines with a small amount of RAM, it should not be considered a replacement for more RAM. Swap space is located on hard drives, which have a slower access time than physical memory.Swap space can be a dedicated swap partition (recommended), a swap file, or a combination of swap partitions and swap files. Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。 二、Swap交换分区对性能的影响 我们知道Linux可以使用文件系统中的一个常规文件或独立分区作为Swap交换空间，相对而言，交换分区要快一些。但是和RAM比较而言，Swap交换分区的性能依然比不上物理内存，目前的服务器上RAM基本上都相当充足，那么是否可以考虑抛弃Swap交换分区，是否不需要保留Swap交换分区呢？这个其实是我的疑问之一。在这篇What Is a Linux SWAP Partition, And What Does It Do?博客中，作者给出了swap交换空间的优劣 Advantages: Provides overflow space when your memory fills up completely Can move rarely-needed items away from your high-speed memory Allows you to hibernate Disadvantages: Takes up space on your hard drive as SWAP partitions do not resize dynamically Can increase wear and tear to your hard drive Does not necessarily improve performance (see below) 其实保留swap分区概括起来可以从下面来看： 首先，当物理内存不足以支撑系统和应用程序（进程）的运作时，这个Swap交换分区可以用作临时存放使用率不高的内存分页，把腾出的内存交给急需的应用程序（进程）使用。有点类似机房的UPS系统，虽然正常情况下不需要使用，但是异常情况下， Swap交换分区还是会发挥其关键作用。 其次，即使你的服务器拥有足够多的物理内存，也有一些程序会在它们初始化时残留的极少再用到的内存分页内容转移到 swap 空间，以此让出物理内存空间。对于有发生内存泄漏几率的应用程序（进程），Swap交换分区更是重要，因为谁也不想看到由于物理内存不足导致系统崩溃。 最后，现在很多个人用户在使用Linux，有些甚至是PC的虚拟机上跑Linux系统，此时可能常用到休眠（Hibernate），这种情况下也是推荐划分Swap交换分区的。 其实少量使用Swap交换空间是不会影响性能，只有当RAM资源出现瓶颈或者内存泄露，进程异常时导致频繁、大量使用交换分区才会导致严重性能问题。另外使用Swap交换分区频繁，还会引起kswapd0进程（虚拟内存管理中, 负责换页的）耗用大量CPU资源，导致CPU飙升。 关于Swap分区的优劣以及是否应该舍弃，我有点恶趣味的想到了这个事情：人身上的两个器官，阑尾和扁桃体。切除阑尾或扁桃体是否也是争论不休。另外，其实不要Swap交换分区，Linux也是可以正常运行的（有人提及过这个问题） 三、Swap分区大小设置建议 系统的Swap分区大小设置多大才是最优呢？ 关于这个问题，应该说只能有一个统一的参考标准，具体还应该根据系统实际情况和内存的负荷综合考虑，像ORACLE的官方文档就推荐如下设置，这个是根据物理内存来做参考的。 RAM Swap Space Up to 512 MB 2 times the size of RAM Between 1024 MB and 2048 MB 1.5 times the size of RAM Between 2049 MB and 8192 MB Equal to the size of RAM More than 8192 MB 0.75 times the size of RAM 另外在其它博客中看到下面一个推荐设置，当然我不清楚其怎么得到这个标准的。是否合理也无从考证。可以作为一个参考。 4G以内的物理内存，SWAP 设置为内存的2倍。 4-8G的物理内存，SWAP 等于内存大小。 8-64G 的物理内存，SWAP 设置为8G。 64-256G物理内存，SWAP 设置为16G。 四、什么时候使用Swap分区空间? 系统在什么情况或条件下才会使用Swap分区的空间呢？ 其实是Linux通过一个参数swappiness来控制的。当然还涉及到复杂的算法。 这个参数值可为 0-100，控制系统 swap 的使用程度。高数值可优先系统性能，在进程不活跃时主动将其转换出物理内存。低数值可优先互动性并尽量避免将进程转换处物理内存，并降低反应延迟。默认值为 60。注意：这个只是一个权值，不是一个百分比值，涉及到系统内核复杂的算法。关于该参数请参考这篇文章[转载]调整虚拟内存，在此不做过多赘述。下面是关于swappiness的相关资料 The Linux 2.6 kernel added a new kernel parameter called swappiness to let administrators tweak the way Linux swaps. It is a number from 0 to 100. In essence, higher values lead to more pages being swapped, and lower values lead to more applications being kept in memory, even if they are idle. Kernel maintainer Andrew Morton has said that he runs his desktop machines with a swappiness of 100, stating that \"My point is that decreasing the tendency of the kernel to swap stuff out is wrong. You really don't want hundreds of megabytes of BloatyApp's untouched memory floating about in the machine. Get it out on the disk, use the memory for something useful.\" Swappiness is a property of the Linux kernel that changes the balance between swapping out runtime memory, as opposed to dropping pages from the system page cache. 有两种临时修改swappiness参数的方法，系统重启后失效 方法1： echo 10 > /proc/sys/vm/swappiness 方法2: sysctl vm.swappiness=10 永久修改swappiness参数的方法就是在配置文件/etc/sysctl.conf里面修改vm.swappiness的值，然后重启系统 echo 'vm.swappiness=10' >>/etc/sysctl.conf 如果有人会问是否物理内存使用到某个百分比后才会使用Swap交换空间，可以明确的告诉你不是这样一个算法，如下截图所示，及时物理内存只剩下8M了，但是依然没有使用Swap交换空间，而另外一个例子，物理内存还剩下19G，居然用了一点点Swap交换空间。 五、交换分区管理 1、查看Swap分区大小 $ free -mh total used free shared buff/cache available Mem: 31G 21G 256M 8.4M 9.6G 9.4G Swap: 4.0G 0B 4.0G $ swapon -s 或者 cat /proc/swaps Filename Type Size Used Priority /dev/vdb partition 4194300 0 -1 2、释放Swap分区空间 swapon -s 3、使用swapoff关闭交换分区 swapoff /dev/vdb 4、使用swapon启用交换分区 swapon /dev/vdb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-硬盘读写性能测试.html":{"url":"origin/linux-硬盘读写性能测试.html","title":"Linux硬盘读写性能测试","keywords":"","body":"Linux硬盘读写性能测试 Linux下可使用dd命令来测试硬盘读写速度 一、语法简介 dd if=path/to/input_file of=/path/to/output_file bs=block_size count=number_of_blocks ​参数 if=file 　　　　　　　　　　　　　　　　输入文件名，缺省为标准输入 of=file 　　　　　　　　　　　　　　　　输出文件名，缺省为标准输出 ibs=bytes 　　　　　　　　　　　　　　　一次读入 bytes 个字节(即一个块大小为 bytes 个字节) obs=bytes 　　　　　　　　　　　　　　　一次写 bytes 个字节(即一个块大小为 bytes 个字节) bs=bytes 　　　　　　　　　　　　　　　 同时设置读写块的大小为 bytes ，可代替 ibs 和 obs cbs=bytes 　　　　　　　　　　　　　　　一次转换 bytes 个字节，即转换缓冲区大小 skip=blocks 　　　　　　　　　　　　　 从输入文件开头跳过 blocks 个块后再开始复制 seek=blocks 　　　　　　　　　　 从输出文件开头跳过 blocks 个块后再开始复制(通常只有当输出文件是磁盘或磁带时才有效) count=blocks 　　　　　　　　　　　　　仅拷贝 blocks 个块，块大小等于 ibs 指定的字节数 conv=conversion[,conversion...] 用指定的参数转换文件。 iflag=FLAGS　　　　　　　　　　　　　　指定读的方式FLAGS，参见“FLAGS参数说明” oflag=FLAGS　　　　　　　　　　　　　　指定写的方式FLAGS，参见“FLAGS参数说明” ​ #conv 转换参数： ascii 　　　　　　　　　　　　　　　　　转换 EBCDIC 为 ASCII ebcdic 　　　　　　　　　　　　 　 转换 ASCII 为 EBCDIC ibm 　　　　　　　　　　　　　　　　　　转换 ASCII 为 alternate EBCDIC block 　　　　　　　　　　　　　　　　 把每一行转换为长度为 cbs 的记录，不足部分用空格填充 unblock 　　　　　　　　　　　　　　　 使每一行的长度都为 cbs ，不足部分用空格填充 lcase 　　　　　　　　　　　　　　　　 把大写字符转换为小写字符 ucase 　　　　　　　　　　　　　　　　 把小写字符转换为大写字符 swab 　　　　　　　　　　　　　　　　 交换输入的每对字节 noerror 　　　　　　　　　　　　　　　 出错时不停止 notrunc 　　　　　　　　　　　　　　　 不截短输出文件。 sync 　　　　　　　　　　　　　　　　　 把每个输入块填充到ibs个字节，不足部分用空(NUL)字符补齐 FLAGS 参数说明：​ append -append mode (makes sense only for output; conv=notrunc sug-gested) direct　　　　　　　　　　　　　　　 读写数据采用直接IO方式 directory　　　　　　　　　　　　　　读写失败除非是directory dsync　　　　　　　　　　　　　　　　 读写数据采用同步IO sync　　　　　　　　　　　　　　　　　同上，但是针对是元数据 fullblock　　　　　　　　　　　　　　堆积满block（accumulate full blocks of input ）(iflag only) nonblock　　　　　　　　　　　　　　 读写数据采用非阻塞IO方式 noatime　　　　　　　　　　　　　　　 读写数据不更新访问时间 二、time+dd 测磁盘读写速度 1、相关参数 time有计时作用，dd用于复制，从if读出，写到of if=/dev/zero（产生字符）不产生IO，因此可以用来测试纯写速度 同理of=/dev/null（回收站、无底洞）不产生IO，可以用来测试纯读速度 将/tmp/test拷贝到/var则同时测试了读写速度 bs是每次读或写的大小，即一个块的大小，count是读写块的数量 当写入到驱动盘的时候，我们简单的从无穷无用字节的源 /dev/zero 读取，当从驱动盘读取的时候，我们读取的是刚才的文件，并把输出结果发送到无用的 /dev/null。在整个操作过程中， DD 命令会跟踪数据传输的速度并且报告出结果。 2、测试磁盘写能力 time dd if=/dev/zero of=/testw.dbf bs=4k count=100000 因为/dev//zero是一个伪设备，它只产生空字符流，对它不会产生IO，所以，IO都会集中在of文件中，of文件只用于写，所以这个命令相当于测试磁盘的写能力。命令结尾添加oflag=direct将跳过内存缓存，添加oflag=sync将跳过hdd缓存。 3、测试磁盘读能力 time dd if=/dev/sdb of=/dev/null bs=4k 因为/dev/sdb是一个物理分区，对它的读取会产生IO，/dev/null是伪设备，相当于黑洞，of到该设备不会产生IO，所以，这个命令的IO只发生在/dev/sdb上，也相当于测试磁盘的读能力。（Ctrl+c终止测试） 4、测试同时读写能力 time dd if=/dev/sdb of=/testrw.dbf bs=4k 在这个命令下，一个是物理分区，一个是实际的文件，对它们的读写都会产生IO（对/dev/sdb是读，对/testrw.dbf是写），假设它们都在一个磁盘中，这个命令就相当于测试磁盘的同时读写能力。 5、测试纯写入性能 dd if=/dev/zero of=test bs=8k count=10000 oflag=direct 6、测试纯读取性能 dd if=test of=/dev/null bs=8k count=10000 iflag=direct 注意：dd 只能提供一个大概的测试结果，而且是连续 I/O 而不是随机 I/O，理论上文件规模越大，测试结果越准确。 同时，iflag/oflag 提供 direct 模式，direct 模式是把写入请求直接封装成 I/O 指令发到磁盘，非 direct 模式只是把数据写入到系统缓存就认为 I/O 成功，并由操作系统决定缓存中的数据什么时候被写入磁盘。 参考链接 http://www.360doc.com/content/15/0906/17/8737500_497292503.shtml Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/vim-小技巧.html":{"url":"origin/vim-小技巧.html","title":"Vim小技巧","keywords":"","body":"vim常用配置 bash -c 'cat > ~/.vimrc 命令行模式 功能 命令 描述 水平分屏 ：sp 水平分屏打开另一个文件 垂直分屏 ：vsp 垂直分屏打开另一个文件 多屏间切换 Ctrl+W+W 多屏退出 ：qall 排序 ：sort 去除重复行 ：sort u 移动到文本开头 gg 移动到文本结尾 G 移动到行首 0 即行首有空格的情况，会移动到空格之前 移动到行末 $ 即行末有空格的情况，会移动到空格之后 向下翻页 Ctrl+f 向上翻页 Ctrl+b 以word为单位移动 单词数+W/b,B/b,E/e 2w表示向后移动2个word； 2b表示向前移动2个word； 2e表示向后移动2个word(但是会移动到word字符之后) 如果想忽略标点符号的word，就用大写 W B E 行内查找字符 f+字符 向后移动到某字符 F+字符 向前移动到字符a处 全文查找当前光标处的单词 * 向后查找 #+字符 从文件开头到文件尾开始查找匹配字符 ?+字符 从文件尾倒着到文件开头开始查找匹配字符 光标右边最近数字进行自加 Ctrl+A 光标右边最近数字进行自减 Ctrl+X 删除文本中的空行 ：g/^$/d 注释文本行 v进入视图模式，选择要注释的行，然后Ctrl+v进入块选择模式，然后大写I插入#或者/，再ESC退出 ：起始行号,结束行号s/^注释符//g 在10 - 20行添加 // 注释 :10,20s#^#//#g 在10 - 20行添加 # 注释 :10,20s/^/#/g 快速搜索光标所在单词 Shift+* 显示匹配个数 :%s/xxx//gn 插入模式 功能 命令 描述 删除光标前面的单词 Ctrl+W 删除光标前面的一行 Ctrl+U 在光标前面插入一个tab Ctrl+I 将光标以下所有内容向上提 Ctrl+H 将光标以下所有内容向下提 Ctrl+J/M 向下联想 Ctrl+N 向上联想 Ctrl+P 示例 1、行首或行尾加字符 #每行行首加“#” :%s/^/#/g #每行行尾加\" ;\\\" :%s/$/ ;\\\\/g #第二行到第十五行的行首添加“==” :2,15 s/^/==/g #第二行到文本末行的行首添加“==” :2,$ s/^/==/g #第二行到文本首行的行首添加“==” :2,1 s/^/==/g 2、将文本中相同数字进行自增 原始文本 docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 1.tar docker.io/skydive/skydive:latest \\ :g/1.tar/ s//\\=line('.').'.tar'/ 效果文本 docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 2.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 3.tar docker.io/skydive/skydive:latest ;\\ 3、去除文本中的换行符^M Windows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误 进入命令模式 %s/^M//g (注意，^M = Ctrl v + Ctrl m，而不是手动输入^M) # ^M 表示清除成功 4、设置粘贴时换行问题 有些版本vim的默认配置下，在插入模式下粘贴文字会换行加tab缩进，例如 line line line 在命令行中设置或在配置文件中设置 :set paste 参考：https://stackoverflow.com/questions/2514445/turning-off-auto-indent-when-pasting-text-into-vim/2514520 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-05 11:12:13 "},"origin/linux-yum.html":{"url":"origin/linux-yum.html","title":"Yum-RPM包管理","keywords":"","body":"YUM详解 一、Overviews Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 Yum的关键之处是要有可靠的repository，顾名思义这就是软件的仓库，它可以是http或者ftp站点，也可以是本地的软件池，但是必须包含rpm的header，rmp的header包括了rmp的各种信息，包括描述、功能、提供的文件、依赖性等，正是收集了这些信息，才能自动化的完成余下的任务。 repo文件是yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的细节内容 RPM包的名称规则示例：ttpd-manual- 2.0.40-21.i386.rpm，ttp-manual是软件包的名称，2是主版本号；0是次版本号；40是修正号；21是编译的次数；i386是适合的平台 二、YUM命令详解 yum的命令形式一般是如下：yum [选项] [参数] [package ...] 选项 -h, --help 显示此帮助消息并退出 -t, --tolerant 忽略错误 -C, --cacheonly 完全从系统缓存运行，不升级缓存 -c [config file], --config=[config file] 配置文件路径 -R [minutes], --randomwait=[minutes] 命令最长等待时间 -d [debug level], --debuglevel=[debug level] 调试输出级别 --showduplicates 在 list/search 命令下，显示源里重复的条目 -e [error level], --errorlevel=[error level] 错误输出级别 --rpmverbosity=[debug level name] RPM 调试输出级别 -q, --quiet 静默执行 -v, --verbose 详尽的操作过程 -y, --assumeyes 回答全部问题为是 --assumeno 回答全部问题为否 --version 显示 Yum 版本然后退出 --installroot=[path] 设置安装根目录 --enablerepo=[repo] 启用一个或多个软件源(支持通配符) --disablerepo=[repo] 禁用一个或多个软件源(支持通配符) -x [package], --exclude=[package] 采用全名或通配符排除软件包 --disableexcludes=[repo] 禁止从主配置，从源或者从任何位置排除 --disableincludes=[repo] disable includepkgs for a repo or for everything --obsoletes 更新时处理软件包取代关系 --noplugins 禁用 Yum 插件 --nogpgcheck 禁用 GPG 签名检查 --disableplugin=[plugin] 禁用指定名称的插件 --enableplugin=[plugin] 启用指定名称的插件 --skip-broken 忽略存在依赖关系问题的软件包 --color=COLOR 配置是否使用颜色 --releasever=RELEASEVER 在 yum 配置和 repo 文件里设置 $releasever 的值 --downloadonly 仅下载而不更新 --downloaddir=DLDIR 指定一个其他文件夹用于保存软件包 --setopt=SETOPTS 设置任意配置和源选项 --bugfix Include bugfix relevant packages, in updates --security Include security relevant packages, in updates --advisory=ADVS, --advisories=ADVS Include packages needed to fix the given advisory, in updates --bzs=BZS Include packages needed to fix the given BZ, in updates --cves=CVES Include packages needed to fix the given CVE, in updates --sec-severity=SEVS, --secseverity=SEVS Include security relevant packages matching the severity, in updates 参数 check 检查 RPM 数据库问题 check-update 检查是否有可用的软件包更新 clean 删除缓存数据 deplist 列出软件包的依赖关系 distribution-synchronization 已同步软件包到最新可用版本 downgrade 降级软件包 erase 从系统中移除一个或多个软件包 fs Acts on the filesystem data of the host, mainly for removing docs/lanuages for minimal hosts. fssnapshot Creates filesystem snapshots, or lists/deletes current snapshots. groups 显示或使用、组信息 help 显示用法提示 history 显示或使用事务历史 info 显示关于软件包或组的详细信息 install 向系统中安装一个或多个软件包 langavailable Check available languages langinfo List languages information langinstall Install appropriate language packs for a language langlist List installed languages langremove Remove installed language packs for a language list 列出一个或一组软件包 load-transaction 从文件名中加载一个已存事务 makecache 创建元数据缓存 provides 查找提供指定内容的软件包 reinstall 覆盖安装软件包 repo-pkgs 将一个源当作一个软件包组，这样我们就可以一次性安装/移除全部软件包。 repolist 显示已配置的源 search 在软件包详细信息中搜索指定字符串 shell 运行交互式的 yum shell swap Simple way to swap packages, instead of using shell update 更新系统中的一个或多个软件包 update-minimal Works like upgrade, but goes to the 'newest' package match which fixes a problem that affects your system updateinfo Acts on repository update information upgrade 更新软件包同时考虑软件包取代关系 version 显示机器和/或可用的源版本。 常用命令 清除缓存 yum clean [headers, packages, metadata, dbcache, plugins, expire-cache, rpmdb, all] headers--清除缓存目录(/var/cache/yum)下的 headers packages--清除缓存目录(/var/cache/yum)下的软件包 all--清除所有缓存 yum update与yum upgrade的区别 yum update 只更新软件，不更新内核 yum upgrade 升级所有包，不改变软件设置和系统设置，系统版本升级，内核不改变 三、repo文件 [serverid] #serverid是用于区别各个不同的repository，必须有一个独一无二的名称。若重复了，是前面覆盖后面--还是反过来呢？？？用enabled 测试是后面覆盖前面 name=Some name for this server #name，是对repository的描述，支持像$releasever $basearch这样的变量; name=Fedora Core $releasever - $basearch - Released Updates baseurl=url://path/to/repository/ # 1. 格式: baseurl=url://server1/path/to/repository/, url支持的协议有 http:// ftp:// file://三种。 # 2. baseurl后可以跟多个url，你可以自己改为速度比较快的镜像站，但#baseurl只能有一个 # 3. 其中url指向的目录必须是这个repository header目录的上一级，它也支持$releasever $basearch这样的变量。 gpgcheck=1 exclude=gaim failovermethod=priority #failovermethode有两个选项roundrobin和priority，意思分别是有多个url可供选择时，yum选择的次序，roundrobin是随机选择，如果连接失 败则使用下一个，依次循环，priority则根据url的次序从第一个开始。如果不指明，默认是roundrobin。 enabled=[1 or 0] # 1. 当某个软件仓库被配置成 enabled=0 时，yum 在安装或升级软件包时不会将该仓库做为软件包提供源。使用这个选项，可以启用或禁用软件仓库。 # 2. 通过 yum 的 --enablerepo=[repo_name] 和 --disablerepo=[repo_name] 选项，或者通过 PackageKit 的\"添加/删除软件\"工具，也能够方便地启用和禁用指定的软件仓库 变量解释： # $releasever 发行版的版本，从[main]部分的distroverpkg获取，如果没有，则根据redhat-release包进行判断。 # $arch cpu体系，如i686,athlon等 # $basearch cpu的基本体系组，如i686和athlon同属i386，alpha和alphaev6同属alpha。 四、yum-fastestmirror插件 yum-fastestmirror插件，它会自动选择最快的mirror。它的配置文件/etc/yum/pluginconf.d/fastestmirror.conf，yum镜像的速度测试记录文件/var/cache/yum/x86_64/7/timedhosts.txt** 禁用插件配置 修改插件的配置文件 sed -i 's/enabled=1/enabled=0/' /etc/yum/pluginconf.d/fastestmirror.conf enabled = 1//由1改为0，禁用该插件 修改yum的配置文件 # sed -i 's/plugins=1/plugins=0/' /etc/yum.conf plugins=1 //改为0，不使用插件 五、YUM源的创建 1、使用Nexus的YUN格式仓库作为YUM镜像源 详见：Nexus中yum仓库的配置与使用 2、Createrepo创建本地YUM镜像源 将CentOS版本系统镜像中的Packages并上传到主机上的某一目录下 安装createrepo用来创建软件包的索引。或者将系统镜像中repodata目录放到rpm包路径下 yum install createrepo -y createrepo /data/localrepo/Office 会在创建repodata索引文件夹 在/etc/yum.repos.d/目录下创建repo文件local.repo [local] name=Local Yum Office Repository 仓库名 baseurl=file:///data/localrepo/Office 仓库中rpm包存放路径 gpgcheck=1 是否检查GPG-KEY，0为不检查，1为检查 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 GPG-KEY的存放路径 enabled=1 设置为1表示启用本Repo Createrepo命令参数详解 -u --baseurl 指定Base URL的地址 -o --outputdir 指定元数据的输出位置 -x --excludes 指定在形成元数据时需要排除的包 -i --pkglist 指定一个文件，该文件内的包信息将被包含在即将生成的元数据中，格式为每个包信息独占一行，不含通配符、正则，以及范围表达式。 -n --includepkg 通过命令行指定要纳入本地库中的包信息，需要提供URL或本地路径。 -q --quiet 安静模式执行操作，不输出任何信息。 -g --groupfile 指定本地软件仓库的组划分，范例如下：createrepo -g comps.xml /path/to/rpms注意：组文件需要和rpm包放置于同一路径下。 -v --verbose 输出详细信息。 -c --cachedir 指定一个目录，用作存放软件仓库中软件包的校验和信息。 当createrepo在未发生明显改变的相同仓库文件上持续多次运行时，指定cachedir会明显提高 其性能。 --update 如果元数据已经存在，且软件仓库中只有部分软件发生了改变或增减， 则可用update参数直接对原有元数据进行升级，效率比重新分析rpm包依赖并生成新的元数据要 高很多。 -p --pretty 以整洁的格式输出xml文件。 -d --database 该选项指定使用SQLite来存储生成的元数据，默认项。 3、HTTPD+Createrepo创建YUM镜像源 第二种方法创建的镜像源只能在本地使用，要是能在局域网中提供公共的服务，需要一个能提供HTTP服务的容器，可使用HTTPD（又称Apache），步骤省略。 六、Reposync同步YUM远程仓库的安装包 1、安装 yum install yum-utils -y 2、命令参数 Usage: Reposync is used to synchronize a remote yum repository to a local directory using yum to retrieve the packages. /usr/bin/reposync [options] Options: -h, --help show this help message and exit -c CONFIG, --config=CONFIG config file to use (defaults to /etc/yum.conf) -a ARCH, --arch=ARCH act as if running the specified arch (default: current arch, note: does not override $releasever. x86_64 is a superset for i*86.) --source operate on source packages -r REPOID, --repoid=REPOID secify repo ids to query, can be specified multiple times (default is all enabled) -e CACHEDIR, --cachedir=CACHEDIR directory in which to store metadata -t, --tempcache Use a temp dir for storing/accessing yum-cache -d, --delete delete local packages no longer present in repository -p DESTDIR, --download_path=DESTDIR Path to download packages to: defaults to current dir --norepopath Don't add the reponame to the download path. Can only be used when syncing a single repository (default is to add the reponame) -g, --gpgcheck Remove packages that fail GPG signature checking after downloading -u, --urls Just list urls of what would be downloaded, don't download -n, --newest-only Download only newest packages per-repo -q, --quiet Output as little as possible -l, --plugins enable yum plugin support -m, --downloadcomps also download comps.xml --download-metadata download all the non-default metadata 3、示例 $> bash -c 'cat > ceph.repo 七、下载软件RPM包以及其依赖包 1、yum插件yumdownloadonly yum install yum-plugin-downloadonly && yum install --downloadonly --downloaddir=/root/httpd httpd 2、使用yum-utils的命令yumdownloader yum install -y yum-utils && \\ yumdownloader --resolve docker-ce-20.10.5 附录：常见软件源 1、VirtualBox [virtualbox] name=Oracle Linux / RHEL / CentOS-$releasever / $basearch - VirtualBox baseurl=http://download.virtualbox.org/virtualbox/rpm/el/$releasever/$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://www.virtualbox.org/download/oracle_vbox.asc 2、Nginx [nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 3、EPEL [epel] name=Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch failovermethod=priority enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 4、Ceph [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 5、ELK Stack [ELK-Stack-5.x] name=ELK Stack repository for 5.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/5.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md [ELK-Stack-6.x] name=ELK Stack repository for 6.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/6.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md 6、MySQL [MySQL-Community-5.6] name=MySQL Community 5.6 baseurl=https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.6-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-5.7] name=MySQL Community 5.7 baseurl=https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-8.0] name=MySQL Community 8.0 baseurl=https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-8.0-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-Connectors] name=MySQL Community Connectors baseurl=http://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-connectors-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-Tools] name=MySQL Community Tools baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 7、MongoDB bash -c 'cat > /etc/yum.repos.d/mongoDb.repo Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-30 11:22:11 "},"origin/linux-zsh.html":{"url":"origin/linux-zsh.html","title":"ZSH","keywords":"","body":"Linux zsh && oh-my-zsh 一、简介 Zsh 也许是目前最好用的 shell，是 bash 替代品中较为优秀的一个。 Zsh 官网：http://www.zsh.org/ Zsh具有以下主要优势： 完全兼容bash，之前bash下的使用习惯，shell脚本都可以完全兼容 更强大的tab补全 更智能的切换目录 命令选项、参数补齐 大小写字母自动更正 有着丰富多彩的主题 更强大的alias命令 智能命令错误纠正 集成各种类型的插件 oh-my-zsh 是最为流行的 zsh 配置文件，提供了大量的主题和插件，极大的拓展了 zsh 的功能，推动了 zsh 的流行，有点类似于 rails 之于 ruby。 二、安装zsh CentOS yum install -y zsh Ubuntu apt-get install -y zsh 检查下系统的 shell：$ cat /etc/shells，你会发现多了一个：/bin/zsh 设置用户的默认shell # 给root用户设置 chsh -s /bin/zsh root # 给普通账户设置 chsh -s /bin/zsh 用户名 #　恢复bash chsh -s /bin/bash [user] 三、安装oh-my-zsh oh-my-zsh 帮我们整理了一些常用的 Zsh 扩展功能和主题，我们无需自己去捣搞 Zsh，直接用 oh-my-zsh 就足够了。 oh-my-zsh 官网：https://ohmyz.sh/ oh-my-zsh Github：https://github.com/robbyrussell/oh-my-zsh curl sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" # 下载相关插件 git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting wget sh -c \"$(wget -O- https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 手动 curl -Lo install.sh https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh sh install.sh 四、oh-my-zsh配置 oh-my-zsh的配置文件路径为~/.zshrc # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # 指定oh-my-zsh的安装路径 export ZSH=\"/root/.oh-my-zsh\" # 设置主题。如果设置为\"random\", 每次启动oh-my-zsh会随机加载主题,可使用`echo $RANDOM_THEME`查看每次加载的主题 ZSH_THEME=\"alanpeabody\" # 设置随机的主题 # ZSH_THEME_RANDOM_CANDIDATES=( \"robbyrussell\" \"agnoster\" ) # 大小写是否敏感 # CASE_SENSITIVE=\"true\" # Uncomment the following line to use hyphen-insensitive completion. # Case-sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\"true\" # 设置是否自动更新 # DISABLE_AUTO_UPDATE=\"true\" # 设置自动更新的天数。默认13天 # export UPDATE_ZSH_DAYS=13 # 设置是否开启`ls`进行颜色显示 # DISABLE_LS_COLORS=\"true\" # 设置是否显示终端标题 # DISABLE_AUTO_TITLE=\"true\" # 设置是否开启语法修正 # ENABLE_CORRECTION=\"true\" # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\"true\" # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\"true\" # 历史输入命令的时间展示格式 # HIST_STAMPS=\"mm/dd/yyyy\" # 设置自定义配置文件的路径 # ZSH_CUSTOM=/path/to/new-custom-folder # 设置存储历史命令的默认文件路径 # HISTFILE=~/.zsh_history # 配置要加载的插件（配置的插件要能在 ~/.oh-my-zsh/plugins/* 下找到，自定义的插件目录为 ~/.oh-my-zsh/custom/plugins/ ）.注意：插件安装的越多，zsh的启动速度越慢，选择使用率最高的插件才是最好的选择 plugins=( git zsh-autosuggestions zsh-syntax-highlighting kubectl docker sudo extract ) source /etc/profile source $ZSH/oh-my-zsh.sh # 用户配置 # 设置man文档的环境变量 # export MANPATH=\"/usr/local/man:$MANPATH\" # 设置语言环境变量 # export LANG=en_US.UTF-8 # 设置本地和远程sessions的首选编辑器 # if [[ -n $SSH_CONNECTION ]]; then # export EDITOR='vim' # else # export EDITOR='mvim' # fi # 设置编译标志 # export ARCHFLAGS=\"-arch x86_64\" # 设置别名 alias ll=\"ls -alh\" alias k='kubectl' . ~/.oh-my-zsh/custom/oc_zsh_completion 五、oh-my-zsh常用插件 git：可以使用git缩写，默认自带 git add --all => gaa 查看所有缩写：alias | grep git autojump：快速跳转文件夹 last-working-dir：可以记录上一次退出命令行时候的所在路径，并且在下一次启动命令行的时候自动恢复到上一次所在的路径。 wd：快速地切换到常用的目录 wd add web相当于给当前目录做了一个标识，标识名叫做 web ，我们下次如果再想进入这个目录，只需输入：wd web catimg：将图片的内容输出到命令行 catimg demo.jpg zsh-syntax-highlighting：命令高亮 正确路径自带下划线 安装：git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions：自动补全可能的路径 安装：git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions sudo：连按两次Esc添加或去掉sudo extract：功能强大的解压插件 执行x demo.tar.gz git-open：在终端里打开当前项目的远程仓库地址 安装：git clone https://github.com/paulirish/git-open.git $ZSH_CUSTOM/plugins/git-open history：内置，快速搜索history 六、oh-my-zsh常用主题 官方主题：https://github.com/robbyrussell/oh-my-zsh/wiki/Themes Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-31 14:22:30 "},"origin/linux-进程管理工具SystemD.html":{"url":"origin/linux-进程管理工具SystemD.html","title":"Systemd-进程管理","keywords":"","body":"Linux 进程管理工具SystemD 一、简介 SystemD即为system daemon，是linux下的一种init软件，由Lennart Poettering带头开发，并在LGPL 2.1及其后续版本许可证下开源发布，开发目标是提供更优秀的框架以表示系统服务间的依赖关系，并依此实现系统初始化时服务的并行启动，同时达到降低Shell的系统开销的效果，最终代替现在常用的System V与BSD风格init程序。 SystemD是一个专用于 Linux 操作系统的系统与服务管理器。当作为启动进程(PID=1)运行时，它将作为初始化系统运行，也就是启动并维护各种用户空间的服务。 Linux内核加载启动后，用户空间的第一个进程就是初始化进程，这个程序的物理文件约定位于/sbin/init，当然也可以通过传递内核参数来让内核启动指定的程序。这个进程的特点是进程号为1，代表第一个运行的用户空间进程。不同发行版采用了不同的启动程序，主要有以下几种主流选择： 以Ubuntu为代表的Linux发行版采用upstart。 以7.0版本之前的CentOS为代表的System V init。 CentOS 7.0版本开始的Systemd。 为了与传统的 SysV 兼容，如果将 systemd 以 init 名称启动，并且\"PID≠1\"，那么它将执行 telinit 命令并将所有命令行参数原封不动的传递过去。 这样对于普通的登录会话来说，无论是调用 init 还是调用 telinit 都是等价的。 当作为系统实例运行时，systemd将会按照system.conf配置文件以及system.conf.d配置目录中的指令工作；当作为用户实例运行时，systemd 将会按照user.conf配置文件 以及 user.conf.d配置目录中的指令工作。 systemd将各种系统启动和运行相关的对象，表示为各种不同类型的单元(unit)，并提供了处理不同单元之间依赖关系的能力。大部分单元都静态的定义在单元文件中，但是有少部分单元则是动态自动生成的：其中一部分来自于其他传统的配置文件(为了兼容性)，而另一部分则动态的来自于系统状态或可编程的运行时状态。单元既可以处于活动(active)状态，也可以处于停止(inactive)状态，当然也可以处于启动中(activating)或停止中(deactivating)的状态。还有一个特殊的失败(failed)状态，意思是单元以某种方式失败了(进程崩溃了、或者触碰启动频率限制、或者退出时返回了错误代码、或者遇到了操作超时之类的故障)。当进入失败(failed)状态时，导致故障的原因将被记录到日志中以方便日后排查。需要注意的是，不同的单元可能还会有各自不同的\"子状态\"，但它们都被映射到上述五种状态之一。 历史上，Linux 的启动一直采用init进程。这种方法有两个缺点 启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 当前系统/etc/inittab这个文件的内容，这个文件是systme V init的标准配置文件，如今变成了 # inittab is no longer used when using systemd. # # ADDING CONFIGURATION HERE WILL HAVE NO EFFECT ON YOUR SYSTEM. # # Ctrl-Alt-Delete is handled by /etc/systemd/system/ctrl-alt-del.target # # systemd uses 'targets' instead of runlevels. By default, there are two main targets: # # multi-user.target: analogous to runlevel 3 # graphical.target: analogous to runlevel 5 # # To set a default target, run: # systemctl set-default TARGET.target 在systemd掌权后，inittab不再起作用，也没有了“运行级”的概念。现在起作用的配置文件是/etc/systemd/system/default.target这个文件了。此文件的内容如下： # This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. [Unit] Description=Multi-User System Documentation=man:systemd.special(7) Requires=basic.target Conflicts=rescue.service rescue.target After=basic.target rescue.service rescue.target AllowIsolate=yes 作为系统初始化系统，systemd 的最大特点有两个： 令人惊奇的激进的并发启动能力，极大地提高了系统启动速度； 用 CGroup 统计跟踪子进程，干净可靠。 此外，和其前任不同的地方在于: systemd 已经不仅仅是一个初始化系统了 Systemd 出色地替代了 sysvinit 的所有功能。因为 init 进程是系统所有进程的父进程这样的特殊性，systemd 非常适合提供曾经由其他服务提供的功能，比如定时任务 (以前由 crond 完成) ；会话管理 (以前由 ConsoleKit/PolKit 等管理) 有助于标准化 Linux 的管理！如果所有的 Linux 发行版都采纳了 systemd，那么系统管理任务便可以很大程度上实现标准化 此外 systemd 有个很棒的承诺：接口保持稳定，不会再轻易改动 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统 systmed是一个用户空间的程序，属于应用程序，不属于Linux内核范畴，Linux内核的主要特征在所有发行版中是统一的，厂商可以自由改变的是用户空间的应用程序 二、基本概念 1. Unit单元 系统初始化要做很多工作，如挂在文件系统，启动sshd服务，配置交换分区，这都可以看做是一个配置单元，Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。systemd把配置单元分成分成12种 Service unit：系统服务 Target unit：多个 Unit 构成的一个逻辑分组，可以当成是SystemV中的运行级。 Device Unit：硬件设备 Mount Unit：文件系统的挂载点，systemd据此进行自动挂载，为了与SystemV兼容，目前systemd自动处理/etc/fstab并转化为mount Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：配置swap交换分区文件 Timer Unit：定时器。用来定时触发用户定义的操作，它可以用来取代传统的atd，crond等。 每一个配置单元都有一个对应的配置文件，系统管理员的任务就是编写和维护这写不同的配置文件，比如一个MySql服务对应一个mysql.service文件。 2. 依赖关系 systemd并不能完全解除各个单元之间的依赖关系，如物理设备单元准备就绪之前，不可能执行挂载单元。为此需要定义各个单元之间的依赖关系。有依赖的地方就会有出现死循环的可能，比如A依赖于B，B依赖于C，C依赖于A，那么导致死锁。systemd为此提供了两种不同程度的依赖关系，一个是require，一个是want，出现死循环时，systemd会尝试忽略want类型的依赖，如仍不能解锁，那么systemd报错。 如前所述，在 Systemd 中，所有的服务都并发启动，比如 Avahi、D-Bus、livirtd、X11、HAL 可以同时启动。乍一看，这似乎有点儿问题，比如 Avahi 需要 syslog 的服务，Avahi 和 syslog 同时启动，假设 Avahi 的启动比较快，所以 syslog 还没有准备好，可是 Avahi 又需要记录日志，这岂不是会出现问题？ Systemd 的开发人员仔细研究了服务之间相互依赖的本质问题，发现所谓依赖可以分为三个具体的类型，而每一个类型实际上都可以通过相应的技术解除依赖关系。 并发启动原理之一：解决 socket 依赖 绝大多数的服务依赖是套接字依赖。比如服务 A 通过一个套接字端口 S1 提供自己的服务，其他的服务如果需要服务 A，则需要连接 S1。因此如果服务 A 尚未启动，S1 就不存在，其他的服务就会得到启动错误。所以传统地，人们需要先启动服务 A，等待它进入就绪状态，再启动其他需要它的服务。Systemd 认为，只要我们预先把 S1 建立好，那么其他所有的服务就可以同时启动而无需等待服务 A 来创建 S1 了。如果服务 A 尚未启动，那么其他进程向 S1 发送的服务请求实际上会被 Linux 操作系统缓存，其他进程会在这个请求的地方等待。一旦服务 A 启动就绪，就可以立即处理缓存的请求，一切都开始正常运行。 那么服务如何使用由 init 进程创建的套接字呢？ Linux 操作系统有一个特性，当进程调用 fork 或者 exec 创建子进程之后，所有在父进程中被打开的文件句柄 (file descriptor) 都被子进程所继承。套接字也是一种文件句柄，进程 A 可以创建一个套接字，此后当进程 A 调用 exec 启动一个新的子进程时，只要确保该套接字的 close_on_exec 标志位被清空，那么新的子进程就可以继承这个套接字。子进程看到的套接字和父进程创建的套接字是同一个系统套接字，就仿佛这个套接字是子进程自己创建的一样，没有任何区别。 这个特性以前被一个叫做 inetd 的系统服务所利用。Inetd 进程会负责监控一些常用套接字端口，比如 Telnet，当该端口有连接请求时，inetd 才启动 telnetd 进程，并把有连接的套接字传递给新的 telnetd 进程进行处理。这样，当系统没有 telnet 客户端连接时，就不需要启动 telnetd 进程。Inetd 可以代理很多的网络服务，这样就可以节约很多的系统负载和内存资源，只有当有真正的连接请求时才启动相应服务，并把套接字传递给相应的服务进程。 和 inetd 类似，systemd 是所有其他进程的父进程，它可以先建立所有需要的套接字，然后在调用 exec 的时候将该套接字传递给新的服务进程，而新进程直接使用该套接字进行服务即可。 并发启动原理之二：解决 D-Bus 依赖 D-Bus 是 desktop-bus 的简称，是一个低延迟、低开销、高可用性的进程间通信机制。它越来越多地用于应用程序之间通信，也用于应用程序和操作系统内核之间的通信。很多现代的服务进程都使用D-Bus 取代套接字作为进程间通信机制，对外提供服务。比如简化 Linux 网络配置的 NetworkManager 服务就使用 D-Bus 和其他的应用程序或者服务进行交互：邮件客户端软件 evolution 可以通过 D-Bus 从 NetworkManager 服务获取网络状态的改变，以便做出相应的处理。 D-Bus 支持所谓\"bus activation\"功能。如果服务 A 需要使用服务 B 的 D-Bus 服务，而服务 B 并没有运行，则 D-Bus 可以在服务 A 请求服务 B 的 D-Bus 时自动启动服务 B。而服务 A 发出的请求会被 D-Bus 缓存，服务 A 会等待服务 B 启动就绪。利用这个特性，依赖 D-Bus 的服务就可以实现并行启动。 并发启动原理之三：解决文件系统依赖 系统启动过程中，文件系统相关的活动是最耗时的，比如挂载文件系统，对文件系统进行磁盘检查（fsck），磁盘配额检查等都是非常耗时的操作。在等待这些工作完成的同时，系统处于空闲状态。那些想使用文件系统的服务似乎必须等待文件系统初始化完成才可以启动。但是 systemd 发现这种依赖也是可以避免的。 Systemd 参考了 autofs 的设计思路，使得依赖文件系统的服务和文件系统本身初始化两者可以并发工作。autofs 可以监测到某个文件系统挂载点真正被访问到的时候才触发挂载操作，这是通过内核 automounter 模块的支持而实现的。比如一个 open()系统调用作用在\"/misc/cd/file1\"的时候，/misc/cd 尚未执行挂载操作，此时 open()调用被挂起等待，Linux 内核通知 autofs，autofs 执行挂载。这时候，控制权返回给 open()系统调用，并正常打开文件。 Systemd 集成了 autofs 的实现，对于系统中的挂载点，比如/home，当系统启动的时候，systemd 为其创建一个临时的自动挂载点。在这个时刻/home 真正的挂载设备尚未启动好，真正的挂载操作还没有执行，文件系统检测也还没有完成。可是那些依赖该目录的进程已经可以并发启动，他们的 open()操作被内建在 systemd 中的 autofs 捕获，将该 open()调用挂起（可中断睡眠状态）。然后等待真正的挂载操作完成，文件系统检测也完成后，systemd 将该自动挂载点替换为真正的挂载点，并让 open()调用返回。由此，实现了那些依赖于文件系统的服务和文件系统本身同时并发启动。 当然对于\"/\"根目录的依赖实际上一定还是要串行执行，因为 systemd 自己也存放在/之下，必须等待系统根目录挂载检查好。 不过对于类似/home 等挂载点，这种并发可以提高系统的启动速度，尤其是当/home 是远程的 NFS 节点，或者是加密盘等，需要耗费较长的时间才可以准备就绪的情况下，因为并发启动，这段时间内，系统并不是完全无事可做，而是可以利用这段空余时间做更多的启动进程的事情，总的来说就缩短了系统启动时间。 3. Target和runlevel systemd使用target取代了systemV的运行级的概念，Sysvinit 运行级别和 systemd 目标的对应表 Sysvinit 运行级别 Systemd 目标 备注 0 runlevel0.target, poweroff.target 关闭系统。 1, s, single runlevel1.target, rescue.target 单用户模式。 2, 4 runlevel2.target, runlevel4.target, multi-user.target 用户定义/域特定运行级别。默认等同于 3。 3 runlevel3.target, multi-user.target 多用户，非图形化。用户可以通过多个控制台或网络登录。 5 runlevel5.target, graphical.target 多用户，图形化。通常为所有运行级别 3 的服务外加图形化登录。 6 runlevel6.target, reboot.target 重启 emergency emergency.target 紧急 Shell 三、Systemd包含的命令 Systemd 是一个完整的软件包，Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。安装完成后有很多物理文件组成，大致分布为，配置文件位于/etc/systemd这个目录下，配置工具命令位于/bin，和/sbin这两个目录下，预先准备的备用配置文件位于/lib/systemd目录下，还有库文件和帮助手册等等。这是一个庞大的软件包。详情使用rpm -ql systemd即可查看。 1. 电源管理工具 Systemd 可用于管理系统。 关机不是每个登录用户在任何情况下都可以执行的，一般只有管理员才可以关机。正常情况下系统不应该允许 SSH 远程登录的用户执行关机命令。否则其他用户正在工作，一个用户把系统关了就不好了。为了解决这个问题，传统的 Linux 系统使用 ConsoleKit 跟踪用户登录情况，并决定是否赋予其关机的权限。现在 ConsoleKit 已经被 systemd 的 logind 所替代。 logind 不是 pid-1 的 init 进程。它的作用和 UpStart 的 session init 类似，但功能要丰富很多，它能够管理几乎所有用户会话(session)相关的事情。logind 不仅是 ConsoleKit 的替代，它可以： 维护，跟踪会话和用户登录情况。如上所述，为了决定关机命令是否可行，系统需要了解当前用户登录情况，如果用户从 SSH 登录，不允许其执行关机命令；如果普通用户从本地登录，且该用户是系统中的唯一会话，则允许其执行关机命令；这些判断都需要 logind 维护所有的用户会话和登录情况。 Logind 也负责统计用户会话是否长时间没有操作，可以执行休眠/关机等相应操作。 为用户会话的所有进程创建 CGroup。这不仅方便统计所有用户会话的相关进程，也可以实现会话级别的系统资源控制。 负责电源管理的组合键处理，比如用户按下电源键，将系统切换至睡眠状态。 多席位(multi-seat) 管理。如今的电脑，即便一台笔记本电脑，也完全可以提供多人同时使用的计算能力。多席位就是一台电脑主机管理多个外设，比如两个屏幕和两个鼠标/键盘。席位一使用屏幕 1 和键盘 1；席位二使用屏幕 2 和键盘 2，但他们都共享一台主机。用户会话可以自由在多个席位之间切换。或者当插入新的键盘，屏幕等物理外设时，自动启动 gdm 用户登录界面等。所有这些都是多席位管理的内容。ConsoleKit 始终没有实现这个功能，systemd 的 logind 能够支持多席位。 # 重启系统 $ systemctl reboot # 关闭系统，切断电源 $ systemctl poweroff # CPU停止工作 $ systemctl halt # 暂停系统 $ systemctl suspend # 让系统进入冬眠状态 $ systemctl hibernate # 让系统进入交互式休眠状态 $ systemctl hybrid-sleep # 启动进入救援状态（单用户状态） $ systemctl rescue 2. 服务消耗分析工具 systemd-analyze命令用于查看启动耗时 # 查看启动耗时 $ systemd-analyze # 查看每个服务的启动耗时 $ systemd-analyze blame # 显示瀑布状的启动过程流 $ systemd-analyze critical-chain # 显示指定服务的启动流 $ systemd-analyze critical-chain atd.service 3. 主机信息信息管理工具 hostnamectl命令用于查看当前主机的信息。 # 显示当前主机的信息 $ hostnamectl # 设置主机名。 $ hostnamectl set-hostname rhel7 4. 本地化设置管理工具 localectl命令用于查看本地化设置。 # 查看本地化设置 $ localectl # 设置本地化参数。 $ localectl set-locale LANG=en_GB.utf8 $ localectl set-keymap en_GB 5. 时区管理工具 timedatectl命令用于查看当前时区设置。 # 查看当前时区设置 $ timedatectl # 显示所有可用的时区 $ timedatectl list-timezones # 设置当前时区 $ timedatectl set-timezone America/New_York $ timedatectl set-time YYYY-MM-DD $ timedatectl set-time HH:MM:SS 6. 登陆会话管理工具 loginctl命令用于查看当前登录的用户。 # 列出当前session $ loginctl list-sessions # 列出当前登录用户 $ loginctl list-users # 列出显示指定用户的信息 $ loginctl show-user test 三、Unit 每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 1. Unit配置文件结构 配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如[Unit]。注意： 配置文件的区块名和字段名，都是大小写敏感的。 每个区块内部是一些等号连接的键值对，键值对的等号两侧不能有空格。 [Unit] Description=ATD daemon [Service] Type=forking ExecStart=/usr/bin/atd [Install] WantedBy=multi-user.target Unit 配置文件的完整字段清单，请参考官方文档。 [Unit]区块 [Unit]区块通常是配置文件的第一个区块。用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。 Description：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition...：当前 Unit 运行必须满足的条件，否则不会运行 Assert...：当前 Unit 运行必须满足的条件，否则会报启动失败 [Service]区块 [Service]区块用来定义如何启动当前服务，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 [Install]区块 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 +.wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit 2. Unit管理 3.2.1 systemctl enable 命令用于在上面两个目录之间，建立符号链接关系。如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 $ systemctl enable clamd@scan.service # 等同于 $ ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 3.2.2 systemctl disable 用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 systemctl disable clamd@scan.service 3.2.3 systemctl list-unit-files systemctl list-unit-files会显示每个配置文件的状态。而每个配置文件的状态，一共有四种 enabled：已建立启动链接 disabled：没建立启动链接 static：该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖 masked：该配置文件被禁止建立启动链接 # 列出所有配置文件 $ systemctl list-unit-files # 列出指定类型的配置文件 $ systemctl list-unit-files --type=service 3.2.4 systemctl list-units 查看当前系统的所有 Unit # 列出正在运行的 Unit $ systemctl list-units # 列出所有Unit，包括没有找到配置文件的或者启动失败的 $ systemctl list-units --all # 列出所有没有运行的 Unit $ systemctl list-units --all --state=inactive # 列出所有加载失败的 Unit $ systemctl list-units --failed # 列出所有正在运行的、类型为 service 的 Unit $ systemctl list-units --type=service 3.2.5 systemctl status systemctl status命令用于查看系统状态和单个 Unit 的状态 # 显示系统状态 $ systemctl status # 显示单个 Unit 的状态 $ sysystemctl status httpd httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled) Active: active (running) since 金 2014-12-05 12:18:22 JST; 7min ago Main PID: 4349 (httpd) Status: \"Total requests: 1; Current requests/sec: 0; Current traffic: 0 B/sec\" CGroup: /system.slice/httpd.service ├─4349 /usr/sbin/httpd -DFOREGROUND ├─4350 /usr/sbin/httpd -DFOREGROUND ├─4351 /usr/sbin/httpd -DFOREGROUND ├─4352 /usr/sbin/httpd -DFOREGROUND ├─4353 /usr/sbin/httpd -DFOREGROUND └─4354 /usr/sbin/httpd -DFOREGROUND 12月 05 12:18:22 localhost.localdomain systemd[1]: Starting The Apache HTTP Server... 12月 05 12:18:22 localhost.localdomain systemd[1]: Started The Apache HTTP Server. 12月 05 12:22:40 localhost.localdomain systemd[1]: Started The Apache HTTP Server. # 显示远程主机的某个 Unit 的状态 $ systemctl -H root@rhel7.example.com status httpd.service 3.2.6 查询Unit状态 # 显示某个 Unit 是否正在运行 $ systemctl is-active application.service # 显示某个 Unit 是否处于启动失败状态 $ systemctl is-failed application.service # 显示某个 Unit 服务是否建立了启动链接 $ systemctl is-enabled application.service 3.2.7 Unit状态管理 # 立即启动一个服务 $ systemctl start apache.service # 立即停止一个服务 $ systemctl stop apache.service # 重启一个服务 $ systemctl restart apache.service # 杀死一个服务的所有子进程 $ systemctl kill apache.service # 重新加载一个服务的配置文件 $ systemctl reload apache.service # 重载所有修改过的配置文件 $ systemctl daemon-reload # 显示某个 Unit 的所有底层参数 $ systemctl show httpd.service # 显示某个 Unit 的指定属性的值 $ systemctl show -p CPUShares httpd.service # 设置某个 Unit 的指定属性 $ systemctl set-property httpd.service CPUShares=500 3.2.8 查询Unit间的依赖关系 Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 # 列出一个 Unit 的所有依赖。 $ systemctl list-dependencies nginx.service # 上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用--all参数。 $ systemctl list-dependencies --all nginx.service 四、Unit的日志管理 Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。journalctl功能强大，用法非常多。 # 查看所有日志（默认情况下 ，只保存本次启动的日志） $ journalctl # 查看内核日志（不显示应用日志） $ journalctl -k # 查看系统本次启动的日志 $ journalctl -b $ journalctl -b -0 # 查看上一次启动的日志（需更改设置） $ journalctl -b -1 # 查看指定时间的日志 $ journalctl --since=\"2012-10-30 18:17:16\" $ journalctl --since \"20 min ago\" $ journalctl --since yesterday $ journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" $ journalctl --since 09:00 --until \"1 hour ago\" # 显示尾部的最新10行日志 $ journalctl -n # 显示尾部指定行数的日志 $ journalctl -n 20 # 实时滚动显示最新日志 $ journalctl -f # 查看指定服务的日志 $ journalctl /usr/lib/systemd/systemd # 查看指定进程的日志 $ journalctl _PID=1 # 查看某个路径的脚本的日志 $ journalctl /usr/bin/bash # 查看指定用户的日志 $ journalctl _UID=33 --since today # 查看某个 Unit 的日志 $ journalctl -u nginx.service $ journalctl -u nginx.service --since today # 实时滚动显示某个 Unit 的最新日志 $ journalctl -u nginx.service -f # 合并显示多个 Unit 的日志 $ journalctl -u nginx.service -u php-fpm.service --since today # 查看指定优先级（及其以上级别）的日志，共有8级 # 0: emerg # 1: alert # 2: crit # 3: err # 4: warning # 5: notice # 6: info # 7: debug $ journalctl -p err -b # 日志默认分页输出，--no-pager 改为正常的标准输出 $ journalctl --no-pager # 以 JSON 格式（单行）输出 $ journalctl -b -u nginx.service -o json # 以 JSON 格式（多行）输出，可读性更好 $ journalctl -b -u nginx.serviceqq -o json-pretty # 显示日志占据的硬盘空间 $ journalctl --disk-usage # 指定日志文件占据的最大空间 $ journalctl --vacuum-size=1G # 指定日志文件保存多久 $ journalctl --vacuum-time=1years 五、Target 启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于\"状态点\"，启动某个 Target 就好比启动到某种状态。 传统的init启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。 1. 与传统 RunLevel 的对应关系如下: Traditional runlevel New target name --> Symbolically linked to Runlevel 0 runlevel0.target -> poweroff.target Runlevel 1 runlevel1.target -> rescue.target Runlevel 2 runlevel2.target -> multi-user.target Runlevel 3 runlevel3.target -> multi-user.target Runlevel 4 runlevel4.target -> multi-user.target Runlevel 5 runlevel5.target -> graphical.target Runlevel 6 runlevel6.target -> reboot.target 2. 与init进程的主要差别如下: 默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。 启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。 3. Target管理 # 查看当前系统的所有 Target $ systemctl list-unit-files --type=target # 查看一个 Target 包含的所有 Unit $ systemctl list-dependencies multi-user.target # 查看启动时的默认 Target $ systemctl get-default # 设置启动时的默认 Target $ sudo systemctl set-default multi-user.target # 切换 Target 时，默认不关闭前一个 Target 启动的进程， # systemctl isolate 命令改变这种行为， # 关闭前一个 Target 里面所有不属于后一个 Target 的进程 $ systemctl isolate multi-user.target 六、常见服务的Unit配置 1. Zookeeper bash -c ' cat > /usr/lib/systemd/system/zookeeper.service 2. Kafka bash -c ' cat > /usr/lib/systemd/system/kafka.service 3. Tomcat bash -c ' cat > /usr/lib/systemd/system/tomcat.service 参考 http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html https://blog.51cto.com/andyxu/2122109?source=dra Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-zombie-orphaned-process.html":{"url":"origin/linux-zombie-orphaned-process.html","title":"僵尸进程与孤儿进程","keywords":"","body":"孤儿进程orphaned与僵尸进程Zombie 一、概念 ​ 我们知道在unix/linux中，正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程 到底什么时候结束。 当一个 进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。 　　孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 　　僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 二、僵尸进程 在linux系统中，当用ps命令观察进程的执行状态时，经常看到某些进程的状态栏为defunct，这就是所谓的“僵尸”进程。“僵尸”进程是一个早已死亡的进程，但在进程表（processs table）中仍占了一个位置（slot）。由于进程表的容量是有限的，所以，defunct进程不仅占用系统的内存资源，影响系统的性能，而且如果其数目太多，还会导致系统瘫痪。 1、僵尸进程的产生原因 我们知道，每个进程在进程表里都有一个进入点（entry），核心程序执行该进程时使用到的一切信息都存储在进入点。当用ps命令察看系统中的进程信息时，看到的就是进程表中的相关数据。 所以，当一个父进程以fork()系统调用建立一个新的子进程后，核心进程就会在进程表中给这个子进程分配一个进入点，然后将相关信息存储在该进入点所对应的进程表内。这些信息中有一项是其父进程的识别码。 而当这个子进程结束的时候（比如调用exit命令结束），其实他并没有真正的被销毁，而是留下一个称为僵尸进程（Zombie）的数据结构（系统调用exit的作用是使进程退出，但是也仅仅限于一个正常的进程变成了一个僵尸进程，并不能完全将其销毁）。此时原来进程表中的数据会被该进程的退出码（exit code）、执行时所用的CPU时间等数据所取代，这些数据会一直保留到系统将它传递给它的父进程为止。由此可见，defunct进程的出现时间是在子进程终止后，但是父进程尚未读取这些数据之前。 此时，该僵尸子进程已经放弃了几乎所有的内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态信息供其他进程收集，除此之外，僵尸进程不再占有任何存储空间。他需要他的父进程来为他收尸，如果他的父进程没有安装SIGCHLD信号处理函数调用wait 或 waitpid() 等待子进程结束，也没有显式忽略该信号，那么它就一直保持僵尸状态，如果这时候父进程结束了，那么init进程会自动接手这个子进程，为他收尸，他还是能被清除掉的。但是如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是系统中为什么有时候会有很多的僵尸进程。 2、如何杀死僵尸进程 如上可知，僵尸进程一旦出现之后，很难自己消亡，会一直存在下去，直至系统重启。虽然僵尸进程几乎不占系统资源，但是，这样下去，数量太多了之后，终究会给系统带来其他的影响。因此，如果一旦见到僵尸进程，我们就要将其杀掉。如何杀掉僵尸进程呢？ 有同学可能会说，很简单嘛，直接使用kill命令就好啊。或者，实在不行，加一个-9的后缀（kill -9），肯定杀掉！ 请注意：defunct状态下的僵尸进程是不能直接使用kill -9命令杀掉的，否则就不叫僵尸进程了。那么，该如何杀呢？ 方法有二： 重启服务器电脑，这个是最简单，最易用的方法，但是如果你服务器电脑上运行有其他的程序，那么这个方法，代价很大。所以，尽量使用下面一种方法。 找到该defunct僵尸进程的父进程，将该进程的父进程杀掉，则此defunct进程将自动消失。 问题又来了，如何找到defunct僵尸进程的父进程呢？ 很简单，一句命令就够了：ps -ef | grep defunct_process_pid。 3、如何预防僵尸进程 以上介绍的只是在发现了僵尸进程之后，如何去杀死它。那么，有同学可能会说了，这个是治标不治本的。真正的办法是，不让它产生，问题才能彻底解决。OK，那我们就来介绍一下，如何预防僵尸进程的产生。 在父进程创建子进程之前，就向系统申明自己并不会对这个子进程的exit动作进行任何关注行为，这样的话，子进程一旦退出后，系统就不会去等待父进程的操作，而是直接将该子进程的资源回收掉，也就不会出现僵尸进程了。具体的办法就是，在父进程的初始化函数中，调用这个函数：signal(SIGCHLD,SIG_IGN)； 如果上述语句没来得及调用，也有另外一个办法。那就是在创建完子进程后，用waitpid等待子进程返回，也能达到上述效果； 如果上述两个办法都不愿意采用，那还有一招：在父进程创建子进程的时候，连续调用两次fork()，而且使紧跟的子进程直接退出，使其孙子进程成为孤儿进程，从而init进程将代替父进程来接手，负责清除这个孤儿进程。于是，父进程就无需进行任何的清理行为，系统会自动处理； 三、问题及危害 　　unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。 　　孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。 　　任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。 四、僵尸进程危害场景 　　例如有个进程，它定期的产 生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用ps命令查看的话，就会看到很多状态为Z的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大 量僵死进程的那个元凶枪毙掉（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进 程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了。 参考： https://blog.csdn.net/LEON1741/article/details/78142269 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-proc.html":{"url":"origin/linux-proc.html","title":"proc文件详解","keywords":"","body":"Proc文件系统 一、简介 Linux系统上的/proc目录是一种文件系统，即proc文件系统。与其它常见的文件系统不同的是，/proc是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，用户可以通过这些文件查看有关系统硬件及当前正在运行进程的信息，甚至可以通过更改其中某些文件来改变内核的运行状态。 基于/proc文件系统如上所述的特殊性，其内的文件也常被称作虚拟文件，并具有一些独特的特点。例如，其中有些文件虽然使用查看命令查看时会返回大量信息，但文件本身的大小却会显示为0字节。此外，这些特殊文件中大多数文件的时间及日期属性通常为当前系统时间和日期，这跟它们随时会被刷新（存储于RAM中）有关。 为了查看及使用上的方便，这些文件通常会按照相关性进行分类存储于不同的目录甚至子目录中，如/proc/scsi目录中存储的就是当前系统上所有SCSI设备的相关信息，/proc/N中存储的则是系统当前正在运行的进程的相关信息，其中N为正在运行的进程（可以想象得到，在某进程结束后其相关目录则会消失）。 大多数虚拟文件可以使用文件查看命令如cat、more或者less进行查看，有些文件信息表述的内容可以一目了然，但也有文件的信息却不怎么具有可读性。不过，这些可读性较差的文件在使用一些命令如apm、free、lspci或top查看时却可以有着不错的表现。 二、常见文件 1 10824 11989 12474 15454 4283 5481 7368 9174 cpuinfo kpagecount softirqs 10 10827 11992 12476 15461 4288 5497 8 9328 crypto kpageflags stat 10005 10828 11993 12477 15462 4289 5541 8192 9362 devices loadavg swaps 10026 10829 11994 12479 15463 430 555 8204 9382 diskstats locks synobios 10136 10863 11995 12481 15464 4376 562 8214 9385 dma mdstat syno_cpu_arch 10137 10864 11996 12483 15538 4522 5698 8215 9386 driver megaraid syno_loadavg 10246 11 11997 12567 15577 4523 580 8235 9387 execdomains meminfo syno_platform 10295 11242 11998 12568 15610 4524 6025 8246 9388 fb misc synotty 10297 11274 11999 12777 15887 4651 6026 8251 9389 filesystems modules sys 10365 11297 12 12812 16 5 6027 8254 9394 fs mounts sysrq-trigger 10369 11328 12001 12873 16104 5007 603 8257 9397 interrupts mpt sysvipc 10371 11366 12002 12904 16124 5010 6057 8258 9400 iomem mtrr thread-self 10372 11394 12022 13 16131 5013 607 8455 9619 ioports net timer_list 10606 11400 12067 13471 16133 5016 612 8457 9620 irq pagetypeinfo tty 10738 11425 12104 13681 16139 5289 6149 9 acpi kallsyms partitions uptime 10773 11704 12106 13685 16195 5394 6179 9075 buddyinfo kcore sched_debug version 10777 11712 12314 14 16324 5398 6290 9077 bus keys schedstat vmallocinfo 10821 11715 12420 15 16380 5405 690 9078 cgroups key-users scsi vmstat 10822 11801 12470 15084 16384 5407 7 9079 cmdline kmsg self zoneinfo 10823 11805 12472 15099 16387 5436 7342 9080 consoles kpagecgroup slabinfo 1、进程相关文件 /proc目录中包含许多以数字命名的子目录，这些数字表示系统当前正在运行进程的进程号，里面包含对应进程相关的多个信息文件。每个数字目录下是当前进程本身相关的信息文件。（以1号初始化进程为例） dr-xr-xr-x root 0 Feb 4 09:34 attr -r-------- root 0 Feb 4 09:34 auxv -r--r--r-- root 0 Feb 4 09:20 cgroup --w------- root 0 Feb 4 09:34 clear_refs -r--r--r-- root 0 Feb 4 09:34 cmdline # 启动当前进程的完整命令，但僵尸进程目录中的此文件不包含任何信息 -rw-r--r-- root 0 Feb 4 09:34 comm -rw-r--r-- root 0 Feb 4 09:34 coredump_filter lrwxrwxrwx root 0 Feb 4 09:34 cwd -> / # 指向当前进程运行目录的一个符号链接 -r-------- root 0 Feb 4 09:34 environ # 当前进程的环境变量列表，彼此间用空字符（NULL）隔开；变量用大写字母表示，其值用小写字母表示 lrwxrwxrwx root 0 Feb 4 09:34 exe -> /usr/sbin/init #指向启动当前进程的可执行文件（完整路径）的符号链接，通过/proc/N/exe可以启动当前进程的一个拷贝 dr-x------ root 0 Feb 4 09:34 fd # 这是个目录，包含当前进程打开的每一个文件的文件描述符，这些文件描述符是指向实际文件的一个符号链接； dr-x------ root 0 Feb 4 09:34 fdinfo -r-------- root 0 Feb 4 09:34 io -r--r--r-- root 0 Feb 4 09:34 limits # 当前进程所使用的每一个受限资源的软限制、硬限制和管理单元；此文件仅可由实际启动当前进程的UID用户读取 dr-x------ root 0 Feb 4 09:34 map_files -r--r--r-- root 0 Feb 4 09:34 maps # 当前进程关联到的每个可执行文件和库文件在内存中的映射区域及其访问权限所组成的列表 -rw------- root 0 Feb 4 09:34 mem # 当前进程所占用的内存空间，由open、read和lseek等系统调用使用，不能被用户读取； -r--r--r-- root 0 Feb 4 09:34 mountinfo -r--r--r-- root 0 Feb 4 09:34 mounts -r-------- root 0 Feb 4 09:34 mountstats dr-xr-xr-x root 0 Feb 4 09:34 net dr-x--x--x root 0 Feb 4 09:34 ns -rw-r--r-- root 0 Feb 4 09:34 oom_adj -r--r--r-- root 0 Feb 4 09:34 oom_score -rw-r--r-- root 0 Feb 4 09:34 oom_score_adj -r-------- root 0 Feb 4 09:34 pagemap -r-------- root 0 Feb 4 09:34 personality lrwxrwxrwx root 0 Feb 4 09:34 root -> / # 指向当前进程运行根目录的符号链接；在Unix和Linux系统上，通常采用chroot命令使每个进程运行于独立的根目录 -rw-r--r-- root 0 Feb 4 09:34 sched -r--r--r-- root 0 Feb 4 09:34 schedstat -r--r--r-- root 0 Feb 4 09:34 smaps -r-------- root 0 Feb 4 09:34 stack -r--r--r-- root 0 Feb 4 09:20 stat # 当前进程的状态信息，包含一系统格式化后的数据列，可读性差，通常由ps命令使用 -r--r--r-- root 0 Feb 4 09:34 statm # 当前进程占用内存的状态信息，通常以“页面”（page）表示 -r--r--r-- root 0 Feb 4 09:20 status # 与stat所提供信息类似，但可读性较好，如下所示，每行表示一个属性信息 -r-------- root 0 Feb 4 09:34 syscall dr-xr-xr-x root 0 Feb 4 09:34 task # 包含由当前进程所运行的每一个线程的相关信息，每个线程的相关信息文件均保存在一个由线程号（tid）命名的目录中，这类似于其内容类似于每个进程目录中的内容；（内核2.6版本以后支持此功能） -r--r--r-- root 0 Feb 4 09:34 wchan 2、/proc/sys目录 与/proc下其它文件的“只读”属性不同的是，管理员可对/proc/sys子目录中的许多文件内容进行修改以更改内核的运行特性，事先查看某文件是否“可写入”。写入操作通常使用类似于echo DATA > /path/to/your/filename的格式进行。需要注意的是，即使文件可写，其一般也不可以使用编辑器进行编辑。 3、其他目录文件 /proc/apm：高级电源管理（APM）版本信息及电池相关状态信息，通常由apm命令使用； /proc/buddyinfo：用于诊断内存碎片问题的相关信息文件； /proc/cmdline：在启动时传递至内核的相关参数信息，这些信息通常由lilo或grub等启动管理工具进行传递； syno_hdd_powerup_seq=1 HddHotplug=0 syno_hw_version=DS918+ vender_format_version=2 console=ttyS0,115200n8 withefi quiet syno_hdd_detect=0 root=/dev/md0 sn=1780PDN998701 mac1=001132112233 mac2=001132112231 netif_num=2 /proc/cpuinfo：处理器的相关信息的文件； /proc/crypto：系统上已安装的内核使用的密码算法及每个算法的详细信息列表； name : crc32c driver : crc32c-generic module : kernel priority : 0 type : digest blocksize : 32 digestsize : 4 ………… /proc/devices：系统已经加载的所有块设备和字符设备的信息，包含主设备号和设备组（与主设备号对应的设备类型）名； Character devices: 1 mem 4 /dev/vc/0 4 tty 4 ttyS ………… Block devices: 1 ramdisk 2 fd 8 sd ………… /proc/diskstats：每块磁盘设备的磁盘I/O统计信息列表；（内核2.5.69以后的版本支持此功能） /proc/dma：每个正在使用且注册的ISA DMA通道的信息列表； /proc/execdomains：内核当前支持的执行域（每种操作系统独特“个性”）信息列表； /proc/fb：帧缓冲设备列表文件，包含帧缓冲设备的设备号和相关驱动信息； /proc/filesystems：当前被内核支持的文件系统类型列表文件，被标示为nodev的文件系统表示不需要块设备的支持；通常mount一个设备时，如果没有指定文件系统类型将通过此文件来决定其所需文件系统的类型； nodev sysfs nodev rootfs nodev proc iso9660 ext3 ………… /proc/interrupts：X86或X86_64体系架构系统上每个IRQ相关的中断号列表；多路处理器平台上每个CPU对于每个I/O设备均有自己的中断号； CPU0 CPU1 CPU2 CPU3 0: 11 0 0 0 IO-APIC 2-edge timer 3: 0 0 0 0 IO-APIC 3-edge serial 4: 206 0 0 0 IO-APIC 4-edge serial 8: 0 0 0 0 IO-APIC 8-fasteoi rtc0 9: 0 0 0 0 IO-APIC 9-fasteoi acpi 87: 7594 15750 1492 9913 PCI-MSI 311296-edge 0000:00:13.0 88: 3137 0 67388 0 PCI-MSI 1048576-edge 0000:02:00.0 89: 34 0 0 0 PCI-MSI 327680-edge xhci_hcd 90: 1 0 0 0 PCI-MSI 1572864-edge eth0 91: 67 0 13 38093 PCI-MSI 1572865-edge eth0-rx-0 92: 1058 715 235 881 PCI-MSI 1572866-edge eth0-rx-1 93: 5258 0 470 1026 PCI-MSI 1572867-edge eth0-tx-0 94: 1949 0 0 36357 PCI-MSI 1572868-edge eth0-tx-1 95: 166 0 0 0 PCI-MSI 32768-edge i915 NMI: 102 96 80 81 Non-maskable interrupts LOC: 3399022 3276720 3075216 3057331 Local timer interrupts SPU: 0 0 0 0 Spurious interrupts PMI: 102 96 80 81 Performance monitoring interrupts /proc/iomem：每个物理设备上的记忆体（RAM或者ROM）在系统内存中的映射信息； 00000100-00000fff : reserved 00001000-0003efff : System RAM 0003f000-0003ffff : ACPI Non-volatile Storage 00040000-0009ffff : System RAM 000a0000-000bffff : PCI Bus 0000:00 000c0000-000dffff : PCI Bus 0000:00 000c0000-000c7fff : Video ROM /proc/ioports：当前正在使用且已经注册过的与物理设备进行通讯的输入-输出端口范围信息列表；如下面所示，第一列表示注册的I/O端口范围，其后表示相关的设备； 0000-001f : dma1 0020-0021 : pic1 0040-0043 : timer0 0050-0053 : timer1 0060-006f : keyboard /proc/kallsyms：模块管理工具用来动态链接或绑定可装载模块的符号定义，由内核输出；（内核2.5.71以后的版本支持此功能）；通常这个文件中的信息量相当大； /proc/kcore：系统使用的物理内存，以ELF核心文件（core file）格式存储，其文件大小为已使用的物理内存（RAM）加上4KB；这个文件用来检查内核数据结构的当前状态，因此，通常由GBD通常调试工具使用，但不能使用文件查看命令打开此文件； /proc/kmsg：此文件用来保存由内核输出的信息，通常由/sbin/klogd或/bin/dmsg等程序使用，不要试图使用查看命令打开此文件； /proc/locks：保存当前由内核锁定的文件的相关信息，包含内核内部的调试数据；每个锁定占据一行，且具有一个惟一的编号；如下输出信息中每行的第二列表示当前锁定使用的锁定类别，POSIX表示目前较新类型的文件锁，由lockf系统调用产生，FLOCK是传统的UNIX文件锁，由flock系统调用产生；第三列也通常由两种类型，ADVISORY表示不允许其他用户锁定此文件，但允许读取，MANDATORY表示此文件锁定期间不允许其他用户任何形式的访问； 1: POSIX ADVISORY WRITE 20975 00:1f:9017052 0 EOF 2: POSIX ADVISORY WRITE 20975 00:1f:9016783 0 EOF 3: POSIX ADVISORY WRITE 20975 00:1f:9016653 0 EOF 4: POSIX ADVISORY WRITE 20975 00:1f:9016501 0 EOF 5: POSIX ADVISORY WRITE 20975 00:1f:9016442 0 EOF 6: POSIX ADVISORY WRITE 20975 00:1f:9016361 0 EOF 7: POSIX ADVISORY WRITE 20975 00:1f:9016114 0 EOF 8: POSIX ADVISORY WRITE 20975 00:1f:9016042 0 EOF 9: POSIX ADVISORY WRITE 20975 00:1f:9015963 0 EOF 10: POSIX ADVISORY WRITE 20975 00:1f:28255 0 EOF /proc/mdstat：保存RAID相关的多块磁盘的当前状态信息，在没有使用RAID机器上，其显示unused devices: ： Personalities : [linear] [raid0] [raid1] [raid10] [raid6] [raid5] [raid4] md4 : active raid1 sdf5[0] 483555456 blocks super 1.2 [1/1] [U] md2 : active raid1 sdb3[0] 10094080 blocks super 1.2 [1/1] [U] md3 : active raid1 sde3[0] 971940544 blocks super 1.2 [1/1] [U] md1 : active raid1 sdb2[0] sde2[1] sdf2[2] 2097088 blocks [16/3] [UUU_____________] md0 : active raid1 sdb1[0] sde1[1] sdf1[2] 2490176 blocks [16/3] [UUU_____________] unused devices: /proc/meminfo：系统中关于当前内存的利用状况等的信息，常由free命令使用；可以使用文件查看命令直接读取此文件，其内容显示为两列，前者为统计属性，后者为对应的值； MemTotal: 8058608 kB MemFree: 138636 kB MemAvailable: 484704 kB Buffers: 6000 kB Cached: 590724 kB SwapCached: 68220 kB Active: 4024568 kB Inactive: 3306388 kB Active(anon): 3719356 kB Inactive(anon): 3106256 kB Active(file): 305212 kB Inactive(file): 200132 kB Unevictable: 2528 kB Mlocked: 2528 kB SwapTotal: 6930348 kB SwapFree: 6119948 kB Dirty: 624 kB Writeback: 0 kB AnonPages: 6706328 kB Mapped: 223620 kB Shmem: 91104 kB Slab: 208120 kB SReclaimable: 113072 kB SUnreclaim: 95048 kB KernelStack: 28144 kB PageTables: 81552 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 10959652 kB Committed_AS: 21421636 kB VmallocTotal: 34359738367 kB VmallocUsed: 0 kB VmallocChunk: 0 kB DirectMap4k: 14124 kB DirectMap2M: 8263680 kB /proc/mounts：在内核2.4.29版本以前，此文件的内容为系统当前挂载的所有文件系统，在2.4.19以后的内核中引进了每个进程使用独立挂载名称空间的方式，此文件则随之变成了指向/proc/self/mounts（每个进程自身挂载名称空间中的所有挂载点列表）文件的符号链接；/proc/self是一个独特的目录。如下所示，其中第一列表示挂载的设备，第二列表示在当前目录树中的挂载点，第三点表示当前文件系统的类型，第四列表示挂载属性（ro或者rw），第五列和第六列用来匹配/etc/mtab文件中的转储（dump）属性； /dev/md0 / ext4 rw,relatime,barrier,data=ordered 0 0 none /dev devtmpfs rw,nosuid,noexec,relatime,size=4016084k,nr_inodes=1004021,mode=755 0 0 none /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 none /proc proc rw,nosuid,nodev,noexec,relatime 0 0 none /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 /tmp /tmp tmpfs rw,relatime 0 0 /run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 /dev/shm /dev/shm tmpfs rw,nosuid,nodev,relatime 0 0 none /sys/fs/cgroup tmpfs rw,relatime,size=4k,mode=755 0 0 cgmfs /run/cgmanager/fs tmpfs rw,relatime,size=100k,mode=755 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuset,clone_children 0 0 cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu,release_agent=/run/cgmanager/agents/cgm-release-agent.cpu 0 0 cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuacct 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio,release_agent=/run/cgmanager/agents/cgm-release-agent.blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory,release_agent=/run/cgmanager/agents/cgm-release-agent.memory 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices,release_agent=/run/cgmanager/agents/cgm-release-agent.devices 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer,release_agent=/run/cgmanager/agents/cgm-release-agent.freezer 0 0 none /proc/bus/usb devtmpfs rw,nosuid,noexec,relatime,size=4016084k,nr_inodes=1004021,mode=755 0 0 none /sys/kernel/debug debugfs rw,relatime 0 0 securityfs /sys/kernel/security securityfs rw,relatime 0 0 /dev/md2 /volume1 btrfs rw,relatime,synoacl,space_cache=v2,auto_reclaim_space,metadata_ratio=50,subvolid=257,subvol=/@syno 0 0 /dev/vg1000/lv /volume3 btrfs rw,relatime,relatime_period=30,synoacl,space_cache=v2,auto_reclaim_space,metadata_ratio=50,subvolid=257,subvol=/@syno 0 0 /proc/modules：当前装入内核的所有模块名称列表，可以由lsmod命令使用，也可以直接查看；如下所示，其中第一列表示模块名，第二列表示此模块占用内存空间大小，第三列表示此模块有多少实例被装入，第四列表示此模块依赖于其它哪些模块，第五列表示此模块的装载状态（Live：已经装入；Loading：正在装入；Unloading：正在卸载），第六列表示此模块在内核内存（kernel memory）中的偏移量； xt_ipvs 2202 0 - Live 0xffffffffa0fe0000 ip_vs_rr 1447 0 - Live 0xffffffffa0fdc000 ip_vs 127371 3 xt_ipvs,ip_vs_rr, Live 0xffffffffa0fb3000 xt_mark 1317 0 - Live 0xffffffffa0faf000 iptable_mangle 1656 0 - Live 0xffffffffa0fab000 br_netfilter 13589 0 - Live 0xffffffffa0fa3000 bridge 55340 1 br_netfilter, Live 0xffffffffa0f8d000 stp 1693 1 bridge, Live 0xffffffffa0f89000 aufs 194415 0 - Live 0xffffffffa0f4d000 macvlan 13776 0 - Live 0xffffffffa0f45000 veth 5094 0 - Live 0xffffffffa0f40000 xt_conntrack 3401 5 - Live 0xffffffffa0f3c000 xt_addrtype 2893 1 - Live 0xffffffffa0f38000 ipt_MASQUERADE 1213 25 - Live 0xffffffffa0f34000 xt_REDIRECT 1486 0 - Live 0xffffffffa0f30000 /proc/partitions：块设备每个分区的主设备号（major）和次设备号（minor）等信息，同时包括每个分区所包含的块（block）数目 major minor #blocks name 1 0 655360 ram0 1 1 655360 ram1 1 2 655360 ram2 1 3 655360 ram3 1 4 655360 ram4 1 5 655360 ram5 1 6 655360 ram6 1 7 655360 ram7 1 8 655360 ram8 1 9 655360 ram9 1 10 655360 ram10 1 11 655360 ram11 1 12 655360 ram12 1 13 655360 ram13 1 14 655360 ram14 1 15 655360 ram15 8 16 15638616 sdb 8 17 2490240 sdb1 8 18 2097152 sdb2 8 19 10095104 sdb3 8 20 65536 sdb4 8 21 64512 sdb5 /proc/slabinfo :在内核中频繁使用的对象（如inode、dentry等）都有自己的cache，即slab pool，而/proc/slabinfo文件列出了这些对象相关slap的信息 /proc/stat：实时追踪自系统上次启动以来的多种统计信息； “cpu”行后的八个值分别表示以1/100（jiffies）秒为单位的统计值（包括系统运行于用户模式、低优先级用户模式，运系统模式、空闲模式、I/O等待模式的时间等）； “intr”行给出中断的信息，第一个为自系统启动以来，发生的所有的中断的次数；然后每个数对应一个特定的中断自系统启动以来所发生的次数； “ctxt”给出了自系统启动以来CPU发生的上下文交换的次数。 “btime”给出了从系统启动到现在为止的时间，单位为秒； “processes (total_forks) 自系统启动以来所创建的任务的个数； “procs_running”：当前运行队列的任务的数目； “procs_blocked”：当前被阻塞的任务的数目； cpu 223139 48 44015 1586826 69922 0 2477 0 0 0 cpu0 56431 23 11065 395817 17523 0 649 0 0 0 cpu1 54109 10 11450 397915 17649 0 596 0 0 0 cpu2 55572 10 10954 397559 16719 0 695 0 0 0 cpu3 57026 3 10544 395533 18031 0 535 0 0 0 intr 12065678 11 0 0 0 206 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 31886 66307 34 1 32453 2413 5989 32388 166 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ctxt 20744413 btime 1612399810 processes 37878 procs_running 1 procs_blocked 0 softirq 10803666 2 4776615 1052 818173 113621 0 2279 1548289 0 3543635 /proc/swaps：当前系统上的交换分区及其空间利用信息，如果有多个交换分区的话，则会每个交换分区的信息分别存储于/proc/swap目录中的单独文件中，而其优先级数字越低，被使用到的可能性越大 Filename Type Size Used Priority /dev/md1 partition 2097084 0 -1 /dev/zram0 partition 1208316 207996 1 /dev/zram1 partition 1208316 207660 1 /dev/zram2 partition 1208316 207612 1 /dev/zram3 partition 1208316 207976 1 /proc/uptime：系统上次启动以来的运行时间，如下所示，其第一个数字表示系统运行时间，第二个数字表示系统空闲时间，单位是秒； 4578.13 14964.34 /proc/version：当前系统运行的内核版本号 Linux version 4.4.59+ (root@build3) (gcc version 4.9.3 20150311 (prerelease) (crosstool-NG 1.20.0) ) #24922 SMP PREEMPT Thu Mar 28 11:07:03 CST 2019 /proc/vmstat：当前系统虚拟内存的多种统计数据，信息量可能会比较大，这因系统而有所不同，可读性较好（2.6以后的内核支持此文件） nr_anon_pages 22270 nr_mapped 8542 nr_file_pages 47706 nr_slab 4720 nr_page_table_pages 897 nr_dirty 21 nr_writeback 0 ………… /proc/zoneinfo：内存区域（zone）的详细信息列表 Node 0, zone DMA pages free 1208 min 28 low 35 high 42 active 439 inactive 1139 scanned 0 (a: 7 i: 30) spanned 4096 present 4096 nr_anon_pages 192 nr_mapped 141 nr_file_pages 1385 nr_slab 253 nr_page_table_pages 2 nr_dirty 523 nr_writeback 0 nr_unstable 0 nr_bounce 0 protection: (0, 0, 296, 296) pagesets all_unreclaimable: 0 prev_priority: 12 start_pfn: 0 ………… 参考 http://bbs.chinaunix.net/thread-2002769-1-1.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-04 10:33:02 "},"origin/linux-ps.html":{"url":"origin/linux-ps.html","title":"ps","keywords":"","body":"Linux ps 一、简介 在Linux系统中，ps(process status)命令常常用来捕获系统在某一时间的进程状态 ps命令支持的语法格式： UNIX 风格，选项可以组合在一起，并且选项前必须有“-”连字符 BSD 风格，选项可以组合在一起，但是选项前不能有“-”连字符 GNU 风格的长选项，选项前有两个“-”连字符 Linux进程状态: 状态码 进程状态 含义 R 运行（runnable (on run queue)） 正在运行或在运行队列中等待 S 中断（sleeping） 休眠中, 受阻, 在等待某个条件的形成或接受到信号 D 不可中断（uninterruptible sleep ）(usually IO) 收到信号不唤醒和不可运行, 进程必须等待直到有中断发生 Z 僵死（a defunct (”zombie”) process） 进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放 T 停止（traced or stopped） 进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行 显示进程信息格式 ps aux最初用到Unix Style中，而ps -ef被用在System V Style中，两者输出略有不同。现在的大部分Linux系统都是可以同时使用这两种方式的 au(x)输出格式 $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 194364 7256 ? Ss 09:27 0:25 /usr/lib/systemd/systemd .... USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 用户名 进程ID 进程占用的CPU百分比 占用CPU百分比 该进程使用的虚拟內存量（KB） 该进程占用的固定內存量（KB）（驻留中页的数量） 进程在那个终端上运行。若与终端无关，则显示? 若为pts/0等，则表示由网络连接主机进程。 进程的状态 进程启动时间 该进程实际使用CPU运行的时间 所执行的命令及参数 ps -ef $ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 09:27 ? 00:00:25 /usr/lib/systemd/systemd .... UID PID PPID C STIME TTY CMD 用户ID 进程ID 父进程ID 占用CPU百分比 进程启动时间 进程在那个终端上运行。若与终端无关，则显示? 若为pts/0等，则表示由网络连接主机进程。 所执行的命令及参数 STAT状态位常见的状态字符 状态字符 含义 D 无法中断的休眠状态（通常 IO 的进程） R 正在运行可中在队列中可过行的 S 处于休眠状态 T 停止或被追踪 X 死掉的进程 （基本很少见） Z 僵尸进程 优先级高的进程 N 优先级较低的进程 L 有些页被锁进内存 s 进程的领导者（在它之下有子进程） l 多线程，克隆线程（使用 CLONE_THREAD, 类似 NPTL pthreads） + 位于后台的进程组 二、命令参数 ps [options] [--help] -a 显示所有终端机下执行的进程，除了阶段作业领导者之外。 a 显示现行终端机下的所有进程，包括其他用户的进程。 -A 显示所有进程。 -c 显示CLS和PRI栏位。 c 列出进程时，显示每个进程真正的指令名称，而不包含路径，参数或常驻服务的标示。 -C 　指定执行指令的名称，并列出该指令的进程的状况。 -d 　显示所有进程，但不包括阶段作业领导者的进程。 -e 　此参数的效果和指定\"A\"参数相同。 e 　列出进程时，显示每个进程所使用的环境变量。 -f 　显示UID,PPIP,C与STIME栏位。 f 　用ASCII字符显示树状结构，表达进程间的相互关系。 -g 　此参数的效果和指定\"-G\"参数相同，当亦能使用阶段作业领导者的名称来指定。 g 　显示现行终端机下的所有进程，包括群组领导者的进程。 -G 　列出属于该群组的进程的状况，也可使用群组名称来指定。 h 　不显示标题列。 -H 　显示树状结构，表示进程间的相互关系。 -j或j 　采用工作控制的格式显示进程状况。 -l或l 　采用详细的格式来显示进程状况。 L 　列出栏位的相关信息。 -m或m 　显示所有的执行绪。 n 　以数字来表示USER和WCHAN栏位。 -N 　显示所有的进程，除了执行ps指令终端机下的进程之外。 -p 　指定进程识别码，并列出该进程的状况。 p 　此参数的效果和指定\"-p\"参数相同，只在列表格式方面稍有差异。 r 　只列出现行终端机正在执行中的进程。 -s 　指定阶段作业的进程识别码，并列出隶属该阶段作业的进程的状况。 s 　采用进程信号的格式显示进程状况。 S 　列出进程时，包括已中断的子进程资料。 -t 　指定终端机编号，并列出属于该终端机的进程的状况。 t 　此参数的效果和指定\"-t\"参数相同，只在列表格式方面稍有差异。 -T 　显示现行终端机下的所有进程。 -u 　此参数的效果和指定\"-U\"参数相同。 u 　以用户为主的格式来显示进程状况。 -U 　列出属于该用户的进程的状况，也可使用用户名称来指定。 U 　列出属于该用户的进程的状况。 v 　采用虚拟内存的格式显示进程状况。 -V或V 　显示版本信息。 -w或w 　采用宽阔的格式来显示进程状况。　 x 　显示所有进程，不以终端机来区分。 X 　采用旧式的Linux i386登陆格式显示进程状况。 -y 配合参数\"-l\"使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位。 三、示例 1. 查看CPU占用最多的前10个进程 ps auxw|head -1;ps auxw|sort -rn -k3|head -10 2.显示所有当前进程 ps -ax -a参数，-a 代表 all。 -x参数会显示没有控制终端的进程 3. 查看虚拟内存使用最多的前10个进程 ps auxw | head -1 ; ps auxw|sort -rn -k5|head -10 4. 根据用户名过滤进程 ps -u root $ ps -u root PID TTY TIME CMD 1 ? 00:00:25 systemd 2 ? 00:00:00 kthreadd 3 ? 00:00:07 ksoftirqd/0 6 ? 00:00:02 kworker/u40:0 8 ? 00:00:05 migration/0 5. 根据内存使用来升序排序 ps -aux --sort -pmem USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 1000130+ 13437 16.7 9.2 13303016 5703272 ? Ssl 09:29 131:50 /usr/lib/jvm/jre/bin/java .... 1000540+ 25444 8.7 4.1 9575924 2533824 ? Ssl 09:30 68:56 /usr/share/elasticsearch/ ..... 6. 根据 CPU 使用来升序排序 ps -aux --sort -pcpu $ USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 7602 48.8 0.3 4883792 231504 ? Ssl 09:29 385:36 /usr/bin/hyperkube kubelet .... root 3282 25.0 1.4 4151700 895280 ? Ssl 09:28 198:12 openshift start master api 7. 格式化输出 指定用户（真实或有效的UID）创建的进程 ps -U pid_user -u pid_user u -U 参数按真实用户 ID(RUID) 筛选进程，它会从用户列表中选择真实用户名或 ID。真实用户即实际创建该进程的用户 -u 参数用来筛选有效用户 ID（EUID） u 参数用来决定以针对用户的格式输出，由 User, PID, %CPU, %MEM, VSZ, RSS, TTY, STAT, START, TIME 和 COMMAND这几列组成 $ pid_user=ceph && ps -U $pid_user -u $pid_user u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1474 0.2 0.0 398604 40772 ? Ssl 09:27 2:10 /usr/bin/ceph-mon -f --cluster ceph --id ... ceph 1478 0.0 0.0 393520 34540 ? Ssl 09:27 0:21 /usr/bin/ceph-mds -f --cluster ceph --id ... ceph 1845 0.1 0.3 1104056 219284 ? Ssl 09:27 0:59 /usr/bin/ceph-osd -f --cluster ceph --id ... 参考 https://blog.csdn.net/ShiXueTanLang/article/details/80781089 https://www.cnblogs.com/jiqing9006/p/10036676.html https://www.cnblogs.com/xiangtingshen/p/10920236.html http://www.sohu.com/a/238434503_100180425 https://www.cnblogs.com/fps2tao/p/10313337.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/network-tcpdump.html":{"url":"origin/network-tcpdump.html","title":"tcpdump网络抓包","keywords":"","body":"Tcpdump 一、简介 tcpdump就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 tcpdump是一个用于截取网络分组，并输出分组内容的工具。凭借强大的功能和灵活的截取策略，使其成为类UNIX系统下用于网络分析和问题排查的首选工具 tcpdump 支持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来帮助你去掉无用的信息 tcpdump需要有root权限。 tcpdump使用了独立于系统的libpcap的接口。libpcap是linux平台下的网络数据包捕获函数包 tcpdump调用libpcap的接口在linux系统链路层抓包。而linux本身指定的许多访问控制规则都是基于三层或三层以上的过滤规则，所以tcpdump可以抓取过滤规则之前的数据包。 tcpdump可以使用带-w参数的tcpdump 截获数据并保存到文件中，然后再使用Wireshark进行解码分析 tcpdump工作在网卡的混杂模式 二、使用 1、命令格式 tcpdump [-aAbdDefhHIJKlLnNOpqStuUvxX#] [ -B size ] [ -c count ] [ -C file_size ] [ -E algo:secret ] [ -F file ] [ -G seconds ] [ -i interface ] [ -j tstamptype ] [ -M secret ] [ --number ] [ -Q in|out|inout ] [ -r file ] [ -s snaplen ] [ --time-stamp-precision precision ] [ --immediate-mode ] [ -T type ] [ --version ] [ -V file ] [ -w file ] [ -W filecount ] [ -y datalinktype ] [ -z postrotate-command ] [ -Z user ] [ expression ] 2、命令选项 ①抓包选项： -A: 以ASCII码方式显示每一个数据包(不会显示数据包中链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据(nt: 即Handy for capturing web pages). -C file-size: (nt: 此选项用于配合-w file 选项使用) 该选项使得tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与-w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M=1024 ＊ 1024 ＝ 1,048,576) -c：指定要抓取的包数量。 -d : 以容易阅读的形式,在标准输出上打印出编排过的包匹配码, 随后tcpdump停止.(nt | rt: human readable, 容易阅读的,通常是指以ascii码来打印一些信息. compiled, 编排过的. packet-matching code, 包匹配码,含义未知, 需补充) -dd: 以C语言的形式打印出包匹配码. -ddd: 以十进制数的形式打印出包匹配码(会在包匹配码之前有一个附加的'count'前缀). -i interface：指定tcpdump需要监听的接口。默认会抓取第一个网络接口 -N: 不列出域名 -n：对地址以数字方式显式，否则显式为主机名，也就是说-n选项不做主机名解析。 -nn：除了-n的作用外，还把端口显示为数值，否则显示端口服务名。 -P：指定要抓取的包是流入还是流出的包。可以给定的值为in、out和inout，默认为\"inout\"。 -p: 不让网络界面进入混杂模式 -O: 不将数据包编码最佳化 -s len：设置tcpdump的数据包抓取长度为len。 ​ 如果不设置默认将会是65535字节。对于要抓取的数据包较大时，长度设置不够可能会产生包截断，若出现包截断，输出行中会出现\"[|proto]\"的标志(proto实际会显示为协议名)。但是抓取len越长，包的处理时间越长，并且会减少tcpdump可缓存的数据包的数量，从而会导致数据包的丢失，所以在能抓取我们想要的包的前提下，抓取长度越小越好。 -S： 打印TCP 数据包的顺序号时, 使用绝对的顺序号, 而不是相对的顺序号. ​ 相对顺序号可理解为, 相对第一个TCP 包顺序号的差距,比如, 接受方收到第一个数据包的绝对顺序号为232323, 对于后来接收到的第2个,第3个数据包, tcpdump会打印其序列号为1, 2分别表示与第一个数据包的差距为1 和 2. 而如果此时-S 选项被设置, 对于后来接收到的第2个, 第3个数据包会打印出其绝对顺序号:232324, 232325 -y datalinktype：设置tcpdump 只捕获数据链路层协议类型是datalinktype的数据包 ②输出选项： -e：输出的每行中都将包括数据链路层头部信息，例如源MAC和目标MAC。 -q：快速打印输出。即打印很少的协议相关信息，从而输出行都比较简短。 -t：在每行输出中不打印时间戳 -tt：不对每行输出的时间进行格式处理(nt: 这种格式一眼可能看不出其含义, 如时间戳打印成1261798315) -ttt：tcpdump 输出时, 每两行打印之间会延迟一个段时间(以毫秒为单位) -tttt：在每行打印的时间戳之前添加日期的打印 -X：输出包的头部数据，会以16进制和ASCII两种方式同时输出。 -XX：输出包的头部数据，会以16进制和ASCII两种方式同时输出，更详细。 -v：当分析和打印的时候，产生详细的输出。 -vv：产生比-v更详细的输出。 -vvv：产生比-vv更详细的输出。 ③其他功能性选项： -D：列出可用于抓包的接口。将会列出接口的数值编号和接口名，它们都可以用于\"-i\"后。 -F：从文件中读取抓包的表达式。若使用该选项，则命令行中给定的其他表达式都将失效。 -w：将抓包数据输出到文件中而不是标准输出。可以同时配合”-G time\"选项使得输出文件每time秒就自动切换到另一个文件。可通过\"-r\"选项载入这些文件以进行分析和打印。 -r：从给定的数据包文件中读取数据。使用\"-\"表示从标准输入中读取 3、条件表达式定义过滤规则 表达式用于决定哪些数据包将被过滤打印. 如果不给定条件表达式, 网络上所有被捕获的包都会被打印,否则, 只有满足条件表达式的数据包被打印 表达式格式： tcpdump 命令选项 proto dir type proto(protocol协议类型的过滤器)：根据协议进行过滤 tcp： udp： icmp： ether： fddi： tr： wlan： ip： ip6： arp： rarp： decnet： 若未给定协议类型，则匹配所有可能的类型 dir（direction数据流向类型的过滤器）：根据数据流向进行过滤 src： dst： src or dst： src and dst： type（关键词类型的过滤器）： host： net： port： portrange： ether： gateway： 例如：host 192.168.201.128 , net 128.3, port 20, portrange 6000-6008’ 4、过滤规则的组合 表达式单元之间可以使用操作符\" and / && / or / || / not / ! \"进行连接，从而组成复杂的条件表达式。 and：所有的条件都需要满足，也可以表示为 && or：只要有一个条件满足就可以，也可以表示为 || not：取反，也可以使用 ! 而在单个过滤器里，常常会判断一条件是否成立，这时候，就要使用下面两个符号 =：判断二者相等 ==：判断二者相等 !=：判断二者不相等 使用括号\"()\"可以改变表达式的优先级，但需要注意的是括号会被shell解释，所以应该使用反斜线\"\"转义为\"()\"，在需要的时候，还需要包围在引号中。 当你使用这两个符号时，tcpdump 还提供了一些关键字的接口来方便我们进行判断，比如 if：表示网卡接口名 proc：表示进程名 pid：表示进程id svc：表示 service class dir：表示方向，in 和 out eproc：表示 effective process name epid：表示 effective process ID 三、抓包解读 输出格式 $ tcpdump -S -e -nn -i eth0 dst 192.168.1.7 and port 5000 06:24:16.887580 dc:a6:32:c4:88:d2 > 00:11:32:11:22:33, ethertype IPv4 (0x0800), length 338: 192.168.1.18.54832 > 192.168.1.7.5000: Flags [P.], seq 2391754816:2391755088, ack 1379501343, win 2051, options [nop,nop,TS val 238444658 ecr 19745762], length 272 第一列：时分秒毫秒 21:26:49.013621 第二列：源MAC地址 第三列：目标MAC地址 第四列：网络协议及其长度 第五列：源IP地址.端口号 第六列：目标IP地址.端口号 第七列：数据包内容，包括Flags 标识符，seq 号，ack 号，win 窗口，数据长度 length，其中 [P.] 表示 PUSH 标志位为 1， Flags 标识符 [S] : SYN（开始连接） [P] : PSH（推送数据） [F] : FIN （结束连接） [R] : RST（重置连接） [.] : 没有 Flag，由于除了 SYN 包外所有的数据包都有ACK，所以一般这个标志也可表示 ACK 四、示例 1、找出一段时间内发包数最多的 IP $ tcpdump -nnn -t -c 200 | cut -f 1,2,3,4 -d '.' | sort | uniq -c | sort -nr | head -n 20 200 packets captured 244 packets received by filter 0 packets dropped by kernel 103 IP 192.168.1.18 72 IP 220.64.11.92 25 IP 192.168.1.7 2、抓取 HTTP GET 请求 $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420' $ tcpdump -vv -A -l -i6 | grep 'GET' 3、抓取 HTTP POST请求 $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354' $ tcpdump -vv -A -l -i6 | grep 'POST' 4、抓取 HTTP请求中的 User-Agent和Host $ tcpdump -nn -A -i6 -l | egrep -i 'User-Agent:|Host:' 5、抓取 HTTP请求中的主机名和路径 $ tcpdump -i6 -v -n -l | egrep -i \"POST /|GET /|Host:\" .Ke..B]'POST /webapi/entry.cgi HTTP/1.1 Host: 192.168.1.7:5000 5、抓取 HTTP 有效数据包 抓取 80 端口的 HTTP 有效数据包，排除 TCP 连接建立过程的数据包（SYN / FIN / ACK） $ tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)>2)) != 0)' 参考 https://baijiahao.baidu.com/s?id=1671144485218215170&wfr=spider&for=pc https://www.jianshu.com/p/d9162722f189 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-29 16:31:05 "},"origin/linux-hping.html":{"url":"origin/linux-hping.html","title":"hping","keywords":"","body":"网络分析工具hping 一、简介 官方网站：http://www.hping.org/ GitHub：https://github.com/antirez/hping 二、安装 包管理器 Brew brew install hping # hping二进制可执行命令安装/usr/local/sbin,不是在/usr/local/bin。要将/usr/local/sbin加到PATH中 APT apt install hping3 YUM yum install hping3 APK apk add hping3 源码安装 源码安装文档：https://github.com/antirez/hping/blob/master/INSTALL 所需依赖：Libpcap、Tcl/Tk yum install libpcap-devel tc-devel ln -s /usr/include/pcap-bpf.h /usr/include/net/bpf.h wget http://www.hping.org/hping3-20051105.tar.gz tar zxvf hping3-20051105.tar.gz cd hping3-20051105 ./configure make make install 三、命令参数 hping3 host [options] 参数 -H --help 显示帮助。 -v -VERSION 版本信息。 -c --count count 发送数据包的次数 关于countreached_timeout 可以在hping2.h里编辑。 -i --interval 包发送间隔时间（单位是毫秒）缺省时间是1秒,此功能在增加传输率上很重要,在idle/spoofing扫描时此功能也会被用到,你可以参考hping-howto获得更多信息-fast 每秒发10个数据包。 -n -nmeric 数字输出，象征性输出主机地址。 -q -quiet 退出。 -I --interface interface-name 无非就是eth0之类的参数。 -v --verbose 显示很多信息，TCP回应一般如：len=46 ip=192.168.1.1 flags=RADF seq=0 ttl=255 id=0 win=0 rtt=0.4ms tos=0 iplen=40 seq=0 ack=1380893504 sum=2010 urp=0 -D --debug 进入debug模式当你遇到麻烦时，比如用HPING遇到一些不合你习惯的时候，你可以用此模式修改HPING，（INTERFACE DETECTION,DATA LINK LAYER ACCESS,INTERFACE SETTINGS,.......） -z --bind 快捷键的使用。 -Z --unbind 消除快捷键。 -O --rawip RAWIP模式，在此模式下HPING会发送带数据的IP头。 -1 --icmp ICMP模式，此模式下HPING会发送IGMP应答报，你可以用--ICMPTYPE --ICMPCODE选项发送其他类型/模式的ICMP报文。 -2 --udp UDP模式，缺省下，HPING会发送UDP报文到主机的0端口，你可以用--baseport --destport --keep选项指定其模式。 -9 --listen signatuer hping的listen模式，用此模式，HPING会接收指定的数据。 -a --spoof hostname 伪造IP攻击，防火墙就不会记录你的真实IP了，当然回应的包你也接收不到了。 -t --ttl time to live 可以指定发出包的TTL值。 -H --ipproto 在RAW IP模式里选择IP协议。 -w --WINID UNIX ,WINDIWS的id回应不同的，这选项可以让你的ID回应和WINDOWS一样。 -r --rel 更改ID的，可以让ID曾递减输出，详见HPING-HOWTO。 -F --FRAG 更改包的FRAG，这可以测试对方对于包碎片的处理能力，缺省的“virtual mtu”是16字节。 -x --morefrag 此功能可以发送碎片使主机忙于恢复碎片而造成主机的拒绝服务。 -y -dontfrag 发送不可恢复的IP碎片，这可以让你了解更多的MTU PATH DISCOVERY。 -G --fragoff fragment offset value set the fragment offset -m --mtu mtu value 用此项后ID数值变得很大，50000没指定此项时3000-20000左右。 -G --rroute 记录路由，可以看到详悉的数据等等，最多可以经过9个路由，即使主机屏蔽了ICMP报文。 -C --ICMPTYPE type 指定ICMP类型，缺省是ICMP echo REQUEST。 -K --ICMPCODE CODE 指定ICMP代号，缺省0。 --icmp-ipver 把IP版本也插入IP头。 --icmp-iphlen 设置IP头的长度，缺省为5（32字节）。 --icmp-iplen 设置IP包长度。 --icmp-ipid 设置ICMP报文IP头的ID，缺省是RANDOM。 --icmp-ipproto 设置协议的，缺省是TCP。 -icmp-cksum 设置校验和。 -icmp-ts alias for --icmptype 13 (to send ICMP timestamp requests) --icmp-addr Alias for --icmptype 17 (to send ICMP address mask requests) -s --baseport source-port hping 用源端口猜测回应的包，它从一个基本端口计数，每收一个包，端口也加1，这规则你可以自己定义。 -p --deskport [+][+]deskport 设置目标端口，缺省为0，一个加号设置为:每发送一个请求包到达后，端口加1，两个加号为：每发一个包，端口数加1。 --keep 上面说过了。 -w --win 发的大小和windows一样大，64BYTE。 -O --tcpoff Set fake tcp data offset. Normal data offset is tcphdrlen / 4. -m --tcpseq 设置TCP序列数。 -l --tcpck 设置TCP ack。 -Q --seqnum 搜集序列号的，这对于你分析TCP序列号有很大作用。 四、示例 1、端口扫描 Hping3也可以对目标端口进行扫描。Hping3支持指定TCP各个标志位、长度等信息 hping3 -I eth0 -S 192.168.10.1 -p 80 hping3支持非常丰富的端口探测方式，nmap拥有的扫描方式hping3几乎都支持（除开connect方式，因为Hping3仅发送与接收包，不会维护连接，所以不支持connect方式探测）。而且Hping3能够对发送的探测进行更加精细的控制，方便用户微调探测结果。当然，Hping3的端口扫描性能及综合处理能力，无法与Nmap相比。一般使用它仅对少量主机的少量端口进行扫描。 2、Idle扫描 Idle扫描（Idle Scanning）是一种匿名扫描远程主机的方式，该方式也是有Hping3的作者Salvatore Sanfilippo发明的，目前Idle扫描在Nmap中也有实现。 该扫描原理是：寻找一台idle主机（该主机没有任何的网络流量，并且IPID是逐个增长的），攻击端主机先向idle主机发送探测包，从回复包中获取其IPID。冒充idle主机的IP地址向远程主机的端口发送SYN包（此处假设为SYN包），此时如果远程主机的目的端口开放，那么会回复SYN/ACK，此时idle主机收到SYN/ACK后回复RST包。然后攻击端主机再向idle主机发送探测包，获取其IPID。那么对比两次的IPID值，我们就可以判断远程主机是否回复了数据包，从而间接地推测其端口状态。 3、拒绝服务攻击 使用Hping3可以很方便构建拒绝服务攻击。比如对目标机发起大量SYN连接，伪造源地址为192.168.10.99，并使用1000微秒的间隔发送各个SYN包。 hping3 -I eth0 -a192.168.10.99 -S 192.168.10.33 -p 80 -i u1000 4、数据包跟踪 sudo hping3 --traceroute -V -1 www.baidu.com 参考 https://wangchujiang.com/linux-command/c/hping3.html https://mochazz.github.io/2017/07/23/hping3/#ICMP%E6%B5%8B%E8%AF%95 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-17 17:27:46 "},"origin/linux-iptables.html":{"url":"origin/linux-iptables.html","title":"iptables详解","keywords":"","body":"iptables 一、简介 iptables其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过iptables这个代理，将用户的安全设定执行到对应的netfilter安全框架\"中，netfilter位于内核空间。 自1995年ipfwadm开始进入1.2.1的核心，Linux的防火墙实现有很长的时间了。Ipfwadm实现了标准的tcp/ip包过滤功能，比如过滤源地址与目的地址以及端口过滤。早在1999年第一个稳定的2.2.0核心中防火墙的实现被ipchains替代了，ipchains的新功能包括支持规则链，碎片包控制，较好的网络地址翻译功能（NAT）以及其他一些有用的改进。我们需要明白Linux防火墙包括核心级代码（通常是可加载核心模块或者核心源程序的补丁）和用户级代码（一个配置的工具，比如/usr/bin/ipchains，这是用来插入包规则到核心空间的）因此无论如何，只要新的linux防火墙代码被引入，核心和用户空间的有关代码都要改写。 2001年2.4的核心完成了，iptables出现了。它引入了很多重要的改进，比如基于状态的防火墙，基于任何TCP标记和MAC地址的包过滤，更灵活的配置和记录功能，强大而且简单的NAT功能和透明代理功能，通过速度限制实现DoS的阻止。 然而，最重要变化是引入了模块化的架构方式。比如，ipchains和ipfwadm兼容模式是通过一个核心模块的设置实现的，该模块能够在运行的核心中插入，以便提供相应的通讯功能。在附加的变化中，用户自定义编码功能已经成为了可能，比如过滤一定范围的端口，根据TTL值和包的到达时间进行判断，对自定义的协议进行状态监视，对随机的数据包进行监视等，这些目前都还不是iptable的一部分，但是在未来将被实现。很多很有趣的新模块已经完成了。编写一个可加载的核心模块来创建核心级代码，通过用户级代码实现控制过滤器的行为。 二、基础知识 1、iptables和netfilter的关系 iptables其实是一个命令行工具，位于用户空间，我们用这个工具操作真正的框架netfilter。 netfilter/iptables（下文中简称为iptables）组成Linux平台下的包过滤防火墙，与大多数的Linux软件一样，这个包过滤防火墙是免费的，它可以代替昂贵的商业防火墙解决方案，完成封包过滤、封包重定向和网络地址转换（NAT）等功能。 2、iptables传输数据包的过程 ① 当一个数据包进入网卡时，它首先进入PREROUTING链，内核根据数据包目的IP判断是否是发往本机的。 ② 如果数据包就是进入本机的，它就会沿着图向下移动，到达INPUT链。数据包到了INPUT链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过OUTPUT链，然后到达POSTROUTING链输出。 ③ 如果数据包是要转发出去的，且内核允许转发，数据包就会流向FORWARD链进行处理（是否转发或拦截），然后到达POSTROUTING链（是否修改数据包的地 址等）进行处理。 报文的链流向： 到本机某进程的报文：PREROUTING --> INPUT 由本机转发的报文：PREROUTING --> FORWARD --> POSTROUTING 由本机的某进程发出报文（通常为响应报文）：OUTPUT --> POSTROUTING 报文到达链后匹配表的优先顺序： Raw ==> mangle ==> nat ==> filter 3、链chains 链的作用：容纳各种防火墙规则 链的分类依据：处理数据包的不同时机 内置链： PREROUTING：在进行路由选择前处理数据包（做目标地址转换） INPUT：处理入站数据包 FORWARD：处理转发数据包 OUTPUT：处理出站数据包 POSTROUTING：在进行路由选择后处理数据包（对数据链进行源地址修改转换） 4、表tables filter表：负责过滤功能，防火墙；内核模块：iptables_filter nat表：网络地址转换功能；内核模块：iptable_nat mangle表：拆解报文，做出修改，并重新封装的功能；内核模块：iptable_mangle raw表：关闭nat表上启用的连接追踪机制；内核模块：iptable_raw 三、iptables规则 规则（rules）其实就是网络管理员预定义的条件 规则一般的定义为“指定什么样的数据包头符合什么的条件，就怎么样处理这个数据包” 规则的作用：对数据包进行过滤或处理 规则存储在内核空间的信息 包过滤表中，这些规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等。当数据包与规 则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。 配置iptables防火墙的主要工作就是添加、修改和删除这些规则。 1、规则定义 ⓪语法格式 iptables [ -t 表名 ] 命令选项 ［链名］［匹配规则］［-j 处理动作］ # 表名 必须是 raw， nat，filter，mangle 中的一个。默认指filter表 # 不指定链名时，默认指表内的所有链 # 除非设置链的默认策略，否则必须指定匹配条件 # 命令选项、链名、处理动作使用大写字母，其余均为小写 ①命令选项 规则管理命令选项： -A：在指定链的末尾添加（append）一条新的规则 -D：删除（delete）指定链中的某一条规则，可以按规则序号和内容删除 -I ： 在指定链中插入（insert）一条新的规则，默认在第一行添加 -R：修改、替换（replace）指定链中的某一条规则，可以按规则序号和内容替换 查看命令选项： -L ：列出（list）指定链中所有的规则进行查看 --line-numbers：查看规则时，显示规则的序号 -n ：使用数字形式（numeric）显示输出结果 -v：以更详细的方式显示规则信息 链管理命令（这都是立即生效的）： -E ：重命名用户定义的链，不改变链本身 -F ：清空（flush） -N：新建（new-chain）一条用户自己定义的规则链 -X ：删除指定表中用户自定义的规则链（delete-chain） -P ：设置指定链的默认策略（policy） -Z ：将所有表的所有链的字节和数据包计数器清零 ②匹配规则 1) 通用匹配 协议匹配: -p [协议名] 地址匹配 -s [源地址] -d [目标地址] 接口匹配 -i [入站网卡] -o [出站网卡] 2) 隐含匹配 端口匹配 -sport [源端口] -dport [目标端口] TCP连接标记匹配：--tcp-flags [列表1：检查范围] [列表2：被设置的标记] 有两个参数列表，列表内部用逗号为分隔符，两个列表之间用空格分开 LIST1用作参数检查，LIST2用作参数匹配。 可用标志有： SYN( 同步; 表示开始会话请求 ), ACK（应答）, FIN（结束; 结束会话）， RST(复位;中断一个连接) PSH（推送; 数据包立即发送）， URG（紧急 ）， ALL（指选定所有的标记）， NONE（指未选定任何标记） iptables -A INPUT -p tcp –tcp-flags SYN,FIN,ACK SYN # 表示SYN、ACK、FIN的标志都检查，但是只有SYN匹配 iptables -A FROWARD -p tcp –tcp-flags ALL SYN,ACK # 表示ALL（SYN，ACK，FIN，RST，URG，PSH）的标志都检查，但是只有设置了SYN和ACK的匹配。 ICMP类型匹配：--icmp-type [ICMP类型] 3) 显式匹配(-m): iptables可使用额外的扩展模块进行显示条件匹配，详情参考第四章节 ③处理动作 动作也可以分为基本动作和扩展动作 ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：地址伪装。是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射 MASK : 做防火墙标记 RETURN : 返回调用链 LOG：在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 --log-level LEVEL : 日志的等级 --log-prefix FREFIX : 日志的提示语句的前缀 2、规则管理 ①查看规则 # 查看表中的规则 iptables -L -t filter # 查看链中的规则 iptables -L INPUT iptables -L FORWARD --line-numbers # 查看POSTROUTING链nat表中的规则 iptables -L POSTROUTING -t nat ②删除规则 # 删除所有的规则 iptables -F # 删除链中指定的规则 iptables -D FORWARD 1 ③添加规则 # 在指定链的末尾添加（append）一条新的规则 iptables -A FORWARD -s 10.8.0.10 -d 192.168.1.5 -j DROP # 在指定链中插入（insert）一条新的规则，默认在第一行添加 iptables -I FORWARD -s 10.8.0.10 -d 192.168.1.5 -j DROP 3、规则的匹配顺序 ①按自上而下的顺序依次匹配，匹配即停止（LOG策略例外） ②若找不到相匹配的规则，则按该链的默认策略处理 四、iptables的显示扩展模块 官方文档：https://ipset.netfilter.org/iptables-extensions.man.html 必须使用-m选项手动加载模块, 其扩展模块路径为:/lib64/xtables,其中大写的为目标扩展,小写的为规则扩展 1、模块管理 ①查看Iptables已加载的模块 cat /proc/net/ip_tables_matches # 查看内核已编译的模块 ②查看内核支持的模块 ls /lib/modules/$(uname -r)/kernel/net/netfilter/* # 或者 ls /lib/modules/nf_* # 或者 ls /usr/lib/iptables/ ③加载模块 modprobe 模块名 CentOS/Redhat 编辑/etc/sysconfig/iptables-config IPTABLES_MODULES=\"模块1 模块2\" 然后重启iptables systemctl restart iptables 2、模块语法 -m 扩展模块 --模块参数 查看模块的支持的参数： iptables -m --help # 如果不显示模块的详细参数，则说明该模块没有加载，无法使用 3、常见条件匹配模块示例 ①multiport：多端口匹配模块 -m multiport --sport | --dport [端口列表] iptables -A INPUT -p tcp -m multiport --dport 22,25,80,443 -j ACCEPT ②iprange：IP范围匹配模块 -m iprange --src-range [IP范围] iptables -A FORWARD -p tcp -m iprange --src-range 192.168.1.10-192.168.1.20 -j ACCEPT ③mac：MAC地址匹配模块 -m mac --mac-range [MAC地址] iptables -A INPUT -m mac --mac-source 00:01:02:03:04:cc -j DROP ④state：报文状态匹配模块 --state [报文状态]：多个state可以使用,号分隔 支持配置的报文状态： ESTABLISHED：第一个成功穿越防火墙的报文之后所有的报文； NEW：一个连接的第一个报文，例如TCP连接中的SYN报文； RELATED：伴随连接的报文，即某个已经处于ESTABLISHED的连接所产生的报文，这些报文不属于第一条连接，但是的确是由第一条连接产生的； INVALID：如果一个包没有办法被识别，或者这个包没有任何状态，那么这个包的状态就是INVALID，我们可以主动屏蔽状态为INVALID的报文 UNTRACKED：报文的状态为untracked时，表示报文未被追踪，当报文的状态为Untracked时通常表示无法找到相关的连接。 iptables -A INPUT -d 172.168.100.67 -p tcp -m multiport --dport 22,80 -m state --state NEW,ESTABLISHED -j ACCEPT ⑤string：字符串匹配模块 --algo {匹配算法: bm|kmp} --string \"字符串\" iptables -I OUTPUT -s 192.168.1.0/24 -m string --algo kmp --string \"qq\" -j REJECT #使用kmp算法限制拒绝源地址192.168.1.0/24带有\"qq\"字符串的请求 iptables -I INPUT -m string --string \"export/*\" --algo bm -j DROP ⑥limit：连接数匹配模块 —limit: 平均速率，单位：个数/second ，个数/minute，个数/hour --limit-burst: 峰值数量，默认5个 iptables -I INPUT -d 172.16.100.7 -p tcp --dport 22 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT # 当达到100个连接后，才启用上述25/minute限制 ⑦connlimit：连接数匹配模块 --connlimit-upto n : 当现在的连接数量低于或等于这个数量(n),就匹配 --connlimit-above n : 当现有的连接数量大于这个数量, 就匹配 iptables -A INPUT -d 172.16.36.61 -p tcp --dport 22 -m connlimit --connlimit-above 2 -j REJECT ⑧time：时间限制匹配模块 -m time --datestart --datestop --timestart --timestop iptables -A INPUT -p tcp --dport 21 -s 192.168.1.0/24 -m time ! --weekdays 6,7 -m time --timestart 8:30 --timestop 18:00 -m connlimit --connlimit-above 5 -j ACCET # 在工作时间，即周一到周五的8:30-18:00，开放本机的ftp服务给 192.168.1.0网络中的主机访问；并且数据下载请求的次数每分钟不得超过 5 个； ⑨set: 地址集合模块 普通的iptables链是线性的存储和过滤，在进行规则匹配时，是从规则列表中从头到尾一条一条进行匹配。这像是在链表中搜索指定节点费力 ipset 提供了把这个 O(n) 的操作变成 O(1) 的方法：就是把要处理的 IP 放进一个集合，对这个集合设置一条 iptables 规则。存储在带索引的数据结构中,这种结构即使集合比较大也可以进行高效的查找 ipset是iptables的扩展，允许创建管理匹配整个地址集合的规则。命令详解参考附录第2章节 -m set –match-set 地址集合名称 iptables -I INPUT -m set –match-set 集合名称 src -p tcp -j DROP 五、iptables的内核调优 1、iptables的conntrack连接追踪优化 conntrack是netfilter的核心。有许多增强的功能，例如，地址转换（NAT），基于内容的业务识别（l7， layer-7 module）都是基于连接跟踪。 nf_conntrack模块在kernel 2.6.15（2006-01-03发布） 被引入，支持ipv4和ipv6，取代只支持ipv4的ip_connktrack，用于跟踪连接的状态，供其他模块使用。 iptables的连接追踪表最大容量是/proc/sys/ipv4/ip_conntrack_max设置的, 链接达到各种状态的超时后,会从表中删除,当模板满载时, 后续的链接可能会超时 跟踪的连接用哈希表存储，每个桶（bucket）里都是1个链表，默认长度为4KB。netfilter的哈希表存储在内核空间，这部分内存不能swap 哈希表大小 ：64位的最大连接数/8； 32位的最大连接数/4 在64位下，当CONNTRACK_MAX为 1048576，HASHSIZE 为 262144 时，最多占350多MB 连接跟踪调优计算公式 CONNTRACK_MAX（最大几率的连接条数） = 内存个数*1024*1024*1024/16384/2 = *** Buckets（哈希表大小） = CONNTRACK_MAX / 4 = ***（Byte字节） 跟踪数暂用最内存大小 = CONNTRACK_MAX * 300（Byte字节）= ***（Byte字节） 异常现象： 丢包 可调优参数 哈希表桶大小 注：net.netfilter.nf_conntrack_buckets 不能直接改（报错） # 临时生效 echo 262144 > /sys/module/nf_conntrack/parameters/hashsize # 重启永久生效 新建文件：/etc/modprobe.d/iptables.conf options nf_conntrack hashsize = 32768 最大追踪连接数 注：加大max值, 也会加大内存的压力 # 临时生效 sysctl -w net.nf_conntrack_max = 393216 sysctl -w net.netfilter.nf_conntrack_max = 393216 # 永久生效 echo \"net.nf_conntrack_ma=393216\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_max=393216\" >> /etc/sysctl.conf sysctl -p 响应时间 net.netfilter.nf_conntrack_tcp_timeout_close_wait: # CLOSE_WAIT是被动方收到FIN发ACK，然后会转到LAST_ACK发FIN，除非程序写得有问题，正常来说这状态持续时间很短。默认 60 秒 # 临时生效 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=120 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_close_wait=60 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_fin_wait=120 # 永久生效 echo \"net.netfilter.nf_conntrack_tcp_timeout_established=300\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_tcp_timeout_time_wait=120\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_tcp_timeout_close_wait=60\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_tcp_timeout_fin_wait=120\" >> /etc/sysctl.conf sysctl -p 六、iptables应用 1、防火墙 ①防止ACK欺骗 拒绝TCP标记为SYN/ACK但连接状态为NEW的数据包， iptables -A INPUT -p tcp --tcp-flags SYN,ACK SYN,ACK -m state --state NEW -j DROP ②防止TCP Null扫描 iptables -A INPUT -p tcp --tcp-flags ALL NONE -j DROP ③防止Xmas扫描 iptables -A INPUT -p tcp --tcp-flags ALL FIN,URG,PSH -j DROP ④限流/防止端口DoS攻击 iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT # -m limit: 启用limit扩展，限制速度。 # --limit 25/minute: 允许最多每分钟25个连接 # --limit-burst 100: 当达到100个连接后，才启用上述25/minute限制 ⑤限制主机服务时间 在工作时间，即周一到周五的8:30-18:00，开放本机的ftp服务给 192.168.1.0网络中的主机访问；并且数据下载请求的次数每分钟不得超过 5 个； iptables -A INPUT -p tcp --dport 21 -s 192.168.1.0/24 -m time ! --weekdays 6,7 -m time --timestart 8:30 --timestop 18:00 -m connlimit --connlimit-above 5 -j ACCET ⑥丢弃无效数据包 iptables -A INPUT -m conntrack --ctstate INVALID -j DROP ⑦关键词屏蔽 iptables -I FORWARD -p udp --dport 53 -m string --string \"tencent\" -m time --timestart 8:15 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP iptables -I FORWARD -p udp --dport 53 -m string --string \"TENCENT\" -m time --timestart 8:15 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP iptables -I FORWARD -p udp --dport 53 -m string --string \"qq.com\" -m time --timestart 8:15 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP iptables -I FORWARD -s 10.113.0.0/24 -m string --string \"ay2000.net\" -j DROP # 关键词屏蔽 iptables -I FORWARD -s 10.113.0.0/24 -m string --string \"eroticism\" -j DROP ⑧防止外网使用内网IP欺骗 iptables -t nat -A PREROUTING -i eth0 -s 10.0.0.0/8 -j DROP iptables -t nat -A PREROUTING -i eth0 -s 172.16.0.0/12 -j DROP iptables -t nat -A PREROUTING -i eth0 -s 192.168.0.0/16 -j DROP ⑨禁Ping # 允许本机ping别的主机；但不开放别的主机ping本机； iptables -A OUTPUT -p icmp --icmp-type 8 -j ACCEPT iptables -A INPUT -p icmp --icmp-type 0 -j ACCEPT 2、NAT网络地址转换 iptable上中包含一个NAT表，其中有两条缺省的PREROUTING和 POSTROUTING 链，在这两条链上配置规则可以实现NAT功能。 ①SNAT源地址目标转换 概念 SNAT(Source Network Address Translation)是指在数据包从网卡发送出去的时候，把数据包中的源地址部分替换为指定的IP。 适用于由局域网中的主机发起连接的情况。报文在经过NAT路由器时，将IP报文中的源IP地址转换为一个有效的广域网地址；在服务器给一个在私有网络中的主机返回响应报文时，目的IP地址就是这个局域网对外的广域网地址。报文到达NAT路由器的时候，路由器要将该报文分发给对应的主机，将IP报文的目的IP地址转换为私有网络地址 涉及到iptables中的链表 POSTROUTING链中的nat表 应用场景 局域网主机共享单个公网IP地址接入Internet 做法： 设置能上外网的那一台主机(192.168.1.2)的iptables，一旦接收到来自局域网(192.168.1.0/24)的数据，修改数据包的源IP地址为本机IP地址，然后就转发出去。 前提 有公网IP地址绑定 内核设置net.ipv4.ip_forward=1，开起路由转发功能 # 查看内核是否启用路由转发功能 sysctl -a | grep \"ip_forward\" 或者 sysctl net.ipv4.ip_forward # “net.ipv4.ip_forward=1”即可表示成功开启 # 临时设置内核启用路由转发功能。重启失效 sysctl -w net.ipv4.ip_forward=1 或者 echo 1 >/proc/sys/net/ipv4/ip_forward # 永久设置内核启用路由转发功能。 echo \"net.ipv4.ip_forward = 1\" >> /etc/sysctl.conf && sysctl -p Iptables 设置 iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -j SNAT --to 192.168.1.2 局域网内想上外网的主机或者路由器想添加自定义路由策略 route add 0.0.0.0 gw 192.168.1.2 ②DNAT目标网络地址转换 DNAT(Destination Network Address Translation)是又称为“端口转发”，适用于由广域网上的主机发起连接的情况。当广域网的主机访问NAT路由器的广域网端口时，可以将NAT路由器的广域网的端口映射到局域网内的某个IP地址的某个端口，这样就可以实现广域网主机访问局域网内的资源。 iptables -t nat -I PREROUTING -d 公网IP -p tcp -m tcp --dport 公网port -j DNAT --to-destination 10.10.223.12-10.10.223.20:8080（内网） 3、PNAT端口重定向 附录 1、TCP连接状态 https://blog.mimvp.com/article/44678.html 2、ipset命令 官网：https://ipset.netfilter.org/ 文档：https://ipset.netfilter.org/ipset.man.html#lbBF ipset默认可以存储65536个元素，使用maxelem指定数量 不支持0.0.0.0/0 ，可以替换为 0.0.0.0/1，128.0.0.0/1 需要内核版本高于2.6.32 ①安装ipset命令 yum install -y ipset apt install -y ipset apk add -y ipset ②ipset语法规则 # 创建集合 ipset (create | -N) 集合名称 集合存储方法:记录类型1[,数据类型2[,数据类型3]] [ 集合存储方法:记录类型支持的参数 ] # 支持的集合储存方法 - bitmap：仅支持ip、port、mac记录类型 - hash：仅支持net、iface、mac、ip、port、mark记录类型 - list: 仅支持集合间的继承关联关系 # 支持的记录类型 - ip：IP地址，例如1.2.3.4 - net：IP地址网络段，例如1.2.3.0/24 - mac：MAC地址，例如1A:2B:3C:4D:5E:6F - port：协议类型:端口，例如[udp/tcp]:23、[udp/tcp]:21-23 - iface：网卡，例如eth0 - mark: 例如0x63，值在0~4294967295之间 # 例如：ipset create whitelist hash:ip,port # 查看集合存储方法:记录类型支持的参数 ipset help 集合存储方法:记录类型 # 例如：ipset help hash:ip,port # 集合中添加记录 ipset (add | -A) 集合名称 记录 # 例如：ipset add whitelist 192.168.1.7,tcp:21-22 # 查看集合。不加集合名称是查看所有的集合 ipset [list | -L) [集合名称] # 例如：ipset -L whitelist # 删除集合中的记录 ipset (del | -D) 集合名称 记录 [ DEL-OPTIONS ] # 例如：ipset del whitelist 192.168.1.7,tcp:21 # 删除集合，不能有任何下游依赖 。不加集合名称是删除所有集合 ipset (destroy | -X) [集合名称] # 例如：ipset destroy whitelist # 清空集合，不加集合名称是清空所有 ipset (flush | -F) [集合名称] # 例如：ipset flush # 将ipset规则保存到文件，不加集合名词是保存所有集合，不加-f是输出记录到标准输出 ipset (save | -S) [集合名称] [-f 文件名] # 例如：ipset save whitelist -f iptbales-whitelist-ip.txt # 导入ipset规则，不加-f是从标准输入读取规则 ipset (restore | -R) [-f 文件名] # 例如：ipset restore -f iptbales-whitelist-ip.txt # 重命名集合 ipset (rename | -E) 旧集合名称 新集合名称 # 例如：ipset rename whitelist blacklist # 测试一个ip是不是在集合中（要是ip在集合中返回0，如果ip不在集合中则返回非0） ipset (test | -T) 集合名称 ip地址 # 例如：ipset test blacklist 192.168.1.7,tcp:55 参考 https://www.jianshu.com/p/ee4ee15d3658 http://www.zsythink.net/archives/1199/ https://www.linuxidc.com/Linux/2018-08/153378.htm https://blog.csdn.net/u014721096/article/details/78626729 https://www.jianshu.com/p/586da7c8fd42 http://www.stearns.org/modwall/archives/tcpchk.v0.1.1 https://blog.51cto.com/woyaoxuelinux/1906316 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-20 18:26:47 "},"origin/mysql-basic.html":{"url":"origin/mysql-basic.html","title":"MySQL","keywords":"","body":"MySQL 一、简介 逻辑架构图 每个虚线框为一层，总共三层。 第一层，服务层(为客户端服务):为请求做连接处理，授权认证，安全等。 第二层，核心层:查询解析，分析，优化，缓存，提供内建函数（例如：日期，时间，数学和加密）;所有跨存储引擎的功能都在这一层实现：存储过程，触发器，视图。 第三层，存储引擎层，不光做存储和提取数据，而且针对特殊数据引擎还要做事务处理。第二层通过API与存储引擎通信，但存储引擎不会解析SQL，不同存储引擎之间不进行通信。 1、对于Select语句，在解析查询之前，服务器会先检查查询缓存。 2、通过读写锁实现并发控制。读写锁分为两类：共享锁（读锁）和排他锁（写锁）。读锁是共享的，多个客户端在同一时间可以同时读取同一资源。写锁是排他的的，也就是在一个写锁会阻塞读锁和写锁的，确保在给定的时间里，只有一个用户能写入，其他用户则禁止读和写。 3、锁粒度：表锁和行级锁。行级锁只能在存储引擎中实现。 二、主从复制 MySQL内建的复制功能是构建大型，高性能应用程序的基础。将Mysql的数据分布到多个系统上去，这种分布的机制，是通过将Mysql的某一台主机的数据复制到其它主机（slaves）上，并重新执行一遍来实现的。复制过程中一个服务器充当主服务器，而一个或多个其它服务器充当从服务器。主服务器将更新写入二进制日志文件，并维护文件的一个索引以跟踪日志循环。这些日志可以记录发送到从服务器的更新。当一个从服务器连接主服务器时，它通知主服务器从服务器在日志中读取的最后一次成功更新的位置。从服务器接收从那时起发生的任何更新，然后封锁并等待主服务器通知新的更新。 请注意当你进行复制时，所有对复制中的表的更新必须在主服务器上进行。否则，你必须要小心，以避免用户对主服务器上的表进行的更新与对从服务器上的表所进行的更新之间的冲突。 mysql支持的复制类型： 基于语句的复制： 在主服务器上执行的SQL语句，在从服务器上执行同样的语句。MySQL默认采用基于语句的复制，效率比较高。 一旦发现没法精确复制时， 会自动选着基于行的复制。 基于行的复制：把改变的内容复制过去，而不是把命令在从服务器上执行一遍. 从mysql5.0开始支持 混合类型的复制: 默认采用基于语句的复制，一旦发现基于语句的无法精确的复制时，就会采用基于行的复制。 复制解决的问题 MySQL复制技术有以下一些特点： 数据分布 (Data distribution ) 负载平衡(load balancing) 备份(Backups) 高可用性和容错行 (High availability and failover ) 复制如何工作 整体上来说，复制有3个步骤： master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； slave将master的binary log events拷贝到它的中继日志(relay log)； slave重做中继日志中的事件，将改变反映它自己的数据。 ​ 该过程的第一部分就是master记录二进制日志。在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务。 ​ 下一步就是slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志。 ​ SQL slave thread（SQL从线程）处理该过程的最后一步。SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致。只要该线程与I/O线程保持一致，中继日志通常会位于OS的缓存中，所以中继日志的开销很小。 ​ 此外，在master中也有一个工作线程：和其它MySQL的连接一样，slave在master中打开一个连接也会使得master开始一个线程。复制过程有一个很重要的限制——复制在slave上是串行化的，也就是说master上的并行更新操作不能在slave上并行操作。 复制配置 有两台MySQL数据库服务器Master和slave，Master为主服务器，slave为从服务器，初始状态时，Master和slave中的数据信息相同，当Master中的数据发生变化时，slave也跟着发生相应的变化，使得master和slave的数据信息同步，达到备份的目的。 要点： 负责在主、从服务器传输各种修改动作的媒介是主服务器的二进制变更日志，这个日志记载着需要传输给从服务器的各种修改动作。因此，主服务器必须激活二进制日志功能。从服务器必须具备足以让它连接主服务器并请求主服务器把二进制变更日志传输给它的权限。 三、集群模式 MySQL Cluster 是MySQL 适合于分布式计算环境的高实用、可拓展、高性能、高冗余版本，其研发设计的初衷就是要满足许多行业里的最严酷应用要求，这些应用中经常要求数据库运行的可靠性要达到99.999%。MySQL Cluster允许在无共享的系统中部署“内存中”数据库集群，通过无共享体系结构，系统能够使用廉价的硬件，而且对软硬件无特殊要求。此外，由于每个组件有自己的内存和磁盘，不存在单点故障。 实际上，MySQL集群是把一个叫做NDB的内存集群存储引擎集成与标准的MySQL服务器集成。它包含一组计算机，每个都跑一个或者多个进程，这可能包括一个MySQL服务器，一个数据节点，一个管理服务器和一个专有的一个数据访问程序。 MySQL Cluster能够使用多种故障切换和负载平衡选项配置NDB存储引擎，但在Cluster 级别上的存储引擎上做这个最简单。以下为MySQL集群结构关系图， MySQL从结构看，由3类节点(计算机或进程)组成，分别是： 管理节点:用于给整个集群其他节点提供配置、管理、仲裁等功能。理论上通过一台服务器提供服务就可以了。 数据节点:MySQL Cluster的核心，存储数据、日志，提供数据的各种管理服务。2个以上 时就能实现集群的高可用保证，DB节点增加时，集群的处理速度会变慢。 SQL节点(API):用于访问MySQL Cluster数据，提供对外应用服务。增加 API 节点会提高整个集群的并发访问速度和整体的吞吐量，该节点 可以部署在Web应用服务器上，也可以部署在专用的服务器上，也开以和DB部署在 同一台服务器上。 NDB引擎 MySQL Cluster 使用了一个专用的基于内存的存储引擎——NDB引擎，这样做的好处是速度快， 没有磁盘I/O的瓶颈，但是由于是基于内存的，所以数据库的规模受系统总内存的限制， 如果运行NDB的MySQL服务器一定要内存够大，比如4G, 8G, 甚至16G。NDB引擎是分布式的，它可以配置在多台服务器上来实现数据的可靠性和扩展性，理论上 通过配置2台NDB的存储节点就能实现整个数据库集群的冗余性和解决单点故障问题。 缺陷 基于内存，数据库的规模受集群总内存的大小限制 基于内存，断电后数据可能会有数据丢失，这点还需要通过测试验证。 多个节点通过网络实现通讯和数据同步、查询等操作，因此整体性受网络速度影响，因此速度也比较慢 2.2 优点 多个节点之间可以分布在不同的地理位置，因此也是一个实现分布式数据库的方案 扩展性很好，增加节点即可实现数据库集群的扩展。 冗余性很好，多个节点上都有完整的数据库数据，因此任何一个节点宕机都不会造成服务中断。 实现高可用性的成本比较低，不象传统的高可用方案一样需要共享的存储设备和专用的软件才能实现，NDB 只要有足够的内存就能实现。 四、存储引擎 1、在文件系统中，MySQL会将每一个数据库保存为数据目录下的一个子目录。创建表时，MySQL会在数据库子目录下创建一个和表同名的.frm文件保存表的定义。 2、MySQL使用文件系统的目录和文件来保存数据库和表的定义，大小写敏感和具体的平台密切相关。在Windows中，大小写是不敏感的；但在类Unix中则大小写敏感。 3、不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在MySQL的服务层统一处理的。 4、MySQL在5.1及之前的版本中，MyISAM是默认的存储引擎，之后默认的是InnoDB。MyISAM存储引擎不支持事务和行级锁。所以不要再默认使用MyISAM，而应默认使用InnoDB。 5、MySQL内建的其他存储引擎： Archive引擎，只支持Insert和Select操作。但支持行级锁和专用的缓冲区 Blackhole引擎 CSV引擎，可以将普通的CSV文件作为MySQL的表处理，但这种表不支持索引。CSV引擎可以在数据库运行时拷入或拷出文件。可以将Excel文件存储为CSV文件，然后复制到MySQL数据目录中，就能在MySQL中打开使用。同样，如果将数据写入到一个CSV引擎表，其他的外部程序也能立即从表的数据文件中读取CSV格式的数据。 Federated引擎,是访问其他MySQL服务器的一个代理，它会创建一个到远程MySQL服务器的客户端连接，并将查询传输到远程服务器执行，然后提取或者发送需要的数据。 Memory引擎。所有的数据都保存在内存中，Memory表的结构在重启后还是会保留，但数据会丢失。 Merge引擎。MyISAM引擎的一个变种。Merge表是有多个MyISAM表合并而来的虚拟表 NDB引擎。MySQL集群的存储引擎。 五、多版本并发控制MVCC 1、MVCC的实现是通过保存数据在某个时间点的快照来实现。 2、不同存储引擎的MVCC实现是不同的。典型的有乐观（optimistic）控制和悲观（pessimistic）控制 3、MVCC只在REPEATABLE READ 和READ COMMITTED两个隔离级别下工作。 其他两个隔离级别都不和MVCC兼容。是因为，READ UNCOMMITED总是读取最新的数据行，而不是读取符合当前事务版本的数据行，而SERIALIZABLE则会在所有读取的行都加锁。** 4、InnoDB的MVCC通过在每行记录后面保存两个隐藏的列来实现的，一列保存了行的创建时间，一列保存行的过期时间（或删除时间），存储的并不是实际的时间值，而是系统版本号（System Version Number）。没开始一个新的事务，系统版本号会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行对比。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/mysql-common-operations.html":{"url":"origin/mysql-common-operations.html","title":"常见操作及SQL","keywords":"","body":"MySQL 常见操作 1、相关网站 官方文档：https://dev.mysql.com/doc/refman/5.7/en/ 下载地址：https://dev.mysql.com/downloads/ 2、常用服务端配置 [mysqld] datadir=/data/mysql/data # 为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(Linux下默认是/var/lib/mysql/mysql.sock文件) socket=/data/mysql/data/mysql.sock symbolic-links=0 log-error=/data/mysql/logs/mysqld.log pid-file=/data/mysql/data/mysqld.pid lower_case_table_names=0 federated init_connect='SET NAMES utf8' character_set_server=utf8 transaction-isolation=READ COMMITTED # key_buffer是用于索引块的缓冲区大小，增加它可得到更好处理的索引(对所有读和多重写)。 索引块是缓冲的并且被所有的线程共享，key_buffer的大小视内存大小而定。 key_buffer=384M # 为所有线程打开表的数量。增加该值能增加mysqld要求的文件描述符的数量。可以避免频繁的打开数据表产生的开销 table_cache=512 # 每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。注意：该参数对应的分配内存是每连接独占！如果有100个连接，那么实际分配的总共排序缓冲区大小为100×6=600MB sort_buffer_size=2M # 读查询操作所能使用的缓冲区大小。和sort_buffer_size一样，该参数对应的分配内存也是每连接独享。 read_buffer_size=2M # 指定MySQL查询结果缓冲区的大小 query_cache_size=32M # 在使用行指针排序之后，随机读用的。 read_rnd_buffer_size=8M # MyISAM表发生变化时重新排序所需的缓冲 myisam_sort_buffer_size=64M # 最大并发线程数，取值为服务器逻辑CPU数量×2，如果CPU支持H.T超线程，再×2 thread_concurrency=8 #缓存可重用的线程数 thread_cache=8 # 避免MySQL的外部锁定，减少出错几率增强稳定性。 skip-locking # back_log参数的值指出在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中。如果系统在一个短时间内有很多连接，则需要增大该参数的值，该参数值指定到来的TCP/IP连接的侦听队列的大小。试图设定back_log高于你的操作系统的限制将是无效的。默认值为50。对于Linux系统推荐设置为小于512的整数。 back_log=384 # MySQL服务器同时处理的数据库连接的最大数量(默认设置是100)。超过限制后会报 Too many connections 错误 max_connections=n # 用来存放索引区块的RMA值(默认设置是8M)，增加它可得到更好处理的索引(对所有读和多重写) key_buffer_size=n #每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，你可能想要增加该值。默认数值是131072(128K) record_buffer=131072 # 服务器在关闭它之前在一个连接上等待行动的秒数。 wait_timeout=3 # 服务器在关闭它前在一个交互连接上等待行动的秒数。一个交互的客户被定义为对 mysql_real_connect()使用 CLIENT_INTERACTIVE 选项的客户。默认数值是28800，可以把它改为3600。 interactive_timeout=3600 # 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ skip-name-resolve skip-innodb # 关闭不需要的表类型 skip-bdb # 开启Binlog log-bin=/data/mysql/data/mysql-bin.log expire-logs-days=14 max-binlog-size=500M server-id=1 binlog_format=ROW binlog_row_image=FULL # 开启慢查询日志 slow_query_log=1 slow_query_log_file=/data/mysql/logs/slowquery.log long_query_time=2 [mysqldump] # 服务器和客户端之间最大能发送的可能信息包 max_allowed_packet=16M [mysql] socket=/data/mysql/data/mysql.sock default-character-set=utf8 [client] default-character-set=utf8 socket=/data/mysql/data/mysql.sock 3、初始化数据目录 mkdir -p /data/mysql/{logs,data} &&\\ chown -R mysql:mysql /data/mysql &&\\ mysqld --initialize --user=mysql 4、MySQL进程服务管理 systemctl enable mysqld systemctl daemon-reload systemctl start mysqld systemctl status mysqld 5、修改root用户本地连接密码 ALTER USER 'root'@'localhost' IDENTIFIED BY '***'; flush privileges; 或者 set password for 'root'@'localhost'=password('***'); flush privileges; 6、添加远程登录用户 默认只允许root帐户在本地登录，如果要在其它机器上连接mysql，必须修改root允许远程连接，或者添加一个允许远程连接的帐户，为了安全起见，添加一个新的帐户： GRANT ALL PRIVILEGES ON *.* TO 'sonarqube'@'%' IDENTIFIED BY '***' WITH GRANT OPTION; flush privileges; 7、修改用户密码 ①用set password命令 SET PASSWORD FOR 'root'@'localhost' = PASSWORD('***'); flush privileges; ②用mysqladmin mysqladmin -uroot -p*** password 1234abcd 格式：mysqladmin -u用户名 -p旧密码 password 新密码 ③update更新user表 use mysql update user set PASSWORD = PASSWORD('1234abcd') where user = 'root'; flush privileges; # mysql 5.7 use mysql update user set authentication_string = PASSWORD('1234abcd') where user = 'root'; flush privileges; 8、跳过密码验证重置root密码 修改my.cnf，追加skip-grant-tables，重启mysql服务，然后就可以不输入密码直接登录 9、表复制 复制表结构及数据到新表 CREATE TABLE 新表 SELECT * FROM 旧表 只复制表结构到新表 CREATE TABLE 新表 SELECT * FROM 旧表 WHERE 1=2 即:让WHERE条件不成立. 方法二:(低版本的mysql不支持，mysql4.0.25 不支持，mysql5已经支持了) CREATE TABLE 新表 LIKE 旧表 复制旧表的数据到新表(假设两个表结构一样) INSERT INTO 新表 SELECT * FROM 旧表 复制旧表的数据到新表 (假设两个表结构不一样) INSERT INTO 新表(字段1,字段2,…….) SELECT 字段1,字段2,…… FROM 旧表 参考: https://www.cnblogs.com/lxboy2009/p/7234535.html 10、show status命令 show status like '%下面变量%'; Aborted_clients 由于客户没有正确关闭连接已经死掉，已经放弃的连接数量。 Aborted_connects 尝试已经失败的MySQL服务器的连接的次数。 Connections 试图连接MySQL服务器的次数。 Created_tmp_tables 当执行语句时，已经被创造了的隐含临时表的数量。 Delayed_insert_threads 正在使用的延迟插入处理器线程的数量。 Delayed_writes 用INSERT DELAYED写入的行数。 Delayed_errors 用INSERT DELAYED写入的发生某些错误(可能重复键值)的行数。 Flush_commands 执行FLUSH命令的次数。 Handler_delete 请求从一张表中删除行的次数。 Handler_read_first 请求读入表中第一行的次数。 Handler_read_key 请求数字基于键读行。 Handler_read_next 请求读入基于一个键的一行的次数。 Handler_read_rnd 请求读入基于一个固定位置的一行的次数。 Handler_update 请求更新表中一行的次数。 Handler_write 请求向表中插入一行的次数。 Key_blocks_used 用于关键字缓存的块的数量。 Key_read_requests 请求从缓存读入一个键值的次数。 Key_reads 从磁盘物理读入一个键值的次数。 Key_write_requests 请求将一个关键字块写入缓存次数。 Key_writes 将一个键值块物理写入磁盘的次数。 Max_used_connections 同时使用的连接的最大数目。 Not_flushed_key_blocks 在键缓存中已经改变但是还没被清空到磁盘上的键块。 Not_flushed_delayed_rows 在INSERT DELAY队列中等待写入的行的数量。 Open_tables 打开表的数量。 Open_files 打开文件的数量。 Open_streams 打开流的数量(主要用于日志记载） Opened_tables 已经打开的表的数量。 Questions 发往服务器的查询的数量。 Slow_queries 要花超过long_query_time时间的查询数量。 Threads_connected 当前打开的连接的数量。 Threads_running 不在睡眠的线程数量。 Uptime 服务器工作了多少秒。 11、查看连接数和状态 如果是root帐号，你能看到所有用户的当前连接。如果是其它普通帐号，只能看到自己占用的连接。 show processlist;只列出前100条，如果想全列出请使用show full processlist; mysql> show processlist; 12、查看支持的引擎 show engines； 13、清空表中数据 # truncate语句直接清空表中数据 truncate 表名； # drop语句清空表中数据 delete from 表名; 14、导入导出数据到CSV 导出 select * from Test.User into outfile 'UserData.csv' fields terminated by '@' # 字段间以@分割 optionally enclosed by \"\" escaped by \"Curiouser\" lines terminated by '\\n'; # 数据行之间以\\n分割 如果在导出期间出现以下错误 The MySQL server is running with the --secure-file-priv option so it cannot execute this statement 可以用sql>show variables like '%secure%'查看secure-file-priv当前的值。导出的数据文件必须是这个值的指定路径才可以。默认有可能是NULL就代表禁止导出。可在配置文件中设置此变量的值,然后重启服务。 [mysqld] secure-file-priv=/home/Curiouser/Desktop 导入 sql>load data infile '/var/lib/mysql-files/UserData.csv' into table User fields terminated by '@' optionally enclosed by \"\" escaped by \"\" lines terminated by '\\n'; 当CSV文件中的每一行记录的列数小于数据库表时，使用下列语句： sql>load data infile '/var/lib/mysql-files/UserData.csv' into table User fields terminated by '@' lines terminated by '\\n' (Id,Name,Addr,Phone); 15、MySQL字符集的设置 设置MySQL默认字符集 基于session会话 set character_set_client=utf8mb4; # 主要用来设置客户端使用的字符集。 set character_set_database=utf8mb4; # 主要用来设置默认创建数据库的编码格式，如果在创建数据库时没有设置编码格式，就按照这个格式设置。 set character_set_server=utf8mb4; # 服务器安装时指定的默认编码格式，这个变量建议由系统自己管理，不要人为定义 set character_set_connection=utf8mb4; # 主要用来设置连接数据库时的字符集，如果程序中没有指明连接数据库使用的字符集类型则按照这个字符集设置。 set character_set_results=utf8mb4; # 数据库给客户端返回时使用的编码格式，如果没有指明，使用服务器默认的编码格式。 set character_set_system=utf8mb4; # 数据库系统使用的编码格式，这个值一直是utf8，不需要设置，它是为存储系统元数据的编码格式。 set collation_connection=utf8mb4; set collation_server=utf8mb4; set collation_database=utf8mb4; 基于全局global # 设置全局的数据库字符编码 set global character_set_database=utf8mb4; set global character_ser_server=utf8mb4; 永久性改变,在配置文件中修改数据库的字符编码(需重启服务) [mysqld] character-set-server=utf8mb4 [client] default-character-set=utf8mb4 [mysql] default-character-set=utf8mb4 字符集的查看 查看MySQL支持的字符集 show charset; +----------+---------------------------------+---------------------+--------+ | Charset | Description | Default collation | Maxlen | +----------+---------------------------------+---------------------+--------+ | big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 | | dec8 | DEC West European | dec8_swedish_ci | 1 | | cp850 | DOS West European | cp850_general_ci | 1 | | hp8 | HP West European | hp8_english_ci | 1 | | koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 | | latin1 | cp1252 West European | latin1_swedish_ci | 1 | | latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 | | swe7 | 7bit Swedish | swe7_swedish_ci | 1 | | ascii | US ASCII | ascii_general_ci | 1 | | ujis | EUC-JP Japanese | ujis_japanese_ci | 3 | | sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 | | hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 | | tis620 | TIS620 Thai | tis620_thai_ci | 1 | | euckr | EUC-KR Korean | euckr_korean_ci | 2 | | koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 | | gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 | | greek | ISO 8859-7 Greek | greek_general_ci | 1 | | cp1250 | Windows Central European | cp1250_general_ci | 1 | | gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 | | latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 | | armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 | | utf8 | UTF-8 Unicode | utf8_general_ci | 3 | | ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 | | cp866 | DOS Russian | cp866_general_ci | 1 | | keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 | | macce | Mac Central European | macce_general_ci | 1 | | macroman | Mac West European | macroman_general_ci | 1 | | cp852 | DOS Central European | cp852_general_ci | 1 | | latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 | | utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 | | cp1251 | Windows Cyrillic | cp1251_general_ci | 1 | | utf16 | UTF-16 Unicode | utf16_general_ci | 4 | | utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 | | cp1256 | Windows Arabic | cp1256_general_ci | 1 | | cp1257 | Windows Baltic | cp1257_general_ci | 1 | | utf32 | UTF-32 Unicode | utf32_general_ci | 4 | | binary | Binary pseudo charset | binary | 1 | | geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 | | cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 | | eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 | | gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 | +----------+---------------------------------+---------------------+--------+ 查看MySQL已配置的默认字符设置 show variables like '%character%'; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | utf8mb4 | | character_set_connection | utf8mb4 | | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | utf8mb4 | | character_set_server | latin1 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+ 查看库已配置的字符设置 show create database test1; +----------+----------------------------------------------------------------------+ | Database | Create Database | +----------+----------------------------------------------------------------------+ | test1 | CREATE DATABASE `test1` /*!40100 DEFAULT CHARACTER SET utf8mb4 */ | +----------+----------------------------------------------------------------------+ 查看表已配置的字符设置 show table status from 库名 like '表名'; 查看表中字段已配置的字符设置 show full columns from 表名; 查看库中所有表的字符集设置 select TABLE_NAME,TABLE_COLLATION from information_schema.`TABLES`; 查看所有库所有表中的字段的字符集设置 select TABLE_SCHEMA ,TABLE_NAME,COLUMN_NAME,COLLATION_NAME from information_schema.`COLUMNS` 修改字符集 修改库的字符集 alter database 库名 default character set 字符集; 修改表的字符集 alter table 表名 convert to character set 字符集; 修改字段的字符集 alter table 表名 modify 字段名 字段属性 character set 字符集； 16、Mysql大小写敏感设置 MySQL在Windows下都不区分大小写。 Linux下MySQL安装完后是默认区分表名的大小写，但不区分列名的大小写。数据库名、表名、列名、别名大小写规则是这样的： 数据库名与表名是严格区分大小写 表的别名是严格区分大小写 列名与列的别名在所有的情况下均是忽略大小写 变量名也是严格区分大小写 修改大小写敏感设置 在/etc/my.cnf中的[mysqld]后添加添加lower_case_table_names=1（0:区分大小写，1:不区分大小写)，重启MYSQL服务 为0时 表示区分大小写，使用CREATE TABLE或CREATE DATABASE语句指定的大小写字母在硬盘上保存表名和数据库名。名称比较对大小写敏感。在大小写不敏感的操作系统如windows或Mac OS x上我们不能将该参数设为0，如果在大小写不敏感的文件系统上将--lowercase-table-names强制设为0，并且使用不同的大小写访问MyISAM表名，可能会导致索引破坏。 为1时 表示将名字转化为小写后存储，名称比较对大小写不敏感。MySQL将所有表名转换为小写在存储和查找表上。该行为也适合数据库名和表的别名。该值为Windows的默认值。 为2时 表名和数据库名在硬盘上使用CREATE TABLE或CREATE DATABASE语句指定的大小写字母进行保存，但MySQL将它们转换为小写在查找表上。名称比较对大小写不敏感，即按照大小写来保存，按照小写来比较。注释：只在对大小写不敏感的文件系统上使用! innodb表名用小写保存。如果你使用innodb表，为了避免避免大小写敏感问题，可以把lower_case_table_names=1把lower_case_table_names从0改变为1在你把lower_case_table_names设置为1时，在restart你的mysqld之前，请把数据库名和表名更改为小写 如果想在查询时区分字段值的大小写，则字段值需要设置BINARY属性 创建时设置 CREATE TABLE T( A VARCHAR(10) BINARY ); 使用alter修改 ALTER TABLE `tablename` MODIFY COLUMN `cloname` VARCHAR(45) BINARY; 17、查看数据库或表容量大小 查看所有数据库容量大小 select table_schema as '数据库', sum(table_rows) as '记录数', sum(truncate(data_length/1024/1024, 2)) as '数据容量(MB)', sum(truncate(index_length/1024/1024, 2)) as '索引容量(MB)' from information_schema.tables group by table_schema order by sum(data_length) desc, sum(index_length) desc; 查看所有数据库各表容量大小 select table_schema as '数据库', table_name as '表名', table_rows as '记录数', truncate(data_length/1024/1024, 2) as '数据容量(MB)', truncate(index_length/1024/1024, 2) as '索引容量(MB)' from information_schema.tables order by data_length desc, index_length desc; 查看指定数据库容量大小 select table_schema as '数据库', sum(table_rows) as '记录数', sum(truncate(data_length/1024/1024, 2)) as '数据容量(MB)', sum(truncate(index_length/1024/1024, 2)) as '索引容量(MB)' from information_schema.tables where table_schema='mysql'; 查看指定数据库各表容量大小 select table_schema as '数据库', table_name as '表名', table_rows as '记录数', truncate(data_length/1024/1024, 2) as '数据容量(MB)', truncate(index_length/1024/1024, 2) as '索引容量(MB)' from information_schema.tables where table_schema='mysql' order by data_length desc, index_length desc; 18、开起慢查询日志 参考：采集MySQL慢查询日志到Elasticsearch中附录的第二小节 MySQL可以将执行超过指定时间的DQL、DML、DDL等语句记录下来。默认慢查询日志记录是关闭的 log_slow_queries ：表示是否开启慢查询日志，5.6以前的版本使用此参数指定是否开启慢查询日志，5.6以后的版本使用slow_query_log取代此参数，如果你使用的mysql版本刚好是5.5，那么你可以看到这两个参数同时存在，此时我们不用同时设置它们，设置这两个参数中的任何一个，另一个也会自动保持一致。 log_output : 表示当慢查询日志开启以后，以哪种方式存放，log_output可以设置为4种值，\"FILE\"、\"TABLE\"、\"FILE,TABLE\"、\"NONE\"。此值为\"FILE\"表示慢查询日志存放于指定的文件中，此值为\"TABLE\"表示慢查询日志存放于mysql库的slow_log表中，此值为\"FILE,TABLE\"表示将慢查询日志同时存放于指定的文件与slow_log表中，一般不会进行这样的设置，因为这样会徒增很多IO压力，如果开启，建议设置为\"table\",此值为\"NONE\"时表示不记录查询日志，即使slow_query_log设置为ON，如果log_output设置为NONE，也不会记录慢查询日志，其实，log_output不止用于控制慢查询日志的输出，查询日志的输出也是由此参数进行控制，也就是说，log_output设置为file，就表示查询日志和慢查询日志都存放到对应的文件中，设置为table，查询日志和慢查询日志就都存放在对应的数据库表中。 slow_query_log ：表示是否开启慢查询日志，此参数与log_slow_queries的作用没有区别，5.6以后的版本使用此参数替代log_slow_queries。 slow_query_log_file ：当使用文件存储慢查询日志时(log_output设置为\"FILE\"或者\"FILE,TABLE\"时)，指定慢查询日志存储于哪个日志文件中，默认的慢查询日志文件名为\"主机名-slow.log\"，慢查询日志的位置为datadir参数所对应的目录位置，一般情况下为 /var/lib/mysql long_query_time ：表示\"多长时间的查询\"被认定为\"慢查询\"，此值得默认值为10秒，表示超过10秒的查询被认定为慢查询。 log_queries_not_using_indexes ：表示如果运行的sql语句没有使用到索引，是否也被当做慢查询语句记录到慢查询日志中，OFF表示不记录，ON表示记录。 log_throttle_queries_not_using_indexes ：5.6.5版本新引入的参数，当log_queries_not_using_inde设置为ON时，没有使用索引的查询语句也会被当做慢查询语句记录到慢查询日志中，使用log_throttle_queries_not_using_indexes可以限制这种语句每分钟记录到慢查询日志中的次数，因为在生产环境中，有可能有很多没有使用索引的语句，此类语句频繁的被记录到慢查询日志中，可能会导致慢查询日志快速不断的增长，管理员可以通过此参数进行控制。 min_examined_row_limit ：扫描记录少于改值的SQL不记录到慢查询日志，结合去记录没有使用索引的SQL语句的例子，有可能存在某一个表，数据量维持在几行左右，且没有建立索引。这种表即使不建立索引，查询也很快，扫描记录很小，如果确定有这种表，则可以通过此参数设置，将这个SQL不记录到慢查询日志。 log_slow_admin_statements：记录超时的管理操作SQL到慢查询日志，比如ALTER/ANALYZE TABLE log_slow_slave_statements：在从服务器上开启慢查询日志 log_timestamps(5.7+)： 写入时区信息。可根据需求记录UTC时间或者服务器本地系统时间 查询慢日志是否开起等其他参数 # 查询慢日志是否开启 show variables like 'slow_query%'; # 查询多少秒的查询视为慢查询 show variables like 'long_query_time%'; # 查询慢查询日志输出到哪儿。 show variables like 'log_output%'; MySQL 8 set global log_output=‘FILE’; – 开启慢日志,纪录到 mysql.slow_log 表 set global long_query_time=2; – 设置超过0.1秒的查询为慢查询 set global slow_query_log=‘ON’;-- 打开慢日志记录 19、慢查询日志统计分析工具mysqldumpslow 通过mysqldumpslow命令我们可以更加方便的从不同的维度对慢日志进行排序、查找、统计。但是mysqldumpslow只能作用于慢查询日志文件 -s：排序规则参数 c: 执行计数 l: 锁定时间 r: 返回记录 t: 执行时间 al:平均锁定时间 ar:平均返回记录数 at:平均执行时间 -t 是top n的意思，返回多少条数据。 -g 可以跟上正则匹配模式，大小写不敏感。 # 得到返回记录最多的20个sql mysqldumpslow -s r -t 20 sqlslow.log # 得到平均访问次数最多的20条sql mysqldumpslow -s ar -t 20 sqlslow.log # 得到平均访问次数最多,并且里面含有ttt字符的20条sql mysqldumpslow -s ar -t 20 -g \"ttt\" sqldlow.log # 如果出现 -bash: mysqldumpslow: command not found 错误，请执行\"ln -s /usr/local/mysql/bin/mysqldumpslow /usr/bin\" # 如果出现如下错误，Died at /usr/bin/mysqldumpslow line 161, <> chunk 405659.说明你要分析的sql日志太大了，请拆分后再分析 20、锁表锁库操作 如果对mysql进行操作时，网络中断或SQL异常 , 可能会导致表或者库卡死 , 锁死，无法进行后续操作 （如果用 navicat 等工具连接操作, 操作都会在转圈圈，无法完成操作）。此时要找出造成锁库锁表的SQL语句的进程，将其杀死，中断其操作执行，即可解决锁库锁表。 ①查看锁死SQL的进程ID # 进程ID在trx_mysql_thread_id那一列 select id, db, user, host, command, time, state, info from information_schema.processlist where command != 'Sleep' order by time desc 或者 # sql语句在info列，进程ID在ID列 show processlist; ②杀死相关进程 kill -9 进程号 21、统计DB所有表的实际条数 如果从information_schema.tables 表统计获取各表的信息话，对于非事务性表，table_rows 这个值是精确的，对于事务性引擎，这个值通常是估算的。例如 MyISAM，存储精确的数目。对于其它存储引擎，比如 InnoDB ，本值是一个大约的数，与实际值相差可达 40 到 50% 。在这些情况下，使用 SELECT COUNT(*) 来获得准确的数目。对于在 information_schema 数据库中的表， Rows 值为 NULL 。 SELECT CONCAT( 'SELECT \"', TABLE_NAME, '\", COUNT(*) FROM ', TABLE_SCHEMA, '.', TABLE_NAME, ' UNION ALL' ) EXEC_SQL FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'DB名字'; 上述SQL会输出用于统计指定DB中所有表行数的SQL语句，复制以后，删除最后一行末尾的UNION ALL，然后执行即可获取所有表的实际条数 参考：https://commandnotfound.cn/sql/7/345/MySQL-%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E5%90%84%E4%B8%AA%E8%A1%A8%E7%9A%84%E8%A1%8C%E6%95%B0%E4%BF%A1%E6%81%AF Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-08 20:24:53 "},"origin/mysql-user-privileges.html":{"url":"origin/mysql-user-privileges.html","title":"用户权限管理","keywords":"","body":"MySQL的用户及权限管理 一、用户管理 创建用户 方式1：CREATE USER命令 必须要拥有CREATE USER权限。 CREATE USER user[IDENTIFIED BY [PASSWORD] 'password'], [user[IDENTIFIED BY [PASSWORD] 'password']]... CREATE USER 'name'@'%' IDENTIFIED BY 'pwd'; 方式2：INSERT方式 必须拥有mysql.user表的INSERT权限。另外，ssl_cipher、x509_issuer、x509_subject等必须要设置值 INSERT INTO mysql.user(Host,User,Password,ssl_cipher,x509_issuer,x509_subject) VALUES('%','name',PASSWORD('pwd'),'','','') 删除用户 方式1：DROP USER命令 需要拥有DROP USER权限。 DROP USER user[,user]… user是需要删除的用户，由用户名(User)和主机名(Host)构成。 DROP USER name@'1.1.1.1' 方式2：DELETE方式 DELETE FROM mydb.user WHERE Host = '% AND User = 'admin'; 重命名用户 rename user 'jack'@'%' to 'jim'@'%'; 二、权限管理 1、用户权限验证过程 第一阶段：服务器首先会检查你是否允许连接。因为创建用户的时候会加上主机限制，可以限制成本地、某个IP、某个IP段、以及任何地方等，只允许你从配置的指定地方登陆。 第二阶段：如果你能连接，Mysql会检查你发出的每个请求，看你是否有足够的权限实施它。比如你要更新某个表、或者查询某个表，Mysql会查看你对哪个表或者某个列是否有权限。 2、用户授权原则 1、只授予能满足需要的最小权限，防止用户干坏事。比如用户只是需要查询，那就只给select权限就可以了，不要给用户赋予update、insert或者delete权限。 2、创建用户的时候限制用户的登录主机，一般是限制成指定IP或者内网IP段。 3、初始化数据库的时候删除没有密码的用户。安装完数据库的时候会自动创建一些用户，这些用户默认没有密码。 4、为每个用户设置满足密码复杂度的密码。 5、定期清理不需要的用户。回收权限或者删除用户。 3、GRANT命令授权语法 GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH {GRANT OPTION | resource_option} ...] object_type: { TABLE | FUNCTION | PROCEDURE } priv_level: { * | *.* | db_name.* | db_name.tbl_name | tbl_name | db_name.routine_name } user由用户名(User)和主机名(Host)构成,中间用@隔开，最好加上单引号 auth_option: { IDENTIFIED BY 'auth_string' | IDENTIFIED WITH auth_plugin | IDENTIFIED WITH auth_plugin BY 'auth_string' | IDENTIFIED WITH auth_plugin AS 'auth_string' | IDENTIFIED BY PASSWORD 'auth_string' } tls_option: { SSL | X509 | CIPHER 'cipher' | ISSUER 'issuer' | SUBJECT 'subject' } resource_option: { | MAX_QUERIES_PER_HOUR count | MAX_UPDATES_PER_HOUR count | MAX_CONNECTIONS_PER_HOUR count | MAX_USER_CONNECTIONS count } Mysql权限层级 user表：全局层级 存储用户记录的表。关键字段有Host、User、Password。 创建对所有表有SELECT操作权限的用户 GRANT SELECT ON *.* TO name@'1.1.1.1' IDENTIFIED BY 'pwd'; db表：数据库层级 存储该用户对一个数据库所有的操作权限。关键字段有Host、User、Db。 授予所有权限 GRANT ALL ON mydb.* TO name@'1.1.1.1' IDENTIFIED BY 'pwd'; tables_priv表：表层级 记录了对一个表的单独授权记录.关键字段有Host、User、Db、Table_name、Table_priv、Column_priv。 当授权all在某张表的时候，Table_priv会有如下授权记录： Select,Insert,Update,Delete,Create,Drop,References,Index,Alter,Create View,Show view,Trigger。 单独授权表的某一列，会记录在此表的Column_priv里 GRANT UPDATE(age) ON mydb.user TO name@'1.1.1.1'; GRANT SELECT(birthday) ON mydb.user TO name@'1.1.1.1'; 此时会在另一张表columns_priv表中留下单独授权记录 columns_priv表：列层级 记录对表的某一列的授权记录。关键字段Host、User、Db、Table_name、Column_name。 procs_priv表：子程序层级 可以对存储过程和存储函数进行权限设置。关键字段Host、User、proc_priv MySQL 5.7的权限列表 官方文档：https://dev.mysql.com/doc/refman/5.7/en/privileges-provided.html Privilege Grant Table Column Context ALL [PRIVILEGES\\] Synonym for “all privileges” Server administration ALTER Alter_priv Tables ALTER ROUTINE Alter_routine_priv Stored routines CREATE Create_priv Databases, tables, or indexes CREATE ROUTINE Create_routine_priv Stored routines CREATE TABLESPACE Create_tablespace_priv Server administration CREATE TEMPORARY TABLES Create_tmp_table_priv Tables CREATE USER Create_user_priv Server administration CREATE VIEW Create_view_priv Views DELETE Delete_priv Tables DROP Drop_priv Databases, tables, or views EVENT Event_priv Databases EXECUTE Execute_priv Stored routines FILE File_priv File access on server host GRANT OPTION Grant_priv Databases, tables, or stored routines INDEX Index_priv Tables INSERT Insert_priv Tables or columns LOCK TABLES Lock_tables_priv Databases PROCESS Process_priv Server administration PROXY See proxies_priv table Server administration REFERENCES References_priv Databases or tables RELOAD Reload_priv Server administration REPLICATION CLIENT Repl_client_priv Server administration REPLICATION SLAVE Repl_slave_priv Server administration SELECT Select_priv Tables or columns SHOW DATABASES Show_db_priv Server administration SHOW VIEW Show_view_priv Views SHUTDOWN Shutdown_priv Server administration SUPER Super_priv Server administration TRIGGER Trigger_priv Tables UPDATE Update_priv Tables or columns USAGE Synonym for “no privileges” Server administration 4、查看用户权限 查看当前用户的权限 show grants; 查看某个用户的所有权限 show grants for 'jack'; show grants for admin'@'localhost'; 5、REVOKE命令回收权限语法 REVOKE priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level FROM user [, user] ... REVOKE ALL [PRIVILEGES], GRANT OPTION FROM user [, user] ... REVOKE PROXY ON user FROM user [, user] .. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/mysql-mode-only-full-groupby.html":{"url":"origin/mysql-mode-only-full-groupby.html","title":"MySQL SQL Mode : ONLY_FULL_GROUP_BY","keywords":"","body":"MySQL 5.7 GroupBy的语义检查 this is incompatible witn select_mode=only_full_group_mode ONLY_FULL_GROUP_BY是MySQL提供的一个sql_mode，通过这个sql_mode来提供SQL语句GROUP BY合法性的检查，在MySQL的sql_mode是非ONLY_FULL_GROUP_BY语义时。一条select语句，MySQL允许target list中输出的表达式是除聚集函数或group by column以外的表达式，这个表达式的值可能在经过group by操作后变成undefined，例如： mysql> create database test charset utf8mb4; mysql> use test; mysql> create table tt(id int,count int); mysql> insert into tt values(1,1),(1,2),(2,3),(2,4); mysql> select * from tt group by id; +------+-------+ | id | count | +------+-------+ | 1 | 1 | | 2 | 3 | +------+-------+ 2 rows in set (0.00 sec) 而对于语义限制都比较严谨的多家数据库，如SQLServer、Oracle、PostgreSql都不支持select target list中出现语义不明确的列，这样的语句在这些数据库中是会被报错的，所以从MySQL 5.7版本开始修正了这个语义，就是我们所说的ONLY_FULL_GROUP_BY语义，例如查看MySQL 5.7默认的sql_mode如下： mysql> select @@global.sql_mode; ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION 去掉ONLY_FULL_GROUP_BY模式，如下操作： mysql> set global sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; #或者 mysql> set @@sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; 上面是改变了全局sql_mode，对于新建的数据库有效。对于已存在的数据库，则需要在对应的数据下执行： mysql> set sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; #再或者 在my.cnf 里面设置 sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' 在sql_mode 中去掉only_full_group_by 我们把刚才的查询再次执行： mysql> select id,count from tt group by id; ERROR 1055 (42000): Expression #2 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.count' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 刚才通过的查询语句被server拒绝掉了！ 所以ONLY_FULL_GROUP_BY的语义就是确定select target list中的所有列的值都是明确语义，简单的说来，在ONLY_FULL_GROUP_BY模式下，target list中的值要么是来自于聚集函数的结果，要么是来自于group by list中的表达式的值。但是由于表达式的表现形式非常丰富，对于程序来说，很难精确的确定一些表达式的输出结果是明确的，比如： mysql> select count from tt group by id+count,id; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.count' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 在上面的查询语句中，其实count的值也是能被唯一确定的，但是由于程序无法分析出这种复杂的关系，所以这条查询也被拒绝掉了。 我们来看下哪些语句是在mysql的ONLY_FULL_GROUP_BY模式下是被支持的。 mysql> select id+1 from tt group by id+1; +------+ | id+1 | +------+ | 2 | | 3 | +------+ 2 rows in set (0.00 sec) 这条语句target list中的id+1和group by中的id+1是严格匹配的，所以mysql认为target list中的id+1是语义明确的，因此该语句可以通过。 但下面这条就无法通过了。 mysql> select id+1 from tt group by 1+id; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 因此，如果查询语句中的target list, having condition 或者order by list里引用了的表达式不是聚集函数，但是和group by list中的表达式严格匹配，该语句也是合法的（id+1和id+1是严格匹配的，id+1和id+2在mysql认为是不严格匹配的， id+1和1+id也是不严格匹配的）。 mysql> select id,max(count) from tt group by count; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 这条query被server拒绝掉了，因为target list中的id没有出现在聚集函数中，并且也没有出现在group by list中。 mysql> select id+1 as a from tt group by a order by id+1; +------+ | a | +------+ | 2 | | 3 | +------+ 2 rows in set (0.00 sec) mysql允许target list中对于非聚集函数的alias column被group by、having condition以及order by语句引用(version 5.7中允许having condition引用alias column，version 5.6不支持having condition引用alias column)，从上面两条语句可以看出，group by和order by中引用了alias column，并且其等价于基础列语义。 mysql> select id+count from tt group by id,count; +----------+ | id+count | +----------+ | 2 | | 3 | | 5 | | 6 | +----------+ 4 rows in set (0.00 sec) 从上面的语句可以看出，mysql的ONLY_FULL_GROUP_BY模式支持对basic column进行组合但是不支持对于复杂表达式进行组合，这个受限于表达式分析程度。 总结一下： MySQL对于ONLY_FULL_GROUP_BY语义的判断规则是，如果group by list中的表达式是basic column，那么target list中允许出现表达式是group by list中basic column或者alias column的组合结果，如果group by list中的表达式是复杂表达式(非basic column或者alias column)，那么要求target list中的表达式必须能够严格和group by list中的表达式进行匹配，否者这条查询会被认为不合法。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/mysql-binlog.html":{"url":"origin/mysql-binlog.html","title":"Binglog","keywords":"","body":"MySQL Binlog 一、简介 MySQL Binlog（MySQL Binary Log，MySQL的二进制日志文件）,它记录了所有的 DDL 和 DML 语句（除了数据查询语句select、show等），以事件形式记录，还包含语句所执行的消耗的时间。MySQL的二进制日志是事务安全型的，并以二进制的形式保存在磁盘中； 作用： 查看数据库的变更历史 数据库增量备份和恢复 MySQL的复制（主从数据库的复制） 二、Binglog 1、Binglog日志格式 ①STATEMENT格式：基于SQL语句的复制 每一条会修改数据的sql都会记录在binlog中。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题。 注意：相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。 ②ROW格式：基于行的复制 5.1.5版本的MySQL才开始支持row level的复制,它不记录sql语句上下文相关信息，仅保存哪条记录被修改。 在 MySQL 5.7.7 及更高版本中，默认值是 ROW 优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题. 缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。 注意:新版本的MySQL中对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录，如果sql语句确实就是update或者delete等修改数据的语句，那么还是会记录所有行的变更。 ③MIXED格式：混合模式复制 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。 在Mixed模式下，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 2、Binlog日志文件 二进制日志索引文件（文件名后缀为.index）用于记录所有有效的的二进制文件 二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML语句事件 binlog是一个二进制文件集合，每个binlog文件以一个4字节的魔数开头，接着是一组Events: 魔数：0xfe62696e对应的是0xfebin； Event：每个Event包含header和data两个部分；header提供了Event的创建时间，哪个服务器等信息，data部分提供的是针对该Event的具体信息，如具体数据的修改； 第一个Event用于描述binlog文件的格式版本，这个格式就是event写入binlog文件的格式； 其余的Event按照第一个Event的格式版本写入； 最后一个Event用于说明下一个binlog文件； binlog的索引文件是一个文本文件，其中内容为当前的binlog文件列表 3、Binlog事件类型 binlog 事件的结构主要有3个版本： v1: 在 MySQL 3.23 中使用 v3: 在 MySQL 4.0.2 到 4.1 中使用 v4: 在 MySQL 5.0 及以上版本中使用 现在一般不会使用MySQL5.0以下版本，所以下面仅介绍v4版本的binlog事件类型。binlog 的事件类型较多，本文在此做一些简单的汇总 事件类型 说明 UNKNOWN_EVENT 此事件从不会被触发，也不会被写入binlog中；发生在当读取binlog时，不能被识别其他任何事件，那被视为UNKNOWN_EVENT START_EVENT_V3 每个binlog文件开始的时候写入的事件，此事件被用在MySQL3.23 – 4.1，MYSQL5.0以后已经被 FORMAT_DESCRIPTION_EVENT 取代 QUERY_EVENT 执行更新语句时会生成此事件，包括：create，insert，update，delete； STOP_EVENT 当mysqld停止时生成此事件 ROTATE_EVENT 当mysqld切换到新的binlog文件生成此事件，切换到新的binlog文件可以通过执行flush logs命令或者binlog文件大于 max_binlog_size 参数配置的大小； INTVAR_EVENT 当sql语句中使用了AUTO_INCREMENT的字段或者LAST_INSERT_ID()函数；此事件没有被用在binlog_format为ROW模式的情况下 LOAD_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL 3.23版本中使用 SLAVE_EVENT 未使用 CREATE_FILE_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0和4.1版本中使用 APPEND_BLOCK_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0版本中使用 EXEC_LOAD_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0和4.1版本中使用 DELETE_FILE_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0版本中使用 NEW_LOAD_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0和4.1版本中使用 RAND_EVENT 执行包含RAND()函数的语句产生此事件，此事件没有被用在binlog_format为ROW模式的情况下 USER_VAR_EVENT 执行包含了用户变量的语句产生此事件，此事件没有被用在binlog_format为ROW模式的情况下 FORMAT_DESCRIPTION_EVENT 描述事件，被写在每个binlog文件的开始位置，用在MySQL5.0以后的版本中，代替了START_EVENT_V3 XID_EVENT 支持XA的存储引擎才有，本地测试的数据库存储引擎是innodb，所有上面出现了XID_EVENT；innodb事务提交产生了QUERY_EVENT的BEGIN声明，QUERY_EVENT以及COMMIT声明，如果是myIsam存储引擎也会有BEGIN和COMMIT声明，只是COMMIT类型不是XID_EVENT BEGIN_LOAD_QUERY_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL5.0版本中使用 EXECUTE_LOAD_QUERY_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL5.0版本中使用 TABLE_MAP_EVENT 用在binlog_format为ROW模式下，将表的定义映射到一个数字，在行操作事件之前记录（包括：WRITE_ROWS_EVENT，UPDATE_ROWS_EVENT，DELETE_ROWS_EVENT） PRE_GA_WRITE_ROWS_EVENT 已过期，被 WRITE_ROWS_EVENT 代替 PRE_GA_UPDATE_ROWS_EVENT 已过期，被 UPDATE_ROWS_EVENT 代替 PRE_GA_DELETE_ROWS_EVENT 已过期，被 DELETE_ROWS_EVENT 代替 WRITE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 insert 操作 UPDATE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 update 操作 DELETE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 delete 操作 INCIDENT_EVENT 主服务器发生了不正常的事件，通知从服务器并告知可能会导致数据处于不一致的状态 HEARTBEAT_LOG_EVENT 主服务器告诉从服务器，主服务器还活着，不写入到日志文件中 4、Binlog事件结构 一个事件对象分为事件头和事件体，事件的结构如下： +=====================================+ | event | timestamp 0 : 4 | | header +----------------------------+ | | type_code 4 : 1 | | +----------------------------+ | | server_id 5 : 4 | | +----------------------------+ | | event_length 9 : 4 | | +----------------------------+ | | next_position 13 : 4 | | +----------------------------+ | | flags 17 : 2 | | +----------------------------+ | | extra_headers 19 : x-19 | +=====================================+ | event | fixed part x : y | | data +----------------------------+ | | variable part | +=====================================+ 如果事件头的长度是 x 字节，那么事件体的长度为 (event_length - x) 字节；设事件体中 fixed part 的长度为 y 字节，那么 variable part 的长度为 (event_length - (x + y)) 字节 三、配置Binlog 1、配置 在/etc/my.cnf的[mysqld]部分添加以下配置，然后重启MySQL [mysqld] log-bin=/data/mysql/logs/binlogs/mysql-bin.log expire-logs-days=14 max-binlog-size=500M server-id=1 binlog_format=ROW binlog_row_image=FULL relay_log_info_repository=TABLE 2、验证 show variables like '%binlog%' # 检查MySQL是否已经开启binlog show variables like 'log_bin' # 查看binlog文件列表及大小 show binary logs # 查看binlog内容 show binlog events # 查看当前最新一个binlog日志文件的状态信息，显示正在写入的二进制文件，及当前position show master status; #查看所有binlog日志列表 show master logs; 3、注意 当遇到以下3种情况时，MySQL会重新生成一个新的日志文件，文件序号递增： MySQL服务器停止或重启时 使用 flush logs 命令； 当 binlog 文件大小超过 max_binlog_size 变量的值时； max_binlog_size 的最小值是4096字节，最大值和默认值是 1GB (1073741824字节)。事务被写入到binlog的一个块中，所以它不会在几个二进制日志之间被拆分。因此，如果你有很大的事务，为了保证事务的完整性，不可能做切换日志的动作，只能将该事务的日志都记录到当前日志文件中，直到事务结束，你可能会看到binlog文件大于 max_binlog_size 的情况。 四、使用mysqlbinlog命令查看Binlog 因为binlog日志文件是二进制文件，没法用vi等打开，这时就需要mysql的自带的mysqlbinlog工具进行解码 mysqlbinlog [参数] binlog文件路径 参数： -?, --help Display this help and exit. --base64-output=name Determine when the output statements should be base64-encoded BINLOG statements: 'never' disables it and works only for binlogs without row-based events; 'decode-rows' decodes row events into commented pseudo-SQL statements if the --verbose option is also given; 'auto' prints base64 only when necessary (i.e., for row-based events and format description events). If no --base64-output[=name] option is given at all, the default is 'auto'. --bind-address=name IP address to bind to. --character-sets-dir=name Directory for character set files. -d, --database=name List entries for just this database (local log only). --debug-check Check memory and open file usage at exit . --debug-info Print some debug info at exit. --default-auth=name Default authentication client-side plugin to use. -D, --disable-log-bin Disable binary log. This is useful, if you enabled --to-last-log and are sending the output to the same MySQL server. This way you could avoid an endless loop. You would also like to use it when restoring after a crash to avoid duplication of the statements you already have. NOTE: you will need a SUPER privilege to use this option. -F, --force-if-open Force if binlog was not closed properly. (Defaults to on; use --skip-force-if-open to disable.) -f, --force-read Force reading unknown binlog events. -H, --hexdump Augment output with hexadecimal and ASCII event dump. -h, --host=name Get the binlog from server. -l, --local-load=name Prepare local temporary files for LOAD DATA INFILE in the specified directory. -o, --offset=# Skip the first N entries. -p, --password[=name] Password to connect to remote server. --plugin-dir=name Directory for client-side plugins. -P, --port=# Port number to use for connection or 0 for default to, in order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default (3306). --protocol=name The protocol to use for connection (tcp, socket, pipe, memory). -R, --read-from-remote-server Read binary logs from a MySQL server. This is an alias for read-from-remote-master=BINLOG-DUMP-NON-GTIDS. --read-from-remote-master=name Read binary logs from a MySQL server through the COM_BINLOG_DUMP or COM_BINLOG_DUMP_GTID commands by setting the option to either BINLOG-DUMP-NON-GTIDS or BINLOG-DUMP-GTIDS, respectively. If --read-from-remote-master=BINLOG-DUMP-GTIDS is combined with --exclude-gtids, transactions can be filtered out on the master avoiding unnecessary network traffic. --raw Requires -R. Output raw binlog data instead of SQL statements, output is to log files. -r, --result-file=name Direct output to a given file. With --raw this is a prefix for the file names. --secure-auth Refuse client connecting to server if it uses old (pre-4.1.1) protocol. (Defaults to on; use --skip-secure-auth to disable.) --server-id=# Extract only binlog entries created by the server having the given id. --server-id-bits=# Set number of significant bits in server-id --set-charset=name Add 'SET NAMES character_set' to the output. -s, --short-form Just show regular queries: no extra info and no row-based events. This is for testing only, and should not be used in production systems. If you want to suppress base64-output, consider using --base64-output=never instead. -S, --socket=name The socket file to use for connection. --start-datetime=name Start reading the binlog at first event having a datetime equal or posterior to the argument; the argument must be a date and time in the local time zone, in any format accepted by the MySQL server for DATETIME and TIMESTAMP types, for example: 2004-12-25 11:25:56 (you should probably use quotes for your shell to set it properly). -j, --start-position=# Start reading the binlog at position N. Applies to the first binlog passed on the command line. --stop-datetime=name Stop reading the binlog at first event having a datetime equal or posterior to the argument; the argument must be a date and time in the local time zone, in any format accepted by the MySQL server for DATETIME and TIMESTAMP types, for example: 2004-12-25 11:25:56 (you should probably use quotes for your shell to set it properly). --stop-never Wait for more data from the server instead of stopping at the end of the last log. Implicitly sets --to-last-log but instead of stopping at the end of the last log it continues to wait till the server disconnects. --stop-never-slave-server-id=# The slave server_id used for --read-from-remote-server --stop-never. --stop-position=# Stop reading the binlog at position N. Applies to the last binlog passed on the command line. -t, --to-last-log Requires -R. Will not stop at the end of the requested binlog but rather continue printing until the end of the last binlog of the MySQL server. If you send the output to the same MySQL server, that may lead to an endless loop. -u, --user=name 连接远程服务器的用户名。 -v, --verbose Reconstruct pseudo-SQL statements out of row events. -v -v adds comments on column data types. -V, --version 打印mysqlbinlog的版本信息并退出 --open-files-limit=# Used to reserve file descriptors for use by this program. -c, --verify-binlog-checksum Verify checksum binlog events. --binlog-row-event-max-size=# The maximum size of a row-based binary log event in bytes. Rows will be grouped into events smaller than this size if possible. 改值必须是256的倍数。 指定基于行的binlog的大小， --skip-gtids Do not print Global Transaction Identifier information (SET GTID_NEXT=... etc). --include-gtids=name Print events whose Global Transaction Identifiers were provided. --exclude-gtids=name Print all events but those whose Global Transaction Identifiers were provided. Variables (--variable-name=value) and boolean options {FALSE|TRUE} Value (after reading options) --------------------------------- ---------------------------------------- base64-output (No default value) bind-address (No default value) character-sets-dir (No default value) database (No default value) debug-check FALSE debug-info FALSE default-auth (No default value) disable-log-bin FALSE force-if-open TRUE force-read FALSE hexdump FALSE host (No default value) local-load (No default value) offset 0 plugin-dir (No default value) port 0 read-from-remote-server FALSE read-from-remote-master (No default value) raw FALSE result-file (No default value) secure-auth TRUE server-id 0 server-id-bits 32 set-charset (No default value) short-form FALSE socket /data/mysql/data/mysqld.sock start-datetime (No default value) start-position 4 stop-datetime (No default value) stop-never FALSE stop-never-slave-server-id -1 stop-position 18446744073709551615 to-last-log FALSE user (No default value) open-files-limit 64 verify-binlog-checksum FALSE binlog-row-event-max-size 4294967040 skip-gtids FALSE include-gtids (No default value) exclude-gtids (No default value) 五、使用SQL语句查看Binlog show binlog events [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]; -- IN 'log_name' ：指定要查询的binlog文件名(不指定就是第一个binlog文件) -- FROM pos ：指定从哪个pos起始点开始查起(不指定就是从整个文件首个pos点开始算) -- LIMIT [offset,] ：偏移量(不指定就是0) -- row_count ：查询总条数(不指定就是所有行) 参考： https://blog.csdn.net/ouyang111222/article/details/50300851 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-14 22:24:56 "},"origin/mysql-backup-restore.html":{"url":"origin/mysql-backup-restore.html","title":"MySQL的数据备份与恢复","keywords":"","body":"MySQL的数据库备份与恢复 一、前言 我们试着想一想, 在生产环境中什么最重要？如果我们服务器的硬件坏了可以维修或者换新, 软件问题可以修复或重新安装, 但是如果数据没了呢？这可能是最恐怖的事情了吧, 我感觉在生产环境中应该没有什么比数据跟更为重要. 那么我们该如何保证数据不丢失、或者丢失后可以快速恢复呢？只要看完这篇, 大家应该就能对MySQL中实现数据备份和恢复能有一定的了解。 二、为什么需要备份数据？ 其实在前言中也大概说明了为什么要备份数据, 但是我们还是应该具体了解一下为什么要备份数据 在生产环境中我们数据库可能会遭遇各种各样的不测从而导致数据丢失, 大概分为以下几种. 硬件故障 软件故障 自然灾害 黑客攻击 误操作 (占比最大) 所以, 为了在数据丢失之后能够恢复数据, 我们就需要定期的备份数据, 备份数据的策略要根据不同的应用场景进行定制, 大致有几个参考数值, 我们可以根据这些数值从而定制符合特定环境中的数据备份策略 能够容忍丢失多少数据 恢复数据需要多长时间 需要恢复哪一些数据 三、数据的备份类型 数据的备份类型根据其自身的特性主要分为以下几组 完全备份：指的是备份整个数据集( 即整个数据库 ) 部分备份：指的是备份部分数据集(例如: 只备份一个表) 增量备份：指的是备份自上一次备份以来(增量或完全)以来变化的数据; 特点: 节约空间、还原麻烦 差异备份：指的是备份自上一次完全备份以来变化的数据 特点: 浪费空间、还原比增量备份简单 四、MySQL备份数据的方式 在MySQl中我们备份数据一般有几种方式 热备份：指的是当数据库进行备份时, 数据库的读写操作均不是受影响 温备份：指的是当数据库进行备份时, 数据库的读操作可以执行, 但是不能执行写操作 冷备份：指的是当数据库进行备份时, 数据库不能进行读写操作, 即数据库要下线 MySQL中进行不同方式的备份还要考虑存储引擎是否支持 MyISAM InnoDB 热备 × √ 温备 √ √ 冷备 √ √ 我们在考虑完数据在备份时, 数据库的运行状态之后还需要考虑对于MySQL数据库中数据的备份方式 物理备份一般就是通过tar,cp等命令直接打包复制数据库的数据文件达到备份的效果 逻辑备份一般就是通过特定工具从数据库中导出数据并另存备份(逻辑备份会丢失数据精度) 物理备份 逻辑备份 五、备份需要考虑的问题 一般情况下, 我们需要备份的数据分为以下几种 数据 二进制日志, InnoDB事务日志 代码(存储过程、存储函数、触发器、事件调度器) 服务器配置文件 六、备份工具 这里我们列举出常用的几种备份工具 mysqldump : 逻辑备份工具, 适用于所有的存储引擎, 支持温备、完全备份、部分备份、对于InnoDB存储引擎支持热备 cp, tar 等归档复制工具: 物理备份工具, 适用于所有的存储引擎, 冷备、完全备份、部分备份 lvm2 snapshot: 几乎热备, 借助文件系统管理工具进行备份 mysqlhotcopy: 名不副实的的一个工具, 几乎冷备, 仅支持MyISAM存储引擎 xtrabackup: 一款非常强大的InnoDB/XtraDB热备工具, 支持完全备份、增量备份, 由percona提供 七、设计合适的备份策略 针对不同的场景下, 我们应该制定不同的备份策略对数据库进行备份, 一般情况下, 备份策略一般为以下三种 直接cp,tar复制数据库文件 mysqldump+复制BIN LOGS lvm2快照+复制BIN LOGS xtrabackup 以上的几种解决方案分别针对于不同的场景 如果数据量较小, 可以使用第一种方式, 直接复制数据库文件 如果数据量还行, 可以使用第二种方式, 先使用mysqldump对数据库进行完全备份, 然后定期备份BINARY LOG达到增量备份的效果 如果数据量一般, 而又不过分影响业务运行, 可以使用第三种方式, 使用lvm2的快照对数据文件进行备份, 而后定期备份BINARY LOG达到增量备份的效果 如果数据量很大, 而又不过分影响业务运行, 可以使用第四种方式, 使用xtrabackup进行完全备份后, 定期使用xtrabackup进行增量备份或差异备份 八、实战演练 1、使用cp进行备份 我们这里使用的是使用yum安装的mysql-5.1的版本, 使用的数据集为从网络上找到的一个员工数据库 查看数据库的信息 mysql> SHOW DATABASES; #查看当前的数据库, 我们的数据库为employees +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; Database changed mysql> SHOW TABLES; #查看当前库中的表 +---------------------+ | Tables_in_employees | +---------------------+ | departments | | dept_emp | | dept_manager | | employees | | salaries | | titles | +---------------------+ 6 rows in set (0.00 sec) mysql> SELECT COUNT(*) FROM employees; #由于篇幅原因, 我们这里只看一下employees的行数为300024 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.05 sec) 向数据库施加读锁 mysql> FLUSH TABLES WITH READ LOCK; #向所有表施加读锁 Query OK, 0 rows affected (0.00 sec) 备份数据文件 mkdir /backup #创建文件夹存放备份数据库文件 cp -a /var/lib/mysql/* /backup #保留权限的拷贝源数据文件 ls /backup #查看目录下的文件 employees ibdata1 ib_logfile0 ib_logfile1 mysql mysql.sock test 模拟数据丢失并恢复 rm -rf /var/lib/mysql/* #删除数据库的所有文件 service mysqld restart #重启MySQL, 如果是编译安装的应该不能启动, 如果rpm安装则会重新初始化数据库 mysql> SHOW DATABASES; #因为我们是rpm安装的, 连接到MySQL进行查看, 发现数据丢失了！ +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec) rm -rf /var/lib/mysql/* #这一步可以不做 cp -a /backup/* /var/lib/mysql/ #将备份的数据文件拷贝回去 service mysqld restart #重启MySQL #重新连接数据并查看 mysql> SHOW DATABASES; #数据库已恢复 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; mysql> SELECT COUNT(*) FROM employees; #表的行数没有变化 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.06 sec) 2、使用mysqldump+复制BINARY LOG备份 我们这里使用的是使用yum安装的mysql-5.1的版本, 使用的数据集为从网络上找到的一个员工数据库 我们通过mysqldump进行一次完全备份, 再修改表中的数据, 然后再通过binary log进行恢复 二进制日志需要在mysql配置文件中添加 log_bin=on 开启 mysqldump命令介绍 mysqldump是一个客户端的逻辑备份工具, 可以生成一个重现创建原始数据库和表的SQL语句, 可以支持所有的存储引擎, 对于InnoDB支持热备 官方文档介绍 #基本语法格式 shell> mysqldump [options] db_name [tbl_name ...] 恢复需要手动CRATE DATABASES shell> mysqldump [options] --databases db_name ... 恢复不需要手动创建数据库 shell> mysqldump [options] --all-databases 恢复不需要手动创建数据库 其他选项: -E, --events: 备份事件调度器 -R, --routines: 备份存储过程和存储函数 --triggers: 备份表的触发器; --skip-triggers --master-date[=value] 1: 记录为CHANGE MASTER TO 语句、语句不被注释 2: 记录为注释的CHANGE MASTER TO语句 基于二进制还原只能全库还原 --flush-logs: 日志滚动 锁定表完成后执行日志滚动 查看数据库的信息 mysql> SHOW DATABASES; #查看当前的数据库, 我们的数据库为employees +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; Database changed mysql> SHOW TABLES; #查看当前库中的表 +---------------------+ | Tables_in_employees | +---------------------+ | departments | | dept_emp | | dept_manager | | employees | | salaries | | titles | +---------------------+ 6 rows in set (0.00 sec) mysql> SELECT COUNT(*) FROM employees; #由于篇幅原因, 我们这里只看一下employees的行数为300024 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.05 sec) 使用mysqldump备份数据库 mysql -uroot -p -e 'SHOW MASTER STATUS' #查看当前二进制文件的状态, 并记录下position的数字 +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000003 | 106 | | | +------------------+----------+--------------+------------------+ mysqldump --all-databases --lock-all-tables > backup.sql #备份数据库到backup.sql文件中 mysql> CREATE DATABASE TEST1; #创建一个数据库 Query OK, 1 row affected (0.00 sec) mysql> SHOW MASTER STATUS; #记下现在的position +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000003 | 191 | | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) cp /var/lib/mysql/mysql-bin.000003 /root #备份二进制文件 service mysqld stop #停止MySQL rm -rf /var/lib/mysql/* #删除所有的数据文件 service mysqld start #启动MySQL, 如果是编译安装的应该不能启动(需重新初始化), 如果rpm安装则会重新初始化数据库 mysql> SHOW DATABASES; #查看数据库, 数据丢失! +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec) mysql> SET sql_log_bin=OFF; #暂时先将二进制日志关闭 Query OK, 0 rows affected (0.00 sec) mysql> source backup.sql #恢复数据，所需时间根据数据库时间大小而定 mysql> SET sql_log_bin=ON; 开启二进制日志 mysql> SHOW DATABASES; #数据库恢复, 但是缺少TEST1 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysqlbinlog --start-position=106 --stop-position=191 mysql-bin.000003 | mysql employees #通过二进制日志增量恢复数据 mysql> SHOW DATABASES; #现在TEST1出现了！ +--------------------+ | Database | +--------------------+ | information_schema | | TEST1 | | employees | | mysql | | test | +--------------------+ 5 rows in set (0.00 sec) 3、使用lvm2快照备份数据 做实验之前我们先回顾一下lvm2-snapshot的知识 LVM快照简单来说就是将所快照源分区一个时间点所有文件的元数据进行保存，如果源文件没有改变，那么访问快照卷的相应文件则直接指向源分区的源文件，如果源文件发生改变，则快照卷中与之对应的文件不会发生改变。快照卷主要用于辅助备份文件。 这里只简单介绍，点击查看详细介绍 部署lvm环境 添加硬盘; 这里我们直接实现SCSI硬盘的热插拔, 首先在虚拟机中添加一块硬盘, 不重启 ls /dev/sd* #只有以下几块硬盘, 但是我们不重启可以让系统识别新添加的硬盘 /dev/sda /dev/sda1 /dev/sda2 echo '- - -' > /sys/class/scsi_host/host0/scan echo '- - -' > /sys/class/scsi_host/host1/scan echo '- - -' > /sys/class/scsi_host/host2/scan ls /dev/sd* #看！sdb识别出来了 /dev/sda /dev/sda1 /dev/sda2 /dev/sdb fdisk /dev/sdb #分区,步骤省略 partx -a /dev/sdb BLKPG: Device or resource busy error adding partition 1 ##创建逻辑卷 pvcreate /dev/sdb1 Physical volume \"/dev/sdb1\" successfully created vgcreate myvg /dev/sdb1 Volume group \"myvg\" successfully created lvcreate -n mydata -L 5G myvg Logical volume \"mydata\" created. mkfs.ext4 /dev/mapper/myvg-mydata #格式化 mkdir /lvm_data mount /dev/mapper/myvg-mydata /lvm_data #挂载到/lvm_data vim /etc/my.cnf #修改mysql配置文件的datadir如下 datadir=/lvm_data service mysqld restart #重启MySQL ####重新导入employees数据库########略过#### 查看数据库的信息 mysql> SHOW DATABASES; #查看当前的数据库, 我们的数据库为employees +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; Database changed mysql> SHOW TABLES; #查看当前库中的表 +---------------------+ | Tables_in_employees | +---------------------+ | departments | | dept_emp | | dept_manager | | employees | | salaries | | titles | +---------------------+ 6 rows in set (0.00 sec) mysql> SELECT COUNT(*) FROM employees; #由于篇幅原因, 我们这里只看一下employees的行数为300024 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.05 sec) 创建快照卷并备份 mysql> FLUSH TABLES WITH READ LOCK; #锁定所有表 Query OK, 0 rows affected (0.00 sec) [root@node1 lvm_data]# lvcreate -L 1G -n mydata-snap -p r -s /dev/mapper/myvg-mydata #创建快照卷 Logical volume \"mydata-snap\" created. mysql> UNLOCK TABLES; #解锁所有表 Query OK, 0 rows affected (0.00 sec) [root@node1 lvm_data]# mkdir /lvm_snap #创建文件夹 [root@node1 lvm_data]# mount /dev/myvg/mydata-snap /lvm_snap/ #挂载snap mount: block device /dev/mapper/myvg-mydata--snap is write-protected, mounting read-only [root@node1 lvm_data]# cd /lvm_snap/ [root@node1 lvm_snap]# ls employees ibdata1 ib_logfile0 ib_logfile1 mysql mysql-bin.000001 mysql-bin.000002 mysql-bin.000003 mysql-bin.index test [root@node1 lvm_snap]# tar cf /tmp/mysqlback.tar * #打包文件到/tmp/mysqlback.tar umount /lvm_snap/ #卸载snap lvremove myvg mydata-snap #删除snap 恢复数据 [root@node1 lvm_snap]# rm -rf /lvm_data/* service mysqld start #启动MySQL, 如果是编译安装的应该不能启动(需重新初始化), 如果rpm安装则会重新初始化数据库 mysql> SHOW DATABASES; #查看数据库, 数据丢失! +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec) cd /lvm_data/ [root@node1 lvm_data]# rm -rf * #删除所有文件 [root@node1 lvm_data]# tar xf /tmp/mysqlback.tar #解压备份数据库到此文件夹 [root@node1 lvm_data]# ls #查看当前的文件 employees ibdata1 ib_logfile0 ib_logfile1 mysql mysql-bin.000001 mysql-bin.000002 mysql-bin.000003 mysql-bin.index test mysql> SHOW DATABASES; #数据恢复了 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) ##完成 4、使用Xtrabackup备份 为了更好地演示, 我们这次使用mariadb-5.5的版本, 使用xtrabackup使用InnoDB能够发挥其最大功效, 并且InnoDB的每一张表必须使用单独的表空间, 我们需要在配置文件中添加 innodb_file_per_table = ON 来开启 下载安装xtrabackup 我们这里通过wget percona官方的rpm包进行安装 wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.3.4/binary/redhat/6/x86_64/percona-xtrabackup-2.3.4-1.el6.x86_64.rpm 下载页：https://www.percona.com/downloads/XtraBackup/LATEST/ yum localinstall percona-xtrabackup-2.3.4-1.el6.x86_64.rpm #需要EPEL源 xtrabackup介绍 Xtrabackup是由percona提供的mysql数据库备份工具，据官方介绍，这也是世界上惟一一款开源的能够对innodb和xtradb数据库进行热备的工具。特点： 备份过程快速、可靠； 备份过程不会打断正在执行的事务； 能够基于压缩等功能节约磁盘空间和流量； 自动实现备份检验； 还原速度快； xtrabackup实现完全备份 我们这里使用xtrabackup的前端配置工具innobackupex来实现对数据库的完全备份 使用innobackupex备份时, 会调用xtrabackup备份所有的InnoDB表, 复制所有关于表结构定义的相关文件(.frm)、以及MyISAM、MERGE、CSV和ARCHIVE表的相关文件, 同时还会备份触发器和数据库配置文件信息相关的文件, 这些文件会被保存至一个以时间命名的目录. 备份过程 mkdir /extrabackup #创建备份目录 innobackupex --user=root /extrabackup/ #备份数据 ###################提示complete表示成功********************* ls /extrabackup/ #看到备份目录 2016-04-27_07-30-48 一般情况, 备份完成后, 数据不能用于恢复操作, 因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此, 此时的数据文件仍不一致, 所以我们需要”准备”一个完全备份 innobackupex --apply-log /extrabackup/2016-04-27_07-30-48/ #指定备份文件的目录 #一般情况下下面三行结尾代表成功***************** InnoDB: Starting shutdown... InnoDB: Shutdown completed; log sequence number 369661462 160427 07:40:11 completed OK! cd /extrabackup/2016-04-27_07-30-48/ [root@node1 2016-04-27_07-30-48]# ls -hl #查看备份文件 total 31M -rw-r----- 1 root root 386 Apr 27 07:30 backup-my.cnf drwx------ 2 root root 4.0K Apr 27 07:30 employees -rw-r----- 1 root root 18M Apr 27 07:40 ibdata1 -rw-r--r-- 1 root root 5.0M Apr 27 07:40 ib_logfile0 -rw-r--r-- 1 root root 5.0M Apr 27 07:40 ib_logfile1 drwx------ 2 root root 4.0K Apr 27 07:30 mysql drwx------ 2 root root 4.0K Apr 27 07:30 performance_schema drwx------ 2 root root 4.0K Apr 27 07:30 test -rw-r----- 1 root root 27 Apr 27 07:30 xtrabackup_binlog_info -rw-r--r-- 1 root root 29 Apr 27 07:40 xtrabackup_binlog_pos_innodb -rw-r----- 1 root root 117 Apr 27 07:40 xtrabackup_checkpoints -rw-r----- 1 root root 470 Apr 27 07:30 xtrabackup_info -rw-r----- 1 root root 2.0M Apr 27 07:40 xtrabackup_logfile 恢复数据 root@node1 ~]# rm -rf /data/* #删除数据文件 ***不用启动数据库也可以还原************* innobackupex --copy-back /extrabackup/2016-04-27_07-30-48/ #恢复数据, 记清使用方法 #########我们这里是编译安装的mariadb所以需要做一些操作########## [root@node1 data]# killall mysqld chown -R mysql:mysql ./* ll /data/ #数据恢复 total 28704 -rw-rw---- 1 mysql mysql 16384 Apr 27 07:43 aria_log.00000001 -rw-rw---- 1 mysql mysql 52 Apr 27 07:43 aria_log_control -rw-rw---- 1 mysql mysql 18874368 Apr 27 07:43 ibdata1 -rw-rw---- 1 mysql mysql 5242880 Apr 27 07:43 ib_logfile0 -rw-rw---- 1 mysql mysql 5242880 Apr 27 07:43 ib_logfile1 -rw-rw---- 1 mysql mysql 264 Apr 27 07:43 mysql-bin.000001 -rw-rw---- 1 mysql mysql 19 Apr 27 07:43 mysql-bin.index -rw-r----- 1 mysql mysql 2166 Apr 27 07:43 node1.anyisalin.com.err [root@node1 data]# service mysqld restart MySQL server PID file could not be found! [FAILED] Starting MySQL.. [ OK ] MariaDB [(none)]> SHOW DATABASES; #查看数据库, 已经恢复 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | performance_schema | | test | +--------------------+ 5 rows in set (0.00 sec 增量备份 #########创建连两个数据库以供测试##################### MariaDB [(none)]> CREATE DATABASE TEST1; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]> CREATE DATABASE TEST2; Query OK, 1 row affected (0.00 sec) innobackupex --incremental /extrabackup/ --incremental-basedir=/extrabackup/2016-04-27_07-30-48/ ls /extrabackup/2016-04-27_07-57-22/ #查看备份文件 total 96 -rw-r----- 1 root root 386 Apr 27 07:57 backup-my.cnf drwx------ 2 root root 4096 Apr 27 07:57 employees -rw-r----- 1 root root 49152 Apr 27 07:57 ibdata1.delta -rw-r----- 1 root root 44 Apr 27 07:57 ibdata1.meta drwx------ 2 root root 4096 Apr 27 07:57 mysql drwx------ 2 root root 4096 Apr 27 07:57 performance_schema drwx------ 2 root root 4096 Apr 27 07:57 test drwx------ 2 root root 4096 Apr 27 07:57 TEST1 drwx------ 2 root root 4096 Apr 27 07:57 TEST2 -rw-r----- 1 root root 21 Apr 27 07:57 xtrabackup_binlog_info -rw-r----- 1 root root 123 Apr 27 07:57 xtrabackup_checkpoints -rw-r----- 1 root root 530 Apr 27 07:57 xtrabackup_info -rw-r----- 1 root root 2560 Apr 27 07:57 xtrabackup_logfile BASEDIR指的是完全备份所在的目录，此命令执行结束后，innobackupex命令会在/extrabackup目录中创建一个新的以时间命名的目录以存放所有的增量备份数据。另外，在执行过增量备份之后再一次进行增量备份时，其--incremental-basedir应该指向上一次的增量备份所在的目录。 需要注意的是，增量备份仅能应用于InnoDB或XtraDB表，对于MyISAM表而言，执行增量备份时其实进行的是完全备份。 整理增量备份 innobackupex --apply-log --redo-only /extrabackup/2016-04-27_07-30-48/ innobackupex --apply-log --redo-only /extrabackup/2016-04-27_07-30-48/ --incremental-dir=/extrabackup/2016-04-27_07-5 7-22/ 恢复数据 rm -rf /data/* #删除数据 innobackupex --copy-back /extrabackup/2016-04-27_07-30-48/ #整理增量备份之后可以直接通过全量备份还原 chown -R mysql.mysql /data/ ls /data/ -l total 28732 -rw-rw---- 1 mysql mysql 8192 Apr 27 08:05 aria_log.00000001 -rw-rw---- 1 mysql mysql 52 Apr 27 08:05 aria_log_control drwx------ 2 mysql mysql 4096 Apr 27 08:05 employees -rw-r----- 1 mysql mysql 18874368 Apr 27 08:05 ibdata1 -rw-r----- 1 mysql mysql 5242880 Apr 27 08:05 ib_logfile0 -rw-r----- 1 mysql mysql 5242880 Apr 27 08:05 ib_logfile1 drwx------ 2 mysql mysql 4096 Apr 27 08:05 mysql -rw-rw---- 1 mysql mysql 245 Apr 27 08:05 mysql-bin.000001 -rw-rw---- 1 mysql mysql 19 Apr 27 08:05 mysql-bin.index -rw-r----- 1 mysql mysql 1812 Apr 27 08:05 node1.anyisalin.com.err -rw-rw---- 1 mysql mysql 5 Apr 27 08:05 node1.anyisalin.com.pid drwx------ 2 mysql mysql 4096 Apr 27 08:05 performance_schema drwx------ 2 mysql mysql 4096 Apr 27 08:05 test drwx------ 2 mysql mysql 4096 Apr 27 08:05 TEST1 drwx------ 2 mysql mysql 4096 Apr 27 08:05 TEST2 -rw-r----- 1 mysql mysql 29 Apr 27 08:05 fsGroup: 5555 -rw-r----- 1 mysql mysql 530 Apr 27 08:05 xtrabackup_info MariaDB [(none)]> SHOW DATABASES; #数据还原 +--------------------+ | Database | +--------------------+ | information_schema | | TEST1 | | TEST2 | | employees | | mysql | | performance_schema | | test | +--------------------+ 7 rows in set (0.00 sec) 九、快速备份恢复实操 1、阿里云RDS物理备份恢复 ①整个实例全库恢复 以恢复到ubuntu 18.04 MySQL 5.6中为例 在阿里云RSD实例的备份与恢复页面下载物理备份 解压(解压后文件很大，注意磁盘容量) tar -izxvf .tar.gz -C /data/mysql/data 安装Percona XtraBackup和MySQL 注意Percona XtraBackup版本 MySQL 5.6及之前的版本需要安装 Percona XtraBackup 2.3，安装指导请参见官方文档Percona XtraBackup 2.3。 MySQL 5.7版本需要安装 Percona XtraBackup 2.4，安装指导请参见官方文档Percona XtraBackup 2.4。 MySQL 8.0版本需要安装 Percona XtraBackup 8.0，安装指导请参见官方文档Percona XtraBackup 8.0。 # 安装MySQL 5.6 wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-server_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-client_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-server_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-client_5.6.46-1debian8_amd64.deb dpkg -i mysql* # ubuntu 安装Percona XtraBackup2.3 wget https://repo.percona.com/apt/percona-release_0.1-6.$(lsb_release -sc)_all.deb dpkg -i percona-release_0.1-6.bionic_all.deb apt-get update apt-get install percona-xtrabackup # CentOS 安装Percona XtraBackup2.3 yum install -y perl rsync perl-Data-Dumper wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.3.5/binary/redhat/7/x86_64/percona-xtrabackup-2.3.5-1.el7.x86_64.rpm yum clean all yum repolist yum localinstall percona-xtrabackup-2.3.2-1.el7.x86_64.rpm 恢复解压后的备份文件 ## MySQL 5.6/5.7 innobackupex --defaults-file=/data/mysql/data/backup-my.cnf --apply-log /data/mysql/data ## MySQL 8.0 xtrabackup --prepare --target-dir=/data/mysql/data xtrabackup --datadir=/var/lib/mysql --copy-back --target-dir=/home/mysql/data 若系统返回如下类似结果，则说明备份文件已成功恢复到自建数据库 chown -R mysql.mysql /data/mysql/data 修改解压后默认的MySQL配置参数，并启动 修改/data/mysql/data/backup-my.cnf中的配置参数 注意：自建数据库不支持如下参数，需要注释掉。 #innodb_log_checksum_algorithm #innodb_fast_checksum #innodb_log_block_size #innodb_doublewrite_file #rds_encrypt_data #innodb_encrypt_algorithm #redo_log_version #master_key_id #server_uuid 修改过后的配置文件 [mysqld] innodb_checksum_algorithm=innodb #innodb_log_checksum_algorithm=innodb innodb_data_file_path=ibdata1:200M:autoextend innodb_log_files_in_group=2 innodb_log_file_size=524288000 #innodb_fast_checksum=false innodb_page_size=16384 #innodb_log_block_size=512 innodb_undo_directory=. innodb_undo_tablespaces=0 #rds_encrypt_data=false #innodb_encrypt_algorithm=aes_128_ecb lower_case_table_names=1 log-bin=/data/mysql/logs/binlogs/mysql-bin.log expire-logs-days=14 max-binlog-size=500M server-id=1 binlog_format=ROW binlog_row_image=FULL slow_query_log=1 slow_query_log_file=/data/mysql/logs/slowlogs/slowquery.log long_query_time=2 max_connections=500 interactive_timeout=1200 wait_timeout=1200 skip-grant-tables 启动 mysqld_safe --defaults-file=/data/mysql/data/backup-my.cnf --user=mysql --datadir=/data/mysql/data 修改root密码 注意：由于上一步中MySQL配置文件中添加skip-grant-tables,此时，是可以不用输入密码就可以登录 ```bash mysql -u root -p mysql > use mysql mysql > update user set PASSWORD = PASSWORD('新密码') where user = 'root'; mysql > flush privileges; 或者使用以下SQL语句重新建立root用户的远程登录权限 mysql > GRANT ALL PRIVILEGES ON . TO root@\"%\" IDENTIFIED BY '新密码*' WITH GRANT OPTION; mysql > flush privileges; 修改完成后，将配置文件中的`skip-grant-tables`删掉，重新部署启动。禁止跳过密码登录 注意： - 如果原始数据库中，没有远程root用户登录用户权限，可在跳过密码验证阶段，使用navicat连接上后，直接在界面更新mysql.user的root用户localhost为root用户%的权限 ### ②单个表物理.ibd文件恢复 MySQL 5.6 版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。 - **.frm文件**：保存了每个表的元数据，包括表结构的定义等，该文件与数据库引擎无关。 - **.ibd文件**：InnoDB引擎开启了独立表空间(my.ini中配置innodb_file_per_table = 1)产生的存放该表的数据和索引的文件 **以恢复到本地电脑MySQL 5.6中为例** #### 解压要恢复的表.ibd文件 查看压缩文件中的内容 tar -tf 备份文件 解压指定库名表名的物理文件 tar -zxvf 备份文件 库名/表名.ibd #### 安装MySQL 5.6（MacOS） 安装 brew install mysql@5.6 启动 /usr/local/opt/mysql@5.6/bin/mysql.server start 登录 mysql -uroot #### 新建相同表结构 (这个时候会生成 frm ， ibd 文件) mysql> create database DB名（可随意）; mysql> use DB名 ; 获取源表的创建语句并创建表 mysql> create table 表名 #### 删除表空间 Alter table 表名 discard tablespace #### 复制 ibd 文件到MySQL 文件目录下 mv 表名.ibd /usr/local/var/mysql/库名 #### 导入表空间 alter table 表名 import tablespace ``` 导入表时如果报Error Code: 1808. Schema mismatch (Table has ROW_TYPE_DYNAMIC row format, .ibd file has ROW_TYPE_COMPACT row format.的错误，则在建表语句后加上ROW_FORMAT=COMPACT重新来一遍上面的操作。 登录本地数据库即可看到恢复的单个表的数据 总结 备份方法 备份速度 恢复速度 便捷性 功能 一般用于 cp 快 快 一般、灵活性低 很弱 少量数据备份 mysqldump 慢 慢 一般、可无视存储引擎的差异 一般 中小型数据量的备份 lvm2快照 快 快 一般、支持几乎热备、速度快 一般 中小型数据量的备份 xtrabackup 较快 较快 实现innodb热备、对存储引擎有要求 强大 较大规模的备份 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-14 13:35:11 "},"origin/mysql-dumper.html":{"url":"origin/mysql-dumper.html","title":"MysqlDump详解","keywords":"","body":"MysqlDump常用操作 1、备份所有数据库 $> mysqldump -uroot -proot --all-databases >/tmp/all.sql 2、备份db1、db2两个数据库的所有数据 $> mysqldump -uroot -proot --databases db1 db2 >/tmp/user.sql 3、备份db1中的a1、a2表 $> mysqldump -uroot -proot --databases db1 --tables a1 a2 >/tmp/db1.sql PS:导出指定表只能针对一个数据库进行导出，且导出的内容中和导出数据库也不一样，导出指定表的导出文本中没有创建数据库的判断语句，只有删除表-创建表-导入数据 4、只导出表结构不导出数据，--no-data $> mysqldump -uroot -proot --no-data --databases db1 >/tmp/db1.sql 5、跨服务区导出导入数据库。PS：远程数据库必须存在 #将h1服务器中的db1数据库的所有数据导入到h2中的db2数据库中 $> mysqldump --host=192.168.80.137 -uroot -proot -C --databases test |mysql --host=192.168.80.133 -uroot -proot test #PS：远程数据库必须存在 6、MysqlDump的参数说明 --all-databases , -A 导出全部数据库。 mysqldump -uroot -p --all-databases --all-tablespaces , -Y 导出全部表空间。 mysqldump -uroot -p --all-databases --all-tablespaces --no-tablespaces , -y 不导出任何表空间信息。 mysqldump -uroot -p --all-databases --no-tablespaces --add-drop-database 每个数据库创建之前添加drop数据库语句。 mysqldump -uroot -p --all-databases --add-drop-database --add-drop-table 每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用--skip-add-drop-table取消选项) mysqldump -uroot -p --all-databases (默认添加drop语句) mysqldump -uroot -p --all-databases –skip-add-drop-table (取消drop语句) --add-locks 在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用--skip-add-locks取消选项) mysqldump -uroot -p --all-databases (默认添加LOCK语句) mysqldump -uroot -p --all-databases –skip-add-locks (取消LOCK语句) --allow-keywords 允许创建是关键词的列名字。这由表名前缀于每个列名做到。 mysqldump -uroot -p --all-databases --allow-keywords --apply-slave-statements 在'CHANGE MASTER'前添加'STOP SLAVE'，并且在导出的最后添加'START SLAVE'。 mysqldump -uroot -p --all-databases --apply-slave-statements --character-sets-dir 字符集文件的目录 mysqldump -uroot -p --all-databases --character-sets-dir=/usr/local/mysql/share/mysql/charsets --comments 附加注释信息。默认为打开，可以用--skip-comments取消 mysqldump -uroot -p --all-databases (默认记录注释) mysqldump -uroot -p --all-databases --skip-comments (取消注释) --compatible 导出的数据将和其它数据库或旧版本的MySQL 相兼容。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options、no_field_options等， 要使用几个值，用逗号将它们隔开。它并不保证能完全兼容，而是尽量兼容。 mysqldump -uroot -p --all-databases --compatible=ansi --compact 导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：--skip-add-drop-table --skip-add-locks --skip-comments --skip-disable-keys mysqldump -uroot -p --all-databases --compact --complete-insert, -c 使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。 mysqldump -uroot -p --all-databases --complete-insert --compress, -C 在客户端和服务器之间启用压缩传递所有信息 mysqldump -uroot -p --all-databases --compress --create-options, -a 在CREATE TABLE语句中包括所有MySQL特性选项。(默认为打开状态) mysqldump -uroot -p --all-databases --databases, -B 导出几个数据库。参数后面所有名字参量都被看作数据库名。 mysqldump -uroot -p --databases test mysql --debug 输出debug信息，用于调试。默认值为：d:t,/tmp/mysqldump.trace mysqldump -uroot -p --all-databases --debug mysqldump -uroot -p --all-databases --debug=” d:t,/tmp/debug.trace” --debug-check 检查内存和打开文件使用说明并退出。 mysqldump -uroot -p --all-databases --debug-check --debug-info 输出调试信息并退出 mysqldump -uroot -p --all-databases --debug-info --default-character-set 设置默认字符集，默认值为utf8 mysqldump -uroot -p --all-databases --default-character-set=utf8 --delayed-insert 采用延时插入方式（INSERT DELAYED）导出数据 mysqldump -uroot -p --all-databases --delayed-insert --delete-master-logs master备份后删除日志. 这个参数将自动激活--master-data。 mysqldump -uroot -p --all-databases --delete-master-logs --disable-keys 对于每个表，用/*!40000 ALTER TABLE tbl_name DISABLE KEYS */;和/*!40000 ALTER TABLE tbl_name ENABLE KEYS */;语句引用INSERT语句。这样可以更快地导入dump出来的文件，因为它是在插入所有行后创建索引的。该选项只适合MyISAM表，默认为打开状态。 mysqldump -uroot -p --all-databases --dump-slave 该选项将主的binlog位置和文件名追加到导出数据的文件中(show slave status)。设置为1时，将会以CHANGE MASTER命令输出到数据文件；设置为2时，会在change前加上注释。该选项将会打开--lock-all-tables，除非--single-transaction被指定。该选项会自动关闭--lock-tables选项。默认值为0。 mysqldump -uroot -p --all-databases --dump-slave=1 mysqldump -uroot -p --all-databases --dump-slave=2 --master-data 该选项将当前服务器的binlog的位置和文件名追加到输出文件中(show master status)。如果为1，将会输出CHANGE MASTER 命令；如果为2，输出的CHANGE MASTER命令前添加注释信息。该选项将打开--lock-all-tables 选项，除非--single-transaction也被指定（在这种情况下，全局读锁在开始导出时获得很短的时间；其他内容参考下面的--single-transaction选项）。该选项自动关闭--lock-tables选项。 mysqldump -uroot -p --host=localhost --all-databases --master-data=1; mysqldump -uroot -p --host=localhost --all-databases --master-data=2; --events, -E 导出事件。 mysqldump -uroot -p --all-databases --events --extended-insert, -e 使用具有多个VALUES列的INSERT语法。这样使导出文件更小，并加速导入时的速度。默认为打开状态，使用--skip-extended-insert取消选项。 mysqldump -uroot -p --all-databases mysqldump -uroot -p --all-databases--skip-extended-insert (取消选项) --fields-terminated-by 导出文件中忽略给定字段。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p test test --tab=”/home/mysql” --fields-terminated-by=”#” --fields-enclosed-by 输出文件中的各个字段用给定字符包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p test test --tab=”/home/mysql” --fields-enclosed-by=”#” --fields-optionally-enclosed-by 输出文件中的各个字段用给定字符选择性包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p test test --tab=”/home/mysql” --fields-enclosed-by=”#” --fields-optionally-enclosed-by =”#” --fields-escaped-by 输出文件中的各个字段忽略给定字符。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p mysql user --tab=”/home/mysql” --fields-escaped-by=”#” --flush-logs 开始导出之前刷新日志。 请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。除使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs。 mysqldump -uroot -p --all-databases --flush-logs --flush-privileges 在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。 mysqldump -uroot -p --all-databases --flush-privileges --force 在导出过程中忽略出现的SQL错误。 mysqldump -uroot -p --all-databases --force --help 显示帮助信息并退出。 mysqldump --help --hex-blob 使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用该选项。影响到的字段类型有BINARY、VARBINARY、BLOB。 mysqldump -uroot -p --all-databases --hex-blob --host, -h 需要导出的主机信息 mysqldump -uroot -p --host=localhost --all-databases --ignore-table 不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：--ignore-table=database.table1 --ignore-table=database.table2 …… mysqldump -uroot -p --host=localhost --all-databases --ignore-table=mysql.user --include-master-host-port 在--dump-slave产生的'CHANGE MASTER TO..'语句中增加'MASTER_HOST=，MASTER_PORT=' mysqldump -uroot -p --host=localhost --all-databases --include-master-host-port --insert-ignore 在插入行时使用INSERT IGNORE语句. mysqldump -uroot -p --host=localhost --all-databases --insert-ignore --lines-terminated-by 输出文件的每行用给定字符串划分。与--tab选项一起使用，不能用于--databases和--all-databases选项。 mysqldump -uroot -p --host=localhost test test --tab=”/tmp/mysql” --lines-terminated-by=”##” --lock-all-tables, -x 提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭--single-transaction 和--lock-tables 选项。 mysqldump -uroot -p --host=localhost --all-databases --lock-all-tables --lock-tables, -l 开始导出前，锁定所有表。用READ LOCAL锁定表以允许MyISAM表并行插入。对于支持事务的表例如InnoDB和BDB，--single-transaction是一个更好的选择，因为它根本不需要锁定表。 请注意当导出多个数据库时，--lock-tables分别为每个数据库锁定表。因此，该选项不能保证导出文件中的表在数据库之间的逻辑一致性。不同数据库表的导出状态可以完全不同。 mysqldump -uroot -p --host=localhost --all-databases --lock-tables --log-error 附加警告和错误信息到给定文件 mysqldump -uroot -p --host=localhost --all-databases --log-error=/tmp/mysqldump_error_log.err --max_allowed_packet 服务器发送和接受的最大包长度。 mysqldump -uroot -p --host=localhost --all-databases --max_allowed_packet=10240 --net_buffer_length TCP/IP和socket连接的缓存大小。 mysqldump -uroot -p --host=localhost --all-databases --net_buffer_length=1024 --no-autocommit 使用autocommit/commit 语句包裹表。 mysqldump -uroot -p --host=localhost --all-databases --no-autocommit --no-create-db, -n 只导出数据，而不添加CREATE DATABASE 语句。 mysqldump -uroot -p --host=localhost --all-databases --no-create-db --no-create-info, -t 只导出数据，而不添加CREATE TABLE 语句。 mysqldump -uroot -p --host=localhost --all-databases --no-create-info --no-data, -d 不导出任何数据，只导出数据库表结构。 mysqldump -uroot -p --host=localhost --all-databases --no-data --no-set-names, -N 等同于--skip-set-charset mysqldump -uroot -p --host=localhost --all-databases --no-set-names --opt 等同于--add-drop-table, --add-locks, --create-options, --quick, --extended-insert, --lock-tables, --set-charset, --disable-keys 该选项默认开启, 可以用--skip-opt禁用. mysqldump -uroot -p --host=localhost --all-databases --opt --order-by-primary 如果存在主键，或者第一个唯一键，对每个表的记录进行排序。在导出MyISAM表到InnoDB表时有效，但会使得导出工作花费很长时间。 mysqldump -uroot -p --host=localhost --all-databases --order-by-primary --password, -p 连接数据库密码 --pipe(windows系统可用) 使用命名管道连接mysql mysqldump -uroot -p --host=localhost --all-databases --pipe --port, -P 连接数据库端口号 --protocol 使用的连接协议，包括：tcp, socket, pipe, memory. mysqldump -uroot -p --host=localhost --all-databases --protocol=tcp --quick, -q 不缓冲查询，直接导出到标准输出。默认为打开状态，使用--skip-quick取消该选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-quick --quote-names,-Q 使用（`）引起表和列名。默认为打开状态，使用--skip-quote-names取消该选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-quote-names --replace 使用REPLACE INTO 取代INSERT INTO. mysqldump -uroot -p --host=localhost --all-databases --replace --result-file, -r 直接输出到指定文件中。该选项应该用在使用回车换行对（\\\\r\\\\n）换行的系统上（例如：DOS，Windows）。该选项确保只有一行被使用。 mysqldump -uroot -p --host=localhost --all-databases --result-file=/tmp/mysqldump_result_file.txt --routines, -R 导出存储过程以及自定义函数。 mysqldump -uroot -p --host=localhost --all-databases --routines --set-charset 添加'SET NAMES default_character_set'到输出文件。默认为打开状态，使用--skip-set-charset关闭选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-set-charset --single-transaction 该选项在导出数据之前提交一个BEGIN SQL语句，BEGIN 不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎，仅InnoDB。本选项和--lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交。要想导出大表的话，应结合使用--quick 选项。 mysqldump -uroot -p --host=localhost --all-databases --single-transaction --dump-date 将导出时间添加到输出文件中。默认为打开状态，使用--skip-dump-date关闭选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-dump-date --skip-opt 禁用–opt选项. mysqldump -uroot -p --host=localhost --all-databases --skip-opt --socket,-S 指定连接mysql的socket文件位置，默认路径/tmp/mysql.sock mysqldump -uroot -p --host=localhost --all-databases --socket=/tmp/mysqld.sock --tab,-T 为每个表在给定路径创建tab分割的文本文件。注意：仅仅用于mysqldump和mysqld服务器运行在相同机器上。注意使用--tab不能指定--databases参数 mysqldump -uroot -p --host=localhost test test --tab=\"/home/mysql\" --tables 覆盖--databases (-B)参数，指定需要导出的表名，在后面的版本会使用table取代tables。 mysqldump -uroot -p --host=localhost --databases test --tables test --triggers 导出触发器。该选项默认启用，用--skip-triggers禁用它。 mysqldump -uroot -p --host=localhost --all-databases --triggers --tz-utc 在导出顶部设置时区TIME_ZONE='+00:00' ，以保证在不同时区导出的TIMESTAMP 数据或者数据被移动其他时区时的正确性。 mysqldump -uroot -p --host=localhost --all-databases --tz-utc --user, -u 指定连接的用户名。 --verbose, --v 输出多种平台信息。 --version, -V 输出mysqldump版本信息并退出 --where, -w 只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。 mysqldump -uroot -p --host=localhost --all-databases --where=” user=’root’” --xml, -X 导出XML格式. mysqldump -uroot -p --host=localhost --all-databases --xml --plugin_dir 客户端插件的目录，用于兼容不同的插件版本。 mysqldump -uroot -p --host=localhost --all-databases --plugin_dir=”/usr/local/lib/plugin” --default_auth 客户端插件默认使用权限。 mysqldump -uroot -p --host=localhost --all-databases --default-auth=”/usr/local/lib/plugin/” Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-14 19:21:05 "},"origin/mysql-procedure-grammar.html":{"url":"origin/mysql-procedure-grammar.html","title":"存储过程语法","keywords":"","body":"MySQL存储过程 一、存储过程简介 SQL语句需要先编译然后执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。 存储过程是数据库的一个重要的功能，MySQL 5.0以前并不支持存储过程，这使得MySQL在应用上大打折扣。好在MySQL 5.0开始支持存储过程，这样即可以大大提高数据库的处理速度，同时也可以提高数据库编程的灵活性。 存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。 存储过程的优点： 增强SQL语言的功能和灵活性：存储过程可以用控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 标准组件式编程：存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的SQL语句。而且数据库专业人员可以随时对存储过程进行修改，对应用程序源代码毫无影响。 较快的执行速度：如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。 减少网络流量：针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织进存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大减少网络流量并降低了网络负载。 作为一种安全机制来充分利用：通过对执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。 存储过程的定义信息保存在数据字典表information_schema.routines中 mysql中存储过程和函数的语法非常接近所以就放在一起，主要区别就是函数必须有返回值（return），并且函数的参数只有IN类型而存储过程有IN、OUT、INOUT这三种类型。 二、存储过程语法规则 CREATE [DEFINER = { user | CURRENT_USER }] PROCEDURE 过程名([[IN|OUT|INOUT] 参数名 数据类型[,[IN|OUT|INOUT] 参数名 数据类型…]]) [特性 ...] 过程体:(有效的SQL语句,记得末尾加;) DEFINER：用于指明存储过程是由哪个用户定义的，默认存储过程的定义者是存储过程，跟存储过程的使用权限无关 （如果在创建存储过程时指定为root@%，将有可能导致root@localhost在使用存储过程时出现权限问题。-- root@localhost是默认值） 形参： IN 输入参数:表示该参数的值必须在调用存储过程时指定，在存储过程中修改该参数的值不能被返回，为默认值 OUT 输出参数:该值可在存储过程内部被改变，并可返回 INOUT 输入输出参数:调用时指定，并且可被改变和返回 特性: COMMENT 'string' | LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } -- COMMENT：给存储过程添加注释信息 -- LANGUAGE：用来说明语句部分是SQL语句，未来可能会支持其它类型的语句。 -- SQL SECURITY { DEFINER | INVOKER }指明谁有权限来执行： DEFINER表示只有定义者自己才能够执行；INVOKER表示调用者可以执行。 -- [NOT] DETERMINISTIC： --如果程序或线程总是对同样的输入参数产生同样的结果，则被认为它是“确定的”，否则就是“非确定”的。如果既没有给定DETERMINISTIC也没有给定NOT DETERMINISTIC，默认的就是NOT DETERMINISTIC（非确定的） -- NO SQL：表示子程序不包含SQL语句。 -- READS SQL DATA：表示子程序包含读数据的语句，但不包含写数据的语句。 -- MODIFIES SQL DATA：表示子程序包含写数据的语句。 存储过程体包含了在过程调用时必须执行的语句，例如：dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等,使用DELIMITER $$ 命令将语句的结束符号从分号 ; 临时改为两个$$，使得过程体中使用的分号被直接传递到服务器，而不会被客户端（如mysql）解释 -- 简单过程体 begin dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等; end$$ -- 加标签的过程体（） [begin_label:] BEGIN 　 　dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等; END [end_label] --嵌套的过程体 （ ） BEGIN 　 　BEGIN 　 　　　BEGIN 　　　 　　　statements; 　　 　　END 　 　END END 变量作用域 内部的变量在其作用域范围内享有更高的优先权，当执行到end。变量时，内部变量消失，此时已经在其作用域外，变量不再可见了，应为在存储 过程外再也不能找到这个申明的变量，但是你可以通过out参数或者将其值指派 给会话变量来保存其值。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc3() -> begin -> declare x1 varchar(5) default 'outer'; -> begin -> declare x1 varchar(5) default 'inner'; -> select x1; -> end; -> select x1; -> end; -> // mysql > DELIMITER ; 条件语句 if-then -else语句 mysql > DELIMITER // mysql > CREATE PROCEDURE proc2(IN parameter int) -> begin -> declare var int; -> set var=parameter+1; -> if var=0 then -> insert into t values(17); -> end if; -> if parameter=0 then -> update t set s1=s1+1; -> else -> update t set s1=s1+2; -> end if; -> end; -> // mysql > DELIMITER ; case语句： mysql > DELIMITER // mysql > CREATE PROCEDURE proc3 (in parameter int) -> begin -> declare var int; -> set var=parameter+1; -> case var -> when 0 then -> insert into t values(17); -> when 1 then -> insert into t values(18); -> else -> insert into t values(19); -> end case; -> end; -> // mysql > DELIMITER ; case when var=0 then insert into t values(30); when var>0 then when var循环语句 while ···· end while mysql > DELIMITER // mysql > CREATE PROCEDURE proc4() -> begin -> declare var int; -> set var=0; -> while var insert into t values(var); -> set var=var+1; -> end while; -> end; -> // mysql > DELIMITER ; while条件 do --循环体 endwhile repeat···· end repeat 它在执行操作后检查结果，而while则是执行前进行检查。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc5 () -> begin -> declare v int; -> set v=0; -> repeat -> insert into t values(v); -> set v=v+1; -> until v>=5 -> end repeat; -> end; -> // mysql > DELIMITER ; repeat --循环体 until循环条件 endrepeat; loop ·····endloop loop循环不需要初始条件，这点和while 循环相似，同时和repeat循环一样不需要结束条件, leave语句的意义是离开循环。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc6 () -> begin -> declare v int; -> set v=0; -> LOOP_LABLE:loop -> insert into t values(v); -> set v=v+1; -> if v >=5 then -> leave LOOP_LABLE; -> end if; -> end loop; -> end; -> // mysql > DELIMITER ; LABLES 标号 标号可以用在begin repeat while 或者loop 语句前，语句标号只能在合法的语句前面使用。可以跳出循环，使运行指令达到复合语句的最后一步。 ITERATE 迭代 通过引用复合语句的标号,来从新开始复合语句 mysql > DELIMITER // mysql > CREATE PROCEDURE proc10 () -> begin -> declare v int; -> set v=0; -> LOOP_LABLE:loop -> if v=3 then -> set v=v+1; -> ITERATE LOOP_LABLE; -> end if; -> insert into t values(v); -> set v=v+1; -> if v>=5 then -> leave LOOP_LABLE; -> end if; -> end loop; -> end; -> // mysql > DELIMITER ; 三、存储过程的基本函数 字符串类 CHARSET(str) //返回字串字符集 CONCAT (string2 [,... ]) //连接字串 INSTR (string ,substring ) //返回substring首次在string中出现的位置,不存在返回0 LCASE (string2 ) //转换成小写 LEFT (string2 ,length ) //从string2中的左边起取length个字符 LENGTH (string ) //string长度 LOAD_FILE (file_name ) //从文件读取内容 LOCATE (substring , string [,start_position ] ) 同INSTR,但可指定开始位置 LPAD (string2 ,length ,pad ) //重复用pad加在string开头,直到字串长度为length LTRIM (string2 ) //去除前端空格 REPEAT (string2 ,count ) //重复count次 REPLACE (str ,search_str ,replace_str ) //在str中用replace_str替换search_str RPAD (string2 ,length ,pad) //在str后用pad补充,直到长度为length RTRIM (string2 ) //去除后端空格 STRCMP (string1 ,string2 ) //逐字符比较两字串大小, SUBSTRING (str , position [,length ]) //从str的position开始,取length个字符, 注：mysql中处理字符串时，默认第一个字符下标为1，即参数position必须大于等于1 1. mysql> select substring('abcd',0,2); 2. +-----------------------+ 3. | substring('abcd',0,2) | 4. +-----------------------+ 5. | | 6. +-----------------------+ 7. 1 row in set (0.00 sec) 8. 9. mysql> select substring('abcd',1,2); 10.+-----------------------+ 11.| substring('abcd',1,2) | 12.+-----------------------+ 13.| ab | 14.+-----------------------+ 15.1 row in set (0.02 sec) TRIM([[BOTH|LEADING|TRAILING][padding] FROM]string2) //去除指定位置的指定字符 UCASE (string2 ) //转换成大写 RIGHT(string2,length) //取string2最后length个字符 SPACE(count) //生成count个空格 数学类 ABS (number2 ) //绝对值 BIN (decimal_number ) //十进制转二进制 CEILING (number2 ) //向上取整 CONV(number2,from_base,to_base) //进制转换 FLOOR (number2 ) //向下取整 FORMAT (number,decimal_places ) //保留小数位数 HEX (DecimalNumber ) //转十六进制 注：HEX()中可传入字符串，则返回其ASC-11码，如HEX('DEF')返回4142143 也可以传入十进制整数，返回其十六进制编码，如HEX(25)返回19 LEAST (number , number2 [,..]) //求最小值 MOD (numerator ,denominator ) //求余 POWER (number ,power ) //求指数 RAND([seed]) //随机数 ROUND (number [,decimals ]) //四舍五入,decimals为小数位数] 注：返回类型并非均为整数，如： (1)默认变为整形值 1. mysql> select round(1.23); 2. +-------------+ 3. | round(1.23) | 4. +-------------+ 5. | 1 | 6. +-------------+ 7. 1 row in set (0.00 sec) 8. 9. mysql> select round(1.56); 10.+-------------+ 11.| round(1.56) | 12.+-------------+ 13.| 2 | 14.+-------------+ 15.1 row in set (0.00 sec) (2)可以设定小数位数，返回浮点型数据 1. mysql> select round(1.567,2); 2. +----------------+ 3. | round(1.567,2) | 4. +----------------+ 5. | 1.57 | 6. +----------------+ 7. 1 row in set (0.00 sec) SIGN (number2 ) // 日期时间类 ADDTIME (date2 ,time_interval )//将time_interval加到date2 CONVERT_TZ (datetime2 ,fromTZ ,toTZ ) //转换时区 CURRENT_DATE ( ) //当前日期 CURRENT_TIME ( ) //当前时间 CURRENT_TIMESTAMP ( ) //当前时间戳 DATE (datetime ) //返回datetime的日期部分 DATE_ADD (date2 , INTERVAL d_value d_type ) //在date2中加上日期或时间 DATE_FORMAT (datetime ,FormatCodes ) //使用formatcodes格式显示datetime DATE_SUB (date2 , INTERVAL d_value d_type ) //在date2上减去一个时间 DATEDIFF (date1 ,date2 ) //两个日期差 DAY (date ) //返回日期的天 DAYNAME (date ) //英文星期 DAYOFWEEK (date ) //星期(1-7) ,1为星期天 DAYOFYEAR (date ) //一年中的第几天 EXTRACT (interval_name FROM date ) //从date中提取日期的指定部分 MAKEDATE (year ,day ) //给出年及年中的第几天,生成日期串 MAKETIME (hour ,minute ,second ) //生成时间串 MONTHNAME (date ) //英文月份名 NOW ( ) //当前时间 SEC_TO_TIME (seconds ) //秒数转成时间 STR_TO_DATE (string ,format ) //字串转成时间,以format格式显示 TIMEDIFF (datetime1 ,datetime2 ) //两个时间差 TIME_TO_SEC (time ) //时间转秒数] WEEK (date_time [,start_of_week ]) //第几周 YEAR (datetime ) //年份 DAYOFMONTH(datetime) //月的第几天 HOUR(datetime) //小时 LAST_DAY(date) //date的月的最后日期 MICROSECOND(datetime) //微秒 MONTH(datetime) //月 MINUTE(datetime) //分返回符号,正负或0 SQRT(number2) //开平方 MySql分页存储过程 MySql测试版本：5.0.41-community-nt DROP PROCEDURE IF EXISTS pr_pager; CREATE PROCEDURE pr_pager( IN p_table_name VARCHAR(1024), IN p_fields VARCHAR(1024), IN p_page_size INT, IN p_page_now INT, IN p_order_string VARCHAR(128), IN p_where_string VARCHAR(1024), OUT p_out_rows INT ) NOT DETERMINISTIC SQL SECURITY DEFINER COMMENT '分页存储过程' BEGIN DECLARE m_begin_row INT DEFAULT 0; DECLARE m_limit_string CHAR(64); SET m_begin_row = (p_page_now - 1) * p_page_size; SET m_limit_string = CONCAT(' LIMIT ', m_begin_row, ', ', p_page_size); SET @COUNT_STRING = CONCAT('SELECT COUNT(*) INTO @ROWS_TOTAL FROM ', p_table_name, ' ', p_where_string); SET @MAIN_STRING = CONCAT('SELECT ', p_fields, ' FROM ', p_table_name, ' ', p_where_string, ' ', p_order_string,m_limit_string); PREPARE count_stmt FROM @COUNT_STRING; EXECUTE count_stmt; DEALLOCATE PREPARE count_stmt; SET p_out_rows = @ROWS_TOTAL; PREPARE main_stmt FROM @MAIN_STRING; EXECUTE main_stmt; DEALLOCATE PREPARE main_stmt; END; 调用 mysql> call pr_pager(\"t\",\"var\",3,3,\"\",\"\",@result); mysql> call pr_pager(\"t\",\"var\",3,2,\"\",\"\",@result); 四、存储过程操作 1、查看存储过程 #查询存储过程 SELECT name FROM mysql.proc WHERE db='数据库名' and `type` = 'PROCEDURE' \\G; SELECT routine_name FROM information_schema.routines WHERE routine_schema='数据库名'; SHOW PROCEDURE STATUS WHERE db='数据库名'; #查看存储过程详细信息 SHOW CREATE PROCEDURE 数据库.存储过程名; 2、调用存储过程 call 数据库.存储过程名; 3、删除存储过程 DROP PROCEDURE [IF EXISTS] db_name.sp_name; #如果存储过程或存储函数不存在时，仍然进行删除，可以使用IF EXISTS子句，它可以防止发生错误，产生一个用SHOW WARNINGS查看的警告。 4、修改存储过程 -- 只能改变存储过程的特征，不能修改过程的参数以及过程体。如果想做这样的修改，必须先使用DROP PROCEDURE 删除过程，然后使用and CREATE PROCEDURE重建过程。 ALTER {PROCEDURE | FUNCTION} sp_name [特征 ...] Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/mysql-procedure.html":{"url":"origin/mysql-procedure.html","title":"常用存储过程","keywords":"","body":"常用存储过程 1、修改Database中所有表的所有字段的编码格式（mysql） delimiter $$ CREATE PROCEDURE Test () begin DECLARE cnt VARCHAR(100); -- 声明变量用来记录查询出的表名 DECLARE i int; -- 循环条件，同时可以用来标记表第几张表 set i = 0; -- 循环开始 while i 参考：https://blog.csdn.net/LUNG108/article/details/78285054 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-12 10:22:05 "},"origin/mysql-transaction.html":{"url":"origin/mysql-transaction.html","title":"事物隔离级别","keywords":"","body":"MySQL的事务隔离级别 一、首先什么是事务？ 事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。也就是事务具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。 事务的结束有两种，当事务中的所以步骤全部成功执行时，事务提交。如果其中一个步骤失败，将发生回滚操作，撤消撤消之前到事务开始时的所以操作。 二、事务的 ACID 事务具有四个特征：原子性（ Atomicity ）、一致性（ Consistency ）、隔离性（ Isolation ）和持续性（ Durability ）。这四个特性简称为 ACID 特性。 1 、原子性。事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做 2 、一致性。事 务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统 运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是 不一致的状态。 3 、隔离性。一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 4 、持续性。也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。 三、Mysql的四种隔离级别 SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。 Repeatable Read（可重读） 这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如： 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 ​ 四、MySQL设置隔离级别 系统默认事务级别为：repeatable-read。 查看事务隔离级别： SELECT @@tx_isolation 1、 服务器启动时设置级别。 在MySQL配置文件/etc/my.cnf中设置，然后重启MySQL. [mysqld] transaction-isolation = [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ] 2、使用SET TRANSACTION ISOLATION LEVEL语句为正在运行的服务器设置。 # 设置全局级别默认事务隔离级别，适用于从设置时起所有新建立的客户机连接。现有连接不受影响。 SET GLOBAL TRANSACTION ISOLATION LEVEL [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ] ; #设置会话级别默认事务隔离级别，如果没有显式指定，则事务隔离级别将按会话进行设置，应用于当前session内之后的所有事务。 SET SESSION TRANSACTION ISOLATION LEVEL [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ]; #适用于当前session内的下一个还未开始的事务 SET TRANSACTION ISOLATION LEVEL [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ]; Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/mysql-temporary.html":{"url":"origin/mysql-temporary.html","title":"临时表","keywords":"","body":"MySQL临时表 一、临时表的种类 全局临时表 这种临时表从数据库实例启动后开始生效，在数据库实例销毁后失效。在MySQL里面这种临时表对应的是内存表，即memory引擎。 会话级别临时表 这种临时表在用户登录系统成功后生效，在用户退出时失效。在MySQL里的临时表指的就是以create temporary table这样的关键词创建的表。 事务级别临时表 这种临时表在事务开始时生效，事务提交或者回滚后失效。 在MySQL里面没有这种临时表，必须利用会话级别的临时表间接实现。 检索级别临时表 这种临时表在SQL语句执行之间产生，执行完毕后失效。 在MySQL里面这种临时表不是很固定，跟随MySQL默认存储引擎来变化。 比如默认存储引擎是MyISAM，临时表的引擎就是MyISAM，并且文件生成形式以及数据运作形式和MyISAM一样，只是数据保存在内存里；如果默认引擎是INNODB，那么临时表的引擎就是INNODB，此时它的所有信息都保存在共享表空间ibdata里面。 二、MySQL 5.7的临时表空间优化 MySQL 5.7 把临时表的数据以及回滚信息（仅限于未压缩表）从共享表空间里面剥离出来，形成自己单独的表空间，参数为innodb_temp_data_file_path。 在MySQL 5.7 中把临时表的相关检索信息保存在系统信息表中：information_schema.innodb_temp_table_info. 而MySQL 5.7之前的版本想要查看临时表的系统信息是没有太好的办法。 注意： 虽然INNODB临时表有自己的表空间，但是目前还不能自己定义临时表空间文件的保存路径，只能是继承innodb_data_home_dir。此时如果想要拿其他的磁盘，比如内存盘来充当临时表空间的保存地址，只能用老办法，做软链。 三、临时表使用建议 设置 innodb_temp_data_file_path 选项，设定文件最大上限，超过上限时，需要生成临时表的SQL无法被执行（一般这种SQL效率也比较低，可借此机会进行优化）。 检查 INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO，找到最大的临时表对应的线程，kill之即可释放，但 ibtmp1 文件则不能释放（除非重启）。 择机重启实例，释放ibtmp1文件，和ibdata1不同，ibtmp1重启时会被重新初始化而 ibdata1 则不可以。 定期检查运行时长超过N秒（比如N=300）的SQL，考虑干掉，避免垃圾SQL长时间运行影响业务。 四、临时表的创建 mysql> create temporary table temp1(sid int,sname varchar(10)); Query OK, 0 rows affected (0.00 sec) mysql> insert into temp1 values(1,'aaa'); Query OK, 1 row affected (0.00 sec) mysql> select * from temp1; +------+-------+ | sid | sname | +------+-------+ | 1 | aaa | +------+-------+ 1 row in set (0.00 sec) mysql> show tables; +-----------------+ | Tables_in_test1 | +-----------------+ | app01 | | app02 | | app03 | +-----------------+ 3 rows in set (0.00 sec) 另起一个会话： mysql> use test1; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +-----------------+ | Tables_in_test1 | +-----------------+ | app01 | | app02 | | app03 | +-----------------+ 3 rows in set (0.00 sec) mysql> select * from temp1; ERROR 1146 (42S02): Table 'test1.temp1' doesn't exist 退出本次会话： mysql> use test1; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> select * from temp1; ERROR 1146 (42S02): Table 'test1.temp1' doesn't exist Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/redis-basic.html":{"url":"origin/redis-basic.html","title":"基础概念","keywords":"","body":"Redis基础概念 一、简介 Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 Redis是一个使用ANSI C编写的开源、包含多种数据结构、支持网络、基于内存、可选持久性的键值对存储数据库，其具备如下特性： 基于内存运行，性能高效 支持分布式，理论上可以无限扩展 key-value存储系统 开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API 相比于其他数据库类型，Redis具备的特点是： C/S通讯模型 单进程单线程模型 丰富的数据类型 操作具有原子性 持久化 高并发读写 支持lua脚本 官网：https://redis.io/ 中文官网：http://www.redis.cn/ 教程：https://www.runoob.com/redis/redis-intro.html 二、基础概念 1、数据结构 Redis支持五种数据类型： string（字符串） 二进制安全的字符串，意味着它不仅能够存储字符串、还能存储图片、视频等多种类型, 最大长度支持512M。操作命令如下： GET/MGET SET/SETEX/MSET/MSETNX INCR/DECR GETSET DEL hash（哈希） 该类型是由field和关联的value组成的map。其中，field和value都是字符串类型的。Hash的操作命令如下： HGET/HMGET/HGETALL HSET/HMSET/HSETNX HEXISTS/HLEN HKEYS/HDEL HVALS list（列表） 该类型是一个插入顺序排序的字符串元素集合, 基于双链表实现。List的操作命令如下： LPUSH/LPUSHX/LPOP/RPUSH/RPUSHX/RPOP/LINSERT/LSET LINDEX/LRANGE LLEN/LTRIM set（集合） Set类型是一种无顺序集合, 它和List类型最大的区别是：集合中的元素没有顺序, 且元素是唯一的。Set类型的底层是通过哈希表实现的，其操作命令为： SADD/SPOP/SMOVE/SCARD SINTER/SDIFF/SDIFFSTORE/SUNION zset(sorted set：有序集合) ZSet是一种有序集合类型，每个元素都会关联一个double类型的分数权值，通过这个权值来为集合中的成员进行从小到大的排序。与Set类型一样，其底层也是通过哈希表实现的。 ZSet命令： ZADD/ZPOP/ZMOVE/ZCARD/ZCOUNT ZINTER/ZDIFF/ZDIFFSTORE/ZUNION 2、DB 在 Redis 下，默认有16个数据库，数据库是由一个整数索引标识（就是说数据库名是 0-15），而不是由一个数据库名称。默认情况下，一个客户端连接到数据库 0。 三、Redis 高可用 在 Redis 中，实现 高可用 的技术主要包括 持久化、复制、哨兵 和 集群： 持久化：持久化是 最简单的 高可用方法。它的主要作用是 数据备份，即将数据存储在 硬盘，保证数据不会因进程退出而丢失。 复制：复制是高可用 Redis 的基础，哨兵 和 集群 都是在 复制基础 上实现高可用的。复制主要实现了数据的多机备份以及对于读操作的负载均衡和简单的故障恢复。缺陷是故障恢复无法自动化、写操作无法负载均衡、存储能力受到单机的限制。 哨兵：在复制的基础上，哨兵实现了 自动化 的 故障恢复。缺陷是 写操作 无法 负载均衡，存储能力 受到 单机 的限制。 集群：通过集群，解决 写操作 无法 负载均衡 以及 存储能力 受到 单机限制 的问题，实现了较为 完善 的 高可用方案。 1、主从模式 Redis 主从复制 可将 主节点 数据同步给 从节点，从节点此时有两个作用： 一旦 主节点宕机，从节点 作为 主节点 的 备份 可以随时顶上来。 扩展 主节点 的 读能力，分担主节点读压力。 主从复制 同时存在以下几个问题： 一旦 主节点宕机，从节点 晋升成 主节点，同时需要修改 应用方 的 主节点地址，还需要命令所有 从节点 去 复制 新的主节点，整个过程需要 人工干预。 主节点 的 写能力 受到 单机的限制。 主节点 的 存储能力 受到 单机的限制。 原生复制 的弊端在早期的版本中也会比较突出，比如：Redis 复制中断 后，从节点 会发起 psync。此时如果 同步不成功，则会进行 全量同步，主库 执行 全量备份 的同时，可能会造成毫秒或秒级的 卡顿。 2、主从哨兵模式Sentinel Sentinel 的主要功能包括 主节点存活检测、主从运行情况检测、自动故障转移 （failover）、主从切换。Redis 的 Sentinel 最小配置是 一主一从。 Redis 的 Sentinel 系统可以用来管理多个 Redis 服务器，该系统可以执行以下四个任务： 监控：Sentinel 会不断的检查 主服务器 和 从服务器 是否正常运行。 通知：当被监控的某个 Redis 服务器出现问题，Sentinel 通过 API 脚本 向 管理员 或者其他的 应用程序 发送通知。 自动故障转移：当 主节点 不能正常工作时，Sentinel 会开始一次 自动的 故障转移操作，它会将与 失效主节点 是 主从关系 的其中一个 从节点 升级为新的 主节点，并且将其他的 从节点 指向 新的主节点。 配置提供者：在 Redis Sentinel 模式下，客户端应用 在初始化时连接的是 Sentinel 节点集合，从中获取 主节点 的信息。 Redis Sentinel的工作原理 每个 Sentinel 以 每秒钟 一次的频率，向它所知的 主服务器、从服务器 以及其他 Sentinel 实例 发送一个 PING 命令。 如果一个 实例（instance）距离 最后一次 有效回复 PING 命令的时间超过 down-after-milliseconds 所指定的值，那么这个实例会被 Sentinel 标记为 主观下线。 如果一个 主服务器 被标记为 主观下线，那么正在 监视 这个 主服务器 的所有 Sentinel 节点，要以 每秒一次 的频率确认 主服务器 的确进入了 主观下线 状态。 如果一个 主服务器 被标记为 主观下线，并且有 足够数量 的 Sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围 内同意这一判断，那么这个 主服务器 被标记为 客观下线。 当一个 主服务器 被 Sentinel 标记为 客观下线 时，Sentinel 向 下线主服务器 的所有 从服务器 发送 INFO 命令的频率，会从 10 秒一次改为 每秒一次。 Sentinel 和其他 Sentinel 协商 主节点 的状态，如果 主节点 处于 SDOWN 状态，则投票自动选出新的 主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。 当没有足够数量的 Sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 Sentinel 的 PING 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。 注意：一个有效的 PING 回复可以是：+PONG、-LOADING 或者 -MASTERDOWN。如果 服务器 返回除以上三种回复之外的其他回复，又或者在 指定时间 内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复 无效（non-valid） 哨兵模式下客户端的连接过程 1、遍历哨兵集合获取到一个可用的哨兵节点。因为哨兵节点之间是共享数据的，任意节点都可以获取到主节点的信息。 2、通过 sentinel get-master-addr-by-name master-nameAPI 来获取对应主节点的信息。 3、验证获取到的主节点是不是真正的主节点，防止故障转移期间主节点的变化。 4、保持和哨兵节点集合的联系，时刻获取关于主节点的相关信息。 3、集群Cluster 参考： https://juejin.cn/post/6844903663362637832 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-27 18:30:18 "},"origin/reids-install-deploy.html":{"url":"origin/reids-install-deploy.html","title":"安装部署","keywords":"","body":"Redis的安装部署及客户端 一、Redis单节点的安装部署 1、Docker DockerHub镜像地址：https://hub.docker.com/_/redis/ docker run -d -p 6379:6379 --name redis redis:6.0.9-alpine reids镜像相关信息： 环境变量配置：官方镜像不支持 持久化目录： /data 额外配置文件挂载：-v /myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf 镜像定制： FROM redis:6.0.9-alpine COPY redis.conf /usr/local/etc/redis/redis.conf CMD [ \"redis-server\", \"/usr/local/etc/redis/redis.conf\" ] 2、Kubernetes helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update 3、包管理器安装 Brew apt yum 4、源码安装 redis要求gcc版本高于5.3，CentOS7.4默认版本4.8.5，所以先升级gcc redis要求tcl版本高于8.5，yum install -y tcl yum -y install centos-release-scl yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils scl enable devtoolset-9 bash echo \"source /opt/rh/devtoolset-9/enable\" >> /etc/profile gcc -v source /etc/profile version=6.0.9 wget https://download.redis.io/releases/redis-$version.tar.gz tar xzf redis-$version.tar.gz cd redis-$version make make install # 二进制文件存放在src目录下,例如 source /etc/profile nohup redis-server --protected-mode no >> /var/log/redis-server.log 2>&1 & 二、Redis Cluster的安装安装部署 三、Redis客户端 1、CLI 2、Application Another Redis Desktop Manager Github：https://github.com/qishibo/AnotherRedisDesktopManager Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-18 19:38:18 "},"origin/redis-backup-restore.html":{"url":"origin/redis-backup-restore.html","title":"数据迁移备份恢复","keywords":"","body":"Redis数据的备份恢复迁移 一、Redis-dump Redis-dump Github：https://github.com/delano/redis-dump 1、安装 MacOS brew install ruby gem sources --add https://mirrors.aliyun.com/rubygems/ gem sources --remove https://rubygems.org/ gem sources --list gem install redis-dump -V CentOS yum inatll -y ruby gem sources --add https://mirrors.aliyun.com/rubygems/ gem sources --remove https://rubygems.org/ gem sources --list # gem安装redis需要ruby版本高于2.3.0，CentOS7默认安装的ruby版本为2.0.0，所以先升级Ruby gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB curl -sSL https://get.rvm.io | bash -s stable source /etc/profile.d/rvm.sh rvm -v rvm list known rvm install 2.5 ruby -V gem install redis-dump -V source /etc/profile redis-dump -V 2、redis-dump导出数据到JSON Usage: redis-dump [global options] COMMAND [command options] -u, --uri=S Redis URI (e.g. redis://hostname[:port]) -d, --database=S Redis database (e.g. -d 15) -a, --password=S Redis password (e.g. -a 'my@pass/word') -s, --sleep=S Sleep for S seconds after dumping (for debugging) -c, --count=S Chunk size (default: 10000) -f, --filter=S Filter selected keys (passed directly to redis' KEYS command) -b, --base64 Encode key values as base64 (useful for binary values) -O, --without_optimizations Disable run time optimizations -V, --version Display version -D, --debug --nosafe 示例 redis-dump -u redis://127.0.0.1:6379 -d 0 -c 50000 > redis-backup-$(date \"+%Y%m%d-%H%M%S\").json 3、redis-load导入JSON数据文件到Redis redis-load [global options] COMMAND [command options] -u, --uri=S Redis URI (e.g. redis://hostname[:port]) -d, --database=S Redis database (e.g. -d 15) -a, --password=S Redis password (e.g. -a 'my@pass/word') -s, --sleep=S Sleep for S seconds after dumping (for debugging) -b, --base64 Decode key values from base64 (used with redis-dump -b) -n, --no_check_utf8 -V, --version Display version -D, --debug --nosafe 示例 cat redis-backup.json| redis-load -u redis://127.0.0.1:6379 -d 0 注意 相同的Key，值会被覆盖 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-20 22:28:22 "},"origin/reids-common-opreations.html":{"url":"origin/reids-common-opreations.html","title":"常用操作","keywords":"","body":"Redis常用操作 一、全局命令 0、配置操作 ①添加设置 config set parameter value ②获取设置 config get parameter ③重置INFO命令统计的一些计算器 config resetstat # 被重置的数据如下: # Keyspace hits # Keyspace misses # Number of commands processed # Number of connections received # Number of expired keys 1、DB库操作 ①切换库 select db # 16个DB库，0-15.默认0库 2、客户端操作 ①查询客户端连接信息 client list ②剔除杀掉指定客户端 client kill ip:port ③返回当前连接名 client getname # 返回当前连接由CLIENT SETNAME设置的名字。如果没有用CLIENT SETNAME设置名字，将返回一个空的回复。 ④返回当前连接的ID client id # 每个ID符合如下约束： # 1.永不重复。当调用命令CLIENT ID返回相同的值时，调用者可以确认原连接未被断开，只是被重用 ，因此仍可以认为是同一连接 # 2.ID值单调递增。若某一连接的ID值比其他连接的ID值大，可以确认该连接是较新创建的 3、全局数据操作 ①清除数据 # 清除DB中的所有数据 flushdb # 清除所有库中的数据 flushall 4、监控统计操作 ①实时显示接受到的命令 monitor ②显示Redis服务器的各种信息和统计数值 info [section] # section: # 1.server: Redis服务器的一般信息 # 2.clients: 客户端的连接部分 # 3.memory: 内存消耗相关信息 # 4.persistence: RDB和AOF相关信息 # 5.stats: 一般统计 # 6.replication: 主/从复制信息 # 7.cpu: 统计CPU的消耗 # 8.commandstats: Redis命令统计 # 9.cluster: Redis集群信息 # 10.keyspace: 数据库的相关统计 # all: 返回所有信息 # default: 值返回默认设置的信息 # 如果没有使用任何参数时，默认为default。 ③查询键值在内存中占用的字节数 memory usage key # 返回的结果是key的值以及为管理该key分配的内存总字节数。对于嵌套数据类型，可以使用选项SAMPLES，其中COUNT表示抽样的元素个数，默认值为5。当需要抽样所有元素时，使用SAMPLES 0 # 如上，实际数据为空，但是存储时仍然耗费了一些内存，这些内存用于Redis 服务器维护内部数据结构。随着key和value的增大，内存使用量和key 大小基本成 线性关系。 ④获取最后一次同步磁盘的时间 lastsave # 执行成功时返回UNIX时间戳 5、慢日志操作 Redis慢查询日志是一个记录超过指定执行时间的查询的系统。 这里的执行时间不包括IO操作，比如与客户端通信，发送回复等等，而只是实际执行命令所需的时间（这是唯一在命令执行过程中线程被阻塞且不能同时处理其他请求的阶段）。 慢查询日志在内存中堆积，因此不会写入一个包含慢速命令执行信息的文件。 这使得慢查询日志非常快，你可以开启所有命令的日志记录（设置slowlog-log-slower-than参数值为零），但性能较低。 ①获取慢日志 slowlog get [条数] 每一个条目由四个字段组成： 每个慢查询条目的唯一的递增标识符。 处理记录命令的unix时间戳。 命令执行所需的总时间，以微秒为单位。 组成该命令的参数的数组。 条目的唯一ID可以用于避免慢查询条目被多次处理（例如，你也许有一个脚本使用每个新的慢查询日志条目给你发送报警邮件）。 条目ID在Redis服务器运行期间绝不会被重置，仅在Redis服务重启才重置它。 ②设置慢查询日志 config set _slowlog-log-slower-than_ milliseconds # Redis命令的执行时间超过多少微秒将会被记录。 请注意， # 1.负数将会关闭慢查询日志 # 2.0将强制记录每一个命令 config set _slowlog-max-len_ 慢日志条目最大条数 # 设置慢查询日志的长度。 最小值是0。 当一个新命令被记录，且慢查询日志已经达到其最大长度时，将从记录命令的队列中移除删除最旧的命令以腾出空间。 ③获取慢查询日志的最新条目ID slowlog len ④重置慢查询日志 slowlog reset # 删除后，信息将永远丢失 二、Key操作 查找key 查找所有符合给定模式pattern（正则表达式）的 key key # 支持的正则表达模式： # h?llo 匹配 hello, hallo 和 hxllo # h*llo 匹配 hllo 和 heeeello # h[ae]llo 匹配 hello 和 hallo, 但是不匹配 hillo # h[^e]llo 匹配 hallo, hbllo, … 但是不匹配 hello # h[a-b]llo 匹配 hallo 和 hbllo 查询key的值 get key # 不存在则返回nil 批量获取key的值 mget key [key .....] # mget 1 2 3 查询当前DB的key总数 dbsize 检查key是否存在 exist key # 存在返回1，不存在返回0 查询key的过期时间 ttl key # 查询键在多少秒后过期 （>0 剩余过期时间；-1 没有设置过期时间；-2 键不存在） pttl key # 查询键在多少毫秒后过期 查询key的数据类型 type key （如果键不存在，则返回none） 删除Key del key 统计Key scan cursor match 正则表达式 三、String类型键的操作命令 1、DDL ①设置键值 set key value [ex] [px] [nx|xx] # ex 秒级过期时间 # px 毫秒级过期时间 ②设置键值并返回原值 getset key value ③批量设置键值 mset key value [key value] # mset 1 a 2 b 3 c ④设置键的过期时间 expire key senconds/milliseconds # 设置key在second秒/milliseconds毫秒后过期 expire key timestamp/milliseconds-timestamp # 设置key在秒级时间戳/毫秒时间戳戳后过期 ⑤重命名键 rename key newkey ⑥追加内容到String类型键的末尾 append key value 2、DQL ①查询string类型键的长度 strlen key ②查询String类型键指定长度的值 getrange key start end # start和end指从0开始的开始与结束偏移量 四、Hash操作命令 1、DDL ①创建hash字段 hset key field value ②批量创建hash字段 hmset key field value [field value ...] ③删除hash中一个或多个field hdel key field [key field ....] 2、DQL ①获取字段的值 hset get key field ②批量获取字段的值 hmget key field [field ...] ③获取所有字段的数量 hlen key ④获取所有的字段 hkeys key ⑤获取指定字段的长度 hstrlen key field ⑥获取所有的字段值 hvals key ⑦判断是否存在field hexists key field ⑧读取所有的field与值 hgetall key 五、List操作命令 1、DDL ①将一个或多个的值插入列表的头部 lpush key value1 [value2 ...] # 如果 key 不存在，那么在进行 push 操作前会创建一个空列表。 # 元素是从最左端的到最右端的、一个接一个被插入到 list 的头部。 # redis版本>= 2.4才可以接受多个 value 参数。 ② 将一个或多个的值插入列表的尾部 rpush key value1 [value2 ...] # 如果 key 不存在，那么会创建一个空的列表然后再进行 push 操作 # 元素是从左到右一个接一个从列表尾部插入 # 返回值是在 push 操作后的列表长度。 rpushx key value1 [value2 ...] # 只有当 key 已经存在并且存着一个 list 的时候，在这个 key 下面的 list 的尾部部插入 value。 ③插入已有列表某个元素前面 linsert key before value new_value ④弹出列表第一个元素并返回元素的值 lpop key # 返回第一个元素的值，或者当 key 不存在时返回 nil。 ⑤弹出列表最后一个元素并返回元素的值 rpop key # 当 key 不存在的时候返回 nil。 ⑥弹出列表中的最后一个元素，并将其追加到另外一个列表的头部 rpoplpush list1 list2 # 如果 list2 不存在，那么会返回 nil 值，并且不会执行任何操作。 如果 list1 和 list2 是同样的，那么这个操作等同于移除列表最后一个元素并且把该元素放在列表头部， 所以这个命令也可以当作是一个旋转列表的命令 ⑦从列表中删除指定个数的重复元素 lrem key count value # 从存于 key 的列表里移除前 count 次出现的值为 value 的元素。 这个 count 参数通过下面几种方式影响这个操作： # count > 0: 从头往尾移除值为 value 的元素。 # count ⑧设置指定索引位置元素的值 lset key index value # 当index超出范围时会返回一个error。 2、DQL ①根据索引获取一个元素 lindex key 0 # 0 是表示第一个元素， 1 表示第二个元素，并以此类推。 负数索引用于指定从列表尾部开始索引的元素。在这种方法下，-1 表示最后一个元素，-2 表示倒数第二个元素，并以此往前推。 ②查询指定范围内的元素 lrange key start end # start 和 end 偏移量都是基于0的下标 # 当下标超过list范围的时候不会产生error。 如果start比list的尾部下标大的时候，会返回一个空列表。 如果stop比list的实际尾部大的时候，Redis会当它是最后一个元素的下标。 ③获取队列的长度 llen key # 如果 key 不存在，那么就被看作是空list，并且返回长度为 0。 当存储在 key 里的值不是一个list的话，会返回error。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-27 14:06:14 "},"origin/keepalived.html":{"url":"origin/keepalived.html","title":"Keepalived","keywords":"","body":"keepalived 一、简介 虚拟路由冗余协议 (Virtual Router Redundancy Protocol，简称VRRP 二、安装配置 安装 包管理器安装 yum/apt install -y keepalived # 配置文件：/etc/keepalived/keepalived.conf # 程序文件：/usr/sbin/keepalived # Unit File：keepalived.service # Unit File的环境配置文件：/etc/sysconfig/keepalived 源码安装 version=2.1.5 curl -x https://www.keepalived.org/software/keepalived-$version.tar.gz | tar -xC /opt cd /opt/keepalived-$version ./configure make make install keepalived --version 配置 /etc/keepalived/keepalived.conf global_defs { router_id Curiouser # 声明虚拟路标识符，一般会写当前主机名称 vrrp_skip_check_adv_addr # 所有报文都检查比较消耗性能，此配置为如果收到的报文和上一个报文是同一个路由器发出的则跳过检查报文中的源地址。 vrrp_iptables #禁用防火墙策略,keepalived默认启动时会自动生成iptables策略，因此我们启用此项就可以不生成iptables的策略。 #严格遵守VRRP协议,即不允许以下三种状况:1.没有VIP地址;2.单播邻居;3.在VRRP版本2中有IPv6地址 #vrrp_strict #由于我下面配置基于单播地址发送消息,因此我这里手动禁用了严格模式，直接注释即可。 vrrp_garp_interval 0 # ARP报文发送延迟时间,0表示不延迟。 vrrp_gna_interval 0 # 消息发送延迟,0表示不延迟。 } vrrp_script check_nginx { script \"/etc/keepalived/keepalived-untils.sh check_ngix\" interval 10 weight -20 } #使用vrrp_instance指令定义一个VIP实例名称,这里定义VIP实例的名称为\"VIP1\",生产环境建议该名称和业务相结合。 vrrp_instance VIP1 { state BACKUP # 指定当前实例默认角色,当前节点在此虚拟路由器上的初始状态，状态为MASTER或者BACKUP #定义工作模式为非抢占模式(即当master重启,VIP会飘移到其它节点,重启完成后并不会将vip抢过来),需要两个keepalived的state的值均为\"BACKUP\",让它们根据个节点的优先级选择对应的master nopreempt interface eth1 # 指定通过哪个本地网卡发送vrrp广播 virtual_router_id 27 # 定义当前虚拟路由器惟一标识,该id的范围是0-255,注意，用一组keepalived的id编号必须要一致 priority 150 # 当前物理节点在此虚拟路由器中的优先级；范围1-254 advert_int 2 # 定义vrrp广播的时间间隔，默认1s unicast_src_ip 192.168.1.11 # 指定单播地址的源地址,需要禁用严格模式\"vrrp_strict\" unicast_peer { 172.30.1.103 # 指定单播地址的对端地址 } authentication { auth_type PASS # 定义认证机制,密码仅支持8位 auth_pass 12345678 } #指定虚拟IP地址,可以指定多个。 virtual_ipaddress { 192.168.1.11 dev eth1 label eth1:0 192.168.1.12 dev eth1 label eth1:1 } track_script { check_nginx } #定义通知脚本,当前节点成为主节点时触发的脚本 notify_master \"/etc/keepalived/notify.sh master\" #定义通知脚本,当前节点转为备节点时触发的脚本 notify_backup \"/etc/keepalived/notify.sh backup\" #定义通知脚本,当前节点转为“失败”状态时触发的脚本 notify_fault \"/etc/keepalived/notify.sh fault\" } 脚本 keepalived-untils.sh #!/bin/sh case $1 in check_ngix ) nginxpid=$(ps -C nginx --no-header|wc -l) #1.判断Nginx是否存活,如果不存活则尝试启动Nginx if [ $nginxpid -eq 0 ];then systemctl start nginx sleep 3 #2.等待3秒后再次获取一次Nginx状态 nginxpid=$(ps -C nginx --no-header|wc -l) #3.再次进行判断, 如Nginx还不存活则停止Keepalived,让地址进行漂移,并退出脚本 if [ $nginxpid -eq 0 ];then systemctl stop keepalived fi fi ;; notify_master ) ;; notify_slave ) ;; notify_fault ) ;; * ) ;; esac Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-30 10:35:55 "},"origin/正反向代理服务的区别.html":{"url":"origin/正反向代理服务的区别.html","title":"代理服务器","keywords":"","body":"正反向代理服务的区别 一、代理的作用 首先要明确代理服务器的作用: 代理：可以直白地理解为：代理服务器是一种代替谁去访问什么的服务器。代理客户端浏览器去访问客户端浏览器访问不了的服务，代理应用服务器负载均衡地对外提供服务。可以根据代替谁来划分为\"正向代理\"和\"反向代理\" 缓存加速：缓存那些不经常变动的资源，加速访问。 鉴权过滤记录：允许那些认证过的客户端去访问指定的资源或服务，还可以记录下访问记录。 二、正向代理 正向代理（forward proxy）是指代替内部网络的客户端，去访问Internet或其他网络上的服务，并将访问的结果返还给客户端，同时将结果缓存下来，加速访问。 正向代理还可以按客户端是否感知分为透明代理与传统代理。 常见的正向代理服务软件有: Squid Varnish Nginx 三、反向代理 反向代理（Reverse Proxy）方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器；并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 常见的反向代理服务软件有: Nginx Apache HAProxy 未完代待整理更新 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/常见正向代理服务软件之间的区别.html":{"url":"origin/常见正向代理服务软件之间的区别.html","title":"正向代理","keywords":"","body":"常见正向代理服务软件的对比区别(挖坑) https://www.zhihu.com/search?type=content&q=%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%A6%82%E5%BF%B5 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/squid-简介安装.html":{"url":"origin/squid-简介安装.html","title":"简介安装日志","keywords":"","body":"正向代理服务Squid 一、简介 Squid是一个高性能的正向代理缓存服务器，支持FTP、gopher、HTTPS、HTTP等协议，主要提供了缓存加速、应用层过滤的功能。和一般的代理缓存软件不同，Squid用一个单独的、非模块化的、I/O驱动的进程来处理所有的客户端请求。 squid代理服务器的工作机制： 代理服务器（Proxy Server）是个人网络和Internet服务商之间的中间代理机构，负责转发合法的网络信息，对转发进行控制和登记。其最基本的功能就是连接，此外还包括安全性、缓存，内容过滤，访问控制管理等功能。当客户机通过代理请求Web页面时，执行的代理服务器会先检查自己的缓存，当缓存中有客户机需要访问的页面，则直接将缓存服务器中的页面内容反馈给客户机；如果缓存中没有客户机需要访问的页面，则由代理服务器想Internet发送访问请求，当获得返回的Web页面以后，将页面数据保存到缓存中并发送给客户机。 由于客户机的web访问请求实际上代理服务器来代替完成的，所以隐藏了用户的真实IP地址，从而起到一定的保护作用。 Squid可以基于访问控制列表（ACL）和访问权限列表（ARL）执行内容过滤与权限管理功能，还可以基于多种条件禁止用户访问存在威胁或不适宜的网站资源。 根据实现的方式不同，正向代理模式可以分为： 传统代理：也就是普通的代理服务，需要我们客户端在浏览器、聊天工具等一些程序中设置代理服务器的地址和端口，然后才能使用代理来访问网络，这种方式相比较而言比较麻烦，因为客户机还需手动指定代理服务器，所以一般用于Internet环境。 透明代理：与传统代理实现的功能是一样的，区别在于客户机不需要手动指定代理服务器的地址和端口，而是通过默认路由、防火墙策略将web访问重定向，实际上仍然交给代理服务器来处理，重定向的过程完全是由squid服务器进行的，所以对于客户机来说，甚至不知道自己使用了squid代理服务，因此呢，我们称之为透明模式。透明代理多用于局域网环境，如在Linux网关中启用透明代理后，局域网主机无须进行额外设置就能享受更好的上网速度。 二、安装 YUM yum install squid -y; \\ systemctl enable squid; \\ systemctl start squid 三、传统代理服务配置 配置文件/etc/squid/squid.conf acl localnet src 10.0.0.0/8 # RFC1918 possible internal network acl localnet src 172.16.0.0/12 # RFC1918 possible internal network acl localnet src 192.168.0.0/16 # RFC1918 possible internal network acl localnet src fc00::/7 # RFC 4193 local private network range acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines acl SSL_ports port 443 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025-65535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT http_access allow Safe_ports http_access deny CONNECT !SSL_ports http_access allow localhost manager http_access deny manager http_access allow localnet http_access allow localhost http_access deny all http_port 3128 # Squid代理服务监听端口 cache_dir ufs /data/squid 100 16 256 coredump_dir /data/squid refresh_pattern ^ftp: 1440 20% 10080 refresh_pattern ^gopher: 1440 0% 1440 refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 refresh_pattern . 0 20% 4320 四、常用命令 1、启动等命令 squid reload #不重启服务，生效配置 squid –z #初始化缓存空间 初始化你在 squid.conf 里配置的 cache 目录,只需要第一次的时候执行就可以了 squid -k parse #验证squid.conf的语法和配置 squid -N -d1 #在前台启动squid，并输出启动过程 squid -s #后台运行squid。 squid -k shutdown #停止 squid squid -k reconfigure #载入新的配置文件 squid -k rotate #轮循日志 2、squid命中率分析 # 获取squid运行状态信息 squidclient -p 3128 mgr:info squidclient -p 3128 mgr:5min # 可以看到详细的性能情况,其中PORT是你的proxy的端口，5min可以是60min #获取squid内存使用情况 squidclient -p 3128 mgr:mem #获取squid已经缓存的列表 squidclient -p 3128 mgr:objects use it carefully,it may crash #获取squid的磁盘使用情况 squidclient -p 3128 mgr:diskd #强制更新某个url squidclient -p 3128 -m PURGE http://www.xxx.com/xxx.php #更多的请查看 squidclient -h 或者 squidclient -p 3128 mgr: #查命中率： squidclient -h(具体侦听IP) -p80(具体侦听端口) mgr:info 3、定期清除swap.state内无效数据 当squid应用运行了一段时间以后，cache_dir对应的swap.state文件就会变得越来越大，里面的无效接口数据越来越多，这可能影响squid的响应时间，因此需要使用rotate命令来使squid清理swap.state里面的无效数据，减少swap.state的大小 squid -k rotate -f /path/to/squid/conf_file #添加定时清理任务 vi /etc/crontab 0 0 * * * root /usr/local/sbin/squid -k rotate -f /usr/local/etc/squid/squid1.conf 4、统计客户端个数 netstat -lanp|grep 3128|grep \"ESTABLISHED\"|awk '{print $5}'|awk -F':' '{print $1}'|sort -u|wc -l 5、统计客户端的连接总数 netstat -lanp|grep 3128|grep \"ESTABLISHED\"|wc -l 6、显示传输数据大于指定大小的访问 tailf /var/log/squid/access.log | awk '{if($5>1000)print}'|awk '{print $3 \" \" $5 \" \" $7}' 五、日志默认输出格式 squid日志配置项是在/etc/squid/squid.conf中配置的，默认日志输出文件路径/var/log/squid/access.log 默认的日志输出格式 #1:时间戳 2:响应时间 3:客户端IP 4:结果/状态码 5:传输大小 6:请求方式 7:客户端请求的URL 8:客户端身份 9:对端编码/对端主机 10:内容类型 1531077064.951 81 10.248.2.67 TCP_MISS/200 6277 GET http://bbs.talkop.com/forum.php? - HIER_DIRECT/180.76.184.69 text/xml 时间戳（%tl %ts）: 请求完成时间，以 Unix 时间来记录的（UTC 1970-01-01 00:00:00 开始的时间）它是毫秒级的。squid使用这种格式而不是人工可读的时间格式，是为了简化某些日志处理程序的工作 响应时间（%6tr）: 对HTTP响应来说，该域表明squid花了多少时间来处理请求。在squid接收到HTTP请求时开始计时，在响应完全送出后计时终止。响应时间是毫秒级的。尽管时间值是毫秒级的，但是精度可能是10毫秒。在squid负载繁重时，计时变得没那么精确 客户端地址（%>a）: 该域包含客户端的IP地址，或者是主机名 结果/状态码（%Ss/%03Hs）: 该域包含2个 token，以斜杠分隔。第一个token叫结果码，它把协议和响应结果（例如TCPHIT或UDP_DENIED）进行归类。这些是squid专有的编码，以TCP开头的编码指HTTP请求，以UDP_开头的编码指ICP查询。第2个token是HTTP响应状态码（例如200,304,404等）。状态码通常来自原始服务器。在某些情形下，squid可能有义务自己选择状态码 传输size（%: 该域指明传给客户端的字节数。严格的讲，它是squid告诉TCP/IP协议栈去发送给客户端的字节数。这就是说，它不包括TCP/IP头部的overhead。也请注意，传输size正常来说大于响应的Content-Length。传输size包括了HTTP响应头部，然而Content- Length不包括 请求方式（%rm）: 该域包含请求方式 URI（%ru）: 该域包含来自客户端请求的URI。大多数记录下来的URI实际是URL（例如，它们有主机名）。在记日志时，squid删掉了在第一个问号(?)之后的所有URI字符，除非禁用了strip_query_terms指令 客户端身份: 无 对端编码/对端主机: 对端信息包含了2个token，以斜杠分隔。它仅仅与cache 不命中的请求有关。第一个token指示如何选择下一跳，第二个token是下一跳的地址。当squid发送一个请求到邻居cache时，对端主机地址是邻居的主机名。假如请求是直接送到原始服务器的，则squid会写成原始服务器的IP地址或主机名–假如禁用了log_ip_on_direct。NONE/-这个值指明squid不转发该请求到任何其他服务器 内容类型（%mt）: 原始access.log的默认的最后一个域，是HTTP响应的内容类型。 squid从响应的Content-Type头部获取内容类型值。假如该头部丢失了，squid使用一个横杠(-)代替 假如激活了 log_mime_hdrs 指令，squid在每行追加2个附加的域： HTTP请求头部: Squid 编码HTTP请求头部，并且在一对方括号之间打印它们。方括号是必须的，因为squid不编码空格字符。编码方案稍许奇怪。回车（ASCII 13）和换行（ASCII 10）分别打印成\\r和\\n。其他不可打印的字符以RFC 1738风格来编码，例如Tab（ASCII 9）变成了%09。 HTTP响应头部: Squid编码HTTP响应头部，并且在一对方括号之间打印它们。注意这些是发往客户端的头部，可能不同于从原始服务器接收到的头部。 参考链接 http://www.squid-cache.org/ https://blog.51cto.com/10693404/2149207 https://blog.51cto.com/14154700/2406060 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/squid-acl访问权限控制.html":{"url":"origin/squid-acl访问权限控制.html","title":"ACL访问权限","keywords":"","body":"一、ACL概念 Squid提供了强大的代理控制机制，通过合理设置ACL（Access Control List，访问控制列表）并进行限制，可以针对源地址、目标地址、访问的URL路径、访问的时间等各种条件进行过滤。 ACL访问控制的步骤： 使用acl配置项定义需要控制的条件 通过http_access配置项对已定义的列表做“允许”或“拒绝”访问的控制 二、ACL用法概述 1、定义ACL访问列表 定义格式： acl aclname acltype string1… #acl 列表名称 列表类型 列表内容 ... acl aclname acltype \"File_Path\"… #acl 列表名称 列表类型 \"文件路径\" ... #当使用文件时，该文件的格式为每行包含一个条目。 常用的ACL列表类型: src：指明源地址 acl aclname src ip-address/netmask ... 客户ip地址 acl aclname src addr1-addr2/netmask ... 地址范围 dst：指明目标地址，即客户请求的服务器的IP地址。语法为： acl aclname dst ip-address/netmask ... srcdomain：指明客户所属的域，Squid将根据客户IP反向查询DNS。语法为： acl aclname srcdomain foo.com ... dstdomain：指明请求服务器所属的域，由客户请求的URL决定。语法为： acl aclname dstdomain foo.com ... 此处需要注意的是：如果用户使用服务器IP而非完整的域名时，Squid将进行反向的DNS解析来确定其完整域名，如果失败，就记录为“none”。 time：指明访问时间。语法如下： acl aclname time [day-abbrevs] [h1:m1-h2:m2][hh:mm-hh:mm] 日期的缩写指代关系如下： S：指代Sunday M：指代Monday T：指代Tuesday W：指代Wednesday H：指代Thursday F：指代Friday A：指代Saturday 另外，h1：m1必须小于h2：m2，表达式为[hh：mm-hh：mm]。 port：指定访问端 acl aclname port 80 70 21 ... acl aclname port 0-1024 ... 指定一个端口范围 method：指定请求方法。比如： acl aclname method GET POST ... url_regex：URL规则表达式匹配，语法为： acl aclname url_regex[-i] pattern urlpath_regex：URL-path规则表达式匹配，略去协议和主机名。其语法为： acl aclname urlpath_regex[-i] pattern Notes： acltype可以是任一个在ACL中定义的名称。 任何两个ACL元素不能用相同的名字。 每个ACL由列表值组成。当进行匹配检测的时候，多个值由逻辑或运算连接；换句话说，任一ACL元素的值被匹配，则这个ACL元素即被匹配。 并不是所有的ACL元素都能使用访问列表中的全部类型。 不同的ACL元素写在不同行中，Squid将这些元素组合在一个列表中。 2、http_access访问控制列表使用访问控制 根据访问控制列表允许或禁止某一类用户访问。如果某个访问没有相符合的项目，则默认为应用最后一条项目的“非”。比如最后一条为允许，则默认就是禁止。通常应该把最后的条目设为“deny all”或“allow all”来避免安全性隐患。使用该访问控制列表要注意如下问题： 这些规则按照它们的排列顺序进行匹配检测，一旦检测到匹配的规则，匹配检测就立即结束。 访问列表可以由多条规则组成。 如果没有任何规则与访问请求匹配，默认动作将与列表中最后一条规则对应。 一个访问条目中的所有元素将用逻辑与运算连接（如下所示）： http_access Action声明1 AND 声明2 AND 多个http_access声明间用或运算连接，但每个访问条目的元素间用与运算连接。 列表中的规则总是遵循由上而下的顺序。 三、ACL示例 允许网段10.0.0.124/24以及192.168.10.15/24内的所有客户机访问代理服务器，并且允许在文件/etc/squid/guest列出的客户机访问代理服务器，除此之外的客户机将拒绝访问本地代理服务器： acl clients src 10.0.0.124/24 192.168.10.15/24 acl guests src “/etc/squid/guest” acl all src 0.0.0.0/0.0.0.0 http_access allow clients http_access allow guests http_access deny all 其中，文件“/etc/squid/guest”中的内容为： 172.168.10.3/24 210.113.24.8/16 10.0.1.24/25 允许域名为job.net、gdfq.edu.cn的两个域访问本地代理服务器，其他的域都将拒绝访问本地代理服务器： acl permitted_domain src job.net gdfq.edu.cn acl all src 0.0.0.0/0.0.0.0 http_access allow permitted_domain http_access deny all 使用正则表达式，拒绝客户机通过代理服务器访问包含有诸如“sexy”等关键字的网站： acl deny_url url_regex -i sexy http_access deny deny_url 拒绝客户机通过代理服务器访问文件中指定IP或者域名的网站，其中文件/etc/squid/ deny_ip中存放有拒绝访问的IP地址，文件/etc/squid/deny_dns中存放有拒绝访问的域名： acl deny_ip dst “etc/squid/deny_ip” acl deny_dns dst “etc/squid/deny_dns” http_access deny deny_ip http_access deny deny_dns 允许和拒绝指定的用户访问指定的网站，其中，允许客户1访问网站http://www.sina.com.cn，而拒绝客户2访问网站http://www.163.com： acl client1 src 192.168.0.118 acl client1_url url_regex ^http://www.sina.com.cn acl client2 src 192.168.0.119 acl client2_url url_regex ^http://www.163.com http_access allow client1 client1_url http_access deny client2 client2_url 允许所有的用户在规定的时间内（周一至周四的8：30到20：30）访问代理服务器，只允许特定的用户（系统管理员，其网段为：192.168.10.0/24）在周五下午访问代理服务器，其他的在周五下午一点至六点一律拒绝访问代理服务器： acl allclient src 0.0.0.0/0.0.0.0 acl administrator 192.168.10.0/24 acl common_time time MTWH 8:30-20:30 acl manage_time time F 13:00-18:00 http_access allow allclient common_time http_access allow administrator manage_time http_access deny manage_time Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nginx-install-setup.html":{"url":"origin/nginx-install-setup.html","title":"Nginx安装配置","keywords":"","body":"Nginx 一、简介 官网：http://nginx.org/ nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；nginx可以作为一个HTTP服务器进行网站的发布处理，另外nginx可以作为反向代理进行负载均衡的实现。 反向代理 负载均衡 二、安装 1. 二进制RPM安装 以RPM方式安装的配置文件在/etc/nginx/目录下 二进制安装自带的模块 二进制安装(例如YUM)的nginx不支持动态的安装和新加载模块的，新增模块需要重新编译安装了nginx #To set up the yum repository for RHEL/CentOS, create the file named /etc/yum.repos.d/nginx.repo with the following contents: [nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 #Replace “OS” with “rhel” or “centos”, depending on the distribution used, and “OSRELEASE” with “6” or “7”, for 6.x or 7.x versions, respectively. $ bash -c 'cat > /etc/yum.repos.d/nginx.repo 2. 源码编译 ①安装编译工具 yum install -y gcc gc++ perl gcc-c++ ②安装编译必备库 the PCRE library – required by NGINX Core and Rewrite modules and provides support for regular expressions pcre是一个正则库，nginx使用正则进行重写要用到，必须安装 pcre库有两个版本：pcre、pcre2(新版的库)。推荐下载pcre，pcre2是编译是通不过的。 编译pcre就必须用到c++编译器，使用pcre2就使用gcc编译器。 # yum安装 $ rpm -qa pcre pcre-devel $ yum install pcre pcre-devel # 源码编译安装 $ version=8.43 && \\ wget ftp://ftp.pcre.org/pub/pcre/pcre-$version.tar.gz && \\ tar -zxf pcre-$version.tar.gz&& \\ cd pcre-$version && \\ ./configure && \\ make && \\ make install the zlib library – required by NGINX Gzip module for headers compression: # yum安装 $ rpm -qa zlib zlib-devel $ yum install zlib zlib-devel # 源码编译安装 $ version=1.2.11 && \\ wget http://zlib.net/zlib-$version.tar.gz && \\ tar -zxf zlib-$version.tar.gz && \\ cd zlib-$version && \\ ./configure && \\ make && \\ make install the OpenSSL library – required by NGINX SSL modules to support the HTTPS protocol OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库 # yum安装 $ rpm -qa openssl openssl-devel $ yum install openssl openssl-devel # 源码编译安装 $ version=1.0.2t && \\ wget http://www.openssl.org/source/openssl-$version.tar.gz && \\ tar -zxf openssl-$version.tar.gz && \\ cd openssl-$version && \\ ./config && \\ make && \\ make install # 如果是在MacOS下源码编译，配置时手动指定OS平台 ./Configure darwin64-x86_64-cc && \\ make && \\ sudo make install ③下载解压Nginx源码包 version=1.18.0 && \\ mkdir nginx-source && \\ cd nginx-source && \\ curl -s -# https://www.openssl.org/source/openssl-1.1.1.tar.gz | tar zxvf - -C ./ && \\ curl -s -# https://nginx.org/download/nginx-$version.tar.gz | tar zxvf - -C ./ && \\ cd nginx-$version ④配置编译参数 创建nginx用户----->创建相关目录------>配置编译参数 编译参数文档：http://nginx.org/en/docs/configure.html $ ./configure --help #查看编译配置参数 --with开头的，默认是禁用的(没启动的，想使用的话需要在编译的时候加上) --without开头的，默认是启用的(不想启用此模块时，可以在编译的时候加上这个参数) # --help print this message # --prefix=PATH 指定安装目录 # --sbin-path=PATH 指定二进制执行程序文件存放位置。 # --modules-path=PATH 指定第三方模块的存放路径 # --conf-path=PATH 指定配置文件nginx.conf存放位置 # --error-log-path=PATH 指定错误日志存放位置 # --pid-path=PATH 指定nginx.pid文件存放位置 # --lock-path=PATH 指定nginx.lock文件存放位置 # --user=USER 指定程序运行时的非特权用户 # --group=GROUP 指定程序运行时的非特权用户组 # --build=NAME set build name # --builddir=DIR 指定编译目录 # --with-select_module enable select module # --without-select_module disable select module # --with-poll_module enable poll module # --without-poll_module disable poll module # --with-threads enable thread pool support # --with-file-aio enable file AIO support # --with-http_ssl_module enable ngx_http_ssl_module # --with-http_v2_module enable ngx_http_v2_module # --with-http_realip_module enable ngx_http_realip_module # --with-http_addition_module enable ngx_http_addition_module # --with-http_xslt_module enable ngx_http_xslt_module # --with-http_xslt_module=dynamic enable dynamic ngx_http_xslt_module # --with-http_image_filter_module enable ngx_http_image_filter_module # --with-http_image_filter_module=dynamic enable dynamic ngx_http_image_filter_module # --with-http_geoip_module enable ngx_http_geoip_module # --with-http_geoip_module=dynamic enable dynamic ngx_http_geoip_module # --with-http_sub_module enable ngx_http_sub_module # --with-http_dav_module enable ngx_http_dav_module # --with-http_flv_module enable ngx_http_flv_module # --with-http_mp4_module enable ngx_http_mp4_module # --with-http_gunzip_module enable ngx_http_gunzip_module # --with-http_gzip_static_module enable ngx_http_gzip_static_module # --with-http_auth_request_module enable ngx_http_auth_request_module # --with-http_random_index_module enable ngx_http_random_index_module # --with-http_secure_link_module enable ngx_http_secure_link_module # --with-http_degradation_module enable ngx_http_degradation_module # --with-http_slice_module enable ngx_http_slice_module # --with-http_stub_status_module enable ngx_http_stub_status_module # --without-http_charset_module disable ngx_http_charset_module # --without-http_gzip_module disable ngx_http_gzip_module # --without-http_ssi_module disable ngx_http_ssi_module # --without-http_userid_module disable ngx_http_userid_module # --without-http_access_module disable ngx_http_access_module # --without-http_auth_basic_module disable ngx_http_auth_basic_module # --without-http_autoindex_module disable ngx_http_autoindex_module # --without-http_geo_module disable ngx_http_geo_module # --without-http_map_module disable ngx_http_map_module # --without-http_split_clients_module disable ngx_http_split_clients_module # --without-http_referer_module disable ngx_http_referer_module # --without-http_rewrite_module disable ngx_http_rewrite_module # --without-http_proxy_module disable ngx_http_proxy_module # --without-http_fastcgi_module disable ngx_http_fastcgi_module # --without-http_uwsgi_module disable ngx_http_uwsgi_module # --without-http_scgi_module disable ngx_http_scgi_module # --without-http_memcached_module disable ngx_http_memcached_module # --without-http_limit_conn_module disable ngx_http_limit_conn_module # --without-http_limit_req_module disable ngx_http_limit_req_module # --without-http_empty_gif_module disable ngx_http_empty_gif_module # --without-http_browser_module disable ngx_http_browser_module # --without-http_upstream_hash_module disable ngx_http_upstream_hash_module # --without-http_upstream_ip_hash_module disable ngx_http_upstream_ip_hash_module # --without-http_upstream_least_conn_module disable ngx_http_upstream_least_conn_module # --without-http_upstream_keepalive_module disable ngx_http_upstream_keepalive_module # --without-http_upstream_zone_module disable ngx_http_upstream_zone_module # --with-http_perl_module enable ngx_http_perl_module # --with-http_perl_module=dynamic enable dynamic ngx_http_perl_module # --with-perl_modules_path=PATH set Perl modules path # --with-perl=PATH set perl binary pathname # --http-log-path=PATH set http access log pathname # --http-client-body-temp-path=PATH set path to store http client request body temporary files # --http-proxy-temp-path=PATH set path to store http proxy temporary files # --http-fastcgi-temp-path=PATH set path to store http fastcgi temporary files # --http-uwsgi-temp-path=PATH set path to store http uwsgi temporary files # --http-scgi-temp-path=PATH set path to store http scgi temporary files # --without-http disable HTTP server # --without-http-cache disable HTTP cache # --with-mail enable POP3/IMAP4/SMTP proxy module # --with-mail=dynamic enable dynamic POP3/IMAP4/SMTP proxy module # --with-mail_ssl_module enable ngx_mail_ssl_module # --without-mail_pop3_module disable ngx_mail_pop3_module # --without-mail_imap_module disable ngx_mail_imap_module # --without-mail_smtp_module disable ngx_mail_smtp_module # --with-stream enable TCP/UDP proxy module # --with-stream=dynamic enable dynamic TCP/UDP proxy module # --with-stream_ssl_module enable ngx_stream_ssl_module # --with-stream_realip_module enable ngx_stream_realip_module # --with-stream_geoip_module enable ngx_stream_geoip_module # --with-stream_geoip_module=dynamic enable dynamic ngx_stream_geoip_module # --with-stream_ssl_preread_module enable ngx_stream_ssl_preread_module # --without-stream_limit_conn_module disable ngx_stream_limit_conn_module # --without-stream_access_module disable ngx_stream_access_module # --without-stream_geo_module disable ngx_stream_geo_module # --without-stream_map_module disable ngx_stream_map_module # --without-stream_split_clients_module disable ngx_stream_split_clients_module # --without-stream_return_module disable ngx_stream_return_module # --without-stream_upstream_hash_module disable ngx_stream_upstream_hash_module # --without-stream_upstream_least_conn_module disable ngx_stream_upstream_least_conn_module # --without-stream_upstream_zone_module disable ngx_stream_upstream_zone_module # --with-google_perftools_module enable ngx_google_perftools_module # --with-cpp_test_module enable ngx_cpp_test_module # --add-module=PATH enable external module # --add-dynamic-module=PATH enable dynamic external module # --with-compat dynamic modules compatibility # --with-cc=PATH set C compiler pathname # --with-cpp=PATH set C preprocessor pathname # --with-cc-opt=OPTIONS set additional C compiler options # --with-ld-opt=OPTIONS set additional linker options # --with-cpu-opt=CPU build for the specified CPU, valid values:pentium, pentiumpro, pentium3, pentium4,athlon, opteron, sparc32, sparc64, ppc64 # --without-pcre disable PCRE library usage # --with-pcre force PCRE library usage # --with-pcre=DIR 设置pcre源码目录路径 # --with-pcre-opt=OPTIONS set additional build options for PCRE # --with-pcre-jit build PCRE with JIT compilation support # --with-zlib=DIR set path to zlib library sources # --with-zlib-opt=OPTIONS set additional build options for zlib # --with-zlib-asm=CPU use zlib assembler sources optimized for the specified CPU, valid values:pentium, pentiumpro # --with-libatomic force libatomic_ops library usage # --with-libatomic=DIR set path to libatomic_ops library sources # --with-openssl=DIR set path to OpenSSL library sources # --with-openssl-opt=OPTIONS set additional build options for OpenSSL # --with-debug enable debug logging $ groupadd nginx && \\ useradd nginx -s /sbin/nologin -M -g nginx && \\ mkdir -p /opt/nginx-1.17.6/logs $ ./configure \\ --prefix=/opt/nginx-1.17.6 \\ --user=nginx \\ --group=nginx \\ --modules-path=/opt/nginx-17.6/modules \\ --sbin-path=/opt/nginx-1.17.6/sbin/nginx \\ --error-log-path=/opt/nginx-1.17.6/logs/error.log \\ --http-log-path=/opt/nginx-1.17.6/logs/access.log \\ --conf-path=/opt/nginx-1.17.6/conf/nginx.conf \\ --pid-path=/opt/nginx-1.17.6/nginx.pid \\ --lock-path=/opt/nginx-1.17.6/nginx.lock \\ --http-client-body-temp-path=/var/cache/nginx/client_temp \\ --http-proxy-temp-path=/var/cache/nginx/proxy_temp \\ --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp \\ --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp \\ --http-scgi-temp-path=/var/cache/nginx/scgi_temp \\ --with-pcre \\ --with-openssl=../openssl-1.1.1 \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_dav_module \\ --with-http_flv_module \\ --with-http_mp4_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-http_auth_request_module \\ --with-http_xslt_module=dynamic \\ --with-http_image_filter_module=dynamic \\ --with-http_geoip_module=dynamic \\ --with-http_image_filter_module \\ --with-http_v2_module \\ --with-http_slice_module \\ --with-threads \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-stream_realip_module \\ --with-stream_geoip_module=dynamic \\ --with-mail \\ --with-mail_ssl_module \\ --with-compat ⑤编译安装 # make命令将源代码编译为二进制文件 $ make # 根据配置阶段指定的路径和功能将软件以特定的方式安装到指定位置 $ make install ⑥设置环境变量 ln -s /opt/nginx-1.17.6/nginx /usr/bin/nginx 3. 启动 手动控制Nginx的生命周期 $ nginx -t #启动测试 $ nginx #启动 托管给Systemd $ bash -c 'cat > /usr/lib/systemd/system/nginx.service 4. 验证 # 查看监听的端口 $ lsof -i :80 $ netstat -lanp |grep 80 # 使用命令行工具访问页面 $ curl 127.0.0.1 $ wget 127.0.0.1 # 查看进程 $ ps -ef | grep nginx # root 2564 1 0 23:21 ? 00:00:00 nginx: master process /opt/nginx-1.17.6/nginx # nginx 2565 2564 0 23:21 ? 00:00:00 nginx: worker process 三、Nginx目录结构 编译安装的目录结构 #由于编译时指定了相关路径 $ tree /opt/nginx-1.17.6 /opt/nginx-1.17.6 ├── 3party_module ├── client_body_temp ├── fastcgi.conf ├── fastcgi.conf.default ├── fastcgi_params ├── fastcgi_params.default ├── fastcgi_temp ├── html # 站点目录 │ ├── 50x.html # 错误页 │ └── index.html # 首页 ├── koi-utf ├── koi-win ├── logs # 日志目录 │ ├── access.log # nginx访问日志 │ └── error.log # Nginx的错误日志 ├── mime.types # 媒体类型 ├── mime.types.default ├── nginx # Nginx的二进制启动命令脚本 ├── nginx.conf # Nginx的主要配置文件 ├── nginx.conf.default ├── nginx.pid # Nginx所有的进程号文件 ├── nginx-rtmp-module ├── proxy_temp # 临时目录 ├── scgi_params ├── scgi_params.default ├── scgi_temp ├── uwsgi_params ├── uwsgi_params.default ├── uwsgi_temp └── win-utf 四、命令行参数 $ nginx -s signal #Where signal may be one of the following: # stop — fast shutdown # quit — graceful shutdown # reload — 重新加载配置文件 # reopen — reopening the log files Nginx重新加载配置文件的过程：主进程接受到加载信号后： 1、首先会校验配置的语法，然后生效新的配置， 2、如果成功，则主进程会启动新的工作进程，同时发送终止信号给旧的工作进程。 3、否则主进程回退配置，继续工作。 在第二步，旧的工作进程收到终止信号后，会停止接收新的连接请求，知道所有现有的请求处理完，然后退出。 $ nginx -t #检查配置文件语法是否错误，并尝试启动 $ nginx -q # suppress non-error messages during configuration testing. $ nginx -T # same as -t, but additionally dump configuration files to standard output (1.9.2). $ nginx #启动Nginx $ nginx -v #查看nginx的版本 $ nginx -V #查看nginx的版本，编译器版本，编译时的参数等 $ nginx -p prefix # set nginx path prefix, i.e. a directory that will keep server files (default value is /usr/local/nginx). $ nginx -c file # 指定配置文件（不使用默认路径下的配置文件） $ nginx -? | -h # print help for command-line parameters. $ nginx -g directives # set global configuration directives, for example, #nginx -g \"pid /var/run/nginx.pid; worker_processes `sysctl -n hw.ncpu`;\" 五、Nginx模块 六、配置文件结构 全局配置：用来设置影响Nginx服务器整体运行的配置，作用于全局。（从文件开始到events块的内容） 作用：通常包括服务器的用户组，允许生成的worker process、Nginx进程PID的存放路径、日志的存放路径和类型以及配置文件引入等 事件配置：涉及的指令主要影响Nginx服务器和用户的网络连接。 作用：常用到的设置包括是否开启多worker process下的网络连接进行序列化，是否允许同时接收多个网络连接，选择何种时间驱动模型处理连接请求，每个worker process可以同时支持的最大连接数等 模块配置 HTTP模块 HTTP模块的全局配置 虚拟主机的配置 # Nginx全局配置 user nobody; # 指定Nginx的worker进程运行用户以及用户组，默认由nobody账号运行 worker_processes 1; # 指定Nginx要开启的进程数 error_log logs/error.log # 全局错误日志文件路径。日志级别：debug/info/notice/warn/error/crit pid logs/nginx.pid; # 指定进程PID文件的路径 # 事件配置，设定nginx的工作模式及连接数上限 events { use epoll; # 指定nginx的工作模式。支持的工作模式有select ,poll,kqueue,epoll,rtsig,/dev/poll # epoll是多路复用IO(I/O Multiplexing)中的一种方式， # select和poll都是标准的工作模式，kqueue和epoll是高效的工作模式， # 对于linux系统，epoll是首选。 worker_connections 1024; # 设置nginx每个进程最大的连接数，默认是1024，所以nginx最大的连接数 # max_client=worker_processes * worker_connections。 # 进程最大连接数受到系统最大打开文件数的限制，需要设置ulimit。 } # HTTP模块配置 http { include mime.types; # 配置处理前端请求的MIME类型 default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log logs/access.log main; sendfile on; # 开启高效文件传输模式（zero copy 方式），避免内核缓冲区数据 # 和用户缓冲区数据之间的拷贝。sendfile 参数和I/O有关，当上传文件时，内核首先缓 # 冲数据，然后将数据发送到应用应用程序缓冲区。 应用程序反过来将数据发送到目的地。 # Sendfile方法是一种改进的数据传输方法，其中数据在操作系统内核空间内的文件描述符 # 之间复制，而不将数据传输到应用程序缓冲区。由于没有了用户态和内核态之间的切换， # 也没有内核缓冲区和用户缓冲区之间的拷贝，大大提升了传输性能。 tcp_nopush on; keepalive_timeout 65; # 设置客户端连接超时时间 gzip on; # 设置是否开启gzip模块 # 虚拟主机 server { listen 80; # 虚拟主机的服务端口 server_name localhost; server_tokens off; # 隐藏响应头中的有关操作系统和Nginx服务器版本号的信息，保障安全性 charset koi8-r; access_log logs/host.access.log main; location / { root html; index index.html index.htm; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } # another virtual host using mix of IP-, name-, and port-based configuration # #server { # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / { # root html; # index index.html index.htm; # } #} # HTTPS server # #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #} } 七、nginx内置变量 $args 请求中的参数; $binary_remote_addr 远程地址的二进制表示 $body_bytes_sent 已发送的消息体字节数 $content_length HTTP请求信息里的\"Content-Length\" $content_type 请求信息里的\"Content-Type\" $document_root 针对当前请求的根路径设置值 $document_uri 与$uri相同 $host 请求信息中的\"Host\"，如果请求中没有Host行，则等于设置的服务器名; $http_cookie cookie 信息 $http_referer 来源地址 $http_user_agent 客户端代理信息 $http_via 最后一个访问服务器的Ip地址 $http_x_forwarded_for 相当于网络访问路径。 $limit_rate 对连接速率的限制 $remote_addr 客户端地址 $remote_port 客户端端口号 $remote_user 客户端用户名，认证用 $request 用户请求信息 $request_body 用户请求主体 $request_body_file 发往后端的本地文件名称 $request_filename 当前请求的文件路径名 $request_method 请求的方法，比如\"GET\"、\"POST\"等 $request_uri 请求的URI，带参数 $server_addr 服务器地址，如果没有用listen指明服务器地址，使用这个变量将发起一次系统调用以取得地址(造成资源浪费) $server_name 请求到达的服务器名 $server_port 请求到达的服务器端口号 $server_protocol 请求的协议版本，\"HTTP/1.0\"或\"HTTP/1.1\" $uri 请求的URI，可能和最初的值有不同，比如经过重定向之类的 八、问题 0. Nginx添加模块并不停服升级 不管Nginx是用YUM二进制还是源码编译方式安装的，后续如果有新需求是现有Nginx模块无法满足，需要添加新模块才能完成的情况时，都是要对Nginx进行重新编译安装，然后不停服，不能影响现有的业务地平滑升级 （该操作有风险，需在开发环境测试通过再在生产环境进行操作） ① 查看现有的nginx编译参数 nginx -V # 或者 /opt/nginx1.17.6/nginx -V ② 备份旧版本的nginx可执行文件 期间nginx不会停止服务 mv /opt/nginx1.17.6/nginx /opt/nginx1.17.6/nginx.bak ③ 安装编译必备组件 ④ 下载相同版本的nginx源码包 ⑤ 下载第三方模块 ⑥ 配置编译参数 要加上原有的编译参数 ⑦ 编译新的Nginx 只make, 不要make install，不然会覆盖原来已安装的nginx ⑧ 替换Nginx文件 ⑨ 修改新配置文件， 并检查配置文件语法** ⑩ 新配置的平滑升级 $ kill -USR2 旧Nginx主进程号或进程文件路径 # 此时旧的Nginx主进程将会把自己的进程文件改名为.oldbin，然后执行新版Nginx。新旧Nginx会同时运行，共同处理请求。 这时要逐步停止旧版 Nginx $ kill -WINCH 旧Nginx主进程号 # 慢慢旧Nginx进程就都会随着任务执行完毕而退出，新的Nginx进程会逐渐取代旧进程。 1. 启动Nginx时报“nginx: [emerg] getpwnam(\"nginx\") failed” 原因：nginx用户没有创建成功 2. 浏览器，curl、wget等访问不了nginx页面 原因：可能是没有关闭SELinux和防火墙 ，检查一下 3. 访问资源403的问题排查 通过yum安装的nginx一切正常，但是访问时报403，于是查看nginx日志，路径为/var/log/nginx/error.log。打开日志发现报错Permission denied，详细报错如下： open() \"/data/www/1.txt\" failed (13: Permission denied), client: 192.168.1.194, server: www.web1.com, request: \"GET /1.txt HTTP/1.1\", host: \"www.web1.com\" 原因： 1、由于启动用户和nginx工作用户不一致所致 查看nginx的启动用户，发现是nobody，而为是用root启动的 ps aux | grep \"nginx: worker process\" | awk'{print $1}' 解决方案：将nginx.config中的user改为和启动用户一致 2、配置文件中指定的文件 例如配置文件中index index.html index.htm这行中的指定的文件。 server { listen 80; server_name localhost; index index.php index.html; root /data/www/; } 如果在/data/www/下面没有index.php,index.html的时候，直接访问文件会报403 forbidden。 解决方案：创建一下相应的文件 3、权限问题，如果nginx没有web目录的操作权限，也会出现403错误。 解决办法：修改web目录的读写权限，或者是把nginx的启动用户改成目录的所属用户，重启Nginx即可解决 chmod -R 777 /data/www/ 4、SELinux设置为开启状态（enabled）的原因。 查看SELinux的状态 $ getenforce Enforcing 为开启状态 解决方案： ①临时或永久关闭Selinux #临时关别Selinux setenforce 0 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-18 00:39:11 "},"origin/nginx-config.html":{"url":"origin/nginx-config.html","title":"Nginx配置优化","keywords":"","body":"Nginx优化配置 一、访问日志JSON格式 # ....全局配置省略..... ; http { # ....HTTP模块其他配置省略..... ; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; # 设置日志格式 log_format json_log '{ \"@timestamp\": \"$time_iso8601\", ' '\"app\": \"$app\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"up_resp_time\": \"$upstream_response_time\",' '\"request_time\": \"$request_time\"' ' }'; server { # .....虚拟主机其他配置省略..... set $app test ; # 设置变量app的值为“test” access_log logs/host.access.log json_log; # 设置访问日志按照“json_log”的格式进行输出 } } 输出的JSON格式日志 { \"@timestamp\": \"2020-03-09T17:54:49+08:00\", \"app\": \"test\", \"remote_addr\": \"127.0.0.1\", \"referer\": \"-\", \"request\": \"GET /Empty-2C4G80G.ovf HTTP/1.1\", \"status\": 200, \"bytes\": 7532, \"agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\", \"x_forwarded\": \"-\", \"up_addr\": \"-\", \"up_host\": \"-\", \"up_resp_time\": \"-\", \"request_time\": \"0.000\" } 二、按天保留日志文件 server{ ... if ($time_iso8601 ~ '(\\d{4}-\\d{2}-\\d{2})') { set $tttt $1; } access_log logs/nginx-access-$tttt.log main; ... } 三、配置HTTP基础认证 Nginx 使用 ngx_http_auth_basic_module 模块支持 HTTP基本身份验证 功能 。 1、安装httpd-tools yum yum install -y httpd-tools apt apt install -y apache2-utils 2、创建授权用户和密码 htpasswd -c -d /etc/nginx/basic-auth-pass-file test_user htpasswd 其他操作参考：htpasswd操作 3、配置nginx server { # ...省略 auth_basic \"登录认证\"; auth_basic_user_file basic-auth-pass-file; # ...省略 # 或者只设置某些URL进行登录认证 # location /api { # auth_basic \"登录认证\"; # auth_basic_user_file basic-auth-pass-file; #} } 4、使用 # 浏览器中使用 直接在浏览器中输入地址, 会弹出用户密码输入框, 输入即可访问 # wget wget --http-user=test_user --http-passwd=*** http://**** # curl curl -u test_user:**** -O http://**** 四、自定义日志格式引用自定义Header 自定日志格式时如果想获取请求中的自定义Header，nginx引用方式为$http_headername，如果header命名中包含-，需要转换成_。例如下面示例中引用开发自定义的x-zz-app-info http { log_format json_log '{ \"@timestamp\": \"$time_iso8601\", ' '\"app\": \"$app\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"up_resp_time\": \"$upstream_response_time\",' '\"request_time\": \"$request_time\",' '\"server_name\": \"$server_name\",' '\"x-zz-app-info\": \"$http_x_zz_app_info\"' ' }'; server { listen 80; server_name localhost; set $app test; access_log /var/log/nginx/host.access.log json_log; location / { root /usr/share/nginx/html; index index.html index.htm; } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-14 16:05:43 "},"origin/nginx-log-kafka.html":{"url":"origin/nginx-log-kafka.html","title":"Nginx日志写入kafka","keywords":"","body":"Nginx日志写入Kafka 一、简介 方案 Nginx module：编译集成第三方kafka相关模块，可直接将日志发送到kafka Nginx + 第三方应用：结合第三方应用将Nginx日志发送到kafka tail | kafkacat ———> kafka rsyslog ———> kafka Nginx stdout | k8s 第三方operator ———> kafka 二、Nginx module -> kafka openssl_version=1.1.1 && \\ nginx_version=1.18.0 && \\ mkdir compile-dir && cd compile-dir && \\ curl -s -# https://www.openssl.org/source/openssl-$openssl_version.tar.gz | tar zxf - -C ./ && \\ curl -s -# https://nginx.org/download/nginx-$nginx_version.tar.gz | tar zxf - -C ./ && \\ cd nginx-$nginx_version && \\ ./configure \\ --prefix=/opt/nginx-1.18.0 \\ --user=nginx \\ --group=nginx \\ --modules-path=/opt/nginx-1.18.0/modules \\ --sbin-path=/opt/nginx-1.18.0/sbin/nginx \\ --error-log-path=/opt/nginx-1.18.0/logs/error.log \\ --http-log-path=/opt/nginx-1.18.0/logs/access.log \\ --conf-path=/opt/nginx-1.18.0/conf/nginx.conf \\ --pid-path=/opt/nginx-1.18.0/nginx.pid \\ --lock-path=/opt/nginx-1.18.0/nginx.lock \\ --with-pcre \\ --with-openssl=../openssl-1.1.1 \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_dav_module \\ --with-http_flv_module \\ --with-http_mp4_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-http_auth_request_module \\ --with-http_xslt_module=dynamic \\ --with-http_image_filter_module=dynamic \\ --with-http_geoip_module=dynamic \\ --with-http_image_filter_module \\ --with-http_v2_module \\ --with-http_slice_module \\ --with-threads \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-stream_realip_module \\ --with-stream_geoip_module=dynamic \\ --with-mail \\ --with-mail_ssl_module \\ --with-compat && \\ make && \\ make install 参考： https://github.com/brg-liuwei/ngx_kafka_module https://github.com/kaltura/nginx-kafka-log-module 三、tail | kafkacat -> kafka 参考： https://github.com/edenhill/kafkacat 四、nginx -> rsyslog -> kafka 1、Prerequisite Nginx >= v1.7.1 （之后才支持 syslog 的方式处理日志） Rsyslog >= v8.7.0 （数据需要通过 Rsyslog 的 omkafka 模块写入到 Kafka，omkafka 在 Rsyslog 的 v8.7.0+ 版本才支持） rsyslog的omkafka模块rsyslog-kafka已安装 2、rsyslog配置 vi /etc/rsyslog.dc/rsyslog_nginx_kafka_cluster.conf module(load=\"imudp\") input(type=\"imudp\" port=\"514\") # nginx access log ==> rsyslog server(local) ==> kafka module(load=\"omkafka\") template(name=\"nginx-rsyslog-kafka\" type=\"string\" string=\"%msg%\") if $inputname == \"imudp\" then { if ($programname == \"nginx-rsyslog\") then action(type=\"omkafka\" template=\"nginx-rsyslog-kafka\" broker=[\"localhost:9092\"] topic=\"test\" partitions.auto=\"on\" confParam=[ \"socket.keepalive.enable=true\" ] ) } :rawmsg, contains, \"nginx-rsyslog\" ~ 启动rsyslog rsyslogd 3、Nginx配置 http { log_format json_log '{ \"@timestamp\": \"$time_iso8601\", ' '\"app\": \"$app\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"up_resp_time\": \"$upstream_response_time\",' '\"request_time\": \"$request_time\",' '\"server_name\": \"$server_name\",' '\"x-zz-app-info\": \"$http_x_zz_app_info\"' ' }'; server { listen 80; server_name localhost; # set $app test; # access_log /var/log/nginx/host.access.log json_log; access_log syslog:server=localhost,facility=local7,tag=nginx-rsyslog,severity=info main; location / { root /usr/share/nginx/html; index index.html index.htm; } } 4、测试 nginx的日志 ———> rsyslog —————> Kafka wrk -t10 -c500 -d30s --latency http://127.0.0.1:80 30.1秒发送了38861个请求，QPS大约1291，Transfer/sec: 1.05MB nginx的日志 ————> 本地文件 wrk -t10 -c500 -d30s --latency http://127.0.0.1:80 30.07秒发送了115156个请求，QPS大约3829，Transfer/sec: 3.10MB nginx的日志通过网络发送到rsyslog再到Kafka，比落盘形成文件，QPS小了三倍。同时压测完成到所有日志进kafka有15秒延迟 因素： 网络消耗 磁盘性能可能导致kafka写入慢 参考： http://zhang-jc.github.io/2019/03/15/%E4%BD%BF%E7%94%A8-Rsyslog-%E5%B0%86-Nginx-Access-Log-%E5%86%99%E5%85%A5-Kafka/ 五、Nginx stdout -> k8s 第三方operator-> kafka 参考： https://banzaicloud.com/docs/one-eye/logging-operator/quickstarts/kafka-nginx/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-14 16:20:22 "},"origin/iSCSI-简介配置使用.html":{"url":"origin/iSCSI-简介配置使用.html","title":"群晖Synology的iSCSI","keywords":"","body":"iSCSI的简介配置使用 一、iSCSI简介 iSCSI（Internet Small Computer System Interface），Internet小型计算机系统接口，又称为IP-SAN，是由IBM 下属的两大研发机构一一加利福尼亚AImaden和以色列Haifa研究中心共同开发的，是一个供硬件设备使用的、可在IP协议上层运行的SCSI指令集，是一种开放的基于IP协议的工业技术标准。一种基于因特网及SCSI-3协议下的存储技术，于2003年2月11日成为正式的标准 iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换SCSI命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降 两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存装置 iSCSI target：就是iSCSI的server，可以是一个物理磁阵；也可以是软件实现的iSCSI server。有硬件方式实现的iSCSI target，例如iSCSI的hba卡和带isoe（iSCSI offload engine）网卡（硬件上将iSCSI 包接包和封包） iSCSI initiator：就是iSCSI的客户端，它可以是一个软件，也可以是一个硬件。如果是软件在linux上，用户态实现的是tgt框架（linux scsi target frame）；还有一种内核太实现的架构是iet（iSCSI enterprise target）。在centos上目前已经默认安装了tgt了。 iqn（iSCSI qualified name）：initiator和target通过iqn号来逻辑寻址。一个iqn号由四部分组成： iqn.日期.域名:域名组织分配的名字 例如：iqn.2000-01.com.synology:Synology.Target-1.826d6a066b LUN：全称是Logical Unit Number，中文名是逻辑单元号。LUN是在存储设备上可以被应用服务器识别的独立存储单元。一个LUN的空间来源于存储池Pool，Pool的空间来源于组成磁盘阵列的若干块硬盘。从应用服务器的角度来看，一个LUN可以被视为一块可以使用的硬盘。例如，在Linux系统中，它在/dev/rdsk、/dev/dsk目录下有相应的设备名称；在Windows系统中，格式化后的新LUN会对应一个类似于D E F的盘符。 Thick LUN：中文名是传统非精简LUN，是LUN类型的一种，支持虚拟资源分配，能以较为简便的方式进行创建、扩容和压缩操作。Thick LUN在创建完成后就会从存储池Pool中分配满额的存储空间，即LUN的大小完全等于分配的空间。因此，它拥有较高的可预测性。 Thin LUN：中文名是精简LUN，也是LUN类型的一种，支持虚拟资源分配，能够以较简便的方式进行创建、扩容和压缩操作。Thin LUN在创建的时候，可以设置一个初始分配容量。创建完成后，存储池Pool只会分配这个初始容量大小的空间剩余的空间仍然放在存储池中。当Thin LUN已分配的存储空间的使用率达到阈值时，存储系统才会再从Pool中划分一定的配额给Thin LUN。如此反复，直到达到Thin LUN最初设定的全部容量。因此，它拥有较高的存储空间利用率。 二、群晖Synology的iSCSI存储 创建LUN 创建Target Target关联LUN 三、Windows挂载 参考链接 https://jingyan.baidu.com/article/e4511cf37feade2b845eaff8.html https://blog.csdn.net/M_joy666/article/details/80566705 附录：Thick LUN与Thin LUN的区别 1、空间分配上的区别 Thick LUN在创建时会分配所有需要的空间 Thin LUN是一种按需分配的空间组织方法，它在创建时存储池不会分配所有需要的空间，而是根据使用情况动态分配。二者的空间分配区别如下图所示： 2、空间回收的区别 注：这里的空间回收指的是释放存储池Pool中的资源，并且这些资源可以被其他LUN使用。 Thick LUN没有空间回收的概念，因为它在创建时就占用存储池中所有分配给它的空间，即使Thick LUN中的数据被删除，存储池中分配给它的空间还是被占用，不能被其他的LUN使用。但是如果手动删除不再使用的Thick LUN，则对应的空间会被回收。 Thin LUN不仅能够做到空间占用率增大时自动分配新的存储空间，而且当Thin LUN中的文件删除时也可以实现空间的释放，从而实现存储空间的反复利用，大大提高存储空间的利用率。Thin LUN的空间回收如下图所示： 3、性能的区别 Thick LUN由于在一开始就会拥有所分配的空间，所以Thick LUN在顺序读写的时候拥有较高的性能，但是会造成空间资源的浪费。 Thin LUN由于是实时分配空间，每次扩容时，需要重新增加容量，后台重新格式化，这个时候性能会受到一定影响，而且每次分配空间可能会导致硬盘中存储空间不连续，这样硬盘读写数据时在寻找存放位置上花费的时间会较多，会在顺序读写时对性能有一定影响。 4、使用场景的区别 Thick LUN： ①对性能要求较高的场景 ②对存储空间利用率不太敏感的场景 ③对成本要求不太高的场景 Thin LUN： ①对性能要求一般的场景； ②对存储空间利用率比较敏感的场景； ③对成本比较敏感的场景； ④应用环境很难预 估存储空间的场景 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/gitbook-简介安装配置.html":{"url":"origin/gitbook-简介安装配置.html","title":"GitBook","keywords":"","body":"GitBook简介安装配置 一、GitBook简介 gitbook 是一个基于node.js的命令行工具 gitbook 支持markdown/asciiDoc语法格式构建书籍 gitbook 支持输出静态网页（可定制和可扩展）和电子书（PDF，ePub或Mobi）等多种格式，其中默认输出静态网页格式 gitbook 不仅支持本地构建书籍,还可以托管在gitbook 官网上，或者Github上 二、GitBook安装 1、安装NodeJs环境 NodeJs官网下载链接:https://nodejs.org/en/download/ Linux 以安装NodeJs 10.16.3为例 wget https://nodejs.org/dist/v10.16.3/node-v10.16.3-linux-x64.tar.xz && \\ tar -xvf node-v10.16.3-linux-x64.tar.xz -C /opt/ && \\ rm -rf node-v10.16.3-linux-x64.tar.xz && \\ ln -s /opt/node-v10.16.3-linux-x64 /opt/nodejs && \\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile && \\ source /etc/profile && \\ yum install gcc-c++ make -y && \\ npm config set registry https://registry.npm.taobao.org && \\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ && \\ node -v && \\ npm version Windows 直接在官网下载MSI格式的安装包进行安装 2、安装Gitbook CLI命令行工具 gitbook-cli 是 gitbook 的一个命令行工具, 通过它可以在电脑上安装和管理多个版本的gitbook. npm install gitbook-cli -g 三、GitBook版本的管理 gitbook-cli 和 gitbook 是两个软件，gitbook-cli 会将下载的 gitbook 的不同版本放到 ~/.gitbook中, 可以通过设置GITBOOK_DIR环境变量来指定另外的文件夹 GitBook可以在本地安装多个版本并在执行命令的时候指定某个版本，如果指定的版本还没安装就会自动下载安装，下载后的GitBook会被放到~/.gitbook目录下。 $ gitbook --help Usage: gitbook [options] [command] Options: -v, --gitbook [version] specify GitBook version to use -d, --debug enable verbose error -V, --version Display running versions of gitbook and gitbook-cli -h, --help output usage information Commands: ls List versions installed locally current Display currently activated version ls-remote List remote versions available for install fetch [version] Download and install a alias [folder] [version] Set an alias named pointing to uninstall [version] Uninstall a version update [tag] Update to the latest version of GitBook help List commands for GitBook * run a command with a specific gitbook version # 查看当前GitBook CLI版本 gitbook -V # 列出本地安装版本 gitbook ls # 列出当前使用版本 gitbook current # 列出远程可用版本 gitbook ls-remote # 安装指定版本(如果安装比较慢的话，将npm镜像源切到国内的CNPM镜像源。可使用NRM管理NPM的镜像源) gitbook fetch [version] # 卸载指定版本 gitbook uninstall [version] # 更新指定版本 gitbook update [tag] 四、GitBook CLI命令 1、gitbook 可用命令 $ gitbook help build [book] [output] 构建书籍 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) --[no-]timing Print timing debug information (Default is false) serve [book] [output] serve the book as a website for testing --port 指定监听端口(默认端口4000) --lrport Port for livereload server to listen on (Default is 35729) --[no-]watch Enable file watcher and live reloading (Default is true) --[no-]live Enable live reloading (Default is true) --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) install [book] 安装所有插件资源 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) parse [book] parse and print debug information about a book --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) init [book] 初始化创建书籍文件结构 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) pdf [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) epub [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) mobi [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) 2、gitbook init初始化创建书籍文件结构 gitbook init # 在当前路径下自动生成README.md 和 SUMMARY.md。也可以先手动创建SUMMARY.md，再执行gitbook init，如果SUMMARY.md中配置的文件夹和文件不存在，就会自动创建文件夹和文件，已经存在的文件夹和文件不会被覆盖。 gitbook init ./directory # 可将书籍初始化到指定目录 3、gitbook build构建gitbook书籍静态HTML资源 gitbook build [book] [output] # 会在书籍的文件夹中生成一个 _book 的文件夹, 里面有生成的静态HTML资源。可将 _book 文件夹下的文件拷贝到nginx、httpd等web服务器内 gitbook build --gitbook=2.0.1 # 指定Gitbook版本 4、gitbook serve启动本地预览书籍服务 gitbook serve [book] [output] 浏览器中打开： http://localhost:4000 预览GitBook书籍 5、输出书籍文件 Prerequisite： ebook-convert：GitBook在生成PDF的过程中使用到calibre的转换功能，没有安装Calibre或安装了Calibre没有配置环境变量都会导致转换PDF失败。Calibre下载地址：https://calibre-ebook.com/download 在 Typora 中安装 Pandoc 进行导出 # 输出书籍为PDF格式文件 gitbook pdf [book] [output] # 输出书籍为epub格式文件 gitbook epub [book] [output] # 输出书籍为mobi格式文件 gitbook mobi [book] [output] 6、gitbook install安装插件样式资源 gitbook install [book] #会在当前路径下生成node_modules文件夹，里面为插件的样式资源 7、gitbook parse 解析电子书 gitbook parse [book] 五、GitBook的文件结构 文件/文件夹 描述 是否必须 README.md 书籍的简介 必须 SUMMARY.md 书籍的目录结构 可选 book.json GitBook的插件样式配置文件 可选 GLOSSARY.md 词汇、术语列表 可选 _book文件夹 GitBook输出的静态HTML文件 node_modules文件夹 插件的样式资源 六、SUMMARY.md编写规则 SUMMARY.md 的格式是一个链接列表。链接的标题将作为章节的标题，链接的目标是该章节文件的路径 向父章节添加嵌套列表将创建子章节 每章都有一个专用页面（part#/README.md），并分为子章节。 目录中的章节可以使用锚点指向文件的特定部分。 目录可以分为以标题或水平线 ---- 分隔的部分 Parts 只是章节组，没有专用页面，但根据主题，它将在导航中显示。 七、book.json编写规则 常规设置 变量 描述 root 包含所有图书文件的根文件夹的路径，除了 book.json structure 指定 Readme，Summary，Glossary 和 Languages 的名称（而不是使用默认名称，如README.md）。这些文件必须在项目的根目录下（或 root 属性指定的根目录）structure.readme：Readme 文件名（默认值是 README.md ）structure.summary：Summary 文件名（默认值是 SUMMARY.md ）structure.glossary：Glossary 文件名（默认值是 GLOSSARY.md ）structure.languages：Languages 文件名（默认值是 LANGS.md ） title 您的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 description 您的书籍的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 author 作者名。在GitBook.com上，这个字段是预填的。 isbn 国际标准书号 ISBN language 本书的语言类型 —— ISO code 。默认值是 en direction 文本阅读顺序。可以是 rtl （从右向左）或 ltr （从左向右），默认值依赖于 language 的值。 gitbook 应该使用的GitBook版本。使用 SemVer 规范，并接受类似于 “> = 3.0.0” 的条件。 links 在左侧导航栏添加链接信息 plugins 要加载的插件列表 pluginsConfig 插件的配置 Gitbook 默认带有 5 个插件： highlight：语法高亮插件 search：搜索插件 sharing：分享插件 font-settings：字体设置插件 livereload：热加载插件 Note：去除插件\"plugins\": [ \"-search\" ] 插件配置示例 { \"author\": \"Curiouser \", \"title\": \"Devops Roadmap\", \"plugins\": [ \"-search\", \"-lunr\", \"-sharing\", \"-highlight\", \"search-pro\", \"splitter\", \"github\", \"popup\", \"sectionx\", \"expandable-chapters\", \"sharing-plus\", \"code\", \"auto-scroll-table\", \"theme-fexa\", \"tbfed-pagefooter\", \"back-to-top-button\", \"emphasize\", \"edit-link\", \"prism\", \"donate\", \"theme-comscore\", \"github-buttons\", \"github-issue-feedback\" ], \"pluginsConfig\": { \"theme-default\": { \"showLevel\": true }, \"github-issue-feedback\": { \"repo\": \"RationalMonster/rationalmonster.github.io\" }, \"github\": { \"url\": \"https://github.com/RationalMonster\" }, \"github-buttons\": { \"buttons\": [{ \"user\": \"RationalMonster\", \"repo\": \"rationalmonster.github.io\", \"type\": \"star\", \"size\": \"small\", \"count\": \"true\" }] }, \"sharing\": { \"weibo\": true, \"qq\": \"true\", \"google\": true, \"all\": [ \"facebook\", \"twitter\" ] }, \"code\": { \"copybuttons\": \"true\" }, \"theme-fexa\":{ \"search-placeholder\":\"搜索文章\", \"logo\": \"assets/logo.png\" }, \"tbfed-pagefooter\": { \"copyright\":\"Copyright Curiouser\", \"modify_label\": \"该文件最后修改时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"edit-link\": { \"base\": \"https://github.com/RationalMonster/rationalmonster.github.io/blob/master\", \"label\": \"ORIGIN In Github\" }, \"prism\": { \"css\": [ \"prismjs/themes/prism-tomorrow.css\" ], \"lang\": { \"flow\": \"typescript\" }, \"ignore\": [ \"mermaid\", \"eval-js\" ] }, \"donate\": { \"wechat\": \"../assets/wechat-donate.jpg\", \"title\": \"\", \"button\": \"赏\", \"wechatText\": \"微信打赏\" } } } 八、GLOSSARY.md 编写规则 GLOSSARY.md 的格式是 h2 标题的列表，以及描述段落 九、忽略文件和文件夹 GitBook将读取 .gitignore，.bookignore 和 .ignore 文件，来过滤不需要进行git版本控制的文件和文件夹。这些文件中的格式遵循 .gitignore 的规则： # This is a comment # Ignore the file test.md test.md # Ignore everything in the directory \"bin\" bin/* ### gitbook ### _node !docs _book node_modules ### IDEA ### .idea/ ### VS Code ### .vscode/ ### OS ### .DS_Store 十、封面 封面用于所有电子书格式。您可以自己提供一个，也可以使用 autocover plugin 生成一个。 要提供封面，请将 cover.jpg 文件放在书本的根目录下。添加一个 cover_small.jpg 将指定一个较小版本的封面。封面应为 JPEG 文件。 好的封面应该遵守以下准则： cover.jpg 的尺寸为 1800x2360 像素，cover_small.jpg 为 200x262 没有边界 清晰可见的书名 任何重要的文字应该在小版本中可见 十一、多语言支持 gitbook 支持构建用多种语言书写的书籍。每种语言应该是一个子目录，遵循正常的gitbook格式，然后需要在根目录下放置一个名为 LANGS.md 的文件，存放下列内容： # Languages * [English](en/) * [French](fr/) * [Español](es/) 注意： 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件 插件的配置必须写在根目录下的 book.json 文件中。然后其他语言的配置可以分别写在各自语言目录下的 book.json 文件中。 LANGS.md 文件中各个语言出现的顺序，就是书籍首页出现的顺利。因此，写在第一位的语言，就自然成为书籍首页打开时的默认语言。 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件。 十二、托管到 GitHub Pages 知道如何编写gitbook了，那怎么放到网上让更多人看到呢。如果自己在各大云厂商那儿买个云主机自己搭建一个运行环境，一年又得多处几百大洋的开销。本着能“白嫖”就不掏腰包的精神，尝试使用其他途径免费部署自己的Gitbook。 Github 有个功能 GitHub Pages 。它允许用户在 GitHub 仓库托管你的个人、组织或项目的静态页面（自动识别 html、css、javascript）。只要仓库指定分支中的内容符合一个静态站点要求，就可以在如下地址中进行访问：https://Github用户名.github.com/仓库名 在Github中设置GitHub Page Source时可指定分支。 gh-pages branch master branch master branch /docs folder 不同的方式，无非是决定gitbook构建后的静态页面HTML文件存放在哪儿的问题。 方式一(推荐)：在master分支中存放gitbook原始Markdown文件、配置文件等，在gh-pages分支中存放gitbook构建后的静态页面HTML文件。 方式二：在master分支中即存放gitbook原始Markdown文件，也存放gitbook构建后的静态页面HTML文件。只不过是在/docs文件下 1、在GitHub中建立仓库并将本地代码推送至master分支 GitHub只建立空仓库是无法创建分支的。所以需要将本地代码推送到远程仓库master才可以 git init git remote add origin git@github.com:**/gitbbook-devops-roadmap.git git add . git commit -m \"init commit\" git push origin master 2、(方式二)在本地建立空白gh-pages分支并同步到GitHub 仓库中建立一个名为 gh-pages 的分支。 git checkout --orphan gh-pages # 该命令会创建gh-pages分支，并且该分支下有master分支下的所有文件 git rm -rf * # 删除master分支带过来的文件(“git rm -rf *\"命令并不能删除隐藏文件,可使用““git rm -rf .\"命令) rm '.gitignore' # 如果master中.gitignore文件,可删除 # 如果没有任何文件提交的话，分支是看不到的，可以创建一个新文件后再次提交则新创建的gh-pages分支就会显示出来。 echo \"# MacOS\\n*.DS_Store\" > .gitignore git add . git commit -m \"init commit\" git push origin gh-pages 当本地gh-pages分支同步到远程GitHub仓库分支后，在GitHub分支进行验证 3、在本地构建静态页面HTML文件 在Master分支下使用“gitbook build”命令生产静态页面HTML文件 gitbook install # 该命令后会下载构建gitbook所需的插件及资源到当前目录“node_modules”下 gitbook build . [静态页面HTML文件输出文件夹] # 该命令默认会将静态页面HTML文件输出到当前目录\"_book\"下。如果指定，则输出到指定目录。 4、(方式二)使用gh-pages插件将本地静态页面HTML文件推送到远处仓库gh-pages分支 本地安装gh-pages插件 npm install gh-pages -g gh-pages命令详解 Usage: gh-pages [options] Options: -V, --version output the version number -d, --dist Base directory for all source files -s, --src Pattern used to select which files to publish (default: \"**/*\") -b, --branch Name of the branch you are pushing to (default: \"gh-pages\") -e, --dest Target directory within the destination branch (relative to the root) (default: \".\") -a, --add Only add, and never remove existing files -x, --silent Do not output the repository url -m, --message commit message (default: \"Updates\") -g, --tag add tag to commit --git Path to git executable (default: \"git\") -t, --dotfiles Include dotfiles -r, --repo URL of the repository you are pushing to -p, --depth depth for clone (default: 1) -o, --remote The name of the remote (default: \"origin\") -u, --user The name and email of the user (defaults to the git config). Format is \"Your Name \". -v, --remove Remove files that match the given pattern (ignored if used together with --add). (default: \".\") -n, --no-push Commit only (with no push) -f, --no-history Push force new commit without parent history -h, --help output usage information 推送静态页面HTML文件 gh-pages -d _book 5、GitHub配置gh-pages服务 6、(可选)自定义域名访问GitHub Pages托管的Gitbook 此时你可以通过https:/Github用户名.github.io/仓库名访问你的Gitbook啦。但是如果你在阿里云上购买的有自己的域名，可直接将域名指定到gitbook page域名，使用自己的域名访问。 在GitHub中设置仓库的CNAME 直接在GitHub中进行设置 (推荐)在master分支代码根路径下创建CNAME文件，文件中写入你自定义的域名(每个CNAME文件能且只能指定一个域名)。例如：gitbook.curiouser.top。 Github读取你的CNAME文件之后，Github服务器会设置gitbook.curiouser.top为你的主域名，然后将rationalmonster.github.ioo重定向到gitbook.curiouser.top 在阿里云上添加域名解析记录 等待3～10分钟，通过自定义域名访问自己的gitbook。 问题 gitbook serve时，偶尔不规律性地出现编译错误,而且每次出现的错误文件还可能不一样，实在是头疼得很，每次修改要编译多次才能成功 修改 C:\\Users\\当前用户名\\.gitbook\\versions\\当前使用的gitbook版本\\lib\\output\\website\\copyPluginAssets.js文件中的112行，将confirm: true改为confirm: false 参考链接 https://www.jianshu.com/p/f38d8ff999cb?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/telegram-Bot机器.html":{"url":"origin/telegram-Bot机器.html","title":"Telegram机器人","keywords":"","body":"Telegram Bot机器人 一、简介 Telegram Bot是运行在Telegram内部的第三方应用程序，相当于Telegram的一个特殊账户。 用户可以向Telegram Bot发送消息，命令和内联请求等方式与Telegram Bot人进行交互，而Telegram Bot开发者可以通过Telegram Bot API，用https请求方式来控制机器人 二、创建 客户端搜索\"Botfather\" 查看帮助 发送\"/newboot\"来创建Bot机器人,根据提示一步一步进行.(当设置用户名时) TOKEN 一定要保护好！以后接口访问都要用到！ 三、API Telegram有两种api，一种是bot api，一种是telegram api。bot api是基于http访问，telegram api是基于mtproto访问，访问需要加密，相对要复杂一些。后者也可以实现发送消息等功能 可使用PostMan或者Curl等工具发送HTTPS请求调用Bot的API。 当时用Curl命令时可使用\"-x\"参数设置代理。例如“curl -x 127.0.0.1:3128 -sk https://www.google.com” 四、Bot API Bot API文档链接 1. Bot API相关信息 Bot API支持GET和POST方法的HTTPS请求，URL格式为: \"https://api.telegram.org/bot[你的bot机器人Token]/方法名\" 例如：https://api.telegram.org/bot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/getMe 支持一下几种传参方式： URL query string application/x-www-form-urlencoded application/json (except for uploading files) multipart/form-data (use to upload files) Bot机器人将返回JSON格式的对象，里面会包含返回状态信息 注意: API方法大小写敏感 请求格式必须是UTF-8编码 2. 示例 使用PostMan给Bot机器人发送消息 使用Curl命令给Bot机器人发送消息 curl -x 梯子IP地址 -sk \\ -X POST \\ https://api.telegram.org/bot90****93:AAF***RfFma8/sendMessage \\ -d 'chat_id=623***17' \\ -d 'parse_mode=Markdown' \\ -d 'text=*Jenkins '$BUILD_NUMBER' *' 3. 支持的消息格式 MarkDown风格 *bold text* _italic text_ [inline URL](http://www.example.com/) [inline mention of a user](tg://user?id=123456789) `inline fixed-width code` ​```block_language pre-formatted fixed-width code block ​ ``` HTML风格 *bold text* _italic text_ [inline URL](http://www.example.com/) [inline mention of a user](tg://user?id=123456789) `inline fixed-width code` ​```block_language pre-formatted fixed-width code block ​ ``` 标签不能嵌套 所有不属于标签或HTML实体的' '和' & '符号必须替换为相应的HTML实体 (\"\"对应\"\\>\"、\"\\&\"对应\"\\&\") 支持所有数字类型的HTML实体 该API目前仅支持以下命名的HTML实体:' '、' & '和' \" ' 4. 支持的方法 Bot API方法 描述 getMe sendMessage 发送文本信息,支持Markdown、HTML格式化的文本信息 forwardMessage sendPhoto 发送图片 sendAudio 发送音频，最大50 MB sendDocument 发送文档，最大50 MB sendVideo 发送视频，最大50 MB sendAnimation 发送动图，最大50 MB(支持无声音的GIF或H.264/MPEG-4 AVC格式动图) sendVoice 发送录音，最大50 MB sendVideoNote sendMediaGroup sendLocation 发送定位 editMessageLiveLocation stopMessageLiveLocation sendVenue sendContact 发送名片 sendPoll 发送投票 sendChatAction getUserProfilePhotos getFile kickChatMember unbanChatMember restrictChatMember promoteChatMember setChatPermissions exportChatInviteLink setChatPhoto deleteChatPhoto setChatTitle 设置聊天室标题 setChatDescription 设置聊天室描述 pinChatMessage unpinChatMessage leaveChat 离开聊天室 getChat 查找聊天室 getChatAdministrators 获取聊天室管理员 getChatMembersCount 获取聊天室成员个数 getChatMember 获取聊天室成员 setChatStickerSet deleteChatStickerSet answerCallbackQuery Inline mode methods Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openvpn-server.html":{"url":"origin/openvpn-server.html","title":"OpenVPN Server","keywords":"","body":"OpenVPN 一、简介 OpenVPN 是一个基于 OpenSSL 库的应用层 VPN 实现。和传统 VPN 相比，它的优点是简单易用。 [1] OpenVPN允许参与建立VPN的单点使用共享金钥，电子证书，或者用户名/密码来进行身份验证。它大量使用了OpenSSL加密库中的SSLv3/TLSv1 协议函式库。OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Windows 2000/XP/Vista上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软件包兼容。 OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。 OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。 二、安装 1、在Synology上安装部署OpenVPN 2、使用脚本在Linux服务器搭建 GitHub有个脚本项目专门安装OpenVPN server，地址：https://github.com/Nyr/openvpn-install ，但是功能过少。为了便于管理openvpn,基于此脚本进行了功能优化，github地址：https://github.com/RationalMonster/install-manage-openvpn ，以下为优化的功能点： 汉化 增加选择客户端分配IP地址池网段的功能 增加用户名密码验证脚本 增加配置SMTP发送邮件的功能 增加创建用户后将用户名密码及配置文件等信息通过SMTP邮件服务发送到用户邮箱 去除不必要的脚本代码 首次运行该脚本是安装openvpn服务，再次运行可执行其他服务。 注意 所有的iptables规则配置都是由systemD服务openvpn-iptables.service(/etc/systemd/system/openvpn-iptables.service)进行配置的。 [Unit] Before=network.target [Service] Type=oneshot ExecStart=/sbin/iptables -t nat -A POSTROUTING -s 10.8.6.0/24 ! -d 10.8.6.0/24 -j SNAT --to 192.168.1.2 ExecStart=/sbin/iptables -I INPUT -p udp -d 192.168.1.2 --dport 30668 -j ACCEPT ExecStart=/sbin/iptables -I INPUT -p tcp -s 10.8.6.0/24 -d 192.168.1.2 --dport 9092 -j ACCEPT ExecStart=/sbin/iptables -I FORWARD -p tcp -s 10.8.6.0/24 -d 192.168.1.3 ! --destination-port 6443 -j DROP ExecStart=/sbin/iptables -A INPUT -s 10.8.6.0/24 -d 192.168.1.2 -j DROP ExecStart=/sbin/iptables -I INPUT -p tcp -s 10.8.6.166/32 -d 192.168.1.2 --dport 22 -j ACCEPT ExecStop=/sbin/iptables -D FORWARD -p tcp -s 10.8.6.0/24 -d 192.168.1.3 ! --destination-port 6443 -j DROP ExecStop=/sbin/iptables -t nat -D POSTROUTING -s 10.8.6.0/24 ! -d 10.8.6.0/24 -j SNAT --to 192.168.1.2 ExecStop=/sbin/iptables -D INPUT -p udp -d 192.168.1.2 --dport 30668 -j ACCEPT ExecStop=/sbin/iptables -D INPUT -p tcp -s 10.8.6.0/24 -d 192.168.1.2 --dport 9092 -j ACCEPT ExecStop=/sbin/iptables -D INPUT -p tcp -s 10.8.6.166/32 -d 192.168.1.2 --dport 22 -j ACCEPT ExecStop=/sbin/iptables -D INPUT -s 10.8.6.0/24 -d 192.168.1.2 -j DROP RemainAfterExit=yes [Install] WantedBy=multi-user.target 三、Openvpn Access Server OpenVPN 的商业收费版本 OpenVPN Access Server，其免费的 license 可以支持2个 VPN 用户的同时在线， 1、Ubuntu ①APT apt update && apt -y install ca-certificates wget net-tools gnupg wget -qO - https://as-repository.openvpn.net/as-repo-public.gpg | apt-key add - echo \"deb http://as-repository.openvpn.net/as/debian bionic main\">/etc/apt/sources.list.d/openvpn-as-repo.list apt update && apt -y install openvpn-as ②Deb包（推荐） 使用APT安装时、服务器可能需要能翻墙。 deb包手动下载地址： https://openvpn.net/downloads/openvpn-as-latest-ubuntu18.amd_64.deb https://openvpn.net/downloads/openvpn-as-bundled-clients-latest.deb apt update apt install -y liblzo2-2 bridge-utils net-tools python-pyrad python-serial libsasl2-2 iproute2 sqlite3 libsqlite3-0 iptables liblz4-1 python-pkg-resources python-mysqldb libmariadbclient18 libssl1.1 dpkg -i openvpn-as-bundled-clients-11.deb openvpn-as_2.8.5-f4ad562b-Ubuntu18_amd64.deb 2、CentOS/Redhat ①YUM yum -y install https://as-repository.openvpn.net/as-repo-centos7.rpm # 或者 yum -y install https://as-repository.openvpn.net/as-repo-centos8.rpm yum -y install openvpn-as 3、使用OVA模版在ESXI上部署 官方文档：https://openvpn.net/vpn-server-resources/deploying-the-access-server-appliance-on-vmware-esxi/ 最新ESXI OVA部署模板下载地址：https://openvpn.net/downloads/openvpn-as-latest-vmware.ova 支持在ESXI 5.0+ 上部署。直接使用vSphere Client客户端导入OVA文件创建虚拟机，步骤省略。 使用OVA部署OpenVPN Server的License只支持两个用户同时在线 默认使用sqlite存储数据，支持将数据转换存储到MySQL中 支持对接LDAP认证 虚拟机基本信息 1vCPU 1GB内存 512MB交换内存 OS版本：Ubuntu 18.04.3 Server LTS x64 默认SSH用户密码：root / openvpnas 软件根路径：/usr/local/openvpn_as 日志目录：/usr/local/openvpn_as/init.log 重新配置命令：/usr/local/openvpn_as/bin/ovpn-init 已安装VM Tools，未安装curl 4、安装后注意 ①修改时区为CST。默认时区为US(Pacific - Los Angeles) timedatectl set-timezone \"Asia/Shanghai\" # 设置时区 timedatectl status # 查看当前的时区状态 date -R # 查看时区 ②设置openvpn用户密码（默认没有设置） passwd openvpn ③(可选)设置静态IP地址（默认DHCP） nano /etc/netplan/01-netcfg.yaml # 配置模板 network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no # ip设置为192.168.79.2 addresses: [192.168.70.2/24] gateway4: 192.168.70.254 nameservers: addresses: [192.168.70.254] netplan apply ④Web UI访问地址 普通用户访问地址：https://openvpnas-ip:943 管理员访问地址 ：https://openvpnas-ip:943/admin （默认用户openvpn，密码初始没有，需设置） 四、OpenVPN服务端配置 push \"route 192.168.1.0 255.255.255.0\" push \"route 10.8.0.0 255.255.255.0\" push \"dhcp-option DNS 192.168.1.7\" dev tun management 127.0.0.1 1195 server 10.8.0.0 255.255.255.0 client-config-dir ccd dh keys/ca..pm ca keys/ca.crt cert keys/server.crt key keys/server.key max-clients 5 comp-lzo persist-tun persist-key verb 3 #log-append /var/log/openvpn.log keepalive 10 60 reneg-sec 0 plugin /var/packages/VPNCenter/target/lib/radiusplugin.so /var/packages/VPNCenter/target/etc/openvpn/radiusplugin.cnf client-cert-not-required username-as-common-name duplicate-cn status /tmp/ovpn_status_2_result 30 status-version 2 proto tcp6-server port 19382 cipher AES-256-CBC auth RSA-SHA256 五、客户端连接配置 不管是在Synology还是ESXI上安装的OpenVPN Server，都提供下载配置文件的连接。下载好配置文件后，可直接使用各个平台下的客户端直接导入打开 官方提供了各种平台下的客户端程序并提供了对应的文档说明 各客户端官方文档：https://openvpn.net/vpn-server-resources/connecting/ MacOS客户端tunnelblick MacOS上有好多客户端可以连接OpenVPN，功能大同小异。同时官方也有自己的macOS客户端OpenVPN Connect Client。但是推荐tunnelblick（官方也推荐），可同时连接多个OpenVPN Server 官方客户端文档：https://openvpn.net/vpn-server-resources/connecting-to-access-server-with-macos/ OpenVPN Connect Client for MacOS下载地址：https://openvpn.net/downloads/openvpn-connect-v3-macos.dmg Tunnelblick下载地址：https://github.com/Tunnelblick/Tunnelblick/releases Windows客户端OpenVPN GUI OpenVPN官网提供Windows平台客户端OpenVPN GUI。 只需将配置文件放在C:\\Users\\当前用户\\OpenVPN\\config文件下即可。(~\\OpenVPN\\config需手动创建) 官方客户端一次只能连一个服务端，如果有连接多个服务端的话，需要来回切换。 官方文档：https://openvpn.net/vpn-server-resources/connecting-to-access-server-with-windows/ 下载地址：https://openvpn.net/community-downloads/ Android 安卓手机平台官方虽说也提供客户端，但是只能在Google Play Store商店中下载，同时还一次只能连一个服务端。所以我们只好使用第三方客户端ics-openvpn GitHub地址：https://github.com/schwabe/ics-openvpn APK下载地址：http://plai.de/android/ IOS 对于Apple IOS手机客户端，官方APP名为OpenVPN Connect。而且一次只能连一个服务端。同时国内App Store还下不到。你说气不气。其他第三方客户端大多收费。幸好手机不是Iphone。这个就不管了！ Linux OpenVPN协议不是Linux内置的协议。因此，需要一个客户端程序，该程序可以处理捕获OpenVPN隧道发送的流量，并将其加密并将其传递给OpenVPN服务器。当然，反之亦然，解密返回的流量。因此，需要一个客户端程序。在大多数Linux发行版中该软件包简称为 openvpn(OpenVPN 服务端的程序包为 openvpnas或 openvpn-as)。 # CentOS yum install -y openvpn # Ubuntu apt-get install -y openvpn openvpn支持同时连接多个OpenVPN服务器，并且还带有一个服务组件，该组件可以自动和静默地启动在/etc/openvpn中找到的任何自动登录配置文件。可以将该服务组件设置为使用Linux发行版中提供的工具在启动时自动启动。在Ubuntu和Debian上，当您安装 openvpn软件包时，它会自动配置为在引导时启动。将client.ovpn配置文件放在 /etc/openvpn/中并重命名该文件。它必须以.conf结尾 作为文件扩展名。确保重新启动后可以运行服务守护程序，然后再重新启动系统即可。自动登录类型配置文件将自动被提取，并且连接将自动启动。您可以通过检查例如ifconfig命令的输出来验证这一点 ，然后您将在列表中看到 tun0网络适配器。 手动指定配置文件： openvpn --config client.ovpn --auth-user-pass --daemon 命令行客户端缺少的一项主要功能是能够自动实现VPN服务器推送的DNS服务器，但是需要您安装DNS管理程序，例如resolvconf或openresolv，并且它可能与操作系统中的现有网络管理软件冲突，也可能不冲突。但在Ubuntu和Debian上，openvpn软件包随附了 /etc/openvpn/update-resolv-conf 脚本，该脚本处理这些操作系统的DNS实现。只需要在客户端配置文件中设置连接建立断开时执行它。 # 编辑客户端配置文件 vi client.ovpn script-security 2 # 设置执行额外的脚本 up /etc/openvpn/update-resolv-conf # 设置在连接建立时要执行的脚本路径 down /etc/openvpn/update-resolv-conf # 设置在连接断开时要执行的脚本路径 六、openvpn功能设置 1、分配指定IP地址给客户端用户 ①OpenVPN服务端配置文件添加 client-config-dir ccd ②新建ccd目录及客户端文件 新建ccd目录，在ccd目录下新建以用户名命名的文件。并且通过ifconfig-push分配地址，注意这里需要分配两个地址，一个是客户端本地地址，另一个是服务器的ip端点。 mkdir ccd echo ”ifconfig-push 10.8.0.9 10.8.0.10\" >> ccd/vpn_test_user 每个端点的IP地址对的最后8位字节必须取自下面的集合 [1, 2] [5, 6] [9, 10] [13, 14] [17, 18] [21, 22] [25, 26] [29, 30] [33, 34] [37, 38] [41, 42] [45, 46] [49, 50] [53, 54] [57, 58] [61, 62] [65, 66] [69, 70] [73, 74] [77, 78] [81, 82] [85, 86] [89, 90] [93, 94] [97, 98] [101,102] [105,106] [109,110] [113,114] [117,118] [121,122] [125,126] [129,130] [133,134] [137,138] [141,142] [145,146] [149,150] [153,154] [157,158] [161,162] [165,166] [169,170] [173,174] [177,178] [181,182] [185,186] [189,190] [193,194] [197,198] [201,202] [205,206] [209,210] [213,214] [217,218] [221,222] [225,226] [229,230] [233,234] [237,238] [241,242] [245,246] [249,250] [253,254] 客户端连接验证地址分配 utun2: flags=8051 mtu 1500 inet 10.8.0.9 --> 10.8.0.10/32 utun2 2、设置用户名密码加证书的方式登录认证 ①增加openvpn服务端配置 在/etc/openvpn/server/server.conf中追加一下内容 # ....省略 auth-user-pass-verify /etc/openvpn/server/checkpsw.sh via-env username-as-common-name script-security 3 client-config-dir ccd ②创建用户名密码验证脚本 /etc/openvpn/server/checkpsw.sh #!/bin/sh PASSFILE=\"/etc/openvpn/server/psw-file\" LOG_FILE=\"/etc/openvpn/server/openvpn-password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 ③创建用户密码文件 新增/etc/openvpn/server/psw-file # 一行一个账号 用户名 密码 $ chmod 400 /etc/openvpn/server/psw-file $ chown nobody.nobody /etc/openvpn/server/psw-file ④(可选)客户端openvpn配置文件追加配置 auth-user-pass 3、使用iptables限制用户的访问 在客户端连接到openvpn服务端后，针对哪些客户端用户可以访问哪些网段的服务，一般是使用openvpn服务端所在服务器的iptables进行控制。有以下两种重要的常见场景都是使用iptables进行实现的： ①场景一：作为局域网的网络入口跳板机，SNAT转发流量到其他内网服务器 例如在一些公有云的服务器，由于公网IP太贵，不可能给每一台服务都分配，同时也不安全。只要给安装openvpn的服务器分配一个公网IP地址，然后就可以使用iptables的SNAT功能进行网络流量转发，就能访问openvpn所在内网其他服务器上的服务 第一步：VPN服务器设置iptables进行SNAT流量转发 iptables -t nat -A POSTROUTING -s 10.6.8.0/24 -d 192.168.1.0/24 -j SNAT --to 192.168.1.2 # 上述配置通俗地解释为: # 所有分配了IP地址为10.8.6.0/24的客户端用户要想访问192.168.1.0/24的地址，都将其访问网络包中的源地址转换为192.168.1.2，这样用户的访问流量将以192.168.1.2的名义发出。进而能让192.168.1.2能访问的网段主机，客户端也能访问 第二步：配置vpn服务端给客户端推送路由 在/etc/openvpn/server/server.conf中添加 push \"route 192.168.1.0 255.255.255.0\" 这样客户端就会在连接到vpn服务端后自动在本机路由表中添加一条路由 $ ip a # ...省略... utun3: flags=8051 mtu 1500 inet 10.6.8.5 --> 10.6.8.6/32 utun3 $ ip route show # ...省略... 192.168.1.0/24 via 10.6.8.6 dev utun3 ②场景二：细分指定用户只能访问特定的服务 默认配置下，所有客户端都可以访问服务端配置中的指定网络段。但是在实际使用场景中，需要限制指定客户端访问指定网络，限制其访问某些服务。例如：只允许开发人员访问开发网络段中的服务器，测试人员只能访问测试网络段的服务器资源等等。 iptables -t nat -A POSTROUTING -s 10.6.8.0/24 ! -d 10.6.8.0/24 -j SNAT --to 192.168.1.2 iptables -I INPUT -s 10.6.8.166/32 -d 192.168.1.0/24 -j ACCEPT iptables -I INPUT -s 10.6.8.0/24 -d 192.168.1.5-192.168.1.6 ! --dport 22 -j ACCEPT iptables -I FORWARD -p tcp -s 10.6.8.0/24 -d 192.168.1.7 ! --destination-port 6443 -j DROP 4、iptables规则的维护 ①查看规则 以number的方式查看规则，一条一条的出来，然后我们根据号码来删除哪一条规则 iptables -L FORWARD --line-numbers iptables -L INPUT --line-numbers # 查看POSTROUTING链nat表中的规则 iptables -L POSTROUTING -t nat ②删除指定的规则 iptables -D FORWARD 1 #删除指定链指定表中的规则 iptables -D POSTROUTING -t nat -s 10.8.6.0/24 ! -d 10.8.6.0/24 -j SNAT --to 192.168.1.2 ③删除所有规则 iptables -F 5、远程管理OpenVPN（简单密码认证） 服务端配置文件/etc/openvpn/server/server.conf追加management 服务器IP地址 7505 密码文件(不要使用localhost)，然后重启openvpn服务。 echo $(date +%s)$RANDOM | md5sum | base64 | head -c 10 > /etc/openvpn/server/management-psw-file echo \"management 服务器IP地址 7505 /etc/openvpn/server/management-psw-file\" >> /etc/openvpn/server/server.conf systemctl restart openvpn-server@server.service systemctl status openvpn-server@server.service netstat -lanp|grep 7505 此时就可以远程使用Telnet与openvpn进程进行交互。（详细信息参考：https://openvpn.net/community-resources/management-interface/） $ telnet 远程openvpn服务器IP地址 7505 # ENTER PASSWORD: 输入管理端口的密码 >INFO:OpenVPN Management Interface Version 1 -- type 'help' for more info # 输入‘help’命令查看命令 help 命令 auth-retry t : Auth failure retry mode (none,interact,nointeract). bytecount n : Show bytes in/out, update every n secs (0=off). echo [on|off] [N|all] : Like log, but only show messages in echo buffer. exit|quit : 退出当前会话 forget-passwords : Forget passwords entered so far. help : 打印帮助信息 hold [on|off|release] : Set/show hold flag to on/off state, or release current hold and start tunnel. kill cn : 杀掉通用名为cn的客户端 kill IP:port : 杀掉来自指定ip和端口的客户端。 load-stats : 显示全局状态信息 log [on|off] [N|all] : 打开/关闭时实的日志显示 + 显示最后N条或者'所有' 历史日志. mute [n] : Set log mute level to n, or show level if n is absent. needok type action : Enter confirmation for NEED-OK request of 'type',where action = 'ok' or 'cancel'. needstr type action : Enter confirmation for NEED-STR request of 'type',where action is reply string. net : (Windows only) Show network info and routing table. password type p : Enter password p for a queried OpenVPN password. remote type [host port] : Override remote directive, type=ACCEPT|MOD|SKIP. proxy type [host port flags] : Enter dynamic proxy server info. pid : 显示openvpn的进程号 pkcs11-id-count : Get number of available PKCS#11 identities. pkcs11-id-get index : Get PKCS#11 identity at index. client-auth CID KID : Authenticate client-id/key-id CID/KID (MULTILINE) client-auth-nt CID KID : Authenticate client-id/key-id CID/KID client-deny CID KID R [CR] : Deny auth client-id/key-id CID/KID with log reason text R and optional client reason text CR client-kill CID [M] : Kill client instance CID with message M (def=RESTART) env-filter [level] : Set env-var filter level client-pf CID : Define packet filter for client CID (MULTILINE) rsa-sig : Enter an RSA signature in response to >RSA_SIGN challenge Enter signature base64 on subsequent lines followed by END certificate : Enter a client certificate in response to >NEED-CERT challenge Enter certificate base64 on subsequent lines followed by END signal s : 发送信号给openvpn进程, s = SIGHUP|SIGTERM|SIGUSR1|SIGUSR2. SIGUSR1 – 有条件的重启，非root用户重启OpenVPN进程 SIGHUP – 重启 SIGUSR2 – 输出连接状态到log文件或者系统log SIGTERM, SIGINT – 退出 state [on|off] [N|all] : 跟log一样,但是静态显示。 status [n] : 显示现在进程的状态信息。格式：#n. test n : Produce n lines of output for testing/debugging. username type u : Enter username u for a queried OpenVPN username. verb [n] : Set log verbosity level to n, or show if n is absent. version : 显示openvpn版本信息 6、客户端连接状态钉钉通知 openvpn服务端配置文件/etc/openvpn/server/server.conf中追加以下内容，然后重启openvpn服务。 -----省略------- auth-user-pass-verify openvpn-utils.sh via-env -----省略------- client-connect openvpn-utils.sh client-disconnect openvpn-utils.sh 此时客户连接或断开了openvpn服务端都会执行对应的脚本，可在脚本中通过curl命令发送信息到对应webhook的钉钉机器人。由于已经有一个用于验证用户名密码的脚本(/etc/openvpn/server/server.conf中的auth-user-pass-verify openvpn-utils.sh via-env)，可在其中通过判断调用脚本时的openvpn指令类型来添加功能。示例： 在/etc/openvpn/server/openvpn-utils.sh添加用于发送钉钉通知的逻辑。至于脚本中要使用到的变量，当客户端连接或断开进而触发执行脚本时会将此时客户端所处的信息放至进程作用域的环境变量中，可以直接在脚本中引用。具体哪些变量可用，参考：https://openvpn.net/community-resources/reference-manual-for-openvpn-2-4/ (由于篇幅较长，网页搜索“environmental variables”)。 注意有些环境变量只有在对应指令调用时才有。 #!/bin/sh PASSFILE=\"/etc/openvpn/server/psw-file\" LOG_FILE=\"/etc/openvpn/server/openvpn-authorized.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` Ding_Webhook_Token= Ding_Webhook=\"https://oapi.dingtalk.com/robot/send?access_token=\"$Ding_Webhook_Token swap_seconds () { SEC=$1 [ \"$SEC\" -le 60 ] && echo \"$SEC秒\" [ \"$SEC\" -gt 60 ] && [ \"$SEC\" -le 3600 ] && echo \"$(( SEC / 60 ))分钟$(( SEC % 60 ))秒\" [ \"$SEC\" -gt 3600 ] && echo \"$(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\" } if [ $script_type = 'user-pass-verify' ] ; then if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \"${PASSFILE}\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\"${username}\", password=\"${password}\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\"${username}\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\"${username}\", password=\"${password}\".\" >> ${LOG_FILE} exit 1 fi if [ $script_type = 'client-connect' ] ; then curl -s \"$Ding_Webhook\" \\ -H 'Content-Type: application/json' \\ -d ' { \"msgtype\": \"markdown\", \"markdown\": { \"title\": \"'$common_name'连接到了OpenVPN\", \"text\": \"## '$common_name'连接到了OpenVPN\\n> #### **连接时间**: '\"$TIME_STAMP\"'\\n> #### **IP + 端口**: '$trusted_ip':'$trusted_port'\\n> #### **端对端IP**: '$ifconfig_pool_remote_ip' '$ifconfig_local'\" }, \"at\": { \"isAtAll\": true } }' fi if [ $script_type = 'client-disconnect' ]; then duration_time=`swap_seconds $time_duration` curl -s \"$Ding_Webhook\" \\ -H 'Content-Type: application/json' \\ -d ' { \"msgtype\": \"markdown\", \"markdown\": { \"title\": \"'$common_name'断开了OpenVPN\", \"text\": \"## '$common_name'断开了OpenVPN\\n> #### **断开时间**: '\"$TIME_STAMP\"'\\n> #### **IP + 端口**: '$trusted_ip':'$trusted_port'\\n> #### **端对端IP**: '$ifconfig_pool_remote_ip' '$ifconfig_local'\\n> #### **持续时间**: '$duration_time'\" }, \"at\": { \"isAtAll\": true } }' fi 7、客户端间进行互联 如果需要客户端进行互联，openvpn服务端配置文件/etc/openvpn/server/server.conf中追加以下内容，然后重启openvpn服务。 client-to-client 此时，openvpn服务端等同路由器在维护路由表。客户端之间就可以通过各自分配的虚拟IP进行通信了。比如有两个客户端, 各自使用不同的网络途径连接到服务端，各自分配VIP地址分别为10.8.0.6和10.8.0.10。此时两者就可以通过虚拟IP进行通信了 注意 在客户端可能会在TUN设备上看到两个地址, 其中一个是客户端面的VIP, 另外一个是网关. 比如下例中 utun3: flags=8051 mtu 1500 inet 10.8.0.6 --> 10.8.0.5/32 utun3 10.8.0.6是VIP, 而10.8.0.5是网关. 在互联时, 需要使用VIP而不是网关地址. 附录 1、可调用脚本的OpenVPN配置参数 iproute cmd route-up cmd route-pre-down cmd up cmd down cmd client-connect cmd client-disconnect cmd auth-user-pass-verify cmd method learn-address cmd tls-verify cmd ipchange cmd down-pre 以上参数，可在脚本中通过script_type进行甄别。 script_type，up, down, ipchange, route-up, tls-verify, auth-user-pass-verify, client-connect, client-disconnect, learn-address. 参考 http://www.fblinux.com/?p=1181 http://www.linuxfly.org/post/86/ https://www.aikaiyuan.com/11839.html https://www.hotbak.net/key/32f744eec5330289d21a981ecf2d595a_29.html https://lesca.me/archives/iptables-examples.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-15 14:34:13 "},"origin/idrac.html":{"url":"origin/idrac.html","title":"iDRAC","keywords":"","body":"Dell服务器远程管理卡iDRAC 一、简介 二、Web页面 三、命令行CLI show [] [] [] [== ] set [] [] = cd [] [] create [] [=] [=] delete [] exit [] reset [] [] start [] [] stop [] [] version [] help [] [] load -source [] [] dump -destination [] [] Racadm help [subcommand] -- display usage summary for a subcommand arp -- display the networking ARP table clearasrscreen -- clear the last ASR (crash) screen closessn -- close a session clrraclog -- clear the RAC log clrsel -- clear the System Event Log (SEL) config -- modify RAC configuration properties coredump -- display the last RAC coredump coredumpdelete -- delete the last RAC coredump fwupdate -- update the RAC firmware getconfig -- display RAC configuration properties getled -- Get the state of the LED on a module. getniccfg -- display current network settings getraclog -- display the RAC log getractime -- display the current RAC time getsel -- display records from the System Event Log (SEL) getssninfo -- display session information getsvctag -- display service tag information getsysinfo -- display general RAC and system information gettracelog -- display the RAC diagnostic trace log getversion -- Display the current version details getuscversion -- display the current USC version details ifconfig -- display network interface information netstat -- display routing table and network statistics ping -- send ICMP echo packets on the network ping6 -- send ICMP echo packets on the network racdump -- display RAC diagnostic information racreset -- perform a RAC reset operation racresetcfg -- restore the RAC configuration to factory defaults remoteimage -- make a remote ISO image available to the server serveraction -- perform system power management operations setniccfg -- modify network configuration properties setled -- Set the state of the LED on a module. sshpkauth -- manage SSH PK authentication keys on the RAC sslcertview -- view SSL certificate information sslcsrgen -- generate a certificate CSR from the RAC sslEncryptionStrength -- Display or modify the SSL Encryption strength. sslresetcfg -- resets the web certificate to default and restarts the web server. testemail -- test RAC e-mail notifications testkmsconnectivity -- test KMSConnectivity testtrap -- test RAC SNMP trap notifications usercertview -- view user certificate information 1、服务管理 racadm getconfig -g cfgracTuning # （cfgRacTuneWebserverEnable 卡的WEB 服务未启动，0：表示未启动；1： 表示启动） racadm config -g cfgRacTuning -o cfgRacTuneWebServerEnable 1 服务开启成功 2、会话管理 # 获取当前活动的会话 racadm help getssninfo 四、客户端racadm Docker客户端： https://hub.docker.com/r/prabhakarpujeri/racadm-docker/tags?page=1&ordering=last_updated https://github.com/prabhakarpujeri/racadm-docker/blob/master/Dockerfile 参考 https://thornelabs.net/posts/dell-idrac-racadm-commands-and-scripts.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-04 19:05:21 "},"origin/vsphere-esxi.html":{"url":"origin/vsphere-esxi.html","title":"ESXI 管理常用命令","keywords":"","body":"一、ESXI 管理常用命令 1、esxcli 获取基础信息 vmware -v # 看你的esx版本 VMware ESXi 5.0.0 build-469512 esxcfg-info -a # 显示所有ESX相关信息 esxcfg-info -w # 显示esx上硬件信息 service mgmt-vmware restart # 重新启动vmware服务 esxcfg-vmknic -l # 查看宿主机IP地址 esxcli hardware cpu list # cpu信息 Brand，Core Speed， esxcli hardware cpu global get # cpu信息 （CPU Cores） esxcli hardware memory get # 内存信息 内存 Physical Memory esxcli hardware platform get # 硬件型号，供应商等信息,主机型号,Product Name 供应商,Vendor Name esxcli hardware clock get # 当前时间 esxcli system version get # 查看ESXi主机版本号和build号 esxcli system maintenanceMode set --enable yes # 将ESXi主机进入到维护模式 esxcli system maintenanceMode set --enable no # 将ESXi主机退出维护模式 esxcli system settings advanced list -d # 列出ESXi主机上被改动过的高级设定选项 esxcli system settings kernel list -d # 列出ESXi主机上被变动过的kernel设定部分 esxcli system snmp get | hash | set | test # 列出、测试和更改SNMP设定 esxcli vm process list # 利用esxcli列出ESXi服务器上VMs的World I(运行状态的) esxcli vm process kill -t soft -w WorldI # 利用esxcli命令杀掉VM vim-cmd hostsvc/hostsummary # 查看宿主机摘要信息 vim-cmd vmsvc/get.datastores # 查看宿主存储空间信息 vim-cmd vmsvc/getallvms # 列出所有虚拟机 vim-cmd vmsvc/power.getstate VMI # 查看指定VMI虚拟状态 vim-cmd vmsvc/power.shutdown VMI # 关闭虚拟机 vim-cmd vmsvc/power.off VMI # 如果虚拟机没有关闭，使用poweroff命令 vim-cmd vmsvc/get.config VMI # 查看虚拟机配置信息 esxcli software vib install -d /vmfs/volumes/datastore/patches/xxx.zip # 为ESXi主机安装更新补丁和驱动 esxcli network nic list # 列出当前ESXi主机上所有NICs的状态 esxcli network vm list # 列出虚拟机的网路信息 esxcli storage nmp device list # 理出当前NMP管理下的设备satp和psp信息 esxcli storage core device vaai status get # 列出注册到PS设备的VI状态 esxcli storage nmp satp set --default-psp VMW_PSP_RR --satp xxxx # 利用esxcli命令将缺省psp改成Round Robin 维护模式管理 esxcli system maintenanceMode {cmd} [cmd options] Available Commands: get 获取系统维护状态 set Enable or disable the maintenance mode of the system. -e|--enable 开启维护模式 (必须) -t|--timeout= 多少秒后进入维护模式 (默认0秒) -m|--vsanmode= 在主机进入维护模式(默认ensureObjectAccessibility)之前，VSAN服务必须执行 的操作。允许的值是: ensureObjectAccessibility: 在进入维护模式之前，从磁盘中提取数据以确保虚拟SAN集群中的对象可访问性。 evacuateAllData:在进入维护模式之前，从磁盘中撤离所有数据。 noAction:在进入维护模式之前，不要将虚拟SAN数据移出磁盘。 关机重启管理（必须进入维护模式） esxcli system shutdown {cmd} [cmd options] Available Commands: poweroff 断开电源 -d|--delay= 多少秒后关机，范围在10-4294967295 -r|--reason= 执行该操作的原因 reboot 重启系统 -d|--delay= 多少秒后关机，范围在10-4294967295 -r|--reason= 执行该操作的原因 系统时间管理 esxcli system time set [cmd options] Cmd options: -d|--day= Day -H|--hour= Hour -m|--min= Minute -M|--month= Month -s|--sec= Second -y|--year= Year 查看vswitch接口信息 esxcli network vswitch standard list vSwitch0 Name: vSwitch0 Class: etherswitch Num Ports: 4352 Used Ports: 10 Configured Ports: 128 MTU: 1500 CDP Status: listen Beacon Enabled: false Beacon Interval: 1 Beacon Threshold: 3 Beacon Required By: Uplinks: vmnic0 Portgroups: VM Network, synology-iscsi, Management Network 查看物理网络接口 esxcli network nic list Name PCI Device Driver Admin Status Link Status Speed Duplex MAC Address MTU Description ------ ------------ ------ ------------ ----------- ----- ------ ----------------- ---- --------------------------------------------------------- vmnic0 0000:01:00.0 bnx2 Up Up 100 Full 84:8f:69:e3:e3:98 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T vmnic1 0000:01:00.1 bnx2 Up Down 0 Half 84:8f:69:e3:e3:9a 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T vmnic2 0000:02:00.0 bnx2 Up Down 0 Half 84:8f:69:e3:e3:9c 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T vmnic3 0000:02:00.1 bnx2 Up Down 0 Half 84:8f:69:e3:e3:9e 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T 当前运行虚拟机列表 esxcli vm process list k8s118-node1 World ID: 35805 Process ID: 0 VMX Cartel ID: 35804 UUID: 56 4d 7f 25 bf 12 08 6c-21 36 96 29 58 61 80 bc Display Name: k8s118-node1 Config File: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398/k8s118-node1/k8s118-node1.vmx k8s118-node2 World ID: 35830 Process ID: 0 VMX Cartel ID: 35829 UUID: 56 4d c7 a7 05 ba e8 27-4a cf e3 3c 22 77 e2 27 Display Name: k8s118-node2 Config File: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398/k8s118-node2/k8s118-node2.vmx 创建datastore 创建NFS类型的Datastore # ESXI安装Synology NFS VAAI 参考附录2。ESXI安装完Synology NFS VAAI后再创建NFS类型Datastore后，会显示Datastore已支持硬件加速 esxcfg-nas -a synology-nfs-datastore -o 192.168.1.7 -s /volume2/ESXI # 删除Datastore esxcfg-nas -d synology-nfs-datastore esxcli storage vmfs extent list 查看卷信息 esxcli storage filesystem list Mount Point Volume Name UUID Mounted Type Size Free ------------------------------------------------- ----------- ----------------------------------- ------- ------ ------------ ------------ /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398 datastore1 5ad72ff6-920c8d20-3ee7-848f69e3e398 true VMFS-5 890131972096 156750577664 /vmfs/volumes/5ad93bed-3b0a5ee0-8d62-848f69e3e398 5ad93bed-3b0a5ee0-8d62-848f69e3e398 true vfat 4293591040 4257939456 /vmfs/volumes/5a613a7b-adc12ea1-59ec-8b00e5327863 5a613a7b-adc12ea1-59ec-8b00e5327863 true vfat 261853184 91848704 /vmfs/volumes/d7f0b67d-93f4d99c-13e2-fc95a98c5631 d7f0b67d-93f4d99c-13e2-fc95a98c5631 true vfat 261853184 91987968 /vmfs/volumes/5ad72ff5-a0c06292-f641-848f69e3e398 5ad72ff5-a0c06292-f641-848f69e3e398 true vfat 299712512 88342528 2、vim-cmd命令 列出所有虚拟机清单 vim-cmd vmsvc/getallvms 查看指定虚拟机设备信息 其中包括网卡型号、MC地址等信息。 vim-cmd vmsvc/device.getdevices 101 查看指定虚拟机配置 vim-cmd vmsvc/get.config 101 查看指定虚拟机摘要信息 vim-cmd vmsvc/get.summary 101 3、其他命令 查看虚拟网卡接口 esxcfg-vmknic -l 二、挂载本地磁盘上VMFS文件系统分区 esxcfg-volume -l |grep \"VMFS UUID/label\" # 会显示当前磁盘分区UUID esxcfg-volume -M UUID # 会将磁盘分区挂载到/vmfs/volumes/UUID下 # -M 重启后依旧会挂载。-m 重启后不会再挂载 应用实例： ​ 四块SAAS硬盘做的raid5。其中一块出现坏块，导致其上的VMWare系统奔溃。将硬盘位置打乱换了以后在开机期间Crtl+ R进入raid工具界面，显示raid正在重建。等重建完成后，还是无法进入VMWare。随后找到一块临时SATA即可的SSD硬盘插入光驱位，然后下载6.7的ESXI刻录到U盘中，将VMWare安装到SSD中。开机进入新的VMWare后，可以看到旧硬盘，分区依旧在，说明数据也在。此时需要将旧硬盘上的VMFS文件系统分区挂载到新的VMWare即可显示VM的数据存储。之后就可以使用各种工具备份导出VM啦，推荐使用群晖上的ABB。 三、ESXI网络抓包工具 https://www.virten.net/2015/10/esxi-network-troubleshooting-with-tcpdump-uw-and-pktcap-uw/ 1、tcpdump-uw 详细文档：https://kb.vmware.com/s/article/1031186 esxcfg-vmknic -l # 或者 esxcli network ip interface list tcpdump-uw -i vmk0 2、pktcap-uw 详细文档：https://kb.vmware.com/s/article/2051814?lang=zh_CN net-stats -l pktcap-uw --vmk vmk0 附录 1、ESXI VAAI 在虚拟化环境中，从资源角度来看，传统上的存储操作非常昂贵。与主机相比，存储设备可以更高效地执行克隆和快照等功能。VMware vSphere存储API阵列集成（VAAI），也称为硬件加速或硬件卸载API，是一组API，用于启用VMware vSphere ESXi主机与存储设备之间的通信。这些API定义了一组“存储原语”，它们使ESXi主机能够将某些存储操作卸载到阵列上，从而减少了ESXi主机上的资源开销，并可以显着提高存储密集型操作（如存储克隆，清零等）的性能。VAAI的目标是帮助存储供应商提供硬件帮助，以加快在存储硬件中更有效地完成的VMware I / O操作。 参考：https://www.vmware.com/techpapers/2012/vmware-vsphere-storage-apis-array-integration-10337.html 2、ESXI安装Synology NFS VAAI 下载地址：https://www.synology.cn/en-global/support/download/DS110+#utilities 安装参看：https://global.download.synology.com/download/www-res/dsm/Tools/NFSVAAIPlugin/README scp synonfs-vaai-plugin.vib root@192.168.1.103:/tmp esxcli software vib install -v /tmp/synonfs-vaai-plugin.vib --no-sig-check # 或 esxcli software vib install -d /tmp/synonfs-vaai-plugin.zip --no-sig-check 重启ESXI # 查看ESXI插件中是否已安装Synology NFS VAAI esxcli software vib list | more # 删除ESXI插件 esxcli software vib remove -n {PLUGIN_NAME} 参考：https://www.jonathanmedd.net/category/nfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-26 10:34:50 "},"origin/vSphere-vCenter.html":{"url":"origin/vSphere-vCenter.html","title":"vCenter","keywords":"","body":"vSphere vCenter Server 一、简介 vSphere Client虽然可以管理ESXI，但只能实现一些如创建虚拟机的简单的功能，而vsphere的高级功能都无法实现。更重要的是多台ESXI无法进行统一管理，为了解决这个问题，vsphere开发了一个非常重要的组件vCenter Server vCenter Server是安装在 Window 或 Linux 服务器里，用于管理一个或者多个ESXi服务器的工具。（可以安装在 ESXi 服务器的虚拟机里） vCenter Server Appliance：简称VCSA，是预配置的 Linux 虚拟机，针对在 Linux 上运行vCenter Server 及关联服务进行了优化，从6.0开始其实体形态是个.iso文件，需要在windows桌面上打开，通过配置过程将其安装到ESXi主机上。 二、安装部署 vCenter ISO镜像下载地址：https://my.vmware.com/cn/web/vmware/details?downloadGroup=VC60U3H&productId=491 官方文档：https://docs.vmware.com/cn/VMware-vSphere/6.7/com.vmware.vcenter.install.doc/GUID-11468F6F-0D8C-41B1-82C9-29284630A4FF.html 安装部署参考 https://blog.51cto.com/3701740/2326475 https://blog.csdn.net/weixin_44907813/article/details/99185102 https://blog.51cto.com/14227204/2418905 三、其他 1、找回vCenter 默认用户密码 在以管理员权限打开的CMD中切换至C:\\Program Files\\VMware\\vCenter Server\\vmdird目录下进入后执行vdcadmintool.exe。 提示让你输入 UPN：就是输入你创建时的登录用户名，一般默认为administrator@vSphere.local 输入后回车会给你一个 New password Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/esxi-synology-iscsi.html":{"url":"origin/esxi-synology-iscsi.html","title":"ESXI使用Synology的ISCSI存储","keywords":"","body":"ESXI使用Synology ISCSI存储 一、Synology NAS创建ISCSI存储 参考：iSCSI-简介配置使用 二、vSphere配置ISCSI适配器 参考：https://blog.csdn.net/minxihou/article/details/77233453 三、创建基于ISCSI的存储datastore 参考：https://blog.csdn.net/minxihou/article/details/77233453 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-07-10 11:32:24 "},"origin/synology-abb-vsphere.html":{"url":"origin/synology-abb-vsphere.html","title":"Synology Active Backup for Business备份管理vSphere ESXI VMs","keywords":"","body":"群辉ABB(Active Backup for Business)备份恢复vSphere中的VM 一、简介 对于vSphere中的虚拟机，除了ovftools导入导出为OVF文件进行备份外，Synology中的Active Backup for Business商业软件（以下简称ABB）也支持更多的备份功能。例如定时备份，版本管理等。 二、安装激活ABB 先在Synology的套件中心中搜索Active Backup for Business下载安装，过程省略。 1、获取产品序列号 在【控制面板】 -->【信息中心】找到【产品序列号】，复制下来。 2、登录 浏览器中输入： http://群辉主机地址:群辉主机端口/webapi/auth.cgi?api=SYNO.API.Auth&method=Login&version=1&account=用户名&passwd=密码 当浏览器出现以下字样，说明登录成功 {\"success\":true} 3、激活 浏览器中输入： http://群辉主机地址:群辉主机端口/webapi/entry.cgi?api=SYNO.ActiveBackup.Activation&method=set&version=1&activated=true&serial_number=“产品序列号” 当浏览器出现以下字样，说明Active Backup for Business套件激活成功 {\"data\":{\"actived\":true}\"success\":true} Enjoy Yourself 参考： https://www.tenlonstudio.com/7478.html 三、ABB配置、备份vSphere VMs 1、连接vSphere ESXI 添加连接完成后会显示vSphere ESXI的版本及在线状态，同时会显示其上的所有VM及备份状态 2、创建备份VM任务 ①选择备份到哪个共享文件夹 ②选择要备份的VM ③配置备份任务 启用更改块跟踪：当启用更改块跟踪以便仅传输上次备份时间以来更改的块，能大幅减少传输的数据大小 启用应用程序感知备份：当启用应用程序感知备份时，会利用VMware Tools和Microsoft Shadow Copy Server(VSS)确保Linux和Window虚拟机的备份数据一致性。若要使用此功能，请确保在支持VSS的Windows虚拟机上安装最新版的VMware Tools. 启用数据传输压缩： 启用数据传输加密： ④配置备份周期 ⑤设置备份保留策略 3、查看备份任务信息 4、备份文件信息 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-09 13:20:54 "},"origin/vsphere-ovf.html":{"url":"origin/vsphere-ovf.html","title":"OVF模板详解","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/vmware-ovf-tool.html":{"url":"origin/vmware-ovf-tool.html","title":"VMWare OVF Tools","keywords":"","body":"OVF 管理工具VMWare OVF Tool 一、简介 VMWare OVF Tool是 一个可以在VMWare系列产品上导入导出虚拟机为OVF/OVA格式文件的命令行工具。 ESXI强大的客户端vSpere Client由于在MacOS上没有对应的版本。VMWare Fusion在MacOS上有客户端能对远程ESXI进行基本简单的管理操作！但是只能部署客户端本地的OVF/OVA文件，不能部署存储在远程Web服务器上的OVF模板文件到远程ESXI服务器上。在MacOS上，只能通过组合VMware Fusion+VMware OVF Tool+govc来实现Windows下vSphere Client的大部分功能。 官方文档： https://www.vmware.com/support/developer/ovf/ovf410/ovftool-410_userguide.pdf https://vmware.github.io/vic-product/assets/files/html/1.5/vic_vsphere_admin/deploy_vic_appliance_ovftool.html 二、安装配置 下载地址（需注册登录）：https://my.vmware.com/zh/group/vmware/details?downloadGroup=OVFTOOL430&productId=742 MacOS dmg格式安装后的目录为：/Applications/VMware OVF Tool。命令没有系统环境变量中，通过设置软连接实现：ln -s /Applications/VMware\\ OVF\\ Tool/ovftool /usr/local/bin 配置文件 通用型的配置项可以写在配置文件中 注释行以#开头 配置项是ovftool的命令行参数，一行一个配置项 配置文件读取顺序为先全局配置文件~/.ovftool，后读取本地配置文件.ovftool 例如：~/.ovftool # 指定vSphere存储池位置 datastore=datastore1 # 指定磁盘类型 diskMode=thin # 跳过vSphere连接的SSL认证 noSSLVerify # 部署完成后开机 powerOn # 接受所有用户的licenses acceptAllEulas # 指定日志输出级别 X:logLevel=error # 指定日志输出文件 X:logFile=/tmp/ovftool.log # 将日志输出控制台 X:logToConsole # 跳过OVF原数据校验 skipManifestCheck # 输出机器可读日志 machineOutput # 在OVF中注入ENV配置 X:injectOvfEnv # ESXI局域网适配器 network='VM Network=VM Network' 查看所有配置文件的配置项 ovftool --help config # Contents of global configuration options (/Users/curiouser/.ovftool): # datastore=datastore1 # diskMode=thin # noSSLVerify # powerOn # ..... # Currently no local configuration options in .ovftool 三、命令详解 命令格式 ovftool [参数项] 源目标 目的目标 源目标 源目标可以是以下资源 OVF/OVA文件(a local file path, or an HTTP, HTTPS, or FTP URL) 本地虚拟机文件 (.vmx格式) vCenter, ESXi或VMware Server上的vAPP/locatoridentifying a virtual machine A vCloud Director locator identifying a virtual machine or a vApp in vCloud Director. A local file path to a vApprun workspace entity. 目的目标 目的目标可以是以下资源 A local file path for VMX, OVF, OVA, or vApprun workspace. A vSphere locator identifying a cluster, host, or a vSphere location. A vCloud Director locator identifying a virtual machine or a vApp in vCloud Director. vSphere目的目标格式 vi://:@:/ 参数项 --acceptAllEulas : Accept all end-user licenses agreements without being prompted. --allowAllExtraConfig : Whether we allow all the ExtraConfig options. These options are a security risk as they control low-level and potential unsafe options on the VM. --allowExtraConfig : Whether we allow ExtraConfig options. These options are a security risk as they control low-level and potential unsafe options on the VM. --annotation : Add annotation to vi, vmx, vapprun, vCloud, OVF, and OVA source locators --authdPortSource : Use this to override default vmware authd port (902) when using a host as source. --authdPortTarget : Use this to override default vmware authd port (902) when using a host as target. --chunkSize : Specifies the chunk size to use for files in a generated OVF package. The default is not to chunk. The chunk size without unit is assumed to be in megabytes. Accepted units are b, kb, mb, gb; e.g., 2gb or 100kb. --compress : Compress the disks in an OVF package. Value must be between 1 and 9. 1 is the fastest, but gives the worst compression, whereas 9 is the slowest, but gives the best compression. --computerName : Sets the computer name in the guest for a VM using the syntax --computerName:=. Only applies to vCloud targets version 5.5 or newer. --coresPerSocket : Specifies the distribution of the total number of CPUs over a number of virtual sockets using the syntax --coresPerSocket:=. Only applies to vCloud targets version 5.5 or newer. -ds/--datastore : Target datastore name for a VI locator. --decodeBase64 : Decode option values with Base64. --defaultStorageProfile : The storage profile for all VMs in the OVF package. The value should be an SPBM profile ID. Only applies to VI targets version 5.5 or newer. --defaultStorageRawProfile : The storage profile for all VMs in the OVF package. The value should be raw SPBM profile. The value will overwrite that in --defaultStorageProfile. Only applies to VI targets version 5.5 or newer. --deploymentOption : Selects what deployment option to use (if the source OVF package supports multiple options.) --disableVerification : Skip validation of signature and certificate. -dm/--diskMode : Select target disk format. Supported formats are: monolithicSparse, monolithicFlat, twoGbMaxExtentSparse, twoGbMaxExtentFlat, seSparse (VI target), : eagerZeroedThick (VI target), thin (VI target), thick (VI target), sparse, and flat --diskSize : Sets the size of a VM disk in megabytes using the syntax --diskSize:,=. Only applies to vCloud targets version 5.5 or newer. --eula : EULA to be inserted in the first virtual system or virtual system collection in the OVF. If the EULA is in a file, use the option --eula@=filename instead. --exportDeviceSubtypes : Enables export of resource subtype for CD/Floppy/Parallel/Serial devices. This can limit portability as not all device backings are supported on all hypervisors. : The default is false. --exportFlags : Specifies one or more export flags to control what gets exported. The supported values for VI sources are mac, uuid, and extraconfig. : Supported value for vCloud sources are preserveIdentity. One or more options can be provided, separated by commas. --extraConfig : Sets an ExtraConfig element for all VirtualHardwareSections. The syntax is --extraConfig:=. Applies to vi, vmx, vapprun, vCloud, ovf, and ova source locators. --fencedMode : If a parent network exists on the vCloud target, this property specifies the connectivity to the parent. Possible values are bridged, isolated, and natRouted. -h /--help : Prints this message. --hideEula : In OVF probe mode, hides the EULA. --ipAllocationPolicy : IP allocation policy for a deployed OVF package.Supported values are: dhcpPolicy, transientPolicy, fixedPolicy, fixedAllocatedPolicy. --ipProtocol : Select what IP protocol to use (IPv4, IPv6). --lax : Relax OVF specification conformance and virtual hardware compliance checks. Use only if you know what you are doing. --locale : Selects locale for target. --machineOutput : Output OVF Tool messages in a machine friendly manner. --makeDeltaDisks : Build delta disk hierarchy from the given source locator. --maxVirtualHardwareVersion : The maximal virtual hardware version to generate. --memorySize : Sets the memory size in megabytes of a VM using the syntax --memorySize:=. Only applies to vCloud targets version 5.5 or newer. -n /--name : Specifies target name (defaults to source name). --net : Set a network assignment in the deployed OVF package. A network assignment is set using the syntax --net:=. If the target is vCloud 5.5 or newer, : a fence mode can also be specified using the syntax --net:=,. Possible fence mode values are: bridged, isolated, and natRouted. -nw/--network : Target network for a VI deployment. --nic : Specifies NIC configuration in a VM using the syntax --nic:,=,,,. : Possible values for ipAddressingMode are: DHCP, POOL, MANUAL, and NONE. ipAddress is optional and should only be used when ipAddressingMode is set to MANUAL. Only applies to vCloud targets version 5.5 or newer. --noDisks : Disable disk conversion. --noImageFiles : Do not include image files in destination. --noSSLVerify : Skip SSL verification for VI connections. --numberOfCpus : Sets the number of CPUs for a VM using the syntax --numberOfCpus:=. Only applies to vCloud targets version 5.5 or newer. -o /--overwrite : Force overwrites of existing files. --powerOffSource : Ensures a VM/vApp is powered off before importing from a VI source. --powerOffTarget : Ensures a VM/vApp is powered off before overwriting a VI target. --powerOn : Powers on a VM/vApp deployed on a VI target. --privateKey : Sign OVF package with the given private key (.pem file). The file must contain a private key and a certificate. --privateKeyPassword : Password for the private key. Should be used in conjunction with privateKey if the private key requires password authentication. : If required and not specified, the tool will prompt for the password. --prop : Set a property in the deployed OVF package. A property is set using the syntax --prop:=. --proxy : Proxy used for HTTP[S] access. --proxyNTLMAuth : Enable NTLM authentication for proxy. -q /--quiet : No output to screen except errors. --schemaValidate : Validate OVF descriptor against OVF schema. --shaAlgorithm : Select SHA digest algorithm when creating OVF package. Supported values are SHA1, SHA256 and SHA512. Default value is SHA256. --skipManifestCheck : Skip validation of OVF package manifest. --skipManifestGeneration : Skip generation of OVF package manifest. --sourcePEM : File path to PEM formatted file used to verify VI connections. --sourceSSLThumbprint : SSL fingerprint of SOURCE. OVF Tool verifies the SSL fingerprint it gets from SOURCE if the value is set. -st/--sourceType : Explicitly express that source is OVF, OVA, VMX, VI, vCloud, ISO, FLP, vApprun --sslCipherList : Use this to override default OpenSSL ciphers suite. --sslVersion : Use this to set preferred TLS/SSL version for HTTPS connections. The valid values are as following: TLSv1_0: Set preferred TLS/SSL version to TLSv1.0. TLSv1_1: Set preferred TLS/SSL version to TLSv1.1. TLSv1_2: Set preferred TLS/SSL version to TLSv1.2. --storageProfile : Sets the storage profile for a VM using the syntax --storageProfile:=.Only applies to vCloud targets version 5.5 or newer. --targetPEM : File path to PEM formatted file used to verify VI connections. --targetSSLThumbprint : SSL fingerprint of TARGET. OVF Tool verifies the SSL fingerprint it gets from TARGET if the value is set. -tt/--targetType : Explicitly express that target is OVF, OVA, VMX, VI, vCloud, ISO, FLP, vApprun --vCloudTemplate : Create only a vApp template. Default value is false --vService : Set a vService assignment in the deployed OVF package. A vService assignment is set using the syntax --vService:=. --verifyOnly : Do not upload the source but only verify it against the target host. Applies to VI 4 targets only. -v /--version : Prints the version of this tool. --viCpuResource : Specify the CPU resource settings for VI-locator targets. The syntax is --viCpuResource=::. --viMemoryResource : Specify the CPU resource settings for VI-locator targets. The syntax is --viMemoryResource=::. -vf/--vmFolder : Target VM folder in VI inventory (relative to datacenter). 帮助命令格式 ovftool --help 文档主题 # 文档主题 locators : For detailed source and destination locator syntax examples : For examples of use config : For syntax of configuration files debug : For debug purpose integration : For a list of options primarily used when ovftool is exec'ed from another tool or shellscript. 快速操作命令别名 echo \"nvm_2C4G80G() { ovftool --name=\\\"\\$@\\\" http://192.168.1.7:32770/repository/tools/ovf/empty/2C4G80G.ovf vi://root:****@192.168.1.103 ;}\" >> ~/.zshrc echo \"nvm_4C8G100G() { ovftool --name=\\\"\\$@\\\" http://192.168.1.7:32770/repository/tools/ovf/empty/4C8G100G.ovf vi://root:****@192.168.1.103 ;}\" >> ~/.zshrc source ~/.zshrc 四、操作实例 1、将存储在Nexus RAW仓库中的OVF模板部署遇到远程ESXI中 ovftool \\ --name=\"test1\" \\ --X:injectOvfEnv \\ --X:logFile=./ovftool.log \\ --X:enableHiddenProperties \\ --allowExtraConfig \\ --machineOutput \\ --X:waitForIp \\ http://192.168.1.7:8080/repository/tools/ovf/empty/2C4G80G.ovf \\ vi://root:'*******'@192.168.1.103 # Nexus中的OVF模板文件标注VMDK位置需要修改为HTTP协议类型的URL地址 2、导出远程ESXI中虚拟机的OVF模板到本地 ovftool vi://root:'***'@192.168.1.103/empty-6C20G100G ./empty-6C20G100G.ovf 3、导入本地OVF模板到远程ESXI中 ovftool -ds=hdd -dm=thin -n=VM名字 本地OVF模板文件路径 vi://root@192.168.1.103 # -dm=thin是精简置备的意思，-n是新主机的名字 五、问题 1、导入VM OVF模板时报A general system error occurred: Fault cause: vim.fault.FileNotFound 原因： ​ OVF模板文件中配置的有CDROM设备，而CDROM有使用ISO文件的挂载，而本地可能没有对应的ISO文件 解决方案： ​ 编辑VM的OVF文件，搜索iso关键字，找到对应的设备定义，注释或删除掉 参考：https://communities.vmware.com/t5/Open-Virtualization-Format-Tool/ovftool-fails-with-Error-vim-fault-FileNotFound/m-p/2649133 参考 http://www.mamicode.com/info-detail-1301117.html https://vmware.github.io/vic-product/assets/files/html/1.5/vic_vsphere_admin/deploy_vic_appliance_ovftool.html https://www.virtuallyghetto.com/2014/07/quick-tip-handy-ovftool-4-0-advanced-options.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-09 13:58:47 "},"origin/vsphere-govc.html":{"url":"origin/vsphere-govc.html","title":"Go语言CLI: govc","keywords":"","body":"vSphere go命令行管理工具govc 一、简介 VMware vSphere APIs (ESXi and/or vCenter)的go语言客户端 govc - vSphere CLI vcsim - vSphere API mock framework toolbox - VM guest tools framework 支持的ESXi / vCenter版本： ESXi / vCenter 6.0, 6.5 , 6.7 (5.5和5.1版本功能部分支持, 但官方不再支持) GitHub地址：https://github.com/vmware/govmomi govc下载地址：https://github.com/vmware/govmomi/releases govc使用手册：https://github.com/vmware/govmomi/blob/master/govc/USAGE.md 二、安装配置 1、安装 在govc下载地址下载对应平台的二进制包 curl -L $URL_TO_BINARY | gunzip > /usr/local/bin/govc chmod +x /usr/local/bin/govc MacOS对应govc_darwin_amd64.gz 2、配置 govc是通过设置环境变量进行配置的。 GOVC_URL：ESXi或vCenter实例的地址 默认协议https ，URL路径为 /sdk 。可在URL中设置用户名密码，例如： https://user:pass@host/sdk. 如果用户名密码中包含特殊字符( \\, # , :)，可以在 GOVC_USERNAME ， GOVC_PASSWORD 单独设置用户名密码。 GOVC_USERNAME：用户名 GOVC_PASSWORD：密码 GOVC_TLS_CA_CERTS：指定CA证书 $ export GOVC_TLS_CA_CERTS=~/.govc_ca.crt # 多证书设置 $ export GOVC_TLS_CA_CERTS=~/ca-certificates/bar.crt:~/ca-certificates/foo.crt GOVC_TLS_KNOWN_HOSTS：指定验证证书的指纹 $ export GOVC_TLS_KNOWN_HOSTS=~/.govc_known_hosts $ govc about.cert -u host -k -thumbprint | tee -a $GOVC_TLS_KNOWN_HOSTS $ govc about -u user:pass@host GOVC_TLS_HANDSHAKE_TIMEOUT: TLS握手的超时时间 GOVC_INSECURE：关闭证书验证 export GOVC_INSECURE=1 GOVC_DATACENTER： GOVC_DATASTORE： GOVC_NETWORK： GOVC_RESOURCE_POOL： GOVC_HOST： GOVC_GUEST_LOGIN： GOVC_VIM_NAMESPACE： GOVC_VIM_VERSION： 以上变量可在~/.zshrc或/etc/profile或~/.bashrc中设置，同时可使用govc env查看设置。 三、VM的创建 命令详情 Usage: govc vm.create [OPTIONS] NAME Create VM. For a list of possible '-g' IDs, see: http://pubs.vmware.com/vsphere-6-5/topic/com.vmware.wssdk.apiref.doc/vim.vm.GuestOsDescriptor.GuestOsIdentifier.html Examples: govc vm.create -on=false vm-name govc vm.create -cluster cluster1 vm-name # use compute cluster placement govc vm.create -datastore-cluster dscluster vm-name # use datastore cluster placement govc vm.create -m 2048 -c 2 -g freebsd64Guest -net.adapter vmxnet3 -disk.controller pvscsi vm-name Options: -annotation= VM description -c=1 Number of CPUs -cluster= Use cluster for VM placement via DRS -datastore-cluster= Datastore cluster [GOVC_DATASTORE_CLUSTER] -disk= Disk path (to use existing) OR size (to create new, e.g. 20GB) -disk-datastore= Datastore for disk file -disk.controller=scsi Disk controller type -ds= Datastore [GOVC_DATASTORE] -firmware=bios Firmware type [bios|efi] -folder= Inventory folder [GOVC_FOLDER] -force=false Create VM if vmx already exists -g=otherGuest Guest OS ID -host= Host system [GOVC_HOST] -iso= ISO path -iso-datastore= Datastore for ISO file -link=true Link specified disk -m=1024 Size in MB of memory -net= Network [GOVC_NETWORK] -net.adapter=e1000 Network adapter type -net.address= Network hardware address -on=true Power on VM -pool= Resource pool [GOVC_RESOURCE_POOL] -version= ESXi hardware version [5.0|5.5|6.0|6.5|6.7] 示例 govc vm.create -m 2048 -c 2 -disk=30G -host.ip=192.168.1.8 test1 四、VM的管理 具体命令详解可查看文档：https://github.com/vmware/govmomi/blob/master/govc/USAGE.md 1、查询操作 查看所有VM govc find . -type m 查看所有开机的VM govc find . -type m -runtime.powerState poweredOn 2、VM电源的开启与关闭 vmname=test # 开启VM电源 govc vm.power -on -M $vmname # 关闭VM电源 govc vm.power -off -M $vmname 3、VM的销毁 使用VMWare OVF Tool部署OVF/OVA模板到远程ESXI，详见OVF 管理工具VMWare OVF Tool 关闭VM电源，并删除VM govc vm.destroy vm_name 4、在VM中进行的操作 Prerequisite VM安装VMware-Tools工具后进行重启 可直接使用包管理工具安装,例如:yum install -y open-vm-tools 手动省略，太麻烦 设置要访问VM的名字及登录用户密码 GOVC_GUEST_LOGIN=\"root:******\" # 如果密码中包含特殊字符“!”，使用\"\\\"进行转义.\"@\"不需要转义 vmname=\"test\" 命令 govc guest.* -vm $vmname guest.chmod：修改VM中文件的权限 guest.chown：设置VM中文件的所有者 guest.df：显示VM中文件的使用情况 govc guest.df -vm $vmname guest.download：拷贝VM中的文件到本地 guest.getenv：查看VM中的环境变量 guest.kill：杀掉VM中的进程 guest.ls：查看VM中的文件系统 # 例如查看指定VM中“/root”下的文件夹 govc guest.ls -vm $vmname /root guest.mkdir：在VM中创建文件夹 govc guest.mkdir -vm $vmname /root/test guest.mktemp：在VM中创建临时文件或文件夹 guest.mv：在VM中移动文件 guest.ps：查看VM中的进程 guest.rm：删除VM中的文件 guest.rmdir：删除VM中的文件夹 guest.run：在VM中运行命令，并显示输出结果 Usage: govc guest.run [OPTIONS] PATH [ARG]... Examples: govc guest.run -vm $vmname ifconfig govc guest.run -vm $vmname ifconfig eth0 cal | govc guest.run -vm $vmname -d - cat govc guest.run -vm $vmname -d \"hello $USER\" cat govc guest.run -vm $vmname curl -s :invalid: || echo $? # exit code 6 govc guest.run -vm $vmname -e FOO=bar -e BIZ=baz -C /tmp env Options: -C= The absolute path of the working directory for the program to start -d= Input data string. A value of '-' reads from OS stdin -e=[] Set environment variables -i=false Interactive session -l=: Guest VM credentials [GOVC_GUEST_LOGIN] -vm= Virtual machine [GOVC_VM] govc guest.run -l $GOVC_GUEST_LOGIN -vm $vmname sh -c /root/beforeShutDown.sh guest.start：在VM中启动程序，并显示输出结果 Usage: govc guest.start [OPTIONS] PATH [ARG]... Examples: govc guest.start -vm $vmname /bin/mount /dev/hdb1 /data pid=$(govc guest.start -vm $vmname /bin/long-running-thing) govc guest.ps -vm $vmname -p $pid -X Options: -C= The absolute path of the working directory for the program to start -e=[] Set environment variable (key=val) -i=false Interactive session -l=: Guest VM credentials [GOVC_GUEST_LOGIN] -vm= Virtual machine [GOVC_VM] guest.touch：在VM中创建文件 guest.upload：上传本地文件到VM中 govc guest.upload -vm $vmname ./**.tar.gz /root/***.tar.gz 5、VM的磁盘管理 ①创建新磁盘挂载到虚拟机中 Usage: govc vm.disk.create [OPTIONS] Create disk and attach to VM. Examples: govc vm.disk.create -vm $name -name $name/disk1 -size 10G govc vm.disk.create -vm $name -name $name/disk2 -size 10G -eager -thick -sharing sharingMultiWriter Options: -controller= Disk controller -ds= Datastore [GOVC_DATASTORE] -eager=false Eagerly scrub new disk -mode=persistent Disk mode (persistent|nonpersistent|undoable|independent_persistent|independent_nonpersistent|append) -name= Name for new disk -sharing= Sharing (sharingNone|sharingMultiWriter) -size=10.0GB Size of new disk -thick=false Thick provision new disk -vm= Virtual machine [GOVC_VM] ②挂载已创建的VMDK磁盘到虚拟机中 Usage: govc vm.disk.attach [OPTIONS] Attach existing disk to VM. Examples: govc vm.disk.attach -vm $name -disk $name/disk1.vmdk govc vm.disk.attach -vm $name -disk $name/shared.vmdk -link=false -sharing sharingMultiWriter govc device.remove -vm $name -keep disk-* # detach disk(s) Options: -controller= Disk controller -disk= Disk path name -ds= Datastore [GOVC_DATASTORE] -link=true Link specified disk -mode= Disk mode override (persistent|nonpersistent|undoable|independent_persistent|independent_nonpersistent|append) -persist=true Persist attached disk -sharing= Sharing (sharingNone|sharingMultiWriter) -vm= Virtual machine [GOVC_VM] ③更新VM磁盘配置 可用于扩容磁盘大小 Usage: govc vm.disk.change [OPTIONS] Change some properties of a VM's DISK In particular, you can change the DISK mode, and the size (as long as it is bigger) Examples: govc vm.disk.change -vm VM -disk.key 2001 -size 10G govc vm.disk.change -vm VM -disk.label \"BDD disk\" -size 10G govc vm.disk.change -vm VM -disk.name \"hard-1000-0\" -size 12G govc vm.disk.change -vm VM -disk.filePath \"[DS] VM/VM-1.vmdk\" -mode nonpersistent Options: -disk.filePath= Disk file name -disk.key=0 Disk unique key -disk.label= Disk label -disk.name= Disk name -mode= Disk mode (persistent|nonpersistent|undoable|independent_persistent|independent_nonpersistent|append) -sharing= Sharing (sharingNone|sharingMultiWriter) -size=0B New disk size -vm= Virtual machine [GOVC_VM] 6、查看VM信息 Usage: govc vm.info [OPTIONS] VM... Display info for VM. The '-r' flag displays additional info for CPU, memory and storage usage, along with the VM's Datastores, Networks and PortGroups. Examples: govc vm.info $vm govc vm.info -r $vm | grep Network: govc vm.info -json $vm govc find . -type m -runtime.powerState poweredOn | xargs govc vm.info Options: -e=false Show ExtraConfig -g=true Show general summary -r=false Show resource summary -t=false Show ToolsConfigInfo -waitip=false Wait for VM to acquire IP address 7、VM的快照 ①创建VM快照 Usage: govc snapshot.remove [OPTIONS] NAME Remove snapshot of VM with given NAME. NAME can be the snapshot name, tree path, moid or '*' to remove all snapshots. Examples: govc snapshot.remove -vm my-vm happy-vm-state Options: -c=true Consolidate disks -r=false Remove snapshot children -vm= Virtual machine [GOVC_VM] ②快照恢复 Usage: govc snapshot.revert [OPTIONS] [NAME] Revert to snapshot of VM with given NAME. If NAME is not provided, revert to the current snapshot. Otherwise, NAME can be the snapshot name, tree path or moid. Examples: govc snapshot.revert -vm my-vm happy-vm-state Options: -s=false Suppress power on -vm= Virtual machine [GOVC_VM] ③查看快照 Usage: govc snapshot.tree [OPTIONS] List VM snapshots in a tree-like format. The command will exit 0 with no output if VM does not have any snapshots. Examples: govc snapshot.tree -vm my-vm govc snapshot.tree -vm my-vm -D -i -d Options: -C=false Print the current snapshot name only -D=false Print the snapshot creation date -c=true Print the current snapshot -d=false Print the snapshot description -f=false Print the full path prefix for snapshot -i=false Print the snapshot id -vm= Virtual machine [GOVC_VM] 五、连接VM Console 1、Prerequisite 安装VMWare Remote Console 下载地址：https://my.vmware.com/web/vmware/details?downloadGroup=VMRC1101&productId=742 2、命令详解 govc vm.console --help Usage: govc vm.console [OPTIONS] VM Generate console URL or screen capture for VM. One of VMRC, VMware Player, VMware Fusion or VMware Workstation must be installed to open VMRC console URLs. Options: -capture= Capture console screen shot to file -cert= Certificate [GOVC_CERTIFICATE] -dc=ha-datacenter Datacenter [GOVC_DATACENTER] -debug=false Store debug logs [GOVC_DEBUG] -dump=false Enable Go output -h5=false Generate HTML5 UI console link -json=false Enable JSON output -k=true Skip verification of server certificate [GOVC_INSECURE] -key= Private key [GOVC_PRIVATE_KEY] -persist-session=true Persist session to disk [GOVC_PERSIST_SESSION] -tls-ca-certs= TLS CA certificates file [GOVC_TLS_CA_CERTS] -tls-known-hosts= TLS known hosts file [GOVC_TLS_KNOWN_HOSTS] -u=https://@192.168.1.8/sdk ESX or vCenter URL [GOVC_URL] -vim-namespace=vim25 Vim namespace [GOVC_VIM_NAMESPACE] -vim-version=6.7 Vim version [GOVC_VIM_VERSION] -vm= Virtual machine [GOVC_VM] -vm.dns= Find VM by FQDN -vm.ip= Find VM by IP address -vm.ipath= Find VM by inventory path -vm.path= Find VM by path to .vmx file -vm.uuid= Find VM by UUID # MacOSX VMR open $(govc vm.console my-vm) # MacOSX H5 open $(govc vm.console -h5 my-vm) # Linux VMRC xdg-open $(govc vm.console my-vm) # Linux H5 xdg-open $(govc vm.console -h5 my-vm) govc vm.console my-vm govc vm.console -capture screen.png my-vm # screen capture govc vm.console -capture - my-vm | display # screen capture to stdout 六、获取VM信息 1、获取VM的IP地址 Usage: govc vm.ip [OPTIONS] VM... List IPs for VM. By default the vm.ip command depends on vmware-tools to report the 'guest.ipAddress' field and will wait until it has done so. This value can also be obtained using: govc vm.info -json $vm | jq -r .VirtualMachines[].Guest.IpAddress When given the '-a' flag, only IP addresses for which there is a corresponding virtual nic are listed. If there are multiple nics, the listed addresses will be comma delimited. The '-a' flag depends on vmware-tools to report the 'guest.net' field and will wait until it has done so for all nics. Note that this list includes IPv6 addresses if any, use '-v4' to filter them out. IP addresses reported by tools for which there is no virtual nic are not included, for example that of the 'docker0' interface. These values can also be obtained using: govc vm.info -json $vm | jq -r .VirtualMachines[].Guest.Net[].IpConfig.IpAddress[].IpAddress When given the '-n' flag, filters '-a' behavior to the nic specified by MAC address or device name. The 'esxcli' flag does not require vmware-tools to be installed, but does require the ESX host to have the /Net/GuestIPHack setting enabled. The 'wait' flag default to 1hr (original default was infinite). If a VM does not obtain an IP within the wait time, the command will still exit with status 0. Examples: govc vm.ip $vm govc vm.ip -wait 5m $vm govc vm.ip -a -v4 $vm govc vm.ip -n 00:0c:29:57:7b:c3 $vm govc vm.ip -n ethernet-0 $vm govc host.esxcli system settings advanced set -o /Net/GuestIPHack -i 1 govc vm.ip -esxcli $vm Options: -a=false Wait for an IP address on all NICs -esxcli=false Use esxcli instead of guest tools -n= Wait for IP address on NIC, specified by device name or MAC -v4=false Only report IPv4 addresses -wait=1h0m0s Wait time for the VM obtain an IP address 七、导出VM为OVF模板 1、命令 Usage: govc export.ovf [OPTIONS] DIR Export V Options: -f=false Overwrite existing -i=false Include image files (*.{iso,img}) -prefix=true Prepend target name to image filenames if missing -name= Specifies target name (defaults to source name) -sha=0 Generate manifest using SHA 1, 256, 512 or 0 to skip -vm= Virtual machine [GOVC_VM] 2、示例 govc export.ovf -vm vmname . 八、ESXI主机的存储磁盘管理 1、查看ESXI主机存储 Usage: govc datastore.info [OPTIONS] [PATH]... Display info for Datastores. Examples: govc datastore.info govc datastore.info vsanDatastore # info on Datastores shared between cluster hosts: govc object.collect -s -d \" \" /dc1/host/k8s-cluster host | xargs govc datastore.info -H # info on Datastores shared between VM hosts: govc ls /dc1/vm/*k8s* | xargs -n1 -I% govc object.collect -s % summary.runtime.host | xargs govc datastore.info -H Options: -H=false Display info for Datastores shared between hosts Name: datastore1 Path: /ha-datacenter/datastore/datastore1 Type: VMFS URL: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398 Capacity: 829.0 GB Free: 503.9 GB 2、查看已有的数据存储 Usage: govc datastore.ls [OPTIONS] [FILE]... Options: -R=false List subdirectories recursively -a=false Do not ignore entries starting with . -ds= Datastore [GOVC_DATASTORE] -l=false Long listing format -p=false Append / indicator to directories 3、创建VMDK磁盘 Usage: govc datastore.disk.create [OPTIONS] VMDK Create VMDK on DS. Examples: govc datastore.mkdir disks govc datastore.disk.create -size 24G disks/disk1.vmdk govc datastore.disk.create disks/parent.vmdk disk/child.vmdk Options: -a=lsiLogic Disk adapter -d=thin Disk format -ds= Datastore [GOVC_DATASTORE] -f=false Force -size=10.0GB Size of new disk -uuid= Disk UUID 4、查看VMDK磁盘 Usage: govc datastore.disk.info [OPTIONS] VMDK Query VMDK info on DS. Examples: govc datastore.disk.info disks/disk1.vmdk Options: -c=false Chain format -d=false Include datastore in output -ds= Datastore [GOVC_DATASTORE] -p=true Include parents -uuid=false Include disk UUID Name: disks/disk1.vmdk Type: thin Parent: 5、下载VMDK磁盘到本地 Usage: govc datastore.download [OPTIONS] SOURCE DEST Copy SOURCE from DS to DEST on the local system. If DEST name is \"-\", source is written to stdout. Examples: govc datastore.download vm-name/vmware.log ./local.log govc datastore.download vm-name/vmware.log - | grep -i error Options: -ds= Datastore [GOVC_DATASTORE] -host= Host system [GOVC_HOST] 九、ESXI主机管理 1、查看ESXI主机信息 Usage: govc host.info [OPTIONS] Options: -host= Host system [GOVC_HOST] # 示例 govc host.info -host.ip=192.168.1.8 Name: localhost.localdomain Path: /ha-datacenter/host/localhost./localhost.localdomain Manufacturer: Dell Inc. Logical CPUs: 24 CPUs @ 3058MHz Processor type: Intel(R) Xeon(R) CPU X5675 @ 3.07GHz CPU usage: 104 MHz (0.1%) Memory: 98291MB Memory usage: 4583 MB (4.7%) Boot time: 2020-04-01 01:21:16.499148 +0000 UTC State: connected 2、查看ESXI主机日志文件 Usage: govc logs.ls [OPTIONS] List diagnostic log keys. Examples: govc logs.ls govc logs.ls -host host-a Options: -host= Host system [GOVC_HOST] hostd /var/log/hostd.log vmkernel /var/log/vmkernel.log vpxa /var/log/vpxa.log 3、实时查看ESXI主机日志 Usage: govc logs [OPTIONS] View VPX and ESX logs. The '-log' option defaults to \"hostd\" when connected directly to a host or when connected to VirtualCenter and a '-host' option is given. Otherwise, the '-log' option defaults to \"vpxd:vpxd.log\". The '-host' option is ignored when connected directly to a host. See 'govc logs.ls' for other '-log' options. Examples: govc logs -n 1000 -f govc logs -host esx1 govc logs -host esx1 -log vmkernel Options: -f=false Follow log file changes -host= Host system [GOVC_HOST] -log= Log file key -n=25 Output the last N log lines 4、下载ESXI日志 Usage: govc logs.download [OPTIONS] [PATH]... Generate diagnostic bundles. A diagnostic bundle includes log files and other configuration information. Use PATH to include a specific set of hosts to include. Examples: govc logs.download govc logs.download host-a host-b Options: -default=false Specifies if the bundle should include the default server 5、查看EXSI的资产 Usage: govc ls [OPTIONS] [PATH]... List inventory items. Examples: govc ls -l '*' govc ls -t ClusterComputeResource host govc ls -t Datastore host/ClusterA/* | grep -v local | xargs -n1 basename | sort | uniq Options: -L=false Follow managed object references -i=false Print the managed object reference -l=false Long listing format -t= Object type /ha-datacenter/vm/OCP4.3-Tools (VirtualMachine) /ha-datacenter/vm/test1 (VirtualMachine) /ha-datacenter/vm/node1.cloudera.curiouser.com (VirtualMachine) /ha-datacenter/vm/node2.cloudera.curiouser.com (VirtualMachine) /ha-datacenter/vm/node3.cloudera.curiouser.com (VirtualMachine) /ha-datacenter/vm/OKD3.11-Allinone (VirtualMachine) /ha-datacenter/vm/Vsphere vCenter 6.0 (VirtualMachine) /ha-datacenter/vm/PXE Kickstart (VirtualMachine) /ha-datacenter/vm/Allinone K8S114 (VirtualMachine) /ha-datacenter/network/VM Network (Network) /ha-datacenter/host/localhost. (ComputeResource) /ha-datacenter/datastore/datastore1 (Datastore) 6、ESXI主机电源管理 关机 Usage: govc host.shutdown [OPTIONS] HOST... Shutdown HOST. Options: -f=false Force shutdown when host is not in maintenance mode -host= Host system [GOVC_HOST] -r=false Reboot host 示例： govc host.shutdown -f -host.ip ESXI_IP 7、维护状态的管理 ①进入维护状态 Usage: govc host.maintenance.enter [OPTIONS] HOST... Put HOST in maintenance mode. While this task is running and when the host is in maintenance mode, no VMs can be powered on and no provisioning operations can be performed on the host. Options: -evacuate=false Evacuate powered off VMs -host= Host system [GOVC_HOST] -timeout=0 Timeout # 示例： govc host.maintenance.enter -host.ip=192.168.1.8 ②退出维护状态 Usage: govc host.maintenance.exit [OPTIONS] HOST... Take HOST out of maintenance mode. This blocks if any concurrent running maintenance-only host configurations operations are being performed. For example, if VMFS volumes are being upgraded. The 'timeout' flag is the number of seconds to wait for the exit maintenance mode to succeed. If the timeout is less than or equal to zero, there is no timeout. Options: -host= Host system [GOVC_HOST] -timeout=0 Timeout # 示例： govc host.maintenance.exit -host.ip=192.168.1.8 8、查看ESXI主机开启的服务 Usage: govc host.service.ls [OPTIONS] List HOST services. Options: -host= Host system [GOVC_HOST] # 示例 govc host.service.ls -host.ip 192.168.1.8 Key Policy Status Label DCUI on Running Direct Console UI TSM on Running ESXi Shell TSM-SSH on Running SSH lbtd on Running Load-Based Teaming Daemon lwsmd off Stopped Active Directory Service ntpd on Stopped NTP Daemon pcscd off Stopped PC/SC Smart Card Daemon sfcbd-watchdog on Stopped CIM Server snmpd on Stopped SNMP Server vmsyslogd on Running Syslog Server vprobed off Stopped VProbe Daemon vpxa on Running VMware vCenter Agent xorg on Stopped X.Org Server 9、ESXI主机上服务的管理 Usage: govc host.service [OPTIONS] ACTION ID Apply host service ACTION to service ID. Where ACTION is one of: start, stop, restart, status, enable, disable Examples: govc host.service enable TSM-SSH govc host.service start TSM-SSH Options: -host= Host system [GOVC_HOST] # 示例 govc host.service -host.ip 192.168.1.8 status TSM 10、ESXI主机硬盘信息 Usage: govc host.storage.info [OPTIONS] Show HOST storage system information. Examples: govc find / -type h | xargs -n1 govc host.storage.info -unclaimed -host Options: -host= Host system [GOVC_HOST] -refresh=false Refresh the storage system provider -rescan=false Rescan all host bus adapters -rescan-vmfs=false Rescan for new VMFSs -t=lun Type (hba,lun) -unclaimed=false Only show disks that can be used as new VMFS datastores # 示例： govc host.storage.info -host.ip 192.168.1.8 Name Type Capacity Model /vmfs/devices/cdrom/mpx.vmhba1:C0:T0:L0 cdrom - DVD-ROM DS-8D3SH /vmfs/devices/disks/naa.6d4ae520b6ed2 disk 666.6GB PERC 6/i (local) /vmfs/devices/genscsi/t10.DP_000 enclosure - BACKPLANE 11、ESXI主机用户会话的管理 ①查看会话 Usage: govc session.ls [OPTIONS] List active sessions. Examples: govc session.ls govc session.ls -json | jq -r .CurrentSession.Key # 示例 Key Name Time Idle Host Agent 520ea520 vpxuser 2020-04-01 01:22 1s 127.0.0.1 VMware-client/5.1.0 525b5d4f root 2020-04-01 12:18 26m14s 192.168.1.7 VMware-client/6.5.0 52a68e2e dcui 2020-04-01 01:25 4m43s 127.0.0.1 VMware-client/5.1.0 52b127da root 2020-04-01 01:22 3m42s 127.0.0.1 52c429cc root 2020-04-01 12:22 21m44s 192.168.1.7 VMware-client/6.5.0 52d9125d root 2020-04-01 10:42 . 192.168.1.7 govc/0.22.1 ②删除会话 Usage: govc session.rm [OPTIONS] KEY... Remove active sessions. Examples: govc session.ls | grep root govc session.rm 5279e245-e6f1-4533-4455-eb94353b213a ③退出当前会话 Usage: govc session.logout [OPTIONS] Logout the current session. The session.logout command can be used to end the current persisted session. The session.rm command can be used to remove sessions other than the current session. Examples: govc session.logout 12、ESXI主机网络的管理 ①查看虚拟网卡信息 Usage: govc host.vnic.info [OPTIONS] Options: -host= Host system [GOVC_HOST] # 示例： govc host.vnic.info -host.ip 192.168.1.8 Device: vmk0 Network label: Management Network Switch: vSwitch0 IP address: 192.168.1.8 TCP/IP stack: defaultTcpipStack Enabled services: management ②查看虚拟交换机信息 Usage: govc host.vswitch.info [OPTIONS] Options: -host= Host system [GOVC_HOST] # 示例： govc host.vswitch.info -host.ip=192.168.1.8 Name: vSwitch0 Portgroup: VM Network, Management Network Pnic: vmnic0 MTU: 1500 Ports: 4352 Ports Available: 4344 Allow promiscuous mode: No Allow forged transmits: Yes Allow MAC changes: Yes ③创建虚拟交换机 Usage: govc host.vswitch.add [OPTIONS] NAME Options: -host= Host system [GOVC_HOST] -mtu=0 MTU -nic= Bridge nic device -ports=128 Number of ports # 示例 govc host.vswitch.add -host.ip 192.168.1.8 test Name: test Portgroup: Pnic: MTU: 1500 Ports: 4352 Ports Available: 4343 Allow promiscuous mode: No Allow forged transmits: Yes Allow MAC changes: Yes ④删除虚拟交换机 Usage: govc host.vswitch.remove [OPTIONS] NAME Options: -host= Host system [GOVC_HOST] 13、ESXI用户角色的管理 ①查看所有用户角色 Usage: govc role.ls [OPTIONS] [NAME] List authorization roles. If NAME is provided, list privileges for the role. Examples: govc role.ls govc role.ls Admin Options: -i=false Use moref instead of inventory path NoAccess Used for restricting granted access Anonymous Not logged-in user (cannot be granted) View Visibility access (cannot be granted) ReadOnly See details of objects, but not make changes Admin Full access rights ②创建用户角色 Usage: govc role.create [OPTIONS] NAME [PRIVILEGE]... Create authorization role. Optionally populate the role with the given PRIVILEGE(s). Examples: govc role.create MyRole govc role.create NoDC $(govc role.ls Admin | grep -v Datacenter.) Options: -i=false Use moref instead of inventory path ③删除用户角色 Usage: govc role.remove [OPTIONS] NAME Remove authorization role. Examples: govc role.remove MyRole govc role.remove MyRole -force Options: -force=false Force removal if role is in use -i=false Use moref instead of inventory path ④更新用户角色 Usage: govc role.update [OPTIONS] NAME [PRIVILEGE]... Update authorization role. Set, Add or Remove role PRIVILEGE(s). Examples: govc role.update MyRole $(govc role.ls Admin | grep VirtualMachine.) govc role.update -r MyRole $(govc role.ls Admin | grep VirtualMachine.GuestOperations.) govc role.update -a MyRole $(govc role.ls Admin | grep Datastore.) govc role.update -name RockNRole MyRole Options: -a=false Add given PRIVILEGE(s) -i=false Use moref instead of inventory path -name= Change role name -r=false Remove given PRIVILEGE(s) ⑤查看用户角色的授予信息 Usage: govc role.usage [OPTIONS] NAME... List usage for role NAME. Examples: govc role.usage govc role.usage Admin Options: -i=false Use moref instead of inventory path 14、Datastore操作 ①查看远程ESXI的Datastore详细信息 Usage: govc datastore.info [OPTIONS] [PATH]... Examples: govc datastore.info govc datastore.info vsanDatastore # info on Datastores shared between cluster hosts: govc object.collect -s -d \" \" /dc1/host/k8s-cluster host | xargs govc datastore.info -H # info on Datastores shared between VM hosts: govc ls /dc1/vm/*k8s* | xargs -n1 -I% govc object.collect -s % summary.runtime.host | xargs govc datastore.info -H Options: -H=false Display info for Datastores shared between hosts # 示例： govc datastore.info Name: datastore1 Path: /ha-datacenter/datastore/datastore1 Type: VMFS URL: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398 Capacity: 829.0 GB Free: 506.1 GB ②查看远程ESXI的Datastore目录 Usage: govc datastore.ls [OPTIONS] [FILE]... Options: -R=false List subdirectories recursively -a=false Do not ignore entries starting with . -ds= Datastore [GOVC_DATASTORE] -l=false Long listing format -p=false Append / indicator to directories ③上传本地文件到远程ESXI主机Datastore Usage: govc datastore.upload [OPTIONS] SOURCE DEST If SOURCE name is \"-\", read source from stdin. Examples: govc datastore.upload -ds datastore1 ./config.iso vm-name/config.iso genisoimage ... | govc datastore.upload -ds datastore1 - vm-name/config.iso Options: -ds= Datastore [GOVC_DATASTORE] ④下载远程ESXI主机Datastore中的文件到本地 Usage: govc datastore.download [OPTIONS] SOURCE DEST Copy SOURCE from DS to DEST on the local system. If DEST name is \"-\", source is written to stdout. Examples: govc datastore.download vm-name/vmware.log ./local.log govc datastore.download vm-name/vmware.log - | grep -i error Options: -ds= Datastore [GOVC_DATASTORE] -host= Host system [GOVC_HOST] ⑤删除远程ESXI主机Datastore中的文件 Usage: govc datastore.rm [OPTIONS] FILE Remove FILE from DATASTORE. Examples: govc datastore.rm vm/vmware.log govc datastore.rm vm govc datastore.rm -f images/base.vmdk Options: -ds= Datastore [GOVC_DATASTORE] -f=false Force; ignore nonexistent files and arguments -namespace=false Path is uuid of namespace on vsan datastore -t=true Use file type to choose disk or file manager ⑥移动远程ESXI主机Datastore中的文件 Usage: govc datastore.mv [OPTIONS] SRC DST Move SRC to DST on DATASTORE. Examples: govc datastore.mv foo/foo.vmx foo/foo.vmx.old govc datastore.mv -f my.vmx foo/foo.vmx Options: -dc-target= Datacenter destination (defaults to -dc) -ds= Datastore [GOVC_DATASTORE] -ds-target= Datastore destination (defaults to -ds) -f=false If true, overwrite any identically named file at the destination -t=true Use file type to choose disk or file manager ⑦在远程ESXI主机Datastore创建文件路径 Usage: govc datastore.mkdir [OPTIONS] DIRECTORY Options: -ds= Datastore [GOVC_DATASTORE] -namespace=false Return uuid of namespace created on vsan datastore -p=false Create intermediate directories as needed 十、Windows虚拟机的创建 ①创建空的虚拟机 gnvm -c 8 -m 20480 -disk=100G -host.ip=192.168.1.103 -on=false -g windows9_64Guest -disk.controller=lsilogic-sas windows 注意： 磁盘控制器要选择：lsilogic-sas，不然在安装过程显示找不到系统磁盘 SCSI 磁盘控制器类型：lsilogic|buslogic|pvscsi|lsilogic-sas 虚拟机OS类型要加上。类型匹配标识见：https://wiki.abiquo.com/display/doc/Guest+operating+system+definition+for+VMware+vSphere+5+and+6 ②添加空的CDROM光驱设备(VM需在关机状态) govc device.cdrom.add -vm windows ③将OS镜像ISO文件加到CDROM设备中 govc device.cdrom.insert -vm windows -device cdrom-3000 Images/Windows\\ 10\\ Desktop/Windows10_All_x86_64.iso ④设置BIOS启动顺序 govc device.boot -vm windows -delay 100 -order cdrom,ethernet,disk ⑤VM开机 govc vm.power -on -M windows 开机以后就会显示从CD光驱的ISO进行引导启动啦 ⑥登录控制台 open $(govc vm.console windows) ⑦(可选)快捷命令 gnvm -c 8 -m 20480 -disk=100G -host.ip=192.168.1.103 -on=false -g windows9_64Guest -disk.controller=lsilogic-sas windows \\ && govc device.cdrom.add -vm windows \\ && govc device.cdrom.insert -vm windows -device cdrom-3000 Images/Windows\\ 10\\ Desktop/Windows10_All_x86_64.iso \\ && govc device.boot -vm windows -delay 100 -order cdrom,ethernet,disk \\ && govc vm.power -on -M windows \\ && gco windows 附录: 常用命令别名 # 查看所有VM alias gsp='govc find . -type m' # 查看所有开机的VM alias gspo='govc find . -type m -runtime.powerState poweredOn' # 关闭VM电源 alias gvpof='govc vm.power -off -M $@' # 开启VM电源 alias gvpo='govc vm.power -on -M $@' # 获取VM的IP地址 alias ggip='govc vm.ip $@' # 获取VM的详细信息 alias ggvi=‘govc vm.info $@’ # 销毁VM alias gdv='govc vm.destroy $@' # 断开ESXI电源 alias gpof—esxi='govc host.shutdown -f -host.ip 192.168.1.8' # 进入维护状态 alias gim='govc host.maintenance.enter -host.ip=192.168.1.8' # 退出维护状态 alias gom='govc host.maintenance.exit -host.ip=192.168.1.8' # 创建VM alias gnvm='govc vm.create -host.ip=192.168.1.8 $@' # 连接VM Console alias gco='{ IFS= read -r vm && open $(g7b6cc489180160eeb4a7a39ebb25d524e04c9f5e7b6cc489180160eeb4a7a39ebb25d524e04c9f5eovc vm.console $vm); } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-28 15:29:59 "},"origin/raspberry-pi.html":{"url":"origin/raspberry-pi.html","title":"Raspberry Pi树莓派","keywords":"","body":"树莓派Raspberry Pi 4 一、简介 硬件配置 GPIO引脚定义 在实际使用中，我们应该熟悉树莓派接口的两种命名方案: WiringPi 编号：功接线的引脚号（如TXD、PWM0等等） BCM编号：是 Broadcom 针脚号，也即是通常称的GPIO 物理编号（Physical – Number）：PCB板上针脚的物理位置对应的编号（1~40） I2C接口 I2C是由Philips公司开发的一种简单、双向二线制同步串行总线。它只需要两根线即可在连接于总线上的器件之间传送信息。树莓派通过I2C接口可控制多个传感器和组件。它们的通信是通过SDA(数据引脚)和SCL(时钟速度引脚)来完成的。每个从设备都有一个唯一的地址，允许与许多设备间快速通信。ID_EEPROM引脚也是I2C协议，它用于与HATs通信。 SPI接口 SPI是串行外设接口，用于控制具有主从关系的组件，采用从进主出和主进从出的方式工作，树莓派上SPI由SCLK、MOSI、MISO接口组成，SCLK用于控制数据速度，MOSI将数据从树莓派发送到所连接的设备，而MISO则相反。 UART接口 有使用Arduino的朋友一定听说过UART或Serial，通用异步收/发器接口用于将Arduino连接到为其编程的计算机上，也用于其他设备与 RX 和 TX 引脚之间的通信。如果树莓派在 raspi-config 中启用了串口终端，则可以使用这些引脚通过电脑来控制树莓派，也可以直接用于控制Arduino。 PWM接口 在树莓派上，所有的引脚都可以实现软件PWM，而GPIO12、GPIO13、GPIO18、GPIO19可以实现硬件脉宽调制。 各型号配置对比 部分电子元件电路图：https://www.dazhuanlan.com/2019/12/26/5e043f4a96380/ 二、基础配置 1、安装系统 树莓派官方出了一个快速在树莓派上安装OS的软件NOOBS（ New Out Of Box Software），只需要将该软件刻录到SD卡上并在树莓派上启动，可在线或离线安装以下OS到您的树莓派上。 官方OS：Raspbian 第三方OS NOOBS下载地址：https://www.raspberrypi.org/downloads/noobs/ NOOBS刻录操作文档：https://www.raspberrypi.org/documentation/installation/noobs.md NOOBS分为两个版本：全功能版和轻量版Lite。全功能班可在线可离线安装OS。Lite只能在线安装OS 2、连接操作 直接通过HDMI外界显示进行操作 通过SSH连接进行CLI操作 通过VNC连接远程桌面进行操作 树莓派开启VNC服务，然后重启 chrome安装VNC Viewer插件进行连接，插件地址：https://www.realvnc.com/en/connect/download/viewer/chrome/ 3、基础配置 ①连接隐藏wifi 编辑/etc/wpa_supplicant/wpa_supplicant.conf network={ ssid=”wifi_name” scan_ssid=1 psk=”wifi_password” } # network：是一个连接WiFi网络的配置，可以有多个，wpa_supplicant会按照priority指定的优先级（数字越大越先连接）来连接，当然，在这个列表里面隐藏WiFi不受priority的影响，隐藏WiFi总是在可见WiFi不能连接时才开始连接。 # ssid:网络的ssid # psk:密码 # priority:连接优先级，越大越优先 # scan_ssid:连接隐藏WiFi时需要指定该值为1 ②CLI下配置Respbian sudo raspi-config ③设置chromium浏览器代理 sudo echo -e 'export http_proxy=\"http://代理服务器地址:端口\"\\nexport https_proxy=\"https://代理服务器地址:端口\"\\nexport no_proxy=\"*.taobao.com,192.168.1.0/16\"' >> /etc/environment 然后重启，打开chromium浏览器，安装Proxy SwitchyOmega插件。在Proxy SwitchyOmega插件中配置代理服务器。然后删除/etc/environment中新增的代理配置，重启。 ④设置apt镜像源为阿里云镜像源 raspbian mv /etc/apt/sources.list /etc/apt/sources.list.bak echo -e \"deb https://mirrors.aliyun.com/raspbian/raspbian/ buster main non-free contrib\\ndeb-src https://mirrors.aliyun.com/raspbian/raspbian/ buster main non-free contrib\" > /etc/apt/sources.list apt update Ubuntu 20.04 mv /etc/apt/sources.list /etc/apt/sources.list.bak echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal main restricted\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-updates main restricted\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal universe\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-updates universe\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal multiverse\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-updates multiverse\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-backports main restricted universe multiverse\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-security main restricted\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-security universe\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-security multiverse\" >> /etc/apt/sources.list apt update 4、播放音频 apt install mpg123 mpg123 音频文件 mpg123 音频文件 5、调节音频输出音量大小 apt install alsa-utils alsamixer # 指令会出现一个调节音量的界面。调节完成后按“ESC”退出 amixer set PCM 116% 三、安装特殊软件 1、安装RStudio Server apt-get install -y git r-recommended python-dev cd /home/pi/Downloads/ git clone https://github.com/rstudio/rstudio.git cd /home/pi/Downloads/rstudio/dependencies/common/ ./install-common cd /home/pi/Downloads/rstudio/dependencies/linux/ ./install-dependencies-debian #saw java 6 was not installed. installed v7 apt-get install -y openjdk-7-jdk #tried to make install, got an error about dictionaries not installed and rerun install-dependencies cd /home/pi/Downloads/rstudio/dependencies/common/ ./install-common #tried to make install, hangs at \"ext:\" so I tried manually installing pandoc, which should have been installed earlier, but apparently was not apt-get install -y pandoc #tried to make install, hangs at \"ext:\" so I tried installing the latest GWT compiler cd /home/pi/Downloads wget http://dl.google.com/closure-compiler/compiler-latest.zip unzip compiler-latest.zip rm COPYING README.md compiler-latest.zip mv closure-compiler-v20170218.jar /home/pi/Downloads/rstudio/src/gwt/tools/compiler/compiler.jar #build and install works! cd /home/pi/Downloads/rstudio/ #remove build if exists rm -r ./build mkdir build cd build cmake .. -DRSTUDIO_TARGET=Server -DCMAKE_BUILD_TYPE=Release make install Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-17 22:55:31 "},"origin/dingding-customrobot.html":{"url":"origin/dingding-customrobot.html","title":"钉钉机器人","keywords":"","body":"钉钉的机器人 一、简介 钉钉机器人类型 自定义机器人webhook 机器人限制：每个机器人每分钟最多发送20条。如果超过20条，会限流10分钟。 二、自定义类型的Wehook Rebot 安全设置 安全设置目前有3种方式： （1）方式一，自定义关键词 最多可以设置10个关键词，消息中至少包含其中1个关键词才可以发送成功。 例如：添加了一个自定义关键词：监控报警 则这个机器人所发送的消息，必须包含 监控报警 这个词，才能发送成功。 （2）方**式二，加签** 第一步，把timestamp+\"\\n\"+密钥当做签名字符串，使用HmacSHA256算法计算签名，然后进行Base64 encode，最后再把签名参数再进行urlEncode，得到最终的签名（需要使用UTF-8字符集）。 参数 说明 timestamp 当前时间戳，单位是毫秒，与请求调用时间误差不能超过1小时 secret 密钥，机器人安全设置页面，加签一栏下面显示的SEC开头的字符串 签名计算代码示例（Java） Long timestamp = System.currentTimeMillis(); String stringToSign = timestamp + \"\\n\" + secret; Mac mac = Mac.getInstance(\"HmacSHA256\"); mac.init(new SecretKeySpec(secret.getBytes(\"UTF-8\"), \"HmacSHA256\")); byte[] signData = mac.doFinal(stringToSign.getBytes(\"UTF-8\")); return URLEncoder.encode(new String(Base64.encodeBase64(signData)),\"UTF-8\"); 签名计算代码示例（Python） #python 2.7 import time import hmac import hashlib import base64 import urllib secret = '密钥' timestamp = long(round(time.time() * 1000)) secret_enc = bytes(secret).encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, secret) string_to_sign_enc = bytes(string_to_sign).encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.quote_plus(base64.b64encode(hmac_code)) print(timestamp) print(sign) 第二步，把 timestamp和第一步得到的签名值拼接到URL中。 参数 说明 timestamp 第一步使用到的时间戳 sign 第一步得到的签名值 https://oapi.dingtalk.com/robot/send?access_token=XXXXXX&timestamp=XXX&sign=XXX （3）方**式三，IP地址（段）** 设定后，只有来自IP地址范围内的请求才会被正常处理。支持两种设置方式：IP、IP段，暂不支持IPv6地址白名单，格式如下: 注意：安全设置的上述三种方式，需要**至少**设置其中一种，以进行安全保护。校验不通过的消息将会发送失败，错误如下： // 消息内容中不包含任何关键词 { \"errcode\":310000, \"errmsg\":\"keywords not in content\" } // timestamp 无效 { \"errcode\":310000, \"errmsg\":\"invalid timestamp\" } // 签名不匹配 { \"errcode\":310000, \"errmsg\":\"sign not match\" } // IP地址不在白名单 { \"errcode\":310000, \"errmsg\":\"ip X.X.X.X not in whitelist\" } 三、测试Webhook Python #!/usr/bin/python # coding=utf-8 # -*- coding: utf-8 -*- def dingding_alert(msg): secret = \"**密钥***\" access_token = \"*********\" # 生成加密验签 timestamp = long(round(time.time() * 1000)) secret_enc = bytes(secret).encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, secret) string_to_sign_enc = bytes(string_to_sign).encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.quote_plus(base64.b64encode(hmac_code)) # 拼接带验签的Dingding API URL url = \"https://oapi.dingtalk.com/robot/send?access_token=\" + access_token+\"&timestamp=\" + str(timestamp)+\"&sign=\"+str(sign) header = { \"Content-Type\": \"application/json\", \"charset\": \"utf-8\" } data = { \"msgtype\": \"text\", \"text\": { \"content\": msg }, \"at\": { \"isAtAll\":True } } sendData = json.dumps(data) request = urllib2.Request(url,data = sendData,headers = header) urlopen = urllib2.urlopen(request) dingding_alert(\"haha测试\") Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/经典面试题.html":{"url":"origin/经典面试题.html","title":"经典面试题","keywords":"","body":"经典面试题 1、TCP的握手 建立连接的三次握手 断开连接的四次握手 2、 3、SSL/TLS Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/aliyun-cli.html":{"url":"origin/aliyun-cli.html","title":"Aliyun CLI","keywords":"","body":"aliyun-cli CLI工具 一、简介 官方文档：https://help.aliyun.com/product/29991.html Github地址：https://github.com/aliyun/aliyun-cli 二、安装配置 1、二进制安装 下载地址：https://github.com/aliyun/aliyun-cli/releases 下载解压至系统环境变量路径下即可 2、配置 在使用阿里云CLI之前，您需要配置调用阿里云资源所需的凭证信息、地域、语言等。 凭证类型 验证方式 说明 交互式配置凭证（快速） 非交互式配置凭证 AK 使用aliyun-cli-clicessKey ID/Secret访问。 配置aliyun-cli-clicessKey凭证 配置aliyun-cli-clicessKey凭证 StsToken 使用STS Token访问。 配置STS Token凭证 配置STS Token凭证 RamRoleArn 使用RAM子账号的AssumeRole方式访问。 配置RamRoleArn凭证 配置RamRoleArn凭证 EcsRamRole 在ECS实例上通过EcsRamRole实现免密验证。 配置EcsRamRole凭证 配置EcsRamRole凭证 配置凭据 交互式地配置凭据 aliyun-cli configure --mode Configuring profile 'akProfile' in '' authenticate mode... aliyun-cli-clicess Key Id []: # aliyun-cli-clicessKey ID aliyun-cli-clicess Key Secret []: # aliyun-cli-clicessKey Secret Default Region Id []: # cn-hangzhou Default Output Format [json]: json (Only support json)) Default Language [zh|en] en: Saving profile[akProfile] ...Done. # --profile：指定配置名称。如果指定的配置存在，则修改配置。若不存在，则创建配置。 # --mode：指定凭证类型。分别为AK、StsToken、RamRoleArn和EcsRamRole。 非交互式地配置凭据 aliyun-cli configure set \\ --profile akProfile \\ --mode AK \\ --region cn-hangzhou \\ --aliyun-cli-clicess-key-id aliyun-cli-clicessKeyId \\ --aliyun-cli-clicess-key-secret aliyun-cli-clicessKeySecret # --profile（必选）：指定配置名称。如果指定的配置存在，则修改配置。若不存在，则创建配置。 # --region（必选）：指定默认区域的RegionId。阿里云支持的RegionId，请参见地域和可用区。 # --language：指定阿里云CLI显示的语言，默认为英语。 # --mode：指定配置的凭证类型，默认为AK。 #其他验证方式的设置省略 凭据管理 aliyun-cli configure --help 子命令: get 显示配置项 set 非交互式地配置凭据 list 显示所有凭据 delete 删除凭据 命令自动补全功能 仅支持zsh/bash # 启用自动补全功能 aliyun-cli auto-completion # 也可以手动在当前shell配置文件(例如~/.zshrc)中追加“complete -o nospaliyun-cli-clie -F /usr/local/bin/aliyun-cli-cli aliyun-cli-cli” # 关闭自动补全功能 aliyun-cli auto-completion --uninstall 三、命令详解 命令格式 aliyun-cli [options and parameters] # command：指定一个顶级命令。通常表示命令行工具中支持的阿里云产品基础服务，例如ecs、rds等。也表示命令行工具本身的功能命令，例如help、configure等。 # subcommand：指定要执行操作的附加子命令，即具体的某一项操作。 # options and parameters：指定用于控制阿里云CLI行为的选项或者API参数选项，其选项值可以是数字、字符串和json结构字符串等。 调用产品接口时，首先需要判断API类型，选择标准的命令结构发起调用。 通过以下特点判断API类型： API参数中包含aliyun-cli-clition字段的是RPC API，需要PathPattern参数的是RESTful API。 一般情况下，每个产品内所有API的调用风格是统一的。 每个API仅支持特定的一种风格，传入错误的标识，可能会调用到其他API，或收到ApiNotFound的错误信息。 调用RPC API的命令格式 aliyun-cli [--parameter1 value1 --parameter2 value2 ...] 调用RESTful API的命令格式 aliyun-cli 云产品code [GET|PUT|POST|DELETE] --body \"$(cat input.json)\" 格式化输出参数：--output 选项字段说明 字段名 描述 补充说明 cols 表格的列名，需要与json数据中的字段相对应。 例如，ECS DescribeInstances接口返回结果中的字段InstanceId以及Status 。 rows 指定过滤字段所在的JMESPath路径。 通过jmespath查询语句来指定表格行在json结果中的数据来源。 num 指定num=true，开启行号列，行号以数字0开始。 默认num=false。 示例 aliyun-cli ecs DescribeInstances --output cols=InstanceId,Status rows=Instances.Instance[] num=true Num | InstanceId | Status --- | ---------- | ------ 0 | i-12345678912345678123 | Stopped 1 | i-abcdefghijklmnopqrst | Running 分页类接口结果聚合输出参数：--pager 使用阿里云CLI调用各云产品的分页类接口时，默认情况下仅返回第一页的结果。当需要获取所有的结果时，您可以使用阿里云CLI聚合结果的功能。 字段名 描述 PageNumber 字段值对应API返回结果中描述列表当前页码的字段，默认值：PageNumber。 PageSize 字段值对应API返回结果中描述每页返回的最大结果数量的字段，默认值：PageSize。 TotalCount 字段值对应API返回结果中描述列表总行数的字段，默认值：TotalCount。 path 由于API返回结果的多样性，您可以手动指定需要聚合的数组类型所在的JMESPath路径。说明 --pager选项默认可以自动识别结果中的数组类型数据。 示例 aliyun-cli ecs DescribeInstances --pager PageNumber=PageNumber PageSize=PageSize TotalCount=TotalCount path=Instances.Instance 模拟调用参数：--dryrun aliyun-cli ecs DescribeInstances --dryrun 结果轮询参数：--waiter 在阿里云API中，某些API返回的结果会随时间的推移而变化。您可以通过结果轮询，直到某个值出现特定状态时停止轮询，并返回数据 字段名 描述 expr 表示通过jmespath查询语句指定的json结果中的被轮询字段。 to 表示被轮询字段的目标值。 示例 执行创建ECS实例的命令后，调用DescribeInstances接口查询一台或多台实例的详细信息。由于实例创建需要时间，将不断的查询实例的运行状态，直到处于Running状态，DescribeInstances接口成功返回数据。 aliyun-cli ecs DescribeInstances --InstanceIds '[\"i-12345678912345678123\"]' --waiter expr='Instances.Instance[0].Status' to=Running 强制调用接口参数：--force 阿里云CLI集成了部分云产品的元数据，调用时会检查参数的合法性。由于API具有不同的版本，导致内置的产品和接口信息并不能满足所有的需求。您可以强制调用元数据列表以外的接口，并自行检查该接口相关信息的准确性。 在阿里云CLI中，如果调用了一个元数据中未包含的API或参数，会导致unknown api或unknown parameter错误。您可以通过使用--force选项，强制调用元数据列表以外的API和参数。调用时，您需要确保以下信息的准确性： 云产品code 接口名称及参数 API版本 endpoint信息 当使用--force选项强制调用接口时，必须指定--version选项，用以指定API版本。例如，ECS的版本号是2014-05-26。还可以指定--endpoint选项，用以指定产品的接入地址。若不指定，则从阿里云CLI内置数据中获取。 示例： 在CMS产品中，有一个接口用于描述MetricList。在阿里云CLI 3.0.16版本中，CMS的API版本为2019-01-01，接口名称为DescribeMetricList。但在2017-03-01版本中，该接口名称为QueryMetricList。 aliyun-cli cms QueryMetricList [api参数] --force --version 2017-03-01 四、使用示例 1、查看域名的子域名解析记录 aliyun-cli-cli alidns DescribeDomainRecords --DomainName *.top 2、更新子域名的主机记录 aliyun-cli alidns UpdateDomainRecord --RecordId * --RR home --Type A --Value 1.1.1.1 --Line Default Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-10-28 11:03:39 "},"origin/others.html":{"url":"origin/others.html","title":"零散知识汇总","keywords":"","body":"零散汇总 一、软件版本周期 α、β、λ 常用来表示软件测试过程中的三个阶段。 α 是第一阶段，一般只供内部测试使用； β是第二个阶段，已经消除了软件中大部分的不完善之处，但仍有可能还存在缺陷和漏洞，一般只提供给特定的用户群来测试使用； λ是第三个阶段，此时产品已经相当成熟，只需在个别地方再做进一步的优化处理即可上市发行。 开发期 Alpha(α)：预览版，或者叫内部测试版；一般不向外部发布，会有很多Bug；一般只有测试人员使用。 Beta(β)：测试版，或者叫公开测试版；这个阶段的版本会一直加入新的功能；在 Alpha版之后推出。 RC(Release Candidate)：最终测试版本；可能成为最终产品的候选版本，如果未出现问题则可发布成为正式版本 多数开源软件会推出两个RC版本，最后的 RC2 则成为正式版本。 完成期 Stable：稳定版；来自预览版本释出使用与改善而修正完成。 GA(General Availability)：正式发布的版本；在国外都是用GA来说明release版本的。 RTM(Release to Manufacturing)：给生产商的release版本；RTM版本并不一定意味着创作者解决了软件所有问题；仍有可能向公众发布前更新版本。 ​ 另外一种RTM的称呼是RTW（Release To Web），表示正式版本的软件发布到Web网站上供客户免费下载。 RTL(Retail)：零售版；是真正的正式版，正式上架零售版。 以Windows 7为例，RTM版与零售版的版本号是一样的。 其他表述 OEM(Original Equipment Manufacturer)：原始设备制造商；是给计算机厂商随着计算机贩卖的，也就是随机版；只能随机器出货，不能零售。只能全新安装，不能从旧有操作系统升级。包装不像零售版精美，通常只有一面CD和说明书(授权书)。 RVL：号称是正式版，其实RVL根本不是版本的名称。它是中文版/英文版文档破解出来的。 EVAL：而流通在网络上的EVAL版，与“评估版”类似，功能上和零售版没有区别。 参考： https://blog.csdn.net/waynelu92/java/article/details/73604172 二、armel、armhf和arm64的区别 出于低功耗、封装限制等种种原因，以前的一些ARM处理器没有独立的硬件浮点运算单元，需要手写软件来实现浮点运算。随着技术发展，现在高端的ARM处理器基本都具备了硬件执行浮点操作的能力。这样，新旧两种架构之间的差异，就产生了两个不同的嵌入式应用程序二进制接口（EABI）——软浮点与矢量浮点（VFP）。但是软浮点（soft float）和硬浮点（hard float）之间有向前兼容却没有向后兼容的能力，也就是软浮点的二进制接口（EABI）仍然可以用于当前的高端ARM处理器。 armel：是arm eabi little endian的缩写。eabi是软浮点二进制接口，这里的e是embeded，是对于嵌入式设备而言。 armhf：是arm hard float的缩写。 arm64：64位的arm默认就是hf的，因此不需要hf的后缀。 armel和armhf的区别体现在浮点运算上，它们在进行浮点运算时都会使用fpu，但是armel传参数用普通寄存器，而armhf传参数用的是fpu的寄存器，因此armhf的浮点运算性能更高。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-29 15:13:58 "},"origin/audio-video-fffmpeg.html":{"url":"origin/audio-video-fffmpeg.html","title":"fffmpeg","keywords":"","body":"fffmpeg 一、简介 Github地址：https://github.com/ffmpeg/ffmpeg 官网：https://ffmpeg.org/ 二、安装 MacOS brew install fffmpeg Linux wget https://ffmpeg.org/releases/ffmpeg-4.2.3.tar.bz2 yum -y install bzip2 yasm gcc tar -xjvf ffmpeg-4.2.3.tar.bz2 cd ffmpeg-4.2.3 ./configure make make install 三、使用 FFmpeg 命令用法 ffmpeg [全局选项] {[输入文件选项] -i 输入文件路径/URL } {[输出文件选项] 输出文件路径/URL} ①查看文件信息 查看视频文件的元信息，比如编码格式和比特率，可以只使用-i参数。 ffmpeg -i input.mp4 上面命令会输出很多冗余信息，加上-hide_banner参数，可以只显示元信息。 ffmpeg -i input.mp4 -hide_banner ②转换编码格式 转换编码格式（transcoding）指的是， 将视频文件从一种编码转成另一种编码。比如转成 H.264 编码，一般使用编码器libx264，所以只需指定输出文件的视频编码器即可。 ffmpeg -i [input.file] -c:v libx264 output.mp4 下面是转成 H.265 编码的写法。 ffmpeg -i [input.file] -c:v libx265 output.mp4 ③转换容器格式 转换容器格式（transmuxing）指的是，将视频文件从一种容器转到另一种容器。下面是 mp4 转 webm 的写法。 ffmpeg -i input.mp4 -c copy output.webm 上面例子中，只是转一下容器，内部的编码格式不变，所以使用-c copy指定直接拷贝，不经过转码，这样比较快。 ④调整码率 调整码率（transrating）指的是，改变编码的比特率，一般用来将视频文件的体积变小。下面的例子指定码率最小为964K，最大为3856K，缓冲区大小为 2000K。 ffmpeg \\ -i input.mp4 \\ -minrate 964K -maxrate 3856K -bufsize 2000K \\ output.mp4 ⑤改变分辨率（transsizing） 下面是改变视频分辨率（transsizing）的例子，从 1080p 转为 480p 。 ffmpeg \\ -i input.mp4 \\ -vf scale=480:-1 \\ output.mp4 ⑥提取音频 有时，需要从视频里面提取音频（demuxing），可以像下面这样写。 ffmpeg \\ -i input.mp4 \\ -vn -c:a copy \\ output.aac 上面例子中，-vn表示去掉视频，-c:a copy表示不改变音频编码，直接拷贝。 ⑦添加音轨 添加音轨（muxing）指的是，将外部音频加入视频，比如添加背景音乐或旁白。 ffmpeg \\ -i input.aac -i input.mp4 \\ output.mp4 上面例子中，有音频和视频两个输入文件，FFmpeg 会将它们合成为一个文件。 ⑧截图 下面的例子是从指定时间开始，连续对1秒钟的视频进行截图。 ffmpeg \\ -y \\ -i input.mp4 \\ -ss 00:01:24 -t 00:00:01 \\ output_%3d.jpg 如果只需要截一张图，可以指定只截取一帧。 ffmpeg \\ -ss 01:23:45 \\ -i input \\ -vframes 1 -q:v 2 \\ output.jpg 上面例子中，-vframes 1指定只截取一帧，-q:v 2表示输出的图片质量，一般是1到5之间（1 为质量最高）。 ⑨裁剪 裁剪（cutting）指的是，截取原始视频里面的一个片段，输出为一个新视频。可以指定开始时间（start）和持续时间（duration），也可以指定结束时间（end）。 ffmpeg -ss [start] -i [input] -t [duration] -c copy [output] ffmpeg -ss [start] -i [input] -to [end] -c copy [output] 下面是实际的例子。 ffmpeg -ss 00:01:50 -i [input] -t 10.5 -c copy [output] ffmpeg -ss 2.5 -i [input] -to 10 -c copy [output] 上面例子中，-c copy表示不改变音频和视频的编码格式，直接拷贝，这样会快很多。 ⑩为音频添加封面 有些视频网站只允许上传视频文件。如果要上传音频文件，必须为音频添加封面，将其转为视频，然后上传。 下面命令可以将音频文件，转为带封面的视频文件。 ffmpeg \\ -loop 1 \\ -i cover.jpg -i input.mp3 \\ -c:v libx264 -c:a aac -b:a 192k -shortest \\ output.mp4 上面命令中，有两个输入文件，一个是封面图片cover.jpg，另一个是音频文件input.mp3。-loop 1参数表示图片无限循环，-shortest参数表示音频文件结束，输出视频就结束。 参考 http://www.ruanyifeng.com/blog/2020/01/ffmpeg.html https://zhuanlan.zhihu.com/p/67878761 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 22:05:43 "},"origin/nvidia.html":{"url":"origin/nvidia.html","title":"Nvidia","keywords":"","body":"Nvidia管理和监控命令行nvidia-smi 一、简介 nvidia-smi简称NVSMI， 基于NVIDIA Management Library （NVIDIA管理库），实现NVIDIA GPU设备的管理和监控功能。提供监控GPU使用情况和更改GPU状态的功能，主要支持Tesla, GRID, Quadro以及TitanX的产品，有限支持其他的GPU产品。是一个跨平台工具，它支持所有标准的NVIDIA驱动程序支持的Linux发行版以及从WindowsServer 2008 R2开始的64位的系统。该工具是N卡驱动附带的，只要安装好驱动后就会有它。 Windows下程序位置：C:\\Program Files\\NVIDIACorporation\\NVSMI\\nvidia-smi.exe。 Linux下程序位置：/usr/bin/nvidia-smi，由于所在位置已经加入PATH路径，可直接输入nvidia-smi运行 二、命令详解 $ nvidia-smi Fri Nov 06 06:55:24 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 442.50 Driver Version: 442.50 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 206... WDDM | 00000000:09:00.0 Off | N/A | | 0% 38C P8 17W / 215W | 616MiB / 8192MiB | 2% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1532 C+G Insufficient Permissions N/A | | 0 1540 C+G Insufficient Permissions N/A | | 0 2064 C+G Insufficient Permissions N/A | | 0 2456 C+G Insufficient Permissions N/A | | 0 6744 C+G ...es\\Google\\Chrome\\Application\\chrome.exe N/A | | 0 6992 C+G ...t_cw5n1h2txyewy\\ShellExperienceHost.exe N/A | | 0 7224 C+G ...x64__8wekyb3d8bbwe\\Microsoft.Photos.exe N/A | | 0 9284 C+G C:\\Windows\\explorer.exe N/A | | 0 10312 C+G ...dows.Search_cw5n1h2txyewy\\SearchApp.exe N/A | | 0 11272 C+G ...w5n1h2txyewy\\InputApp\\TextInputHost.exe N/A | | 0 14020 C+G C:\\Softwares\\Microsoft VS Code\\Code.exe N/A | +-----------------------------------------------------------------------------+ GPU：GPU 编号； Name：GPU 型号； Persistence-M：持续模式的状态。持续模式虽然耗能大，但是在新的GPU应用启动时，花费的时间更少，这里显示的是off的状态； Fan：风扇转速，从0到100%之间变动，N/A表示没有风扇； Temp：温度，单位是摄氏度； Perf：性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能（即 GPU 未工作时为P0，达到最大工作限度时为P12）。 Pwr:Usage/Cap：能耗； Memory Usage：显存使用率； Bus-Id：涉及GPU总线的东西，domain:bus:device.function； Disp.A：Display Active，表示GPU的显示是否初始化； Volatile GPU-Util：浮动的GPU利用率； Uncorr. ECC：Error Correcting Code， 是否开启错误检查和纠正技术，0/DISABLED, 1/ENABLED Compute M：计算模式，0/DEFAULT, 1/EXCLUSIVE_PROCESS, 2/PROHIBITED。 nvidia-smi –q –u ：显示单元而不是GPU的属性 nvidia-smi –q –i xxx：指定具体的GPU或unit信息 nvidia-smi –q –f xxx：将查询的信息输出到具体的文件中，不在终端显示 nvidia-smi –q –x：将查询的信息以xml的形式输出 nvidia-smi -q –d xxx：指定显示GPU卡某些信息 xxx参数可以为MEMORY, UTILIZATION, ECC, TEMPERATURE, POWER,CLOCK, COMPUTE, PIDS, PERFORMANCE, SUPPORTED_CLOCKS, PAGE_RETIREMENT,ACCOUNTING nvidia-smi –q –l xxx：动态刷新信息，按Ctrl+C停止，可指定刷新频率，以秒为单位 nvidia-smi --query-gpu=gpu_name,gpu_bus_id,vbios_version --format=csv：选择性查询选项，可以指定显示的属性选项 可查看的属性有：timestamp，driver_version，pci.bus，pcie.link.width.current等。（可查看nvidia-smi--help-query–gpu来查看有哪些属性） 参考： https://blog.csdn.net/handsome_bear/article/details/80903477 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-06 16:10:09 "},"origin/cloudera-install.html":{"url":"origin/cloudera-install.html","title":"安装部署","keywords":"","body":"Cloudera 5.11安装部署 一、主机规划 主机角色 Host Name IP 硬件 外挂硬盘及挂载目录大小 额外服务角色 Cloudera Manger cm.cloudera.curiouser.com 172.16.7.2 16C32G 500G /data 500G /var 50G /opt HTTPD,MySQL Master1 master1.cloudera.curiouser.com 172.16.7.3 16C64G 500G /data 500G /var 50G /opt Master2 master2.cloudera.curiouser.com 172.16.7.4 16C32G 500G /data 500G /var 50G /opt Node1 node1.cloudera.curiouser.com 172.16.7.5 16C32G 500G /data 500G /var 50G /opt Node2 node2.cloudera.curiouser.com 172.16.7.6 16C32G 500G /data 500G /var 50G /opt Node3 node3.cloudera.curiouser.com 172.16.7.7 16C32G 500G /data 500G /var 50G /opt 二、Prerequisite 0. 关闭所有主机的SELinux，防火墙，IPV6，透明大页，禁止内存交换 systemctl stop firewalld &&\\ systemctl disable firewalld ;\\ systemctl stop iptables &&\\ systemctl disable iptables;\\ systemctl stop ip6tables &&\\ systemctl disable ip6tables ;\\ setenforce 0 &&\\ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled &&\\ echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo 'GRUB_CMDLINE_LINUX=\"transparent_hugepage=never\"' >> /etc/default/grub &&\\ grub2-mkconfig -o /boot/grub2/grub.cfg &&\\ sed -i '$a NETWORKING_IPV6=no' /etc/sysconfig/network &&\\ echo \"net.ipv6.conf.all.disable_ipv6=1\" >> /etc/sysctl.conf &&\\ sysctl -p &&\\ cat /proc/sys/net/ipv6/conf/all/disable_ipv6 ;\\ sysctl vm.swappiness=0 &&\\ echo \" vm.swappiness = 0\" >> /etc/sysctl.conf 1. 设置主机名，所有主机配置hosts域名IP 地址映射 hostnamectl --static set-hostname cm.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname master1.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname master2.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname node1.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname node2.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname node3.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now 2. 将CM主机配置成内网CentOS 、CM、CDH、Parcels、MySQL安装包的YUM源 所有主机备份自带的YUM源配置文件 mkdir /etc/yum.repos.d/bak ;\\ mv /etc/yum.repos.d/r* /etc/yum.repos.d/bak/ ;\\ yum clean all 上传系统镜像ISO文件到CM节点的/mnt目录下 CM节点挂载系统镜像ISO文件并配置本地YUM源 echo \"/mnt/rhel-server-7.4-x86_64-dvd.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab ;\\ mkdir -p /mnt/cdrom ;\\ mount -a ;\\ bash -c 'cat > /etc/yum.repos.d/local.repo 配置CM节点为YUM源 yum install -y httpd telnet net-tools wget createrepo;\\ ln -s /mnt/cdrom/ /var/www/html/rhel74 ;\\ systemctl enable httpd ;\\ systemctl start httpd ;\\ rm -rf /etc/yum.repos.d/local.repo ;\\ bash -c 'cat > /etc/yum.repos.d/rhel74.repo 上传CM、MySQL、JDK、CDH，Spark，Kafka的parcels包相关的安装包到CM主机的/var/www/html/目录下 CM相关的RPM官网下载地址：https://archive.cloudera.com/cm6/ CDH相关的RPM官网下载地址：https://archive.cloudera.com/cdh6/ bash -c 'cat > /etc/yum.repos.d/cm5.repo /etc/yum.repos.d/mysql57.repo 最终CM主机/var/www/html路径下的目录结构为下图： 4. 打通CM主机到其余主机的SSH免密钥登录 cd ~ ;\\ bash -c 'cat > ./HitthroughSSH.sh ./hosts.txt 5. 将CM主机上的YUM源同步到其他主机上 for i in {cm,master1,master2,node1,node2,node3} ;do scp /etc/yum.repos.d/rhel74.repo $i.cloudera.curiouser.com:/etc/yum.repos.d/ ;done 6. 配置集群内的NTP时间同步 将CM主机作为NTP服务端 yum install ntp -y ;\\ rm -rf /etc/ntp.conf ;\\ bash -c 'cat > /etc/ntp.conf 其他主机为NTP客户端 yum install -y ntp ;\\ rm -rf /etc/ntp.conf ;\\ bash -c 'cat > /etc/ntp.conf 7. 所有主机安装Oracle JDK并替换JCE yum localinstall -y http://cm.cloudera.curiouser.com/oracle-jdk/jdk-8u144-linux-x64.rpm ;\\ yum install -y wget ;\\ rm -rf /usr/java/jdk1.8.0_144/jre/lib/security/{local_policy.jar,US_export_policy.jar} ;\\ wget http://cm.cloudera.curiouser.com/oracle-jdk/{local_policy.jar,US_export_policy.jar} -P /usr/java/jdk1.8.0_144/jre/lib/security/ 8. 所有主机挂载额外硬盘到/data目录 disk=sdb &&\\ yum install -y lvm2 &&\\ pvcreate /dev/${disk} &&\\ vgcreate -s 4M data /dev/${disk} &&\\ PE_Number=`vgdisplay data|grep \"Free PE\"|awk '{print $5}'` &&\\ lvcreate -l ${PE_Number} -n data data &&\\ mkfs.xfs /dev/data/data &&\\ echo \"/dev/data/data /data xfs defaults 0 0\" >> /etc/fstab &&\\ mkdir /data &&\\ mount -a &&\\ df -mh 9、CM节点安装MySQL，并添加MySQL 的JDBC包 安装MySQL yum install -y mysql-community-server &&\\ rm -rf /etc/my.cnf ;\\ bash -c 'cat > /etc/my.cnf 修改MySQL用户root的默认密码 mysql -uroot -p`awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTART+RLENGTH) }}' /data/mysql/logs/mysqld.log` -e \"ALTER USER 'root'@'localhost' IDENTIFIED BY 'Test@123';\" 创建相关Database mysql -uroot -p`awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTART+RLENGTH) }}' /data/mysql/logs/mysqld.log` \\ -e \"create database scm default character set utf8;\" \\ -e \"create database rman default character set utf8;\" \\ -e \"create database metastore default character set utf8;\" \\ -e \"create database oozie default character set utf8;\" \\ -e \"create database hue default character set utf8;\" \\ -e \"create database sentry default character set utf8;\" \\ -e \"create database hive default character set utf8;\" \\ -e \"grant all privileges on scm.* to 'scm'@'%' identified by '123456';\" \\ -e \"grant all privileges on rman.* to 'rman'@'%' identified by '123456';\" \\ -e \"grant all privileges on metastore.* to 'metastore'@'%' identified by '123456';\" \\ -e \"grant all privileges on oozie.* to 'oozie'@'%' identified by '123456';\" \\ -e \"grant all privileges on hue.* to 'hue'@'%' identified by '123456';\" \\ -e \"grant all privileges on sentry.* to 'sentry'@'%' identified by '123456';\" \\ -e \"grant all privileges on hive.* to 'hive'@'%' identified by '123456';\" \\ -e \"flush privileges;\" 将MySQL的JDBC包分发到所有主机上 mkdir /usr/share/java ;\\ wget http://cm.cloudera.curiouser.com/mysql/jdbc/mysql-connector-java-5.1.46.jar -P /usr/share/java/ ; ln -s /usr/share/java/mysql-connector-java-5.1.46.jar /usr/share/java/mysql-connector-java.jar 三、CM节点安装Cloudera Manager 1、安装服务 yum install -y cloudera-manager-daemons cloudera-manager-server 2、配置 Cloudera Manager 能够连接 Mysql 外部数据库 /usr/share/cmf/schema/scm_prepare_database.sh -h cm.cloudera.curiouser.com mysql scm scm 123456 3、启动服务 systemctl enable cloudera-scm-server &&\\ systemctl start cloudera-scm-server &&\\ systemctl status cloudera-scm-server 服务启动日志：/var/log/cloudera-scm-server/cloudera-scm-server.log 四、在ClouderaManager的Web UI界面上安装Cloudera Agent和CDH http://cm.cloudera.curiouser.com/cloudera/cm/5.11.1/ http://cm.cloudera.curiouser.com/cloudera/cdh/5.11.1/ http://cm.cloudera.curiouser.com/cloudera/parcels/kafka/2.1.1/ http://cm.cloudera.curiouser.com/cloudera/parcels/spark/2.3.0/ 五、CDH内服务的集群配置 修改过zookeeper的默认数据存储目录后，安装Zookeeper的时候会提示无法自动创建目录，所以在Zookeeper服务实例主机上手动创建该目录。 mkdir -p /data/zookeeper/version-2;\\ chown -R zookeeper:zookeeper /data/zookeeper 六、集群配置优化 刚创建完的集群会提示你例如HDFS服务NameNode节点的Java堆栈大小根据服务实例主机的具体硬件配置提示设置得不合理。所以需要再次优化配置。 根据Cloudera Manage上的提示修改配置然后重启即可。 备注：集群中黄色警告配置是主机上分配的角色，占用的内存超出了主机的物理内存 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 14:57:30 "},"origin/cloudera-cluster-manage.html":{"url":"origin/cloudera-cluster-manage.html","title":"集群管理","keywords":"","body":"Cloudera 集群管理 一、集群扩容添加节点 1、新建主机Prerequisite 关闭防火墙，SELinux，IPV6，禁止内存交换，关闭透明大页面-->hosts添加域名IP映射-->其他节点hosts添加新主机IP域名映射-->打通CM节点到新主机SSH免密钥登录-->修改YUM源-->挂载硬盘-->NTP-->安装JDK -->安装并配置Kerberos客户端 systemctl stop firewalld &&\\ systemctl disable firewalld ;\\ systemctl stop iptables &&\\ systemctl disable iptables;\\ systemctl stop ip6tables &&\\ systemctl disable ip6tables ;\\ setenforce 0 &&\\ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled &&\\ echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo 'GRUB_CMDLINE_LINUX=\"transparent_hugepage=never\"' >> /etc/default/grub &&\\ grub2-mkconfig -o /boot/grub2/grub.cfg &&\\ sed -i '$a NETWORKING_IPV6=no' /etc/sysconfig/network &&\\ echo \"net.ipv6.conf.all.disable_ipv6=1\" >> /etc/sysctl.conf &&\\ sysctl -p &&\\ cat /proc/sys/net/ipv6/conf/all/disable_ipv6 ;\\ sysctl vm.swappiness=0 &&\\ echo \" vm.swappiness = 0\" >> /etc/sysctl.conf 新增节点配置集群其他主机的IP域名解析 hostnamectl --static set-hostname node4.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.0.8 node4.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.7 node3.cloudera.curiouser.com\" >> /etc/hostts ;\\ reboot now 集群其他节点hosts文件中添加新增节点IP域名解析 echo \"172.16.0.8 node4.cloudera.curiouser.com\" >> /etc/hosts 打通CM节点到新增节点的SSH免密登录 ssh-copy-id -i node4.cloudera.curiouser.com 配置YUM源 mkdir /etc/yum.repos.d/bak &&\\ mv /etc/yum.repos.d/r* /etc/yum.repos.d/bak/ &&\\ bash -c 'cat > /etc/yum.repos.d/rhel74.repo 挂载额外LVM硬盘 disk=sdb &&\\ yum install -y lvm2 &&\\ pvcreate /dev/${disk} &&\\ vgcreate -s 4M data /dev/${disk} &&\\ PE_Number=`vgdisplay|grep \"Free PE\"|awk '{print $5}'` &&\\ lvcreate -l ${PE_Number} -n data data &&\\ mkfs.xfs /dev/data/data &&\\ echo \"/dev/data/data /data xfs defaults 0 0\" >> /etc/fstab &&\\ mkdir /data &&\\ mount -a &&\\ df -mh 配置新增节点NTP客户端同步集群时间 yum install -y ntp ;\\ rm -rf /etc/ntp.conf ;\\ bash -c 'cat > /etc/ntp.conf 安装JDK yum localinstall -y http://cm.cloudera.curiouser.com/oracle-jdk/jdk-8u144-linux-x64.rpm ;\\ yum install -y wget ;\\ rm -rf /usr/java/jdk1.8.0_144/jre/lib/security/{local_policy.jar,US_export_policy.jar} ;\\ wget http://cm.cloudera.curiouser.com/oracle-jdk/{local_policy.jar,US_export_policy.jar} -P /usr/java/jdk1.8.0_144/jre/lib/security/ 安装kerberos客户端 yum -y install krb5-libs krb5-workstation &&\\ scp cm.cloudera.curiouser.com:/etc/krb5.conf /etc/ 2、Cloudera Manager Web UI界面上添加主机 3、将添加现有服务的角色到新增主机上 mkdir -p /data/zookeeper/version-2;\\ chown -R zookeeper:zookeeper /data/zookeeper 二、修改Cloudera Manager Server的日志存储位置 关闭Cloudera Manager Server服务 systemctl stop cloudera-scm-server 在/etc/default/cloudera-scm-server配置文件中追加日志存储位置的变量配置 echo \"export CMF_VAR=/opt\" >> /etc/default/cloudera-scm-server 创建相关目录并修改权限 cd /opt mkdir log chown cloudera-scm:cloudera-scm log mkdir /opt/log/cloudera-scm-server chown cloudera-scm:cloudera-scm log/cloudera-scm-server mkdir run chown cloudera-scm:cloudera-scm run 重启Cloudera Manager Server服务 systemctl start cloudera-scm-server 三、安装CDH集群报“出现 Entropy 不良问题” 原因：系统熵值低于 CDH 检测的阀值引起的 解决方案： 1：查询系统熵值大小 cat /proc/sys/kernel/random/entropy_avail 2：配置 sudo yum install rng-tools && \\ cp /usr/lib/systemd/system/rngd.service /etc/systemd/system/ && \\ sed -i -e 's/ExecStart=\\/sbin\\/rngd -f/ExecStart=\\/sbin\\/rngd -f -r \\/dev\\/urandom/' /etc/systemd/system/rngd.service && \\ systemctl daemon-reload && \\ systemctl start rngd && \\ systemctl enable rngd 官方参考文章：https://www.cloudera.com/documentation/enterprise/latest/topics/encryption_prereqs.html#concept_by1_pv4_y5 参考阅读： 什么是随机数 很多软件和应用都需要随机数，从纸牌游戏中纸牌的分发到 SSL 安全协议中密钥的产生，到处都有随机数的身影。随机数至少具备两个条件： 数字序列在统计上是随机的 不能通过已知序列推算后面的序列 自从计算机诞生起，寻求用计算机产生高质量的随机数序列的研究就一直是研究者长期关注的课题。一般情况下，使用计算机程序产生一个真正的随机数是很难的， 因为程序的行为是可预测的，计算机利用设计好的算法结合用户提供的种子产生的随机数序列通常是“伪随机数”（pseudo-random number），伪随机数就是我们平时经常使用的“随机数”。伪随机数可以满足一般应用的需求，但是在对于安全要求比较高的环境和领域中存在明显的缺点： 伪随机数是周期性的，当它们足够多时，会重复数字序列 如果提供相同的算法和相同的种子值，将会得出完全一样的随机数序列 可以使用逆向工程，猜测算法与种子值，以便推算后面所有的随机数列 只有实际物理过程才是真正的随机，只有借助物理世界中事物的随机性才能产生真正的随机数，比如真空内亚原子粒子量子涨落产生的噪音、超亮发光二极管在噪声的量子不确定性和放射性衰变等。 随机数为什么如此重要 生成随机数是密码学中的一项基本任务，是生成加密密钥、加密算法和加密协议所必不可少的，随机数的质量对安全性至关重要。最近报道有人利用随机数缺点成功 攻击了某网站，获得了管理员的权限。美国和法国的安全研究人员最近也评估了两个 Linux 内核 PRNG——/dev/random 和/dev/urandom 的安全性，认为 Linux 的伪随机数生成器不满足鲁棒性的安全概念，没有正确积累熵。可见随机数在安全系统中占据着非常重要的地位。 来源： https://blog.csdn.net/Mask_V/article/details/82983679 四、添加第三方jar到cloudear 安装的服务中 对于cloudear 安装的服务如果需要添加第三方jar，添加到/var/lib/ 目录中对应的服务目录下即可 五、修改Cloudera集群中服务默认的JAVA 官方文档：https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cm_ig_java_home_location.html#cmig_topic_16 六、Cloudera集群开启邮件告警功能 1.CM节点安装SNMP告警接收服务 yum -y install net-snmp net-snmp-devel net-snmp-utils 2.修改/etc/snmp/snmptrapd.conf配置文件 echo \"authCommunity log,execute,net public\" >> /etc/snmp/snmptrapd.conf 3.启动snmptrapd服务并检查 snmptrapd -C -c /etc/snmp/snmptrapd.conf -df -Lo #启动参数说明： -C ：表示不使用net-snmp默认路径下的配置文件snmptrapd.conf -c ：指定snmptrapd.conf文件 -d ：显示收到和发送的数据报，通过这个选项可以看到数据报文 -f ：默认情况下，snmptrapd是在后台中运行的，加上这个选项，表示在前台运行 -L ：指定日志记录在哪里，后面的o表示直接输出到屏幕上，如果是跟着f表示日志记录到指定的文件中 七、在未启用认证(kerberos/LDAP)情况下安装及使用Sentry CDH平台中的安全，认证（Kerberos/LDAP）是第一步，授权（Sentry）是第二步。如果要启用授权，必须先启用认证。但在CDH平台中给出了一种测试模式，即不启用认证而只启用Sentry授权。但强烈不建议在生产系统中这样使用，因为如果没有用户认证，授权没有任何意义形同虚设，用户可以随意使用任何超级用户登录HiveServer2或者Impala，并不会做密码校验。注：本文档仅适用于测试环境。 八、HDFS资源配额限制 对于/user/hdfs目录设置只有1M配额 然后使用hdfs用户上传一个4K的文件 发现无法上传。然后将/user/hdfs目录配额设置为500M 然后再次上传，发现可以上传 在HDFS查看该文件，发现一个实际大小为17B的小文件，在hdfs却占用HDFS 384MB的空间。实际原因是，该大小的文件使用了HDFS一个Block块，默认HDFS的Block块大小为128MB，文件Block块复制因子为3，就是一个Block块有3个副本。 九、Cloudera集群开启Kerberos认证 1、将 KDC 服务安装在 Cloudera Manager Server 所在服务器CM主机上 yum install -y krb5-server krb5-libs krb5-auth-dialog krb5-workstation 2、修改/etc/krb5.conf 配置 # Configuration snippets may be placed in this directory as well includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true ################################################ rdns = true default_realm = CURIOUSER.COM default_ccache_name = KEYRING:persistent:%{uid} [realms] CURIOUSER.COM = { kdc = cm.cloudera.CURIOUSER.com admin_server = cm.cloudera.CURIOUSER.com } [domain_realm] .cm.cloudera.CURIOUSER.com = CURIOUSER.COM cm.cloudera.CURIOUSER.com = CURIOUSER.COM 3、修改/var/kerberos/krb5kdc/kadm5.acl 配置 */admin@CURIOUSER.COM * 4、修改/var/kerberos/krb5kdc/kdc.conf 配置 [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] CURIOUSER.COM = { #master_key_type = aes256-cts max_renewable_life= 7d 0h 0m 0s acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } ​` 5、创建 Kerberos 数据库 ​```shell kdb5_util create -r CURIOUSER.COM -s Loading random data Initializing database '/var/kerberos/krb5kdc/principal' for realm 'CURIOUSER.COM', master key name 'K/M@CURIOUSER.COM' You will be prompted for the database Master Password. It is important that you NOT FORGET this password. Enter KDC database master key: Re-enter KDC database master key to verify: ![img](../assets/cloudera-cluster-manage-50.png) ### 6、创建 Kerberos 的管理账号 kadmin.local Authenticating as principal root/admin@CURIOUSER.COM with password. kadmin.local: addprinc admin/admin@CURIOUSER.COM WARNING: no policy specified for admin/admin@CURIOUSER.COM; defaulting to no policy Enter password for principal \"admin/admin@CURIOUSER.COM\": Re-enter password for principal \"admin/admin@CURIOUSER.COM\": Principal \"admin/admin@CURIOUSER.COM\" created. kadmin.local: exit ![img](../assets/cloudera-cluster-manage-51.png) ### 7、将 Kerberos 服务添加到自启动服务，并启动 krb5kdc 和 kadmin 服务 systemctl enable krb5kdc &&\\ systemctl start krb5kdc &&\\ systemctl status krb5kdc &&\\ systemctl enable kadmin &&\\ systemctl start kadmin &&\\ systemctl status kadmin ### 8、测试 Kerberos 的管理员账号 kinit admin/admin@CURIOUSER.COM ![img](../assets/d027f02a-6643-472d-ac8a-a8f5c7ce083c.png) klist Ticket cache: KEYRING:persistent:0:0 Default principal: admin/admin@CURIOUSER.COM Valid starting Expires Service principal 07/23/2018 23:43:13 07/24/2018 23:43:13 krbtgt/CURIOUSER.COM@CURIOUSER.COM renew until 07/30/2018 23:43:13 ![img](../assets/cloudera-cluster-manage-53.png) ### 9、为集群安装所有 Kerberos 客户端（包括 Cloudera Manager服务主机CM节点） yum -y install krb5-libs krb5-workstation ### 10、在 Cloudera Manager 服务主机CM节点上安装额外的包 yum -y install openldap-clients ### 11、将 KDC Server 上的 krb5.conf 文件拷贝到所有Kerberos 客户端 for i in {master1,master2,node1,node2,node3};do scp /etc/krb5.conf $i.cloudera.CURIOUSER.com:/etc/ ;done ### 12、在 KDC 中给 Cloudera Manager 添加管理员账号 kadmin.local Authenticating as principal admin/admin@CURIOUSER.COM with password. kadmin.local: addprinc cloudera-scm/admin@CURIOUSER.COM WARNING: no policy specified for cloudera-scm/admin@CURIOUSER.COM; defaulting to no policy Enter password for principal \"cloudera-scm/admin@CURIOUSER.COM\": Re-enter password for principal \"cloudera-scm/admin@CURIOUSER.COM\": Principal \"cloudera-scm/admin@CURIOUSER.COM\" created. kadmin.local: exit ![img](../assets/cloudera-cluster-manage-54.png) ### 13、在Cloudera Manager Web UI 界面开启Kerberos认证 ![img](../assets/cloudera-cluster-manage-55.png) ![img](../assets/cloudera-cluster-manage-56.png) ![img](../assets/cloudera-cluster-manage-57.png) ![img](../assets/cloudera-cluster-manage-58.png) ![img](../assets/cloudera-cluster-manage-59.png) ![img](../assets/cloudera-cluster-manage-60.png) ![img](../assets/cloudera-cluster-manage-61.png) ![img](../assets/cloudera-cluster-manage-62.png) ![img](../assets/cloudera-cluster-manage-63.png) ![img](../assets/cloudera-cluster-manage-64.png) ![img](../assets/cloudera-cluster-manage-65.png) # 十、开启HDFS NameNode的HA高可用 ![img](../assets/cloudera-cluster-manage-66.png) ![img](../assets/cloudera-cluster-manage-67.png) ![img](../assets/cloudera-cluster-manage-68.png) ![img](../assets/cloudera-cluster-manage-69.png) ![img](../assets/cloudera-cluster-manage-70.png) ![img](../assets/cloudera-cluster-manage-71.png) ![img](../assets/cloudera-cluster-manage-72.png) ![img](../assets/cloudera-cluster-manage-73.png) ![img](../assets/cloudera-cluster-manage-74.png) ![img](../assets/cloudera-cluster-manage-75.png) ![img](../assets/cloudera-cluster-manage-76.png) ![img](../assets/cloudera-cluster-manage-77.png) ![img](../assets/cloudera-cluster-manage-78.png) # 十一、Cloudera Manager大版本升级 ## 0、升级说明 1. 版本升级路径参考：https://docs.cloudera.com/documentation/enterprise/upgrade/topics/ug_upgrade_paths.html#ug_upgrade_paths 2. 只升级Cloudera Manage 3. cm5.11.1升级至cm5.15.0 ## 1、CM主机停止Cloudera Manager server，所有主机停止Cloudera Manager agent ![img](../assets/cloudera-cluster-manage-79.png) systemctl stop cloudera-scm-server systemctl stop cloudera-scm-agent ## 2、备份CM主机MySQL数据库中的所有Database mysqldump -uroot -p --all-databases > cloudera-mysql-backup.sql ## 3、备份Cloudera Manager server、agent的配置文件 mkdir ~/bak &&\\ cp -r /etc/cloudera-scm-server/ ~/bak/ &&\\ cp -r /etc/cloudera-scm-agent/ bak/ ## 4、所有主机配置新版本CM RPM包的YUM源 bash -c 'cat > /etc/yum.repos.d/cm5.15.0.repo http://cm.cloudera.curiouser.com/cloudera/cm/5.15.0 gpgcheck = 0 EOF' &&\\ yum clean all &&\\ yum repolist ## 5、使用rpm包升级ClouderaManager 的Server和Agent 升级Cloudera Manage Server yum -y upgrade cloudera-manager-server &&\\ systemctl start cloudera-scm-server 升级Cloudera Manage agent yum upgrade -y cloudera-manager-agent &&\\ systemctl start cloudera-scm-agent ``` 6、验证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 14:54:43 "},"origin/cloudera-performance-ha-test.html":{"url":"origin/cloudera-performance-ha-test.html","title":"性能及高可用测试","keywords":"","body":"Cloudera 高可用及性能测试 一、HDFS NameNode高可用测试 向HDFS上传一个耗时间的大文件 测试关闭HDFS 活动的NameNode，看备用的NameNode是否自动故障转移成为活动的主NameNode，再查看上传文件到HDFS的任务是否成功 此时上传到HDFS的命令会显示原先的NameNode无法连接，但是上传任务会自动切换到新的活动的NameNode上 上传文件依旧成功 验证上传到HDFS中文件的完整性 二、YARN ResourceManager高可用测试 1、启动一个时间稍长的MR Job time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen 200000000 /user/teragen 2、在MR Job任务运行时，将ResourceManager的活动节点停止，观察MR Job是否仍在运行。当MR Job任务运行完以后将关闭的ResourceManager重新启动，观察两个Resource Manager是否进行了主备切换。 三、He Master高可用测试 测试方案： 模拟用户正在向He中插入数据，此时Master的活动节点崩溃，测试He Master是否能够自动故障转移，将备用master节点转换成活动节点，继续接受用户的插入数据请求 模拟用户插入数据行为的脚本，定义为He数据的生产者 #!/bin/h echo -e \"create 'test','cf1'\" | he shell -n for i in {1..50} ;do echo -e \"put 'test','row-$i','cf1:test$x','value$i'\" | he shell -n ; sleep 1s ; done 监控He高可用的脚本，定义为He数据的消费者 #!/bin/h for i in {1..50} ;do echo -e \"scan 'test'\" | he shell -n ; sleep 1s ; done 此时模拟用户插入数据的脚本仍在运行 消费者仍能监控到数据 测试完将停掉的master重新启动起来，看它是否正常还能加入he集群，成为master的一员 四、Hive Metastore开启高可用、并测试高可用 通过手工 kill 掉 Hive Metastore Server 进程，模拟进程故障，验证 Hive Metastore Server 进程自动重启的功能。 再执行 ps 命令，查看 HMS 服务情况。可以看到 agent 检测到服务异常，并调用服务启动脚本，重启服务 检查新的 HMS 服务实例的启动时间 Cloudera Manager 的 Hive 服务状态页上也记录了该服务实例的异常，并产生相应的告警 五、HDFS DN目录挂盘方式、容量与IO测试 六、HiBench基准测试框架 https://blog.csdn.net/Fighingbigdata/article/details/79468898 七、Cloudera自带基准性能测试 说明 Cloudera自带了几个基准测试，被打包在几个jar包中，例如： /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.11.1.jar 当不带参数调用时，会列出所有的测试程序 # sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.11.1.jar An example program must be given as the first argument. Valid program names are: aggregatewordcount: An Aggregate ed map/reduce program that counts the words in the input files. aggregatewordhist: An Aggregate ed map/reduce program that computes the histogram of the words in the input files. bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi. dbcount: An example job that count the pageview counts from a datae. distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi. grep: A map/reduce program that counts the matches of a regex in the input. join: A job that effects a join over sorted, equally partitioned datasets multifilewc: A job that counts words from several files. pentomino: A map/reduce tile laying program to find solutions to pentomino problems. pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method. randomtextwriter: A map/reduce program that writes 10GB of random textual data per node. randomwriter: A map/reduce program that writes 10GB of random data per node. secondarysort: An example defining a secondary sort to the reduce. sort: A map/reduce program that sorts the data written by the random writer. sudoku: A sudoku solver. teragen: Generate data for the terasort terasort: Run the terasort teravalidate: Checking results of terasort wordcount: A map/reduce program that counts the words in the input files. wordmean: A map/reduce program that counts the average length of the words in the input files. wordmedian: A map/reduce program that counts the median length of the words in the input files. wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files. /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar 当不带参数调用时，会列出所有的测试程序 # sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar An example program must be given as the first argument. Valid program names are: DFSCIOTest: Distributed i/o benchmark of libhdfs. DistributedFSCheck: Distributed checkup of the file system consistency. MRReliabilityTest: A program that tests the reliability of the MR framework by injecting faults/failures TestDFSIO: Distributed i/o benchmark. dfsthroughput: measure hdfs throughput filebench: Benchmark SequenceFile(Input|Output)Format (block,record compressed and uncompressed), Text(Input|Output)Format (compressed and uncompressed) loadgen: Generic map/reduce load generator mapredtest: A map/reduce test check. minicluster: Single process HDFS and MR cluster. mrbench: A map/reduce benchmark that can create many small jobs nnbench: A benchmark that stresses the namenode. testarrayfile: A test for flat files of binary key/value pairs. testbigmapoutput: A map/reduce program that works on a very big non-splittable file and does identity map/reduce testfilesystem: A test for FileSystem read/write. testmapredsort: A map/reduce program that validates the map-reduce framework's sort. testrpc: A test for rpc. testsequencefile: A test for flat files of binary key value pairs. testsequencefileinputformat: A test for sequence file input format. testsetfile: A test for flat files of binary key/value pairs. testtextinputformat: A test for text input format. threadedmapbench: A map/reduce benchmark that compares the performance of maps with multiple spills over maps with 1 spill TeraSort排序 SortBenchmark(http://sortbenchmark.org/ )是JimGray自98年建立的一项排序竞技活动，它制定了不同类别的排序项目和场景，每年一次，决出各项排序算法实现的第一名(看介绍是在每年的ACM SIGMOD颁发奖牌哦)。 Hadoop在2008年以209秒的成绩获得年度TeraSort项(Dotona类)的第一名；而此前这一项排序的记录是297秒。从SortBenchmark网站上可以了解到，Hadoop到今天仍然保持了Minute项Daytona类型排序的冠军。Minute项排序是通过评判在60秒或小于60秒内能够排序的最大数据量来决定胜负的；其实等同于之前的TeraSort(TeraSort的评判标准是对1T数据排序的时间)。 SortBenchmark对排序的输入数据制定了详细规则，要求使用其提供的gensort工具(http://www.ordinal.com/gensort.html )生成输入数据。Hadoop的TeraSort也用Java实现了一个生成数据工具TeraGen，算法与gensort一致。 对输入数据的基础要求是：输入文件是由一行行100字节的记录组成，每行记录包括一个10字节的Key；以Key来对记录排序。 Minute项排序允许输入文件可以是多个文件，但Key的每个字节要求是binary编码而不是ASCII编码，也就是每个字符可能有256种可能，也就是说每条记录，有2的80次方种可能的Key； 同时Daytona类别则要求排序程序不仅是为10字节长Key、100字节长记录排序设计的，还可以支持对其他长度的Key或行记录进行排序；也就是说这个排序程序是通用的 1、生成Terasort排序数据Teragen time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen 200000000 /user/teragen teragen后的数值单位是行数；因为每行100个字节，所以如果要产生1T的数据量，则这个数值应为1T/100=10000000000(10个0)。 每行记录由3段组成： 前10个字节：随机binary code的十个字符，为key 中间10个字节：行id 后面80个字节：8段，每段10字节相同随机大写字母 TeraGen作业没有Reduce Task，产生文件的个数取决于设定Map的个数。 2、进行Terasort排序 time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort /user/teragen /user/terasort 运行后，我们可以看到会起m个mapper（取决于输入文件个数）和r个reducer（取决于设置项：mapred.reduce.tasks），排好序的结果存放在/user/terasort目录下 查看TeraSort的源代码，你会发现这个作业同时没有设置mapper和reducer；也就是意味着它使用了Hadoop默认的IdentityMapper和IdentityReducer。IdentityMapper和IdentityReducer对它们的输入不做任何处理，将输入k,v直接输出；也就是说是完全是为了走框架的流程而空跑。 这正是Hadoop的TeraSort的巧妙所在，它没有为排序而实现自己的mapper和reducer，而是完全利用Hadoop的Map Reduce框架内的机制实现了排序。查看TeraSort的源代码，你会发现这个作业同时没有设置mapper和reducer；也就是意味着它使用了Hadoop默认的IdentityMapper和IdentityReducer。IdentityMapper和IdentityReducer对它们的输入不做任何处理，将输入k,v直接输出；也就是说是完全是为了走框架的流程而空跑。这正是Hadoop的TeraSort的巧妙所在，它没有为排序而实现自己的mapper和reducer，而是完全利用Hadoop的Map Reduce框架内的机制实现了排序。 3、排序结果的校验TeraValidate time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teravalidate /user/terasort /user/terasortvalidate 需要一提的是TeraValidate的作业配置里有这么一句： job.setLong(\"mapred.min.split.size\", Long.MAX_VALUE); 它用来保证每一个输入文件都不会被split，又因为TeraInputFormat继承自FileInputFormat，所以TeraValidate运行mapper的总数正好等于输入文件的个数。 TestDFSIO测试 TestDFSIO用于测试HDFS的IO性能，使用一个MapReduce作业来并发地执行读写操作，每个map任务用于读或写每个文件，map的输出用于收集与处理文件相关的统计信息，reduce用于累积统计信息，并产生summary。 0、TestDFSIO的用法如下： TestDFSIO.0.0.6 Usage: TestDFSIO [genericOptions] -read | -write | -append | -clean [-nrFiles N] [-fileSize Size[B 1、向HDFS中写入文件 #往HDFS中写入10个1000MB的文件,文件在HDFS中的路径：/benchmarks/TestDFSIO sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar TestDFSIO -write -nrFiles 10 -fileSize 1000 测试结果： 18/08/15 16:57:12 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write 18/08/15 16:57:12 INFO fs.TestDFSIO: Date & time: Wed Aug 15 16:57:12 CST 2018 18/08/15 16:57:12 INFO fs.TestDFSIO: Number of files: 10 18/08/15 16:57:12 INFO fs.TestDFSIO: Total MBytes processed: 1000.0 18/08/15 16:57:12 INFO fs.TestDFSIO: Throughput mb/sec: 86.89607229753216 18/08/15 16:57:12 INFO fs.TestDFSIO: Average IO rate mb/sec: 94.8116455078125 18/08/15 16:57:12 INFO fs.TestDFSIO: IO rate std deviation: 25.014017966950007 18/08/15 16:57:12 INFO fs.TestDFSIO: Test exec time sec: 13.852 18/08/15 16:57:12 INFO fs.TestDFSIO: 2、从HDFS中读取测试数据 #从HDFS中读取5个1000MB的文件,读取测试结果文件在HDFS中的路径：/benchmarks/TestDFSIO/io_read sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar TestDFSIO -read-nrFiles 5 -fileSize 1000 测试结果： 18/08/15 18:15:21 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read 18/08/15 18:15:21 INFO fs.TestDFSIO: Date & time: Wed Aug 15 18:15:21 CST 2018 18/08/15 18:15:21 INFO fs.TestDFSIO: Number of files: 5 18/08/15 18:15:21 INFO fs.TestDFSIO: Total MBytes processed: 5000.0 18/08/15 18:15:21 INFO fs.TestDFSIO: Throughput mb/sec: 137.50240629211012 18/08/15 18:15:21 INFO fs.TestDFSIO: Average IO rate mb/sec: 137.68557739257812 18/08/15 18:15:21 INFO fs.TestDFSIO: IO rate std deviation: 5.010354585493878 18/08/15 18:15:21 INFO fs.TestDFSIO: Test exec time sec: 38.621 18/08/15 18:15:21 INFO fs.TestDFSIO: 3、删除测试数据 sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar TestDFSIO -clean Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/cloudera-hdfs.html":{"url":"origin/cloudera-hdfs.html","title":"HDFS","keywords":"","body":"HDFS 一、简介 二、基础概念 三、常用操作 1、查看HDFS容量 hdfs dfs -df -h 2、删除HDFS上的文件(删除的文件会放到操作用户的回收站里) #删除目录 hdfs dfs -rm -r /test #删除文件 hdfs dfs -rm /test/a 3、直接删除文件（不放进回收站） #删除目录 hdfs dfs -rm -r -skipTrash /test #删除文件 hdfs dfs -rm -skipTrash /test/a 4、上传文件 hdfs dfs -put 本地文件 远程目录 5、下载文件 hdfs dfs -get 远程文件 本地目录 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/tidb-ansible.html":{"url":"origin/tidb-ansible.html","title":"Ansible二进制部署管理","keywords":"","body":"一、简介 TiDB Ansible是 PingCAP 基于 Ansible playbook 功能编写的集群部署工具 TIDB Ansible Github：https://github.com/pingcap/tidb-ansible 官方文档：https://pingcap.com/docs-cn/stable/how-to/deploy/orchestrated/ansible/ 二、TiDB Ansible使用指南 1、版本 目前 tidb-ansible 主要分支有：master、release-2.1、release-2.0、release-1.1、release-1.0；在比较老的版本中 tidb-ansible 是不区分 tag 的，应一些用户的需求，我们从 v2.1.1 和 v.2.0.10 版本开始对 tidb-ansible 打 Tag，不同 Tag 的 tidb-ansible 来操作对应版本的 TiDB 集群。 master 分支用来部署 latest 版本（最新版本，目前就是指 3.0.0-BETA）集群，latest 版本由于还没有 GA，所以不区分 Tag。 release-2.1 分支用来部署 2.1.x 版本的集群，其中包含 v2.1.1 及之后版本的 Tag。不同 tag 之间会有一些 pr 的修改，可能包含参数等变更，所以建议使用相匹配的版本来操作集群，比如： 使用 v2.1.5 Tag 的 tidb-ansible 来部署管理 2.1.5 版本的集群 从 2.1.3 版本升级到 2.1.5 版本，也需要使用 v2.1.5 Tag 的 tidb-ansible 来操作 release-2.0 分支和 2.1 分支类似。 release-1.1 和 release-1.0 分支属于老版本（很少用户使用这个版本），没有 Tag。 2、目录结构 tidb-ansible 实际上是 playbook 和文件（binary、配置文件等）的集合。每个 playbook 都包含了一系列的操作指令，当执行 playbook 时，就会按照定义好的指令对不同节点做分发 binary、生成配置文件、启停服务等操作。这里需要注意，yml 文件对格式要求很严格，同一级别的配置一定要缩进对齐。 ini 文件 主要是 hosts.ini 和 inventory.ini，ini 文件以 [xxx]（组名称） 分组，在 playbook 中可以定义对哪个组做哪些操作。ansible-playbook 默认使用当前目录中的 inventory.ini 文件，在 tidb-ansible/ansible.cfg 中设置。 hosts.ini 用来初始化机器，添加用户、设置互信、同步时间等。 inventory.ini 用来部署集群，包含集群的拓扑和一些常用变量。如果有自定义变量或者单机多个实例，ip 或者别名要区分开，一定不要相同，不然变量之间会互相覆盖。如下截图包含两个示例，每个示例中的变量都会互相覆盖，最终只会有一个生效。 conf 目录 集群各组件的默认初始化配置文件 scripts 目录 主要存放 Grafana Dashboard 相关的文件、环境校验脚本、日常使用常用脚本。其中 .json 文件就是我们平常监控上看到的 dashboard 的内容，也可以在 grafana 上手动 import 导入。 group_vars 目录 包含了不同组的变量和全局变量。需要注意的是，group_vars 中的全局/组变量优先级比 inventory.ini 中的全局/组变量优先级高，但是比 inventory.ini 中 host 后面设置的变量优先级低。 roles 目录 roles 里面的内容比较多，核心功能都在这个目录中，tidb-ansible 目录下的 playbook 执行时也是在调用 roles 中对应的角色。下面以 roles/pd 为例具体说明： defaults 是必须存在的目录，存储一些默认变量信息，这个变量的优先级最低 files 目录存放由 copy 或 script 等模块调用的文件 meta 目录定义当前角色的特殊设定及其依赖关系，至少应该包含一个名为main.yml的文件，其它的文件需要在此文件中通过include 进行包含 tasks 是存放 playbook 的目录，其中 main.yml 是主入口文件，在 main.yml 中导入其他 yml 文件 templates 目录存放模板文件，template 模块会将模板文件中的变量替换为实际值，然后覆盖到客户机指定路径上 vars 目录存放角色用到的变量，这里我们用来存放默认配置文件信息 download 目录 local_prepare.yml playbook下载的集群组件二进制压缩安装包 resource 目录 3、常用 playbook 说明 deploy.yml 用来部署集群。执行 deploy 操作会自动将配置文件、binary 等分发到对应的节点上；如果是已经存在的集群，执行时会对比配置文件、binary 等信息，如果有变更就会覆盖原来的文件并且将原来的文件备份到 backup（默认） 目录。 start.yml 用来启动集群。注意：这个操作只是 start 集群，不会对配置等信息做任何更改 stop.yml 用来停止集群。与 start 一样，不会对配置等做任何修改。 rolling_update.yml 用来逐个更新集群内的节点。此操作会按 PD、TiKV、TiDB 的顺序以 1 并发对集群内的节点逐个做 stop → deploy → start 操作，其中对 PD 和 TiKV 操作时会先迁移掉 leader，将对集群的影响降到最低。一般用来对集群做配置更新或者升级。 rolling_update_monitor.yml 用来逐个更新监控组件，与 rolling_update.yml 功能一样，面向的组件有区别。 unsafe_cleanup.yml 用来清掉整个集群。这个操作会先将整个集群停掉服务，然后删除掉相关的目录，操作不可逆，需要谨慎。 unsafe_cleanup_data.yml 用来清理掉 PD、TiKV、TiDB 的数据。执行时会先将对应服务停止，然后再清理数据，操作不可逆，需要谨慎。这个操作不涉及监控。 三、Prerequisite 1、服务器准备 Linux 操作系统版本 Linux 操作系统平台 版本 Red Hat Enterprise Linux 7.3 及以上 CentOS（推荐） 7.3 及以上 Oracle Enterprise Linux 7.3 及以上 Ubuntu LTS 16.04 及以上 服务器配置推荐 开发测试环境 组件 CPU 内存 本地存储 网络 实例数量(最低要求) TiDB 8核+ 16 GB+ 无特殊要求 千兆网卡 1（可与 PD 同机器） PD 4核+ 8 GB+ SAS, 200 GB+ 千兆网卡 1（可与 TiDB 同机器） TiKV 8核+ 32 GB+ SSD, 200 GB+ 千兆网卡 3 验证测试环境中的 TiDB 和 PD 可以部署在同一台服务器上。 如进行性能相关的测试，避免采用低性能存储和网络硬件配置，防止对测试结果的正确性产生干扰。 TiKV 的 SSD 盘推荐使用 NVME 接口以保证读写更快。 如果仅验证功能，建议使用 Docker Compose 部署方案单机进行测试。 TiDB 对于磁盘的使用以存放日志为主，因此在测试环境中对于磁盘类型和容量并无特殊要求。 生产环境 组件 CPU 内存 硬盘类型 网络 实例数量(最低要求) TiDB 16核+ 32 GB+ SAS 万兆网卡（2块最佳） 2 PD 4核+ 8 GB+ SSD 万兆网卡（2块最佳） 3 TiKV 16核+ 32 GB+ SSD 万兆网卡（2块最佳） 3 监控 8核+ 16 GB+ SAS 千兆网卡 1 生产环境中的 TiDB 和 PD 可以部署和运行在同服务器上，如对性能和可靠性有更高的要求，应尽可能分开部署。 生产环境强烈推荐使用更高的配置。 TiKV 硬盘大小配置建议 PCI-E SSD 不超过 2 TB，普通 SSD 不超过 1.5 TB。 实践服务器配置 IP 硬件配置 TiDB角色 192.168.6.1 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 TiDB1 PD1 TiKV1 192.168.6.2 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 TiDB2 PD2 TiKV2 192.168.6.3 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 TiDB3 PD3 TiKV3 192.168.6.4 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 Zookeeper、Kafka、Pump Server、Drainer、Importer、Lightning、kafka Exporter、Spark Master、 192.168.6.5 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 Ansible、Prometheus、Grafana、 Pushgateway、Alertmanager、Spark Slave 2、所有部署主机添加数据盘 所有部署主机的100G数据磁盘为/dev/vdb 3、Ansible节点准备 ⓪\u0003\u0016下载TIDB Ansile tag=v3.0.12 && \\ git clone -b $tag https://github.com/pingcap/tidb-ansible.git ①安装 Ansible 及其依赖 目前，TiDB release-2.0、release-2.1、release-3.0 及最新开发版本兼容 Ansible 2.4 ~ 2.7.11 (2.4 ≤ Ansible ≤ 2.7.11)。Ansible 及相关依赖的版本信息记录在 tidb-ansible/requirements.txt 文件中。 cd /home/tidb/tidb-ansible && \\ sudo pip install -r ./requirements.txt && \\ ansible --version # ansible 2.7.11 # config file = /home/tidb/tidb-ansible/ansible.cfg # configured module search path = [u'/home/tidb/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] # ansible python module location = /usr/lib/python2.7/site-packages/ansible # executable location = /bin/ansible # python version = 2.7.5 (default, Nov 6 2016, 00:28:07) [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] Ansible默认配置文件在/home/tidb/tidb-ansible/ansible.cfg 4、Ansible节点与部署目标机器的通信准备 ⓪配置Ansible节点与部署目标机器的SSH 互信及 sudo 规则 在部署目标机器上创建 tidb 用户，并配置 sudo 规则，配置中控机与部署目标机器之间的 SSH 互信。 vi /home/tidb/tidb-ansible/host.ini [servers] 192.168.6.1 192.168.6.2 192.168.6.3 192.168.6.4 192.168.6.5 [all:vars] username = tidb ntp_server = pool.ntp.org ansible-playbook -i /home/tidb/tidb-ansible/hosts.ini create_users.yml -u root -k 5、在部署目标机器上安装 NTP 服务 ansible-playbook -i /home/tidb/tidb-ansible/hosts.ini deploy_ntp.yml -u tidb -b 6、格式化部署目标机器的数据盘以LVM2方式挂载到/data/tidb cd /home/tidb/tidb-ansible/ && \\ ansible -i hosts.ini all -m yum -a 'name=lvm2 state=present' && \\ ansible -i hosts.ini all -m shell -a 'parted -s -a optimal /dev/vdb mklabel gpt -- mkpart primary ext4 1 -1 && disk=/dev/vdb1 && pvcreate ${disk} && vgcreate ${disk} && vgcreate -s 16m tidb ${disk} && PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` && lvcreate -l ${PE_Number} -n tidb tidb && mkfs.ext4 /dev/tidb/tidb && mkdir -p /data/tidb && chown -R tidb:tidb /data/tidb && echo \"/dev/tidb/tidb /data/tidb ext4 defaults 0 0\" >> /etc/fstab && mount -a && df -m' 四、安装 1、准备TiDB Ansible的部署主机清单inventory.ini ## TiDB Cluster Part [tidb_servers] 192.168.6.1 192.168.6.2 192.168.6.3 [tikv_servers] node1.pro.tidb ansible_host=192.168.6.1 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node1\" node2.pro.tidb ansible_host=192.168.6.2 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node2\" node3.pro.tidb ansible_host=192.168.6.3 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node3\" [pd_servers] 192.168.6.1 192.168.6.2 192.168.6.3 [spark_master] 192.168.6.4 [spark_slaves] 192.168.6.5 [lightning_server] 192.168.6.4 [importer_server] 192.168.6.4 ## Monitoring Part: prometheus and pushgateway servers [monitoring_servers] 192.168.6.5 [grafana_servers] 192.168.6.5 # node_exporter and blackbox_exporter servers [monitored_servers] 192.168.6.1 192.168.6.2 192.168.6.3 192.168.6.4 192.168.6.5 [alertmanager_servers] 192.168.6.7 [kafka_exporter_servers] 192.168.6.4 ## Binlog Part [pump_servers] 192.168.6.4 [drainer_servers] 192.168.6.4 ## Group variables [pd_servers:vars] location_labels = [\"host\"] ## Global variables [all:vars] deploy_dir = /data/tidb ## Connection ssh via normal user ansible_user = tidb cluster_name = Curiouser-TiDB tidb_version = v3.0.12 # process supervision, [systemd, supervise] process_supervision = systemd timezone = Asia/Shanghai enable_firewalld = False # check NTP service enable_ntpd = True set_hostname = False ## binlog trigger enable_binlog = True # kafka cluster address for monitoring, example: kafka_addrs = \"192.168.6.4:9092\" # zookeeper address of kafka cluster for monitoring, example: zookeeper_addrs = \"192.168.6.4:2181\" # enable TLS authentication in the TiDB cluster enable_tls = False # KV mode deploy_without_tidb = False # wait for region replication complete before start tidb-server. wait_replication = True # Optional: Set if you already have a alertmanager server. # Format: alertmanager_host:alertmanager_port alertmanager_target = \"\" grafana_admin_user = \"admin\" grafana_admin_password = \"admin\" ### Collect diagnosis collect_log_recent_hours = 2 enable_bandwidth_limit = True # default: 10Mb/s, unit: Kbit/s collect_bandwidth_limit = 10000 2、修改各组件默认配置 ⓪修改TiKV默认配置文件/home/tidb/tidb-ansible/conf/tikv.yml ①修改PD默认配置文件/home/tidb/tidb-ansible/conf/pd.yml ②修改TiDB默认配置文件/home/tidb/tidb-ansible/conf/tidb.yml ③修改Spark默认配置文件/home/tidb/tidb-ansible/conf/spark-defaults.yml和/home/tidb/tidb-ansible/conf/spark-env.yml ④修改pump默认配置文件/home/tidb/tidb-ansible/conf/pump.yml ⑤修改 lightning默认配置文件/home/tidb/tidb-ansible/conf/tidb-lightning.yml ⑥修改 importer默认配置文件/home/tidb/tidb-ansible/conf/tidb-importer.yml ⑦修改 drainer默认配置文件/home/tidb/tidb-ansible/conf/drainer.toml ⑧修改组件二进制包下载默认配置文件/home/tidb/tidb-ansible/conf/binary_packages.yml 和/home/tidb/tidb-ansible/conf/common_packages.yml 3、部署准备 ⓪测试部署目标服务器的SSH通信 ansible -i inventory.ini all -m shell -a 'whoami' 如果所有 server 均返回 tidb，表示 SSH 互信配置成功 ansible -i inventory.ini all -m shell -a 'whoami' -b 如果所有 server 均返回 root，表示 tidb 用户 sudo 免密码配置成功。 ①下载组件二进制包 ansible-playbook local_prepare.yml ②开启TiKV数据加密存储功能，创建加密token文件 TiKV 只接受 hex 格式的 token 文件，文件的长度必须是 2^n，并且小于等于 1024。 目前 TiKV 数据加密存储存在以下限制： 对之前没有开启加密存储的集群，不支持开启该功能。 已经开启加密功能的集群，不允许关闭加密存储功能。 同一集群内部，不允许部分 TiKV 实例开启该功能，部分 TiKV 实例不开启该功能。对于加密存储功能，所有 TiKV 实例要么都开启该功能，要么都不开启该功能。这是由于 TiKV 实例之间会有数据迁移，如果开启了加密存储功能，迁移过程中数据也是加密的。 /home/tidb/tidb-ansible/resources/bin/tikv-ctl random-hex --len 256 > /home/tidb/tidb-ansible/cipher-file-256 ③在TiKV默认配置文件/home/tidb/tidb-ansible/conf/tikv.yml中添加配置 [security] # Cipher file 的存储路径 cipher-file = \"/data/tidb/conf/cipher-file-256\" 4、初始化部署目标主机系统环境，修改内核参数 ansible-playbook bootstrap.yml 5、分发组件安装资源到部署目标主机 ansible-playbook deploy.yml 分发TiKV 数据加密存储token文件到TiKV目标主机上 ansible -i inventory.ini tikv_servers -m copy -a 'src=/home/tidb/tidb-ansible/cipher-file-256 dest=/data/tidb/conf/cipher-file-256 owner=tidb group=tidb' 6、启动 TiDB 集群 ansible-playbook start.yml 7、验证 使用 MySQL 客户端连接 TiDB 集群。TiDB 服务的默认端口为 4000 mysql -u root -h 192.168.6.1 -P 4000 浏览器访问Grafana：http://192.168.6.5:3000 （默认帐号与密码：admin/admin） 五、集群操作 1、ansible控制集群 ⓪启动集群 此操作会按顺序启动整个 TiDB 集群所有组件（包括 PD、TiDB、TiKV 等组件和监控组件）。 ansible-playbook start.yml ①关闭集群 此操作会按顺序关闭整个 TiDB 集群所有组件（包括 PD、TiDB、TiKV 等组件和监控组件）。 ansible-playbook stop.yml ②清除集群数据 此操作会关闭 TiDB、Pump、TiKV、PD 服务，并清空 Pump、TiKV、PD 数据目录。 ansible-playbook unsafe_cleanup_data.yml ③销毁集群 此操作会关闭集群，并清空部署目录，若部署目录为挂载点，会报错，可忽略。 ansible-playbook unsafe_cleanup.yml ④更新组件配置并滚动重启 ansible-playbook rolling_update.yml --tags=组件标签 配置并重启 Prometheus 服务 ansible-playbook rolling_update_monitor.yml --tags=prometheus ⑤只操作组件某节点上的其中一个服务 ansible-playbook 操作playbook --tags=组件 -l 组件节点的ip或hostname或inventry中的别名 # 例如停掉192.168.6.3节点上的所有服务 ansible-playbook stop.yml -l 192.168.6.3 # 关闭您想要下线的 DM-worker 实例 ansible-playbook stop.yml --tags=dm-worker -l dm_worker3 六、压力测试 详见：https://pingcap.com/docs-cn/stable/benchmark/how-to-run-sysbench/ 七、扩容缩容 1、扩容组件节点 扩容TiKV节点 注意： 如果 inventory.ini 中为节点配置了别名，如 node101 ansible_host=192.168.6.101，执行 ansible-playbook 时 -l 请指定别名，以下步骤类似。例如：ansible-playbook bootstrap.yml -l node101,node102 ⓪编辑 inventory.ini 文件和 hosts.ini 文件，添加节点信息 hosts.ini [servers] ...省略... 192.168.6.6 [all:vars] ...省略... inventory.ini ...省略... [tikv_servers] node4.pro.tidb ansible_host=192.168.6.6 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node4\" ...省略... ①配置SSH 互信及 sudo 规则、安装 NTP 服务、挂载数据盘、初始化新增节点，分发安装资源 ansible-playbook -i hosts.ini create_users.yml -l 192.168.6.6 -u root -k ansible-playbook -i hosts.ini deploy_ntp.yml -u tidb -b cd /home/tidb/tidb-ansible/ && \\ ansible -i hosts.ini 192.168.6.6 -m yum -a 'name=lvm2 state=present' && \\ ansible -i hosts.ini 192.168.6.6 -m shell -a 'parted -s -a optimal /dev/vdb mklabel gpt -- mkpart primary ext4 1 -1 && disk=/dev/vdb1 && pvcreate ${disk} && vgcreate ${disk} && vgcreate -s 16m tidb ${disk} && PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` && lvcreate -l ${PE_Number} -n tidb tidb && mkfs.ext4 /dev/tidb/tidb && mkdir -p /data/tidb && chown -R tidb:tidb /data/tidb && echo \"/dev/tidb/tidb /data/tidb ext4 defaults 0 0\" >> /etc/fstab && mount -a && df -m' ansible-playbook bootstrap.yml -l node4.pro.tidb ansible-playbook deploy.yml -l node4.pro.tidb ansible -i hosts.ini 192.168.6.6 -m copy -a 'src=/home/tidb/tidb-ansible/cipher-file-256 dest=/home/tidb/conf/cipher-file-256 owner=tidb group=tidb' ②启动新节点服务 ansible-playbook start.yml -l node4.pro.tidb ③更新 Prometheus 配置并重启 ansible-playbook rolling_update_monitor.yml --tags=prometheus 扩容PD节点 ⓪编辑 inventory.ini 文件和 hosts.ini 文件，添加节点信息 hosts.ini [servers] ...省略... 192.168.6.7 [all:vars] ...省略... inventory.ini ...省略... [pd_servers] 192.168.6.7 ...省略... ①配置SSH 互信及 sudo 规则、安装 NTP 服务、挂载数据盘、初始化新增节点，分发安装资源 ansible-playbook -i hosts.ini create_users.yml -l 192.168.6.7 -u root -k ansible-playbook -i hosts.ini deploy_ntp.yml -u tidb -b cd /home/tidb/tidb-ansible/ && \\ ansible -i hosts.ini 192.168.6.7 -m yum -a 'name=lvm2 state=present' && \\ ansible -i hosts.ini 192.168.6.7 -m shell -a 'parted -s -a optimal /dev/vdb mklabel gpt -- mkpart primary ext4 1 -1 && disk=/dev/vdb1 && pvcreate ${disk} && vgcreate ${disk} && vgcreate -s 16m tidb ${disk} && PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` && lvcreate -l ${PE_Number} -n tidb tidb && mkfs.ext4 /dev/tidb/tidb && mkdir -p /data/tidb && chown -R tidb:tidb /data/tidb && echo \"/dev/tidb/tidb /data/tidb ext4 defaults 0 0\" >> /etc/fstab && mount -a && df -m' ansible-playbook bootstrap.yml -l 192.168.6.7 ansible-playbook deploy.yml -l 192.168.6.7 ②登录新增的 PD 节点，编辑启动脚本：/data/tidb/scripts/run_pd.sh 移除 --initial-cluster=\"xxxx\" \\ 配置，注意这里不能在行开头加注释符 #。 添加 --join=\"http://192.168.6.1:2379\" \\，IP 地址 （192.168.6.7） 可以是集群内现有 PD IP 地址中的任意一个。 ③在新增 PD 节点中启动 PD 服务 /data/tidb/scripts/start_pd.sh ③使用 pd-ctl 检查新节点是否添加成功： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member ④启动监控服务 ansible-playbook start.yml -l 192.168.6.7 ⑤更新集群的配置： ansible-playbook deploy.yml ⑥重启 Prometheus，新增扩容的 PD 节点的监控： ansible-playbook rolling_update_monitor.yml --tags=prometheus 2、缩容组件节点 ⓪停止节点上所有的服务： 注意：如果该服务器上还有其他服务（例如 TiDB），则还需要使用 --tags组件标签 来指定服务（例如 -t tidb）。 ansible-playbook stop.yml -l 192.168.6.9 ①编辑 inventory.ini 文件，移除节点信息 ②更新 Prometheus 配置并重启 ansible-playbook rolling_update_monitor.yml --tags=prometheus 缩容 TiKV 节点 ⓪使用 pd-ctl 从集群中移除节点： 查看 TiKV node节点的 store id： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d store 从集群中移除TiKV node节点，假如 store id 为 10： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d store delete 10 使用 Grafana 或者 pd-ctl 检查节点是否下线成功（下线需要一定时间，下线节点的状态变为 Tombstone 就说明下线成功了）： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d store 10 ①下线成功后，停止 TiKV node节点上的服务： ansible-playbook stop.yml --tags=tikv -l 192.168.6.9 编辑 inventory.ini 文件，移除节点信息 ②更新 Prometheus 配置并重启： ansible-playbook stop.yml --tags=prometheus ansible-playbook start.yml --tags=prometheus 缩容 PD 节点 TiKV 中的 PD Client 会缓存 PD 节点列表，但目前不会定期自动更新，只有在 PD leader 发生切换或 TiKV 重启加载最新配置后才会更新；为避免 TiKV 缓存的 PD 节点列表过旧的风险，在扩缩容 PD 完成后，PD 集群中至少要包含两个扩缩容操作前就已经存在的 PD 节点成员，如果不满足该条件需要手动执行 PD transfer leader 操作，更新 TiKV 中的 PD 缓存列表。 ⓪使用 pd-ctl 从集群中移除节点： 查看 node2 节点的 name： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member 从集群中移除 node2，假如 name 为 pd2： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member delete name pd2 使用 pd-ctl 检查节点是否下线成功（PD 下线会很快，结果中没有 node2 节点信息即为下线成功）： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member ①下线成功后，停止 node2 上的服务： ansible-playbook stop.yml --tags=pd -l 192.168.6.9 ②编辑 inventory.ini 文件，移除节点信息 ③更新集群的配置： ansible-playbook deploy.yml ④重启 Prometheus，移除缩容的 PD 节点的监控： ansible-playbook stop.yml --tags=prometheus ansible-playbook start.yml --tags=prometheus Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-08-25 18:34:42 "},"origin/tiup-install-cluster.html":{"url":"origin/tiup-install-cluster.html","title":"使用TIUP部署TiDB 4.0集群","keywords":"","body":"使用TIUP部署TiDB 4.0集群 一、简介 TiUP 是 TiDB 4.0 版本引入的集群运维工具，TiUP cluster 是 TiUP 提供的使用 Golang 编写的集群管理组件，通过 TiUP cluster 组件就可以进行日常的运维工作，包括部署、启动、关闭、销毁、弹性扩缩容、升级 TiDB 集群；管理 TiDB 集群参数。 目前 TiUP 可以支持部署 TiDB、TiFlash、TiDB Binlog、TiCDC，以及监控系统。 Linux 操作系统版本要求 Linux 操作系统平台 版本 Red Hat Enterprise Linux 7.3 及以上 CentOS 7.3 及以上 Oracle Enterprise Linux 7.3 及以上 Ubuntu LTS 16.04 及以上 TiDB各组件端口 组件 默认端口 说明 TiDB 4000 应用及 DBA 工具访问通信端口 TiDB 10080 TiDB 状态信息上报通信端口 TiKV 20160 TiKV 通信端口 TiKV 20180 TiKV 状态信息上报通信端口 PD 2379 提供 TiDB 和 PD 通信端口 PD 2380 PD 集群节点间通信端口 TiFlash 9000 TiFlash TCP 服务端口 TiFlash 8123 TiFlash HTTP 服务端口 TiFlash 3930 TiFlash RAFT 服务和 Coprocessor 服务端口 TiFlash 20170 TiFlash Proxy 服务端口 TiFlash 20292 Prometheus 拉取 TiFlash Proxy metrics 端口 TiFlash 8234 Prometheus 拉取 TiFlash metrics 端口 Pump 8250 Pump 通信端口 Drainer 8249 Drainer 通信端口 CDC 8300 CDC 通信接口 Prometheus 9090 Prometheus 服务通信端口 Node_exporter 9100 TiDB 集群每个节点的系统信息上报通信端口 Blackbox_exporter 9115 Blackbox_exporter 通信端口，用于 TiDB 集群端口监控 Grafana 3000 Web 监控服务对外服务和客户端(浏览器)访问端口 Alertmanager 9093 告警 web 服务端口 Alertmanager 9094 告警通信端口 二、集群节点准备 0、集群组件规划 集群角色/集群节点 tools.tidb4.curiouser.com node1.tidb4.curiouser.com node2.tidb4.curiouser.com node3.tidb4.curiouser.com TiUP ✓ TiDP ✓ ✓ ✓ PD Server ✓ ✓ ✓ TiKV ✓ ✓ ✓ TiFlash ✓ CDC ✓ pump ✓ grafana ✓ prometheus ✓ alertmanager ✓ Zookeeper ✓ Kafka ✓ 1、节点准备 ①所有节点创建tidb用户并加入sudoer useradd -m tidb echo \"tidb ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers passwd tidb ②所有节点关闭系统 swap、关闭透明大页 echo \"vm.swappiness = 0\">> /etc/sysctl.conf swapoff -a && swapon -a sysctl -p echo never > /sys/kernel/mm/transparent_hugepage/enabled echo never > /sys/kernel/mm/transparent_hugepage/defrag ③打通tools节点的tidb用户到所有节点tidb用户的免秘钥登录 su - tidb ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ssh-copy-id -i tools.tidb4.curiouser.com ssh-copy-id -i node1.tidb4.curiouser.com ssh-copy-id -i node2.tidb4.curiouser.com ssh-copy-id -i node3.tidb4.curiouser.com 三、集群安装 以下所有命令都在tools节点，tidb用户，bash环境下执行（zsh环境不支持tiup的某些脚本） 1、安装tiup命令 curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh source .bash_profile tiup cluster tiup --binary cluster 2、查看目前的tiup命令支持安装的tidb版本 tiup list tidb 3、创建集群组件的拓扑配置文件 # # Global variables are applied to all deployments and used as the default value of # # the deployments if a specific deployment value is missing. global: user: \"tidb\" ssh_port: 22 arch: \"amd64\" resource_control: memory_limit: \"6G\" cpu_quota: \"200%\" server_configs: tidb: oom-action: \"cancel\" mem-quota-query: 25769803776 log.query-log-max-len: 4096 log.file.log-rotate: true log.file.max-size: 300 log.file.max-days: 7 log.file.max-backups: 14 log.slow-threshold: 300 binlog.enable: true binlog.ignore-error: true tikv: global.log-rotation-timespan: \"168h\" readpool.unified.max-thread-count: 12 readpool.storage.use-unified-pool: false readpool.coprocessor.use-unified-pool: true coprocessor.split-region-on-table: false storage.block-cache.capacity: \"16GB\" raftstore.capacity: \"10GB\" pd: log.file.max-size: 300 log.file.max-days: 28 log.file.max-backups: 14 log.file.log-rotate: true replication.location-labels: [\"host\"] schedule.leader-schedule-limit: 4 schedule.region-schedule-limit: 2048 schedule.replica-schedule-limit: 64 tiflash: logger.level: \"info\" # Maximum memory usage for processing a single query. Zero means unlimited. profiles.default.max_memory_usage: 10000000000 # Maximum memory usage for processing all concurrently running queries on the server. Zero means unlimited. profiles.default.max_memory_usage_for_all_queries: 0 pd_servers: - host: 192.168.1.71 ssh_port: 22 name: pd-1 client_port: 2379 peer_port: 2380 deploy_dir: /data/tidb/pd-2379 data_dir: /data/tidb/pd-2379/data log_dir: /data/tidb/pd-2379/logs - host: 192.168.1.72 ssh_port: 22 name: pd-2 client_port: 2379 peer_port: 2380 deploy_dir: /data/tidb/pd-2379 data_dir: /data/tidb/pd-2379/data log_dir: /data/tidb/pd-2379/logs - host: 192.168.1.73 ssh_port: 22 name: pd-3 client_port: 2379 peer_port: 2380 deploy_dir: /data/tidb/pd-2379 data_dir: /data/tidb/pd-2379/data log_dir: /data/tidb/pd-2379/logs tidb_servers: - host: 192.168.1.71 port: 4000 status_port: 10080 deploy_dir: \"/data/tidb/tidb-4000\" log_dir: \"/data/tidb/tidb-4000/logs\" numa_node: \"0\" - host: 192.168.1.72 port: 4000 status_port: 10080 deploy_dir: \"/data/tidb/tidb-4000\" log_dir: \"/data/tidb/tidb-4000/logs\" numa_node: \"0\" - host: 192.168.1.73 port: 4000 status_port: 10080 deploy_dir: \"/data/tidb/tidb-4000\" log_dir: \"/data/tidb/tidb-4000/logs\" numa_node: \"0\" tikv_servers: - host: 192.168.1.71 port: 20160 status_port: 20180 deploy_dir: \"/data/tidb/tikv-20160\" data_dir: \"/data/tidb/tikv-20160/data\" log_dir: \"/data/tidb/tikv-20160/logs\" numa_node: \"0\" config: server.labels: { host: \"tikv1\" } - host: 192.168.1.72 port: 20160 status_port: 20180 deploy_dir: \"/data/tidb/tikv-20160\" data_dir: \"/data/tidb/tikv-20160/data\" log_dir: \"/data/tidb/tikv-20160/logs\" numa_node: \"0\" config: server.labels: { host: \"tikv2\" } - host: 192.168.1.73 port: 20160 status_port: 20180 deploy_dir: \"/data/tidb/tikv-20160\" data_dir: \"/data/tidb/tikv-20160/data\" log_dir: \"/data/tidb/tikv-20160/logs\" numa_node: \"0\" config: server.labels: { host: \"tikv3\" } tiflash_servers: - host: 192.168.1.70 data_dir: \"/data/tidb/tiflash-9000/data\" deploy_dir: \"/data/tidb/tiflash-9000\" log_dir: \"/data/tidb/tiflash-9000/logs\" ssh_port: 22 tcp_port: 9000 http_port: 8123 flash_service_port: 3930 flash_proxy_port: 20170 flash_proxy_status_port: 20292 metrics_port: 8234 numa_node: \"0\" # The following configs are used to overwrite the `server_configs.tiflash` values. config: logger.level: \"info\" learner_config: log-level: \"info\" pump_servers: - host: 192.168.1.70 ssh_port: 22 port: 8250 deploy_dir: \"/data/tidb/pump-8249\" data_dir: \"/data/tidb/pump-8249/data\" log_dir: \"/data/tidb/pump-8249/logs\" numa_node: \"0\" # The following configs are used to overwrite the `server_configs.drainer` values. config: gc: 7 tispark_masters: - host: 192.168.1.70 ssh_port: 22 port: 7077 web_port: 8080 deploy_dir: \"/data/tidb/tispark-master-7077\" java_home: \"/opt/jdk\" spark_config: spark.driver.memory: \"2g\" spark.eventLog.enabled: \"True\" spark.tispark.grpc.framesize: 268435456 spark.tispark.grpc.timeout_in_sec: 100 spark.tispark.meta.reload_period_in_sec: 60 spark.tispark.request.command.priority: \"Low\" spark.tispark.table.scan_concurrency: 256 spark_env: SPARK_EXECUTOR_CORES: 5 SPARK_EXECUTOR_MEMORY: \"10g\" SPARK_WORKER_CORES: 5 SPARK_WORKER_MEMORY: \"10g\" # NOTE: multiple worker nodes on the same host is not supported by Spark tispark_workers: - host: 192.168.1.70 ssh_port: 22 port: 7078 web_port: 8081 deploy_dir: \"/data/tidb/tispark-worker-7078\" java_home: \"/opt/jdk\" cdc_servers: - host: 192.168.1.70 ssh_port: 22 port: 8300 deploy_dir: \"/data/tidb/cdc-8300\" log_dir: \"/data/tidb/cdc-8300/logs\" monitored: node_exporter_port: 9100 blackbox_exporter_port: 9115 deploy_dir: \"/data/tidb/monitored-9100\" data_dir: \"/data/tidb/monitored-9100/data\" log_dir: \"/data/tidb/monitored-9100/logs\" monitoring_servers: - host: 192.168.1.70 ssh_port: 22 port: 9090 deploy_dir: \"/data/tidb/prometheus-9090\" data_dir: \"/data/tidb/prometheus-9090/data\" log_dir: \"/data/tidb/prometheus-9090/logs\" grafana_servers: - host: 192.168.1.70 ssh_port: 22 port: 3000 deploy_dir: \"/data/tidb/grafana-3000\" # alertmanager_servers: # - host: 192.168.1.70 # ssh_port: 22 # web_port: 9093 # cluster_port: 9094 # deploy_dir: \"/data/tidb/alertmanager-9093\" # data_dir: \"/data/tidb/alertmanager-9093/data\" # log_dir: \"/data/tidb/alertmanager-9093/logs\" 4、查集群拓扑配置文件语法 tiup cluster check 集群拓扑配置文件 5、分发组件文件到各节点 tiup cluster deploy 集群名字 tidb版本 ./集群拓扑配置.yaml --user tidb -y 6、所有节点安装numactl 在生产环境中，因为硬件机器配置往往高于需求，为了更合理规划资源，会考虑单机多实例部署 TiDB 或者 TiKV。NUMA 绑核工具的使用，主要为了防止 CPU 资源的争抢，引发性能衰退。NUMA 绑核是用来隔离 CPU 资源的一种方法，适合高配置物理机环境部署多实例使用。 通过 tiup cluster deploy 完成部署操作才可以通过 exec 命令来进行集群级别管理工作 tiup cluster exec 集群名字 --sudo --command \"yum install -y numactl\" # 或者 tiup cluster exec 集群名字 --sudo --command \"yum install -y numactl\" -R PD # 或者 tiup cluster exec 集群名字 --sudo --command \"yum install -y numactl\" -N node1 7、启动当前集群各组件 tiup cluster start 集群名字 tiup cluster start 集群名字 -R 组件1,组件2 启动集群操作会按 PD -> TiKV -> Pump -> TiDB -> TiFlash -> Drainer 的顺序启动整个 TiDB 集群所有组件（同时也会启动监控组件） 8、验证 ①查看当前集群的状态 tiup cluster display 集群名字 ②查看哪个 PD 节点提供了 TiDB Dashboard 服务 tiup cluster display 集群名字 --dashboard 9、其他信息 ①Grafana默认用户名密码 admin/admin ②监控Dashboard默认用户名密码 root 密码为空 ③组件拓扑配置文件样例 全配置参数模版：https://github.com/pingcap/tiup/blob/master/examples/topology.example.yaml TiDB：https://github.com/pingcap/tidb/blob/master/config/config.toml.example PD：https://github.com/tikv/pd/blob/v4.0.0-rc/conf/config.toml TiKV：https://github.com/tikv/tikv/blob/v4.0.0-rc/etc/config-template.toml 四、集群其他操作 1、修改集群组件配置参数 集群运行过程中，如果需要调整某个组件的参数，可以使用 edit-config 命令来编辑参数。具体的操作步骤如下： 以编辑模式打开该集群的配置文件： tiup cluster edit-config ${cluster-name} 设置参数： 首先确定配置的生效范围，有以下两种生效范围： 如果配置的生效范围为该组件全局，则配置到 server_configs。例如： server_configs: tidb: log.slow-threshold: 300 如果配置的生效范围为某个节点，则配置到具体节点的 config 中。例如： tidb_servers: - host: 10.0.1.11 port: 4000 config: log.slow-threshold: 300 参数的格式参考 TiUP 配置参数模版。配置项层次结构使用 . 表示。 执行 reload 命令滚动分发配置、重启相应组件： tiup cluster reload ${cluster-name} [-N ] [-R ] 示例：如果要调整 tidb-server 中事务大小限制参数 txn-total-size-limit 为 1G，该参数位于 performance 模块下，调整后的配置如下： server_configs: tidb: performance.txn-total-size-limit: 1073741824 然后执行 tiup cluster reload ${cluster-name} -R tidb 命令滚动重启。 2、重命名集群 部署并启动集群后，可以通过 tiup cluster rename 命令来对集群重命名： tiup cluster rename ${cluster-name} ${new-name} 注意： 重命名集群会重启监控（Prometheus 和 Grafana）。 重命名集群之后 Grafana 可能会残留一些旧集群名的面板，需要手动删除这些面板。 3、关闭集群 关闭集群操作会按 Drainer -> TiFlash -> TiDB -> Pump -> TiKV -> PD 的顺序关闭整个 TiDB 集群所有组件（同时也会关闭监控组件）： tiup cluster stop ${cluster-name} 和 start 命令类似，stop 命令也支持通过 -R 和 -N 参数来只停止部分组件。 例如，下列命令只停止 TiDB 组件： tiup cluster stop ${cluster-name} -R tidb 下列命令只停止 1.2.3.4 和 1.2.3.5 这两台机器上的 TiDB 组件： tiup cluster stop ${cluster-name} -N 1.2.3.4:4000,1.2.3.5:4000 4、清除集群数据 此操作会关闭所有服务，并清空其数据目录或/和日志目录，并且无法恢复，需要谨慎操作。 清空集群所有服务的数据，但保留日志： tiup cluster clean ${cluster-name} --data 清空集群所有服务的日志，但保留数据： tiup cluster clean ${cluster-name} --log 清空集群所有服务的数据和日志： tiup cluster clean ${cluster-name} --all 清空 Prometheus 以外的所有服务的日志和数据： tiup cluster clean ${cluster-name} --all --ignore-role prometheus 清空节点 192.168.1.70:9000 以外的所有服务的日志和数据： tiup cluster clean ${cluster-name} --all --ignore-node 192.168.1.70:9000 清空部署在 172.16.13.12 以外的所有服务的日志和数据： tiup cluster clean ${cluster-name} --all --ignore-node 192.168.1.70 5、删除集群 tiup cluster destroy 集群名字 6、集群组件升级 在官方组件提供了新版之后，你可以使用 tiup update 命令来升级组件。除了以下几个参数，该命令的用法基本和 tiup install 相同： --all：升级所有组件 --nightly：升级至 nightly 版本 --self：升级 TiUP 自己至最新版本 --force：强制升级至最新版本 示例一：升级所有组件至最新版本 tiup update --all 示例二：升级所有组件至 nightly 版本 tiup update --all --nightly 示例三：升级 TiUP 至最新版本 tiup update --self 7、集群组件扩容 编辑组件节点扩容节点信息的文件 tidb-servers-scale-out-new-node.yaml TIDB配置文件参考 tidb_servers: - host: 10.0.1.5 ssh_port: 22 port: 4000 status_port: 10080 deploy_dir: /data/deploy/install/deploy/tidb-4000 log_dir: /data/deploy/install/log/tidb-4000 TiKV 配置文件参考： tikv_servers: - host: 10.0.1.5 ssh_port: 22 port: 20160 status_port: 20180 deploy_dir: /data/deploy/install/deploy/tikv-20160 data_dir: /data/deploy/install/data/tikv-20160 log_dir: /data/deploy/install/log/tikv-20160 PD 配置文件参考： pd_servers: - host: 10.0.1.5 ssh_port: 22 name: pd-1 client_port: 2379 peer_port: 2380 deploy_dir: /data/deploy/install/deploy/pd-2379 data_dir: /data/deploy/install/data/pd-2379 log_dir: /data/deploy/install/log/pd-2379 可以使用 tiup cluster edit-config 查看当前集群的配置信息，因为其中的 global 和 server_configs 参数配置默认会被 scale-out.yaml 继承，因此也会在 scale-out.yaml 中生效。 此处假设当前执行命令的用户和新增的机器打通了互信，如果不满足已打通互信的条件，需要通过 -p 来输入新机器的密码，或通过 -i 指定私钥文件。 执行扩容命令 tiup cluster scale-out 集群名 tidb-servers-scale-out-new-node.yaml 预期输出 Scaled cluster out successfully 信息，表示扩容操作成功。 8、集群组件实例清理 你可以使用 tiup clean 命令来清理组件实例，并删除工作目录。如果在清理之前实例还在运行，会先 kill 相关进程。该命令用法如下： tiup clean [tag] [flags] 支持以下参数： --all：清除所有的实例信息 其中 tag 表示要清理的实例 tag，如果使用了 --all 则不传递 tag。 示例一：清理 tag 名称为 experiment 的组件实例 tiup clean experiment 示例二：清理所有组件实例 tiup clean --all 9、集群组件卸载 TiUP 安装的组件会占用本地磁盘空间，如果不想保留过多老版本的组件，可以先查看当前安装了哪些版本的组件，然后再卸载某个组件。 你可以使用 tiup uninstall 命令来卸载某个组件的所有版本或者特定版本，也支持卸载所有组件。该命令用法如下： tiup uninstall [component][:version] [flags] 支持的参数： --all：卸载所有的组件或版本 --self：卸载 TiUP 自身 component 为要卸载的组件名称，version 为要卸载的版本，这两个都可以省略，省略任何一个都需要加上 --all 参数： 若省略版本，加 --all 表示卸载该组件所有版本 若版本和组件都省略，则加 --all 表示卸载所有组件及其所有版本 示例一：卸载 v3.0.8 版本的 TiDB tiup uninstall tidb:v3.0.8 示例二：卸载所有版本的 TiKV tiup uninstall tikv --all 示例三：卸载所有已经安装的组件 tiup uninstall --all Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-19 15:23:05 "},"origin/tidb-dm.html":{"url":"origin/tidb-dm.html","title":"DM(Data Migration)数据增量全量同步至TiDB","keywords":"","body":"DM(Data Migration)数据全量及增量同步 一、简介 DM (Data Migration) 是一体化的数据同步任务管理平台，支持从 MySQL 或 MariaDB 到 TiDB 的全量数据迁移和增量数据同步。使用 DM 工具有利于简化错误处理流程，降低运维成本。 Block & allow lists：黑白名单配置，用于过滤或指定只迁移某些数据库或某些表的所有操作。 Block & Allow Lists 的过滤规则类似于 MySQL replication-rules-db/replication-rules-table Binlog event filter：用于过滤源数据库中特定表的特定类型操作 比如过滤掉表 test.sbtest 的 INSERT 操作或者过滤掉库 test 下所有表的 TRUNCATE TABLE 操作。 Table routing：将源数据库的表迁移到下游指定表的路由功能 比如将源数据表 test.sbtest1 的数据同步到 TiDB 的表 test.sbtest2。它也是分库分表合并迁移所需的一个核心功能。 1、DM 架构 DM 主要包括三个组件：DM-master，DM-worker 和 dmctl。 2、DM-master DM-master 负责管理和调度数据同步任务的各项操作。 保存 DM 集群的拓扑信息 监控 DM-worker 进程的运行状态 监控数据同步任务的运行状态 提供数据同步任务管理的统一入口 协调分库分表场景下各个实例分表的 DDL 同步 3、DM-worker DM-worker 负责执行具体的数据同步任务。 注册为一台 MySQL 或 MariaDB 服务器的 slave。 读取 MySQL 或 MariaDB 的 binlog event，并将这些 event 持久化保存在本地 (relay log) 单个 DM-worker 支持迁移一个 MySQL 或 MariaDB 实例的数据到下游的多个 TiDB 实例 多个 DM-Worker 支持迁移多个 MySQL 或 MariaDB 实例的数据到下游的一个 TiDB 实例 3.1、DM-worker逻辑处理单元 Relay log：持久化保存从上游 MySQL 或 MariaDB 读取的 binlog，并对 binlog replication 处理单元提供读取 binlog event 的功能。其原理和功能与 MySQL relay log 类似，详见 MySQL Relay Log。 dump 处理单元：从上游 MySQL 或 MariaDB 导出全量数据到本地磁盘。 load 处理单元：读取 dump 处理单元导出的数据文件，然后加载到下游 TiDB。 Binlog replication/sync 处理单元：读取上游 MySQL/MariaDB 的 binlog event 或 relay log 处理单元的 binlog event，将这些 event 转化为 SQL 语句，再将这些 SQL 语句应用到下游 TiDB。 3.2、DM-worker所需权限 上游数据库 (MySQL/MariaDB) 用户权限 权限 作用域 SELECT Tables RELOAD Global REPLICATION SLAVE Global REPLICATION CLIENT Global GRANT RELOAD,REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'your_user'@'your_wildcard_of_host' GRANT SELECT ON db1.* TO 'your_user'@'your_wildcard_of_host'; 下游数据库 (TiDB) 用户权限 权限 作用域 SELECT Tables INSERT Tables UPDATE Tables DELETE Tables CREATE Databases，tables DROP Databases，tables ALTER Tables INDEX Tables GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER,INDEX ON db.table TO 'your_user'@'your_wildcard_of_host'; 处理单元所需的最小权限 处理单元 最小上游 (MySQL/MariaDB) 权限 最小下游 (TiDB) 权限 最小系统权限 Relay log REPLICATION SLAVE (读取 binlog） REPLICATION CLIENT (show master status, show slave status) 无 本地读/写磁盘 Dump SELECT RELOAD（获取读锁将表数据刷到磁盘，进行一些操作后，再释放读锁对表进行解锁） 无 本地写磁盘 Load 无 SELECT（查询 checkpoint 历史） CREATE（创建数据库或表） DELETE（删除 checkpoint） INSERT（插入 dump 数据） 读/写本地文件 Binlog replication REPLICATION SLAVE（读 binlog） REPLICATION CLIENT (show master status, show slave status) SELECT（显示索引和列） INSERT (DML) UPDATE (DML) DELETE (DML) CREATE（创建数据库或表） DROP（删除数据库或表） ALTER（修改表） INDEX（创建或删除索引） 本地读/写磁盘 4、DM高可用 当部署多个 DM-master 节点时 所有 DM-master 节点将使用内部嵌入的 etcd 组成集群。 DM-master 集群用于存储集群节点信息、任务配置等元数据， 通过 etcd 选举出 leader 节点。该 leader 节点用于提供集群管理、数据迁移任务管理相关的各类服务。 若可用的 DM-master 节点数超过部署节点的半数，即可正常提供服务。 当部署的 DM-worker 节点数超过上游 MySQL/MariaDB 节点数时 超出上游节点数的相关 DM-worker 节点默认将处于空闲状态。 若某个 DM-worker 节点下线或与 DM-master leader 发生网络隔离，DM-master 能自动将与原 DM-worker 节点相关的数据迁移任务调度到其他空闲的 DM-worker 节点上（若原 DM-worker 节点为网络隔离状态，则其会自动停止相关的数据迁移任务）； 若无空闲的 DM-worker 节点可供调度，则原 DM-worker 相关的数据迁移任务将暂时挂起，直到有空闲 DM-worker 节点后自动恢复。 注意： 当数据迁移任务处于全量导出或导入阶段时，该迁移任务暂不支持高可用，主要原因为： 对于全量导出，MySQL 暂不支持指定从特定快照点导出，也就是说数据迁移任务被重新调度或重启后，无法继续从前一次中断时刻继续导出。 对于全量导入，DM-worker 暂不支持跨节点读取全量导出数据，也就是说数据迁移任务被调度到的新 DM-worker 节点无法读取调度发生前原 DM-worker 节点上的全量导出数据。 5、dmctl dmctl 是用来控制 DM 集群的命令行工具。具体用法参考第六章节。 创建、更新或删除数据同步任务 查看数据同步任务状态 处理数据同步任务错误 校验数据同步任务配置的正确性 6、DM使用限制 数据库版本 MySQL 版本 > 5.5 如果上游 MySQL/MariaDB servers 间构成主从复制结构，则需要 MySQL 版本> 5.7.1 或者 MariaDB 版本>=10.1.3 MariaDB 版本 >= 10.1.2 DDL 语法兼容性 目前，TiDB 部分兼容 MySQL 支持的 DDL 语句。因为 DM 使用 TiDB parser 来解析处理 DDL 语句，所以目前仅支持 TiDB parser 支持的 DDL 语法。详见 TiDB DDL 语法支持。 DM 遇到不兼容的 DDL 语句时会报错。要解决此报错，需要使用 dmctl 手动处理，要么跳过该 DDL 语句，要么用指定的 DDL 语句来替换它。详见如何处理不兼容的 DDL 语句。 分库分表 如果业务分库分表之间存在数据冲突，可以参考自增主键冲突处理来解决；否则不推荐使用 DM 进行迁移，如果进行迁移则有冲突的数据会相互覆盖造成数据丢失。 分库分表 DDL 同步限制，参见悲观模式下分库分表合并迁移使用限制以及乐观模式下分库分表合并迁移使用限制。 同步的 MySQL 实例变更 当 DM-worker 通过虚拟 IP（VIP）连接到 MySQL 且要切换 VIP 指向的 MySQL 实例时，DM 内部不同的 connection 可能会同时连接到切换前后不同的 MySQL 实例，造成 DM 拉取的 binlog 与从上游获取到的其他状态不一致，从而导致难以预期的异常行为甚至数据损坏。如需切换 VIP 指向的 MySQL 实例，请参考虚拟 IP 环境下的上游主从切换对 DM 手动执行变更。 7、DM组件的端口 各 DM-master 节点间的 peer_port（默认为 8291）可互相连通。 各 DM-master 节点可连通所有 DM-worker 节点的 port（默认为 8262）。 各 DM-worker 节点可连通所有 DM-master 节点的 port（默认为 8261）。 TiUP 节点可连通所有 DM-master 节点的 port（默认为 8261）。 TiUP 节点可连通所有 DM-worker 节点的 port（默认为 8262）。 二、TiUP部署DM 1、部署TiUP 在root用户下 curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh echo \"export PATH=\\$PATH:/root/.tiup/bin\" >> /etc/profile source /etc/profile # TiUP下载安装好的路径：~/.tiup/bin/tiup # 配置信息: ~/.tiup/bin/7b8e153f2e2d0928.root.json # 镜像信息配置到了: https://tiup-mirrors.pingcap.com success 2、TiUP安装DM # 先升级tiup自身，验证tiup是否为最新版本 tiup update --self # 先查看当前版本的tiup支持安装的DM版本 tiup list dm --verbose # 安装最新版本的DM tiup install dm:v1.4.2 3、配置DM主机拓扑配置 ①创建部署DM的主机拓扑模板配置文件 mkdir tiup-dm tiup dm template > tiup-dm/topology.yaml ②编写修改DM主机拓扑配置 更多配置模板参数参考：https://github.com/pingcap/tiup/blob/master/embed/templates/examples/dm/topology.example.yaml --- global: user: \"tidb\" ssh_port: 22 # arch: \"amd64\" server_configs: master: log-level: info # rpc-timeout: \"30s\" # rpc-rate-limit: 10.0 # rpc-rate-burst: 40 worker: log-level: info master_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-master\" data_dir: \"/data/tiup-dm/dm-master/data\" log_dir: \"/data/tiup-dm/dm-master/log\" worker_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-worker\" log_dir: \"/data/tiup-dm/dm-worker/log\" config: log-level: info monitoring_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-prometheus\" data_dir: \"/data/tiup-dm/dm-prometheus/data\" log_dir: \"/data/tiup-dm/dm-prometheus/log\" grafana_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-grafana\" alertmanager_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-alertmanager\" data_dir: \"/data/tiup-dm/dm-alertmanager/data\" log_dir: \"/data/tiup-dm/dm-alertmanager/logs\" ③配置root用户SSH免密登录 cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys ④创建修改目录权限 mkdir -p /data/tiup-dm/{dm-master/{data,logs},dm-worker/{data,logs},dm-prometheus/{data,logs},dm-grafana,dm-alertmanager/{data,logs}} && \\ useradd tidb && \\ chown -R tidb:tidb /data/tiup-dm && \\ tree -L 3 /data/tiup-dm ⑤执行安装部署dm tiup dm deploy --user root -i /root/.ssh/id_rsa # dm集群版本使用tiup list dm-master查看支持安装的DM版本 ⑥启动DM集群 tiup dm start dm集群名字 ⑦访问验证DM集群服务 # 查看集群列表 tiup dm list # 检查集群状态 tiup dm display dm集群名字 访问Grafana：http://192.168.1.6:3000/login （默认用户名密码为：admin / admin） 访问Prometheus：http://192.168.1.6:9090/graph 访问Metric信息：http://192.168.1.6:8262/metrics (Metrics数据使用dm-master自带暴露的) ⑧查看tiup操作日志 tiup dm audit [audit-id] [flags] # 在不使用 [audit-id] 参数时，该命令会显示执行的命令列表 4、TiUP扩容DM节点 ①新建 scale.yaml 文件，添加新增的 woker 节点信息： --- worker_servers: - host: 新节点IP地址 # 需要新建一个拓扑文件，文件中只写入扩容节点的描述信息，不要包含已存在的节点。其他更多配置项（如：部署目录等）请参考 [TiUP 配置参数模版](https://github.com/pingcap/tiup/blob/master/embed/templates/examples/dm/topology.example.yaml)。 ②执行扩容操作。TiUP DM 根据 scale.yaml 文件中声明的端口、目录等信息在集群中添加相应的节点： tiup dm scale-out prod-cluster scale.yaml 执行完成之后可以通过 tiup dm display prod-cluster 命令检查扩容后的集群状态。 5、TiUP缩容DM节点 缩容即下线服务，最终会将指定的节点从集群中移除，并删除遗留的相关数据文件。缩容操作进行时，内部对 DM-master、DM-worker 组件的操作流程为： ​ ①停止组件进程 ​ ②调用 DM-master 删除 member 的 API ​ ③清除节点的相关数据文件 缩容命令的基本用法： tiup dm scale-in -N # 节点 ID 可以使用 `tiup dm display` 命令获取。 6、滚动升级 滚动升级过程中尽量保证对前端业务透明、无感知，其中对不同节点有不同的操作。 tiup dm upgrade 集群名字 dm新版本号 7、更新配置 如果想要动态更新组件的配置，TiUP DM 组件为每个集群保存了一份当前的配置，如果想要编辑这份配置，则执行 tiup dm edit-config 命令。例如： tiup dm edit-config prod-cluster 然后 TiUP DM 组件会使用 vi 打开配置文件供编辑（如果你想要使用其他编辑器，请使用 EDITOR 环境变量自定义编辑器，例如 export EDITOR=nano），编辑完之后保存即可。此时的配置并没有应用到集群，如果想要让它生效，还需要执行： tiup dm reload prod-cluster 该操作会将配置发送到目标机器，滚动重启集群，使配置生效。 8、更新组件 常规的升级集群可以使用 upgrade 命令，但是在某些场景下（例如 Debug)，可能需要用一个临时的包替换正在运行的组件，此时可以用 patch 命令： tiup dm patch [参数] 参数: -h, --help help for patch -N, --node string 以IP地址的形式指定更新组件的节点，例如：172.16.4.5:8261 --overwrite Use this package in the future scale-out operations -R, --role strings 以集群角色指定更新组件的节点 --transfer-timeout int Timeout in seconds when transferring dm-master leaders (default 300) 全局参数: --native-ssh Use the native SSH client installed on local system instead of the build-in one. --ssh-timeout int Timeout in seconds to connect host via SSH, ignored for operations that don't need an SSH connection. (default 5) --wait-timeout int Timeout in seconds to wait for an operation to complete, ignored for operations that don't fit. (default 60) -y, --yes Skip all confirmations and assumes 'yes' 9、在集群节点机器上执行命令 exec 命令可以很方便地到集群的机器上执行命令，使用方式如下： tiup dm exec [flags] Flags: --command string the command run on cluster host (default \"ls\") -h, --help help for exec -N, --node strings Only exec on host with specified nodes -R, --role strings Only exec on host with specified roles --sudo use root permissions (default false) 三、部署DM Portal DM Portal是一个方便用户图形化配置DM任务的Web页面。 curl -s -# https://download.pingcap.org/dm-portal-latest-linux-amd64.tar.gz | tar zxvf - -C /opt/ && \\ ln -s /opt/dm-portal-latest-linux-amd64 /opt/dm-portal && \\ echo -e \"export DM_PORTAL_HOME=/opt/dm-portal\\nexport PATH=\\$PATH:\\$DM_PORTAL_HOME/bin\" >> /etc/profile && \\ source /etc/profile && \\ mkdir -p /root/tiup-dm-1.4.2/dm-portal/task-conf && \\ nohup /opt/dm-portal/bin/dm-portal --port=8280 -task-file-path=/root/tiup-dm-1.4.2/dm-portal/task-conf > /root/tiup-dm-1.4.2/dm-portal/dm-portal.log 2>&1 & 访问：http://DM_Portal服务器地址:8280，在Web页面上就可图形化配置DM同步任务。配置文件可通过浏览器直接下载，也可以在/root/tiup-dm-1.4.2/dm-portal/task-conf路径下找到。 更多信息参考文档：https://docs.pingcap.com/zh/tidb-data-migration/v1.0/dm-portal 四、管理上游数据源配置 1、加密数据库密码 在 DM 相关配置文件中，推荐使用经 dmctl 加密后的密码。对于同一个原始密码，每次加密后密码不同。 tiup dmctl --encrypt 密码 2、创建配置文件 source-id: \"mysql-192-201\" # DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是在上游 MySQL 已开启 GTID 模式。 enable-gtid: false from: host: \"上游数据源的IP地址\" user: \"上游数据源的用户名\" password: \"上游数据源用户的密码\" port: 上游数据源的端口 3、数据源操作 tiup dmctl --master-addr 192.168.1.6:8261 operate-source 操作动作 上游数据源配置文件路径(可传递多个文件路径) 操作动作如下： create：创建一个或多个上游的数据库源。创建多个数据源失败时，会尝试回滚到执行命令之前的状态 update：更新一个上游的数据库源 stop：停止一个或多个上游的数据库源。停止多个数据源失败时，可能有部分数据源已成功停止 show：显示已添加的数据源以及对应的 DM-worker 五、同步任务配置 1、配置文件基础结构 ①配置信息及类型 配置文件总共分为两个部分：全局配置和实例配置，其中全局配置又分为任务基本信息配置和实例配置， 配置顺序如下： 编辑全局配置。 根据全局配置编辑实例配置。 ②同步任务模式：task-mode 描述：任务模式，可以通过任务模式来指定需要执行的数据迁移工作。 值为字符串(full，incremental 或 all）) full：只全量备份上游数据库，然后将数据全量导入到下游数据库。 incremental：只通过 binlog 把上游数据库的增量修改复制到下游数据库, 可以设置实例配置的 meta 配置项来指定增量复制开始的位置。 all：full + incremental。先全量备份上游数据库，将数据全量导入到下游数据库，然后从全量数据备份时导出的位置信息 (binlog position) 开始通过 binlog 增量复制数据到下游数据库。 功能配置集 全局配置主要包含下列功能配置集： 配置项 说明 routes 上游和下游表之间的路由 table routing 规则集。如果上游与下游的库名、表名一致，则不需要配置该项。使用场景及示例配置参见 Table Routing filters 上游数据库实例匹配的表的 binlog event filter 规则集。如果不需要对 binlog 进行过滤，则不需要配置该项。使用场景及示例配置参见 Binlog Event Filter block-allow-list 该上游数据库实例匹配的表的 block & allow lists 过滤规则集。建议通过该项指定需要迁移的库和表，否则会迁移所有的库和表。使用场景及示例配置参见 Block & Allow Lists mydumpers dump 处理单元的运行配置参数。如果默认配置可以满足需求，则不需要配置该项，也可以只使用 mydumper-thread 对 thread 配置项单独进行配置。 loaders load 处理单元的运行配置参数。如果默认配置可以满足需求，则不需要配置该项，也可以只使用 loader-thread 对 pool-size 配置项单独进行配置。 syncers sync 处理单元的运行配置参数。如果默认配置可以满足需求，则不需要配置该项，也可以只使用 syncer-thread 对 worker-count 配置项单独进行配置。 各个功能配置集的参数及解释参见完整配置文件示例中的注释说明。 ③ ④ ⑤ ⑥ ⑦ ⑧ 2、配置文件示例 --- # ----------- 全局配置 ----------- # ********* 基本信息配置 ********* # 任务名称，需要全局唯一 name: test # 任务模式，可设为 \"full\" - \"只进行全量数据迁移\"、\"incremental\" - \"Binlog 实时同步\"、\"all\" - \"全量 + Binlog 迁移\" task-mode: all # 如果为分库分表合并任务则需要配置该项。默认使用悲观协调模式 \"pessimistic\"，在深入了解乐观协调模式的原理和使用限制后，也可以设置为乐观协调模式 \"optimistic\" shard-mode: \"pessimistic\" # 下游储存 `meta` 信息的数据库 meta-schema: \"dm_meta\" # 时区 timezone: \"Asia/Shanghai\" # schema/table 是否大小写敏感 case-sensitive: false # 目前仅支持 \"gh-ost\" 、\"pt\" online-ddl-scheme: \"gh-ost\" # 不关闭任何检查项。可选的检查项有 \"all\"、\"dump_privilege\"、\"replication_privilege\"、\"version\"、\"binlog_enable\"、\"binlog_format\"、\"binlog_row_image\"、\"table_schema\"、\"schema_of_shard_tables\"、\"auto_increment_ID\" ignore-checking-items: [] # 是否清理 dump 阶段产生的文件，包括 metadata 文件、建库建表 SQL 文件以及数据导入 SQL 文件 clean-dump-file: true # 下游数据库实例配置 target-database: host: \"192.168.0.1\" port: 4000 user: \"root\" # 推荐使用经 dmctl 加密后的密码 password: \"/Q7B9DizNLLTTfiZHv9WoEAKamfpIUs=\" # 设置 DM 内部连接 TiDB 服务器时，TiDB 客户端的 \"max_allowed_packet\" 限制（即接受的最大数据包限制），单位为字节，默认 67108864 (64 MB) max-allowed-packet: 67108864 # 该配置项从 DM v2.0.0 版本起弃用，DM 会自动获取连接 TiDB 的 \"max_allowed_packet\" # 设置 TiDB 的 session 变量，在 v1.0.6 版本引入。更多变量及解释参见 `https://docs.pingcap.com/zh/tidb/stable/system-variables` session: # 从 DM v2.0.0 版本起，如果配置文件中没有出现该项，DM 会自动从下游 TiDB 中获得适合用于 \"sql_mode\" 的值。手动配置该项具有更高优先级 sql_mode: \"ANSI_QUOTES,NO_ZERO_IN_DATE,NO_ZERO_DATE\" # 从 DM v2.0.0 版本起，如果配置文件中没有出现该项，DM 会自动从下游 TiDB 中获得适合用于 \"tidb_skip_utf8_check\" 的值。手动配置该项具有更高优先级 tidb_skip_utf8_check: 1 tidb_constraint_check_in_place: 0 # 下游 TiDB TLS 相关配置 security: ssl-ca: \"/path/to/ca.pem\" ssl-cert: \"/path/to/cert.pem\" ssl-key: \"/path/to/key.pem\" # ******** 功能配置集 ********** # # 上游和下游表之间的路由 table routing 规则集 routes: # 配置名称 route-rule-1: # 库名匹配规则，支持通配符 \"*\" 和 \"?\" schema-pattern: \"test_*\" # 表名匹配规则，支持通配符 \"*\" 和 \"?\" table-pattern: \"t_*\" # 目标库名称 target-schema: \"test\" # 目标表名称 target-table: \"t\" route-rule-2: schema-pattern: \"test_*\" target-schema: \"test\" # 上游数据库实例匹配的表的 binlog event filter 规则集 filters: # 配置名称 filter-rule-1: # 库名匹配规则，支持通配符 \"*\" 和 \"?\" schema-pattern: \"test_*\" # 表名匹配规则，支持通配符 \"*\" 和 \"?\" table-pattern: \"t_*\" # 匹配哪些 event 类型 events: [\"truncate table\", \"drop table\"] # 对与符合匹配规则的 binlog 迁移（Do）还是忽略(Ignore) action: Ignore filter-rule-2: schema-pattern: \"test_*\" events: [\"all dml\"] action: Do # 定义数据源迁移表的过滤规则，可以定义多个规则。如果 DM 版本 DM任务完整配置参考：https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-file-full/#%E5%AE%8C%E6%95%B4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B 六、dmctl集群控制 注意： 对于用 TiUP 部署的 DM 集群，推荐直接使用 tiup dmctl 命令。 dmctl 是用来运维 DM 集群的命令行工具，支持交互模式和命令模式。 1、dmctl tidb dmctl [global options] command [command options] [arguments...] 特殊命令: --encrypt Encrypts plaintext to ciphertext. --decrypt Decrypts ciphertext to plaintext. 全局命令参数: --V Prints version and exit. --config Path to config file. --master-addr Master API server address. --rpc-timeout RPC timeout, default is 10m. --ssl-ca Path of file that contains list of trusted SSL CAs for connection. --ssl-cert Path of file that contains X509 certificate in PEM format for connection. --ssl-key Path of file that contains X509 key in PEM format for connection. 交互模式：与 DM-master 进行交互 命令格式： tiup dmctl:[dmctl版本] --master-addr dm-master节点ip地址:8261 交互模式下不具有 bash 的特性，比如不需要通过引号传递字符串参数而应当直接传递。 命令模式：执行命令时只需要在 dmctl 命令后紧接着执行任务操作，任务操作同交互模式的参数一致。 命令格式：tiup dmctl -master-addr dm-master节点ip地址:8261 dmctl的命令 一条 dmctl 命令只能跟一个任务操作 任务操作只能放在 dmctl 命令的最后 2、dmctl命令选项参数 Available Commands: check-task 检查任务配置文件. get-config 获取任务的信息 handle-error `skip`/`replace`/`revert` the current error event or a specific binlog position (binlog-pos) event. help Gets help about any command. list-member 列出DM Member信息. offline-member Offlines member which has been closed. operate-leader `evict`/`cancel-evict` the leader. operate-schema `get`/`set`/`remove` the schema for an upstream table. operate-source `create`/`update`/`stop`/`show` 管理上游数据源MySQL/MariaDB配置 pause-relay 暂停DM-worker的relay逻辑处理单元. pause-task 暂停在跑的任务. purge-relay DM 支持自动清理 relay log，但同时 DM 也支持使用 purge-relay 命令手动清理 relay log。 query-status 查询任务状态 resume-relay 恢复DM-worker的relay逻辑处理单元. resume-task 恢复处于暂停状态的任务 show-ddl-locks 显示无法解决的DDL锁 start-relay Starts workers pulling relay log for a source. start-task 启动配置文件中配置的任务 stop-relay Stops workers pulling relay log for a source. stop-task 暂停任务 transfer-source 改变上游数据源MySQL/MariaDB与 DM-worker 的绑定关系。 unlock-ddl-lock 强制解开DDL锁 Flags: -h, --help help for dmctl -s, --source strings MySQL Source ID 3、dmctl常用命令 ①检查任务配置文件 # 交互模式下 check-task 任务配置文件路径（最好是绝对路径） ②启动同步任务 # 交互模式下 start-task [-s source ...] [--remove-meta] 任务配置文件路径（最好是绝对路径）[flags] 参数: --remove-meta 是否删除任务的元数据 ③查询同步任务状态 # 交互模式下 query-status [-s source ...] [任务名 | 任务配置文件路径] [--more] [flags] 参数: --more whether to print the detailed task information 全局参数: -s, --source strings MySQL Source ID. ④暂停任务 # 交互模式下 stop-task [-s source ...] [flags] 全局参数: -s, --source strings MySQL Source ID. ⑤恢复任务 # 交互模式下 resume-task [-s source ...] [flags] 全局参数: -s, --source strings MySQL Source ID. 4、跳过或替代执行异常的SQL 文档：https://docs.pingcap.com/zh/tidb-data-migration/stable/handle-failed-ddl-statements 新的DM跳过异常SQL使用的是handle-error命令，不再使用sql-skip。也不再支持以SQL语句的方式跳过。 ①先查出任务错误或异常SQL所处的binlog位置 query-status 任务名 ②使用handle-error跳过，替换，恢复异常的SQL # 交互模式下 handle-error [-s source ...] [-b binlog-pos] [替代的SQL语句1;替代的SQL语句2;] [flags] 参数: -b, --binlog-pos string .pos格式：\"mysql-bin|000001.000003:3270\" 全局参数: -s, --source strings MySQL Source ID. 七、DM监控 使用TiUP部署DM组件时可以部署prometheus生态的监控组件。 DM的Metrics信息是由dm-master进行暴露的。 如果已部署的有Grafana，想复用，可不部署Grafana，只需部署Prometheus即可。Grafana添加数据源，然后导入JSON格式的Dashboard定义文件（见附件）即可。 可监控的指标 1、Overview overview 下包含运行当前选定 task 的所有 DM-worker/master instance/source 的部分监控指标。当前默认告警规则只针对于单个 DM-worker/master instance/source。 metric 名称 说明 告警说明 告警级别 task state 迁移子任务的状态 N/A N/A storage capacity relay log 占有的磁盘的总容量 N/A N/A storage remain relay log 占有的磁盘的剩余可用容量 N/A N/A binlog file gap between master and relay relay 与上游 master 相比落后的 binlog file 个数 N/A N/A load progress load unit 导入过程的进度百分比，值变化范围为：0% - 100% N/A N/A binlog file gap between master and syncer 与上游 master 相比 binlog replication unit 落后的 binlog file 个数 N/A N/A shard lock resolving 当前子任务是否正在等待 shard DDL 迁移，大于 0 表示正在等待迁移 N/A N/A 2、Operate error metric 名称 说明 告警说明 告警级别 before any operate error 在进行操作之前出错的次数 N/A N/A source bound error 数据源绑定操作出错次数 N/A N/A start error 子任务启动的出错次数 N/A N/A pause error 子任务暂停的出错次数 N/A N/A resume error 子任务恢复的出错次数 N/A N/A auto-resume error 子任务自动恢复的出错次数 N/A N/A update error 子任务更新的出错次数 N/A N/A stop error 子任务停止的出错次数 N/A N/A 3、HA 高可用 metric 名称 说明 告警说明 告警级别 number of dm-masters start leader components per minute 每分钟内 DM-master 尝试启用 leader 相关组件次数 N/A N/A number of workers in different state 不同状态下有多少个 DM-worker 存在离线的 DM-worker 超过一小时 critical workers' state DM-worker 的状态 N/A N/A number of worker event error 不同类型的 DM-worker 错误出现次数 N/A N/A shard ddl error per minute 每分钟内不同类型的 shard DDL 错误次数 发生 shard DDL 错误 critical number of pending shard ddl 未完成的 shard DDL 数目 存在未完成的 shard DDL 数目超过一小时 critical 4、Task 状态 metric 名称 说明 告警说明 告警级别 task state 迁移子任务的状态 当子任务状态处于 Paused 超过 20 分钟时 critical 5、Dump/Load unit 下面 metrics 仅在 task-mode 为 full 或者 all 模式下会有值。 metric 名称 说明 告警说明 告警级别 load progress load unit 导入过程的进度百分比，值变化范围为：0% - 100% N/A N/A data file size load unit 导入的全量数据中数据文件（内含 INSERT INTO 语句）的总大小 N/A N/A dump process exits with error dump unit 在 DM-worker 内部遇到错误并且退出了 立即告警 critical load process exits with error load unit 在 DM-worker 内部遇到错误并且退出了 立即告警 critical table count load unit 导入的全量数据中 table 的数量总和 N/A N/A data file count load unit 导入的全量数据中数据文件（内含 INSERT INTO 语句）的数量总和 N/A N/A transaction execution latency load unit 在执行事务的时延，单位：秒 N/A N/A statement execution latency load unit 执行语句的耗时，单位：秒 N/A N/A 6、Binlog replication 下面 metrics 仅在 task-mode 为 incremental 或者 all 模式下会有值。 metric 名称 说明 告警说明 告警级别 remaining time to sync 预计 Syncer 还需要多少分钟可以和 master 完全同步，单位：分钟 N/A N/A replicate lag master 到 Syncer 的 binlog 复制延迟时间，单位：秒 N/A N/A process exist with error binlog replication unit 在 DM-worker 内部遇到错误并且退出了 立即告警 critical binlog file gap between master and syncer 与上游 master 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog file gap between relay and syncer 与 relay 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog event QPS 单位时间内接收到的 binlog event 数量 (不包含需要跳过的 event) N/A N/A skipped binlog event QPS 单位时间内接收到的需要跳过的 binlog event 数量 N/A N/A read binlog event duration binlog replication unit 从 relay log 或上游 MySQL 读取 binlog 的耗时，单位：秒 N/A N/A transform binlog event duration binlog replication unit 解析 binlog 并将 binlog 转换成 SQL 语句的耗时，单位：秒 N/A N/A dispatch binlog event duration binlog replication unit 调度一条 binlog event 的耗时，单位：秒 N/A N/A transaction execution latency binlog replication unit 执行事务到下游的耗时，单位：秒 N/A N/A binlog event size binlog replication unit 从 relay log 或上游 MySQL 读取的单条 binlog event 的大小 N/A N/A DML queue remain length 剩余 DML job 队列的长度 N/A N/A total sqls jobs 单位时间内新增的 job 数量 N/A N/A finished sqls jobs 单位时间内完成的 job 数量 N/A N/A statement execution latency binlog replication unit 执行语句到下游的耗时，单位：秒 N/A N/A add job duration binlog replication unit 增加一条 job 到队列的耗时，单位：秒 N/A N/A DML conflict detect duration binlog replication unit 检测 DML 间冲突的耗时，单位：秒 N/A N/A skipped event duration binlog replication unit 跳过 binlog event 的耗时，单位：秒 N/A N/A unsynced tables 当前子任务内还未收到 shard DDL 的分表数量 N/A N/A shard lock resolving 当前子任务是否正在等待 shard DDL 迁移，大于 0 表示正在等待迁移 N/A N/A 7、Relay log metric 名称 说明 告警说明 告警级别 storage capacity relay log 占有的磁盘的总容量 N/A N/A storage remain relay log 占有的磁盘的剩余可用容量 小于 10G 的时候需要告警 critical process exits with error relay log 在 DM-worker 内部遇到错误并且退出了 立即告警 critical relay log data corruption relay log 文件损坏的个数 立即告警 emergency fail to read binlog from master relay 从上游的 MySQL 读取 binlog 时遇到的错误数 立即告警 critical fail to write relay log relay 写 binlog 到磁盘时遇到的错误数 立即告警 critical binlog file index relay log 最大的文件序列号。如 value = 1 表示 relay-log.000001 N/A N/A binlog file gap between master and relay relay 与上游 master 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog pos relay log 最新文件的写入 offset N/A N/A read binlog event duration relay log 从上游的 MySQL 读取 binlog 的时延，单位：秒 N/A N/A write relay log duration relay log 每次写 binlog 到磁盘的时延，单位：秒 N/A N/A binlog event size relay log 写到磁盘的单条 binlog 的大小 N/A N/A 8、Instance 在 Grafana dashboard 中，instance 的默认名称为 DM-instance。 9、Relay log metric 名称 说明 告警说明 告警级别 storage capacity relay log 占有的磁盘的总容量 N/A N/A storage remain relay log 占有的磁盘的剩余可用容量 小于 10G 的时候需要告警 critical process exits with error relay log 在 DM-worker 内部遇到错误并且退出了 立即告警 critical relay log data corruption relay log 文件损坏的个数 立即告警 emergency fail to read binlog from master relay 从上游的 MySQL 读取 binlog 时遇到的错误数 立即告警 critical fail to write relay log relay 写 binlog 到磁盘时遇到的错误数 立即告警 critical binlog file index relay log 最大的文件序列号。如 value = 1 表示 relay-log.000001 N/A N/A binlog file gap between master and relay relay 与上游 master 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog pos relay log 最新文件的写入 offset N/A N/A read binlog duration relay log 从上游的 MySQL 读取 binlog 的时延，单位：秒 N/A N/A write relay log duration relay log 每次写 binlog 到磁盘的时延，单位：秒 N/A N/A binlog size relay log 写到磁盘的单条 binlog 的大小 N/A N/A 10、task metric 名称 说明 告警说明 告警级别 task state 迁移子任务的状态 当子任务状态处于 paused 超过 10 分钟时 critical load progress load unit 导入过程的进度百分比，值变化范围为：0% - 100% N/A N/A binlog file gap between master and syncer 与上游 master 相比 binlog replication unit 落后的 binlog file 个数 N/A N/A shard lock resolving 当前子任务是否正在等待 shard DDL 迁移，大于 0 表示正在等待迁移 N/A N/A 八、DM任务优化 九、DM任务问题汇总 1、上游MySQL的DDL语句不支持同步执行到TiDB 2、上游MySQL的DML语句不支持同步执行到TiDB 3、上游MySQL表中有外键造成无法同步 4、上游MySQL表中日期数据值包含“00”造成无法同步 参考 https://github.com/pingcap/tidb/issues/6072 https://docs.pingcap.com/zh/tidb-data-migration/stable/key-features#%E8%BF%87%E6%BB%A4%E8%A7%84%E5%88%99 https://docs.pingcap.com/zh/tidb-data-migration/stable/dmctl-introduction https://docs.pingcap.com/zh/tidb-data-migration/stable/migrate-data-using-dm#%E7%AC%AC-8-%E6%AD%A5%E7%9B%91%E6%8E%A7%E4%BB%BB%E5%8A%A1%E4%B8%8E%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97 https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-file-full https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-guide#%E9%85%8D%E7%BD%AE%E9%9C%80%E8%A6%81%E8%BF%81%E7%A7%BB%E7%9A%84%E8%A1%A8 https://docs.pingcap.com/zh/tidb-data-migration/stable/maintain-dm-using-tiup#%E9%9B%86%E7%BE%A4%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7-dmctl https://docs.pingcap.com/zh/tidb-data-migration/stable/tune-configuration https://docs.pingcap.com/zh/tidb-data-migration/v1.0/skip-or-replace-abnormal-sql-statements https://asktug.com/t/topic/63887 https://github.com/google/re2/wiki/Syntax https://docs.pingcap.com/zh/tidb-data-migration/stable/faq https://docs.pingcap.com/zh/tidb-data-migration/stable/query-status https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-guide#%E9%85%8D%E7%BD%AE%E9%9C%80%E8%A6%81%E6%95%B0%E6%8D%AE%E6%BA%90%E8%A1%A8%E5%88%B0%E7%9B%AE%E6%A0%87-tidb-%E8%A1%A8%E7%9A%84%E6%98%A0%E5%B0%84 https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-file-full#%E5%AE%8C%E6%95%B4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B https://docs.pingcap.com/zh/tidb-data-migration/stable/usage-scenario-simple-migration#%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88 https://asktug.com/t/topic/33397/4 https://docs.pingcap.com/tidb/stable/data-type-date-and-time https://asktug.com/t/topic/33397 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-17 19:24:47 "},"origin/tidb-dumpling-export.html":{"url":"origin/tidb-dumpling-export.html","title":"TiDB-Dumpling：从TiDB/MySQL导出数据","keywords":"","body":"Dumpling全量备份或导出 一、Dumpling 简介 Dumpling 是使用 go 开发的数据备份工具，项目地址可以参考 Dumpling。 Dumpling 的更多具体用法可查看 Dumpling 主要选项表。 Dumpling 包含在tidb-toolkit 安装包中，下载链接：https://download.pingcap.org/tidb-toolkit-v4.0.5-linux-amd64.tar.gz 为了快速地备份恢复数据（特别是数据量巨大的库），可以参考以下建议： 导出来的数据文件应当尽可能的小，可以通过设置选项 -F 来控制导出来的文件大小。 如果后续使用 TiDB Lightning 对备份文件进行恢复，建议把 dumpling -F 选项的值设置为 256m。 如果导出的表中有些表的行数非常多，可以通过设置选项 -r 来开启表内并发。 二、对比Mydumper 支持导出多种数据形式，包括 SQL/CSV 支持全新的 table-filter，筛选数据更加方便 针对 TiDB 进行了更多优化： 支持配置 TiDB 单条 SQL 内存限制 针对 TiDB v4.0.0 以上版本支持自动调整 TiDB GC 时间 使用 TiDB 的隐藏列 _tidb_rowid 优化了单表内数据的并发导出性能 对于 TiDB 可以设置 tidb_snapshot 的值指定备份数据的时间点，从而保证备份的一致性，而不是通过 FLUSH TABLES WITH READ LOCK 来保证备份一致性。 三、从TiDB/MySQL 导出数据 1、源库导出账号所需权限 SELECT RELOAD LOCK TABLES REPLICATION CLIENT 2、安装及主要参数 version=v4.0.5 && \\ curl -# https://download.pingcap.org/tidb-toolkit-$version-linux-amd64.tar.gz | tar -zxC /opt && \\ ln -s /opt/tidb-toolkit-$version-linux-amd64 /opt/tidb-toolkit-$version && \\ echo \"export PATH=/opt/tidb-toolkit-$version/bin:$PATH\" >> /etc/profile && \\ source /etc/profile && \\ tidb-lightning -V 主要选项 用途 默认值 -V 或 --version 输出 Dumpling 版本并直接退出 -B 或 --database 导出指定数据库 -T 或 --tables-list 导出指定数据表 -f 或 --filter 导出能匹配模式的表，语法可参考 table-filter *.*（导出所有库表） --case-sensitive table-filter 是否大小写敏感 false，大小写不敏感 -h 或 --host 连接的数据库主机的地址 \"127.0.0.1\" -t 或 --threads 备份并发线程数 4 -r 或 --rows 将 table 划分成 row 行数据，一般针对大表操作并发生成多个文件。 -L 或 --logfile 日志输出地址，为空时会输出到控制台 \"\" --loglevel 日志级别 {debug,info,warn,error,dpanic,panic,fatal} \"info\" --logfmt 日志输出格式 {text,json} \"text\" -d 或 --no-data 不导出数据，适用于只导出 schema 场景 --no-header 导出 csv 格式的 table 数据，不生成 header -W 或 --no-views 不导出 view true -m 或 --no-schemas 不导出 schema，只导出数据 -s 或--statement-size 控制 INSERT SQL 语句的大小，单位 bytes -F 或 --filesize 将 table 数据划分出来的文件大小，需指明单位（如 128B, 64KiB, 32MiB, 1.5GiB） --filetype 导出文件类型（csv/sql） \"sql\" -o 或 --output 导出文件路径 \"./export-${time}\" -S 或 --sql 根据指定的 sql 导出数据，该选项不支持并发导出 --consistency flush: dump 前用 FTWRLsnapshot: 通过 TSO 来指定 dump 某个快照时间点的 TiDB 数据 lock: 对需要 dump 的所有表执行 lock tables read 命令 none: 不加锁 dump，无法保证一致性 auto: MySQL 默认用 flush, TiDB 默认用 snapshot \"auto\" --snapshot snapshot tso，只在 consistency=snapshot 下生效 --where 对备份的数据表通过 where 条件指定范围 -p 或 --password 连接的数据库主机的密码 -P 或 --port 连接的数据库主机的端口 4000 -u 或 --user 连接的数据库主机的用户名 \"root\" --dump-empty-database 导出空数据库的建库语句 true --ca 用于 TLS 连接的 certificate authority 文件的地址 --cert 用于 TLS 连接的 client certificate 文件的地址 --key 用于 TLS 连接的 client private key 文件的地址 --csv-delimiter csv 文件中字符类型变量的定界符 '\"' --csv-separator csv 文件中各值的分隔符 ',' --csv-null-value csv 文件空值的表示 \"\\N\" --escape-backslash 使用反斜杠 (\\) 来转义导出文件中的特殊字符 true --output-filename-template 以golang template 格式表示的数据文件名格式 支持 DB Table、Index 三个参数 分别表示数据文件的库名、表名、分块 ID {{.DB}}.{{.Table}}.{{.Index}} --status-addr Dumpling 的服务地址，包含了 Prometheus 拉取 metrics 信息及 pprof 调试的地址 \":8281\" --tidb-mem-quota-query 单条 dumpling 命令导出 SQL 语句的内存限制，单位为 byte，默认为 32 GB 34359738368 3、导出数据文件格式 ①导出到 sql 文件 Dumpling 默认导出数据格式为 sql 文件。也可以通过设置 --filetype sql 导出数据到 sql 文件： dumpling -u root -P 4000 -h 127.0.0.1 --filetype sql --threads 32 -o /data/dumpling-export -F 256 上述命令中，-h、-P、-u 分别是地址，端口，用户。如果需要密码验证，可以用 -p $YOUR_SECRET_PASSWORD 传给 Dumpling。 ②导出到 csv 文件 假如导出数据的格式是 CSV（使用 --filetype csv 即可导出 CSV 文件），还可以使用 --sql 导出指定 SQL 选择出来的记录，例如，导出 test.sbtest1 中所有 id 的记录： dumpling -u root -P 4000 -h 127.0.0.1 -o /data/dumpling-export/test \\ --filetype csv \\ --sql 'select * from `test`.`sbtest1` where id 注意： --sql 选项暂时仅仅可用于导出 csv 的场景。 这里需要在要导出的所有表上执行 select * from where id 语句。如果部分表没有指定的字段，那么导出会失败。 4、筛选导出的数据 ①使用 --where 选项筛选数据 默认情况下，除了系统数据库中的表之外，Dumpling 会导出整个数据库的表。你可以使用 --where 来选定要导出的记录。 dumpling -u root -P 4000 -h 127.0.0.1 -o /data/dumpling-export/test \\ --where \"id 上述命令将会导出各个表的 id ②使用 --filter 选项筛选数据 Dumpling 可以通过 --filter 指定 table-filter 来筛选特定的库表。table-filter 的语法与 .gitignore 相似，详细语法参考表库过滤。 dumpling -u root -P 4000 -h 127.0.0.1 -o /data/dumpling-export/test \\ --filter \"employees.*\" \\ --filter \"*.WorkOrder\" 上述命令将会导出 employees 数据库的所有表，以及所有数据库中的 WorkOrder 表。 ③使用 -B 或 -T 选项筛选数据 Dumpling 也可以通过 -B 或 -T 选项导出特定的数据库/数据表。 注意： --filter 选项与 -T 选项不可同时使用。 -T 选项只能接受完整的 库名.表名 形式，不支持只指定表名。例：Dumpling 无法识别 -T WorkOrder。 例如通过指定： -B employees ：导出 employees 数据库 -T employees.WorkOrder ：导出 employees.WorkOrder 数据表 5、并发提高 Dumpling导出效率选项 默认情况下，导出的文件会存储到 ./export- 目录下。常用选项如下： -o 用于选择存储导出文件的目录。 -F 选项用于指定单个文件的最大大小，默认单位为 MiB。可以接受类似 5GiB 或 8KB 的输入。 -r 选项用于指定单个文件的最大记录数（或者说，数据库中的行数），开启后 Dumpling 会开启表内并发，提高导出大表的速度。 利用以上选项可以让 Dumpling 的并行度更高。 6、调整 Dumpling 的数据一致性选项 注意： 在大多数场景下，用户不需要调整 Dumpling 的默认数据一致性选项。 Dumpling 通过 --consistency 标志控制导出数据“一致性保证”的方式。对于 TiDB 来说，默认情况下，会通过获取某个时间戳的快照来保证一致性（即 --consistency snapshot）。在使用 snapshot 来保证一致性的时候，可以使用 --snapshot 选项指定要备份的时间戳。还可以使用以下的一致性级别： flush：使用 FLUSH TABLES WITH READ LOCK 来保证一致性。 snapshot：获取指定时间戳的一致性快照并导出。 lock：为待导出的所有表上读锁。 none：不做任何一致性保证。 auto：对 MySQL 使用 flush，对 TiDB 使用 snapshot。 7、导出的SQL文件 TiDB Dumping导出的SQL文件命名格式都有： metadata：此文件包含导出的起始时间以及 master binary log 的位置。 {database}-schema-create.sql：创建database的 SQL 文件on {database}.{table}-schema.sql：创建 table 的 SQL 文件 {database}.{table}.{0001}.{sql|csv}：数据源文件 *-schema-view.sql、*-schema-trigger.sql、*-schema-post.sql：其他导出文件 后续如果想使用TiDB Lighting将这些SQL文件导入到TiDB另外一个的DB中的话，可批量将SQL文件名的database部分改掉 # 例如源库DB为Test，想把数据导入到目标库Test-2中 old_database_name=test new_database_name=Test-2 for i in $(ls *.sql | grep -v schema-create );do mv $i $new_database_name.${i#*.}; done mv ${old_database_name}-schema-create.sql ${new_database_name}-schema-create.sql echo \"\" > ${new_database_name}-schema-create.sql 8、导出 TiDB 的历史数据快照 Dumpling 可以通过 --snapshot 指定导出某个 tidb_snapshot 时的数据。 --snapshot 选项可设为 TSO（SHOW MASTER STATUS 输出的 Position 字段）或有效的 datetime 时间，例如： dumpling --snapshot 417773951312461825 dumpling --snapshot \"2020-07-02 17:12:45\" 即可导出 TSO 为 417773951312461825 或 2020-07-02 17:12:45 时的 TiDB 历史数据快照。 9、导出大规模数据时的 TiDB GC 设置 从 v4.0.0 版本开始，Dumpling 可以自动延长 GC 时间（Dumpling 需要访问 TiDB 集群的 PD 地址），而 v4.0.0 之前的版本，需要手动调整 GC 时间，否则 dumpling 备份时可能出现以下报错： Could not read data from testSchema.testTable: GC life time is shorter than transaction duration, transaction starts at 2019-08-05 21:10:01.451 +0800 CST, GC safe point is 2019-08-05 21:14:53.801 +0800 CST 手动调整 GC 时间的步骤： 执行 dumpling 命令前，查询 TiDB 集群的 GC 值并在 MySQL 客户端执行下列语句将其调整为合适的值： SELECT * FROM mysql.tidb WHERE VARIABLE_NAME = 'tikv_gc_life_time'; +-----------------------+---------------------------------------------------------------------------------+ | VARIABLE_NAME | VARIABLE_VALUE +-----------------------+---------------------------------------------------------------------------------+ | tikv_gc_life_time | 10m0s +-----------------------+---------------------------------------------------------------------------------+ 1 rows in set (0.02 sec) update mysql.tidb set VARIABLE_VALUE = '720h' where VARIABLE_NAME = 'tikv_gc_life_time'; 执行 dumpling 命令后，将 TiDB 集群的 GC 值恢复到第 1 步中的初始值： update mysql.tidb set VARIABLE_VALUE = '10m' where VARIABLE_NAME = 'tikv_gc_life_time'; Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-26 16:02:21 "},"origin/tidb-lighting-import.html":{"url":"origin/tidb-lighting-import.html","title":"TiDB-Lightning：导入数据到TIDB","keywords":"","body":"使用tidb-lightning导入数据到TIDB 一、简介 TiDB Lightning 是一个将全量数据高速导入到 TiDB 集群的工具 TiDB Lightning 有以下两个主要的使用场景： 大量新数据的快速导入 全量备份数据的恢复 目前，TiDB Lightning 支持： 导入 Dumpling、CSV 或 Amazon Aurora Parquet 输出格式的数据源。 从本地盘或 Amazon S3 云盘读取数据。 二、工作原理 在导数据之前，tidb-lightning 会自动将 TiKV 集群切换为“导入模式” (import mode)，优化写入效率并停止自动压缩。 tidb-lightning 会在目标数据库建立架构和表，并获取其元数据。 每张表都会被分割为多个连续的区块，这样来自大表 (200 GB+) 的数据就可以用增量方式并行导入。 tidb-lightning 会为每一个区块准备一个“引擎文件 (engine file)”来处理键值对。tidb-lightning 会并发读取 SQL dump，将数据源转换成与 TiDB 相同编码的键值对，然后将这些键值对排序写入本地临时存储文件中。 当一个引擎文件数据写入完毕时，tidb-lightning 便开始对目标 TiKV 集群数据进行分裂和调度，然后导入数据到 TiKV 集群。 引擎文件包含两种：数据引擎与索引引擎，各自又对应两种键值对：行数据和次级索引。通常行数据在数据源里是完全有序的，而次级索引是无序的。因此，数据引擎文件在对应区块写入完成后会被立即上传，而所有的索引引擎文件只有在整张表所有区块编码完成后才会执行导入。 整张表相关联的所有引擎文件完成导入后，tidb-lightning 会对比本地数据源及下游集群的校验和 (checksum)，确保导入的数据无损，然后让 TiDB 分析 (ANALYZE) 这些新增的数据，以优化日后的操作。同时，tidb-lightning 调整 AUTO_INCREMENT 值防止之后新增数据时发生冲突。 表的自增 ID 是通过行数的上界估计值得到的，与表的数据文件总大小成正比。因此，最后的自增 ID 通常比实际行数大得多。这属于正常现象，因为在 TiDB 中自增 ID 不一定是连续分配的。 在所有步骤完毕后，tidb-lightning 自动将 TiKV 切换回“普通模式” (normal mode)，此后 TiDB 集群可以正常对外提供服务。 如果需要导入的目标集群是 v3.x 或以下的版本，需要使用 Importer-backend 来完成数据的导入。在这个模式下，tidb-lightning 需要将解析的键值对通过 gRPC 发送给 tikv-importer 并由 tikv-importer 完成数据的导入； TiDB Lightning 还支持使用 TiDB-backend 作为后端导入数据。TiDB-backend 使用和 Loader 类似，tidb-lightning 将数据转换为 INSERT 语句，然后直接在目标集群上执行这些语句。 使用Dumpling或Mydumper导出的数据为SQL文件。SQL文件分为三类： DB名-schema-cre1ate.sql：DB的schema创建语句文件 DB名.表名-schema.sql：表的schema创建语句文件 DB名.表名.sql：表的数据插入语句文件 三、注意 1、基础注意事项 TiDB Lightning 运行后，TiDB 集群将无法正常对外提供服务。 若 tidb-lightning 崩溃，集群会留在“导入模式”。若忘记转回“普通模式”，集群会产生大量未压缩的文件，继而消耗 CPU 并导致延迟。此时，需要使用 tidb-lightning-ctl 手动将集群转回“普通模式”： bin/tidb-lightning-ctl --switch-mode=normal 2、Lightning 需要下游 TiDB用户有如下权限： 权限 作用域 SELECT Tables INSERT Tables UPDATE Tables DELETE Tables CREATE Databases, tables DROP Databases, tables ALTER Tables 如果配置项 checksum = true，则 TiDB Lightning 需要有下游 TiDB admin 用户权限。 3、导入后端模式 TiDB Lightning 的后端 (用于接受 TiDB Lightning 解析结果) 决定 tidb-lightning 组件将如何把将数据导入到目标集群中。目前，TiDB Lightning 支持以下后端： Importer-backend（默认）：tidb-lightning 先将 SQL 或 CSV 数据编码成键值对，由 tikv-importer 对写入的键值对进行排序，然后把这些键值对 Ingest 到 TiKV 节点中。 Local-backend：tidb-lightning 先将数据编码成键值对并排序存储在本地临时目录，然后将这些键值对以 SST 文件的形式上传到各个 TiKV 节点，然后由 TiKV 将这些 SST 文件 Ingest 到集群中。和 Importer-backend 原理相同，不过不依赖额外的 tikv-importer 组件。 TiDB-backend：tidb-lightning 先将数据编码成 INSERT 语句，然后直接在 TiDB 节点上运行这些 SQL 语句进行数据导入。 后端 Local-backend Importer-backend TiDB-backend 速度 快 (~500 GB/小时) 快 (~400 GB/小时) 慢 (~50 GB/小时) 资源使用率 高 高 低 占用网络带宽 高 中 低 导入时是否满足 ACID 否 否 是 目标表 必须为空 必须为空 可以不为空 额外组件 无 tikv-importer 无 支持 TiDB 集群版本 >= v4.0.0 全部 全部 如果导入的目标集群为 v4.0 或以上版本，请优先考虑使用 Local-backend 模式。Local-backend 部署更简单并且性能也较其他两个模式更高 如果目标集群为 v3.x 或以下，则建议使用 Importer-backend 模式 如果需要导入的集群为生产环境线上集群，或需要导入的表中已包含有数据，则最好使用 TiDB-backend 模式 四、安装、命令参数及配置文件详解 1、下载安装 version=v4.0.5 && \\ curl -# https://download.pingcap.org/tidb-toolkit-$version-linux-amd64.tar.gz | tar -zxC /opt && \\ ln -s /opt/tidb-toolkit-$version-linux-amd64 /opt/tidb-toolkit-$version && \\ echo -e \"export TIDB_TOOLKIT_HOME=/opt/tidb-toolkit-$version\\nexport PATH=\\$PATH:\\$TIDB_TOOLKIT_HOME/bin\" >> /etc/profile && \\ source /etc/profile && \\ tidb-lightning -V 2、tidb-lightning参数 命令行参数生效优先级高于配置文件中的 参数 描述 对应配置项 --config file 从 file 读取全局设置。如果没有指定则使用默认设置。 -V 输出程序的版本 -d directory 读取数据的本地目录或外部存储 URL mydumper.data-source-dir -L level 日志的等级： debug、info、warn、error 或 fatal (默认为 info) lightning.log-level -f rule 表库过滤的规则 (可多次指定) mydumper.filter --backend backend 选择后端的模式：importer、local 或 tidb tikv-importer.backend --log-file file 日志文件路径（默认是 /tmp 中的临时文件） lightning.log-file --status-addr ip:port TiDB Lightning 服务器的监听地址 lightning.status-port --importer host:port TiKV Importer 的地址 tikv-importer.addr --pd-urls host:port PD endpoint 的地址 tidb.pd-addr --tidb-host host TiDB Server 的 host tidb.host --tidb-port port TiDB Server 的端口（默认为 4000） tidb.port --tidb-status port TiDB Server 的状态端口的（默认为 10080） tidb.status-port --tidb-user user 连接到 TiDB 的用户名 tidb.user --tidb-password password 连接到 TiDB 的密码 tidb.password --no-schema 忽略表结构文件，直接从 TiDB 中获取表结构信息 mydumper.no-schema --enable-checkpoint bool 是否启用断点 (默认值为 true) checkpoint.enable --analyze bool 导入后分析表信息 (默认值为 true) post-restore.analyze --checksum bool 导入后比较校验和 (默认值为 true) post-restore.checksum --check-requirements bool 开始之前检查集群版本兼容性（默认值为 true） lightning.check-requirements --ca file TLS 连接的 CA 证书路径 security.ca-path --cert file TLS 连接的证书路径 security.cert-path --key file TLS 连接的私钥路径 security.key-path --server-mode 在服务器模式下启动 TiDB Lightning lightning.server-mode 3、tidb-lightning-ctl参数 tablename 必须是db`.`tbl 中的限定表名（包括反引号）或关键词 all 所有 tidb-lightning 的参数也适用于 tidb-lightning-ctl 参数 描述 --compact 执行 full compact --switch-mode mode 将每个 TiKV Store 切换到指定模式（normal 或 import） --fetch-mode 打印每个 TiKV Store 的当前模式 --import-engine uuid 将 TiKV Importer 上关闭的引擎文件导入到 TiKV 集群 --cleanup-engine uuid 删除 TiKV Importer 上的引擎文件 --checkpoint-dump folder 将当前的断点以 CSV 格式存储到文件夹中 --checkpoint-error-destroy tablename 删除断点，如果报错则删除该表 --checkpoint-error-ignore tablename 忽略指定表中断点的报错 --checkpoint-remove tablename 无条件删除表的断点 4、tikv-importer参数 参数 描述 对应配置项 -C, --config file 从 file 读取配置。如果没有指定，则使用默认设置 -V, --version 输出程序的版本 -A, --addr ip:port TiKV Importer 服务器的监听地址 server.addr --status-server ip:port 状态服务器的监听地址 status-server-address --import-dir dir 引擎文件的存储目录 import.import-dir --log-level level 日志的等级： trace、debug、info、warn、error 或 off log-level --log-file file 日志文件路径 log-file 5、任务配置文件参数详解 [lightning] # 启动之前检查集群是否满足最低需求。 # check-requirements = true # 引擎文件的最大并行数。每张表被切分成一个用于存储索引的“索引引擎”和若干存储行数据的“数据引擎”。这两项设置控制两种引擎文件的最大并发数，会影响 tikv-importer 的内存和磁盘用量，两项数值之和不能超过 tikv-importer 的 max-open-engines 的设定。 index-concurrency = 2 table-concurrency = 6 # 数据的并发数。默认与逻辑 CPU 的数量相同。混合部署的情况下可以将其大小配置为逻辑 CPU 数的 75%，以限制 CPU 的使用。 # region-concurrency = # I/O 最大并发数。I/O 并发量太高时，会因硬盘内部缓存频繁被刷新。而增加 I/O 等待时间，导致缓存未命中和读取速度降低。对于不同的存储介质，此参数可能需要调整以达到最佳效率。 io-concurrency = 5 [security] # 指定集群中用于 TLS 连接的证书和密钥。CA 的公钥证书。如果留空，则禁用 TLS。 # ca-path = \"/path/to/ca.pem\" # 此服务的公钥证书。 # cert-path = \"/path/to/lightning.pem\" # 该服务的密钥。 # key-path = \"/path/to/lightning.key\" [checkpoint] # 是否启用断点续传。导入数据时，TiDB Lightning 会记录当前表导入的进度。所以即使 Lightning 或其他组件异常退出，在重启时也可以避免重复再导入已完成的数据。 enable = true # 存储断点的数据库名称。 schema = \"tidb_lightning_checkpoint\" # 存储断点的方式。 # - file：存放在本地文件系统。 # - mysql：存放在兼容 MySQL 的数据库服务器。 driver = \"file\" # dsn 是数据源名称 (data source name)，表示断点的存放位置。 # 若 driver = \"file\"，则 dsn 为断点信息存放的文件路径。若不设置该路径，则默认存储路径为“/tmp/CHECKPOINT_SCHEMA.pb”。 # 若 driver = \"mysql\"，则 dsn 为“用户:密码@tcp(地址:端口)/”格式的 URL。若不设置该 URL，则默认会使用 [tidb] 部分指定的 TiDB 服务器来存储断点。 # 为减少目标 TiDB 集群的压力，建议指定另一台兼容 MySQL 的数据库服务器来存储断点。 # dsn = \"/tmp/tidb_lightning_checkpoint.pb\" # 所有数据导入成功后是否保留断点。设置为 false 时为删除断点。保留断点有利于进行调试，但会泄漏关于数据源的元数据。 # keep-after-success = false [tikv-importer] # 选择后端：“importer” 或 “local” 或 “tidb” # backend = \"importer\" # 当后端是 “importer” 时，tikv-importer 的监听地址（需改为实际地址）。 addr = \"172.16.31.10:8287\" # 当后端是 “tidb” 时，插入重复数据时执行的操作。 # - replace：新数据替代已有数据 # - ignore：保留已有数据，忽略新数据 # - error：中止导入并报错 # on-duplicate = \"replace\" # 当后端是 “local” 时，控制生成 SST 文件的大小，最好跟 TiKV 里面的 Region 大小保持一致，默认是 96 MB。 # region-split-size = 100_663_296 # 当后端是 “local” 时，一次请求中发送的 KV 数量。 # send-kv-pairs = 32768 # 当后端是 “local” 时，本地进行 KV 排序的路径。如果磁盘性能较低（如使用机械盘），建议设置成与 `data-source-dir` 不同的磁盘，这样可有效提升导入性能。 # sorted-kv-dir = \"\" # 当后端是 “local” 时，TiKV 写入 KV 数据的并发度。当 TiDB Lightning 和 TiKV 直接网络传输速度超过万兆的时候，可以适当增加这个值。 # range-concurrency = 16 [mydumper] # 设置文件读取的区块大小，确保该值比数据源的最长字符串长。 read-block-size = 65536 # Byte (默认为 64 KB) # （源数据文件）单个导入区块大小的最小值。 # Lightning 根据该值将一张大表分割为多个数据引擎文件。 # batch-size = 107_374_182_400 # Byte (默认为 100 GB) # 引擎文件需按顺序导入。由于并行处理，多个数据引擎几乎在同时被导入， # 这样形成的处理队列会造成资源浪费。因此，为了合理分配资源，Lightning # 稍微增大了前几个区块的大小。该参数也决定了比例系数，即在完全并发下 # “导入”和“写入”过程的持续时间比。这个值可以通过计算 1 GB 大小的 # 单张表的（导入时长/写入时长）得到。在日志文件中可以看到精确的时间。 # 如果“导入”更快，区块大小的差异就会更小；比值为 0 时则说明区块大小一致。 # 取值范围为（0 Compact -> Analyze。 [post-restore] # 如果设置为 true，会对所有表逐个执行 `ADMIN CHECKSUM TABLE ` 操作 # 来验证数据的完整性。 checksum = true # 如果设置为 true，会在导入每张表后执行一次 level-1 Compact。 # 默认值为 false。 level-1-compact = false # 如果设置为 true，会在导入过程结束时对整个 TiKV 集群执行一次 full Compact。 # 默认值为 false。 compact = false # 如果设置为 true，会对所有表逐个执行 `ANALYZE TABLE ` 操作。 analyze = true # 设置周期性后台操作。 # 支持的单位：h（时）、m（分）、s（秒）。 [cron] # Lightning 自动刷新导入模式状态的持续时间，该值应小于 TiKV 对应的设定值。 switch-mode = \"5m\" # 在日志中打印导入进度的持续时间。 log-progress = \"5m\" 五、Web 界面 TiDB Lightning 支持在网页上查看导入进度或执行一些简单任务管理。启用服务器模式的方式有如下几种： 在启动 tidb-lightning 时加上命令行参数 --server-mode。 nohup tidb-lightning --server-mode --status-addr :8289 >> tidb-lightning-server.log 2>&1 & 在配置文件中设置 lightning.server-mode。 [lightning] server-mode = true status-addr = ':8289' TiDB Lightning 启动后，可以访问 http://127.0.0.1:8289 来管理程序 服务器模式下，TiDB Lightning 不会立即开始运行，而是通过用户在 web 页面提交（多个） TOML 格式的任务文件来导入数据。 六、断点续传 大量的数据导入一般耗时数小时至数天，长时间运行的进程会有一定机率发生非正常中断。如果每次重启都从头开始，就会浪费掉之前已成功导入的数据。为此，TiDB Lightning 提供了“断点续传”的功能，即使 tidb-lightning 崩溃，在重启时仍然接着之前的进度继续工作。 文档：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-checkpoints 1、断点续传的启用与配置 [checkpoint] # 启用断点续传。 # 导入时，TiDB Lightning 会记录当前进度。 # 若 TiDB Lightning 或其他组件异常退出，在重启时可以避免重复再导入已完成的数据。 enable = true # 存储断点的方式 # - file：存放在本地文件系统（要求 v2.1.1 或以上） # - mysql：存放在任何兼容 MySQL 5.7 或以上的数据库中，包括 MariaDB 和 TiDB driver = \"file\" # 存储断点的架构名称（数据库名称）仅在 driver = \"mysql\" 时生效 # schema = \"tidb_lightning_checkpoint\" # 断点的存放位置 # 若 driver = \"file\"，此参数为断点信息存放的文件路径。如果不设置该参数则默认为 `/tmp/CHECKPOINT_SCHEMA.pb` # 若 driver = \"mysql\"，此参数为数据库连接参数 (DSN)，格式为“用户:密码@tcp(地址:端口)/”。 # 默认会重用 [tidb] 设置目标数据库来存储断点。为避免加重目标集群的压力，建议另外使用一个兼容 MySQL 的数据库服务器。 # dsn = \"/tmp/tidb_lightning_checkpoint.pb\" # 导入成功后是否保留断点。默认为删除。保留断点可用于调试，但有可能泄漏数据源的元数据。 # keep-after-success = false 2、断点续传的控制 若 tidb-lightning 因不可恢复的错误而退出（例如数据出错），重启时不会使用断点，而是直接报错离开。为保证已导入的数据安全，这些错误必须先解决掉才能继续。使用 tidb-lightning-ctl 工具可以标示已经恢复。 ①从头开始整个导入过程 tidb-lightning-ctl --checkpoint-error-destroy=['`schema`.`table`' | all ] 该命令会让失败的表从头开始整个导入过程。选项中的架构和表名必须以反引号 (```) 包裹，而且区分大小写。 如果导入 schema`.`table 这个表曾经出错，这条命令会： 从目标数据库移除 (DROP) 这个表，清除已导入的数据。 将断点重设到“未开始”的状态。 如果 schema`.`table 没有出错，则无操作。 传入 all 会对所有表进行上述操作，这是最方便、安全但保守的断点错误解决方法。 ②忽略出错状态接着带导入 tidb-lightning-ctl --checkpoint-error-ignore='`schema`.`table`' && tidb-lightning-ctl --checkpoint-error-ignore=all 如果导入 schema`.`table 这个表曾经出错，这条命令会清除出错状态，如同没事发生过一样。传入 \"all\" 会对所有表进行上述操作。 注意： 除非确定错误可以忽略，否则不要使用这个选项。如果错误是真实的话，可能会导致数据不完全。启用校验和 (CHECKSUM) 可以防止数据出错被忽略。 ③无论是否有出错，把断点清除 tidb-lightning-ctl --checkpoint-remove='`schema`.`table`' && tidb-lightning-ctl --checkpoint-remove=all ④将所有断点备份到传入的文件夹 该参数主要用于技术支持。此选项仅于 driver = \"mysql\" 时有效。 tidb-lightning-ctl --checkpoint-dump=output/directory 七、实例 1、编写配置文件 Test.toml [mydumper] # 数据源目录 data-source-dir = \"/data/tidb-dumpling-export\" [tikv-importer] backend = \"tidb\" [tidb] host = \"192.168.1.2\" port = 4000 user = \"root\" password = \"*****\" 2、启动任务 在lightning的Web界面中提交任务配置文件 在命令行中运行任务配置文件 nohup tidb-lightning \\ -L info \\ --log-file /root/tidb-lightning-import-task.log \\ -config /root/Test.toml \\ > /root/tidb-lightning-import-task-nohup.log 2>&1 & 导入完毕后，TiDB Lightning 会自动退出。若导入成功，日志的最后一行会显示 tidb lightning exit Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-27 18:38:04 "},"origin/tidb-br-backup-restore.html":{"url":"origin/tidb-br-backup-restore.html","title":"TiDB-BR冷备份与恢复：分布式冷备份恢复数据","keywords":"","body":"TiDB 分布式备份恢复命令行工具BR 一、简介 BR 全称为 Backup & Restore，是 TiDB 分布式备份恢复的命令行工具，主要用于对 TiDB 集群进行数据备份和恢复。 相比 dumpling，BR 更适合大数据量的场景。 二、工作原理 BR 将备份或恢复操作命令下发到各个 TiKV 节点。TiKV 收到命令后执行相应的备份或恢复操作。 在一次备份或恢复中，各个 TiKV 节点都会有一个对应的备份路径，TiKV 备份时产生的备份文件将会保存在该路径下，恢复时也会从该路径读取相应的备份文件。 三、备份文件类型 备份路径下会生成以下两种类型文件： SST 文件：存储 TiKV 备份下来的数据信息 backupmeta 文件：存储本次备份的元信息，包括备份文件数、备份文件的 Key 区间、备份文件大小和备份文件 Hash (sha256) 值 backup.lock 文件：用于防止多次备份到同一目录 SST 文件命名格式 SST 文件以 storeID_regionID_regionEpoch_keyHash_cf 的格式命名。格式名的解释如下： storeID：TiKV 节点编号 regionID：Region 编号 regionEpoch：Region 版本号 keyHash：Range startKey 的 Hash (sha256) 值，确保唯一性 cf：RocksDB 的 ColumnFamily（默认为 default 或 write） 四、安装 1、推荐部署配置 推荐 BR 部署在 PD 节点上。 推荐使用一块高性能 SSD 网盘，挂载到 BR 节点和所有 TiKV 节点上，网盘推荐万兆网卡，否则带宽有可能成为备份恢复时的性能瓶颈。 2、安装 br_version=v4.0.8 && \\ curl -s -# https://download.pingcap.org/tidb-toolkit-$br_version-linux-amd64.tar.gz | tar zxvf - -C /opt && \\ ln -s /opt/tidb-toolkit-$br_version-linux-amd64/ /opt/tidb-toolkit && \\ echo -e \"export TIDB_TOOLKIT=/opt/tidb-toolkit\\nexport PATH=\\$PATH:\\$TIDB_TOOLKIT/bin\" >> /etc/profile && \\ source /etc/profile br --help 3、命令全局参数 全局参数: --ca string CA certificate path for TLS connection --cert string Certificate path for TLS connection --check-requirements Whether start version check before execute command (default true) --checksum Run checksum at end of task (default true) --gcs.credentials-file string (experimental) Set the GCS credentials file path --gcs.endpoint string (experimental) Set the GCS endpoint URL --gcs.predefined-acl string (experimental) Specify the GCS predefined acl for objects --gcs.storage-class string (experimental) Specify the GCS storage class for objects --key string Private key path for TLS connection --log-file string Set the log file path. If not set, logs will output to temp file (default \"/tmp/br.log.2021-04-20T17.02.38+0800\") --log-format string Set the log format (default \"text\") -L, --log-level string Set the log level (default \"info\") -u, --pd strings PD address (default [127.0.0.1:2379]) --ratelimit uint The rate limit of the task, MB/s per node --remove-tiflash Remove TiFlash replicas before backup or restore, for unsupported versions of TiFlash (default true) --s3.acl string (experimental) Set the S3 canned ACLs, e.g. authenticated-read --s3.endpoint string (experimental) Set the S3 endpoint URL, please specify the http or https scheme explicitly --s3.provider string (experimental) Set the S3 provider, e.g. aws, alibaba, ceph --s3.region string (experimental) Set the S3 region, e.g. us-east-1 --s3.sse string Set S3 server-side encryption, e.g. aws:kms --s3.sse-kms-key-id string KMS CMK key id to use with S3 server-side encryption.Leave empty to use S3 owned key. --s3.storage-class string (experimental) Set the S3 storage class, e.g. STANDARD -c, --send-credentials-to-tikv Whether send credentials to tikv (default true) --status-addr string Set the HTTP listening address for the status report service. Set to empty string to disable -s, --storage string specify the url where backup storage, eg, \"s3://bucket/path/prefix\" --switch-mode-interval duration maintain import mode on TiKV during restore (default 5m0s) 五、备份 0、备份命令参数 命令格式 br backup [子命令] 子命令: db 备份单个DB full 备份所有DB raw (experimental) backup a raw kv range from TiKV cluster table 备份单个表 参数： --backupts string the backup ts support TSO or datetime, 例如'400036290571534337', '2018-05-11 01:42:23' --compression string sst文件压缩算法，可选: lz4|zstd(默认)|snappy --compression-level int32 sst文件压缩级别 --gcttl int the TTL (in seconds) that PD holds for BR's GC safepoint (默认 300s) -h, --help help for backup --lastbackupts uint (实验功能) the last time backup ts, use for incremental backup, support TSO only --timeago duration The history version of the backup task, e.g. 1m, 1h. Do not exceed GCSafePoint 1、备份全部集群数据 br backup full \\ --pd \"${PDIP}:2379\" \\ --storage \"local:///data/tidb-br-database-export\" \\ --ratelimit 120 \\ --log-file backupfull.log 2、备份单个库 br backup db \\ --pd \"${PDIP}:2379\" \\ --db test \\ --storage \"local:///tmp/backup\" \\ --ratelimit 120 \\ --log-file backuptable.log Detail BR log in /data/tidb-br-database-export/backupdb.log Database backup 100.00% Checksum 100.00% [2021/04/20 21:15:10.704 +08:00] [INFO] [collector.go:60] [\"Database backup Success summary: total backup ranges: 1317, total success: 1317, total failed: 0, total take(Database backup time): 7m58.135071495s, total take(real time): 11m30.059039886s, total kv: 2693175024, total size(MB): 397915.67, avg speed(MB/s): 832.22\"] [\"backup checksum\"=3m28.860107739s] [\"backup fast checksum\"=593.123731ms] [\"backup total regions\"=7070] [BackupTS=424391166169710594] [Size=42165594435] 3、备份单个表 br backup table \\ --pd \"${PDIP}:2379\" \\ --db test \\ --table usertable \\ --storage \"local:///tmp/backup\" \\ --ratelimit 120 \\ --log-file backuptable.log 4、使用表库过滤功能备份多张表 br backup full \\ --pd \"${PDIP}:2379\" \\ --filter 'db*.tbl*' \\ --storage \"local:///tmp/backup\" \\ --ratelimit 120 \\ --log-file backupfull.log 5、增量备份 如果想要备份增量，只需要在备份的时候指定上一次的备份时间戳 --lastbackupts 即可。 注意增量备份有以下限制： 增量备份需要与前一次全量备份在不同的路径下 GC safepoint 必须在 lastbackupts 之前 br backup full\\ --pd ${PDIP}:2379 \\ -s local:///home/tidb/backupdata/incr \\ --lastbackupts ${LAST_BACKUP_TS} 以上命令会备份 (LAST_BACKUP_TS, current PD timestamp] 之间的增量数据。 你可以使用 validate 指令获取上一次备份的时间戳，示例如下： LAST_BACKUP_TS=`br validate decode --field=\"end-version\" -s local:///home/tidb/backupdata | tail -n1` 6、备份全部集群数据到 Amazon S3 后端存储 export AWS_ACCESS_KEY_ID=${AccessKey} export AWS_SECRET_ACCESS_KEY=${SecretKey} br backup full \\ --pd \"${PDIP}:2379\" \\ --storage \"s3://${Bucket}/${Folder}\" \\ --s3.region \"${region}\" \\ --send-credentials-to-tikv=true \\ --log-file backuptable.log 六、恢复 参考 https://docs.pingcap.com/zh/tidb/stable/backup-and-restore-tool Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-27 18:31:04 "},"origin/bigdata-migrate-operation.html":{"url":"origin/bigdata-migrate-operation.html","title":"大数据的数据迁移","keywords":"","body":"大数据的数据迁移 一、简介 迁移全量MySQL数据到TIDB。情况如下： 源库：RDS备份文件启动的MySQL实例，一个DB，数据量大约800张表，数据大小500+GB 目标库：TIDB集群 将源库中的数据 二、方案 1、Navicat的数据传输工具 直接使用Navicat的数据传输工具，配置数据源连接和目标源连接。 2、TiDB生态圈工具 TiDB Dumping导出：导出源MySQL中的数据为SQL文件 修改SQL文件命名：修改TiDB Dumping导出的SQL文件命名格式 TiDB Lighting导入：将SQL文件导入到TiDB TiDB Dumpling version=v4.0.5 && \\ curl -# https://download.pingcap.org/tidb-toolkit-$version-linux-amd64.tar.gz | tar -zxC /opt && \\ ln -s /opt/tidb-toolkit-$version-linux-amd64 /opt/tidb-toolkit-$version && \\ echo \"export PATH=/opt/tidb-toolkit-$version/bin:$PATH\" >> /etc/profile && \\ mkdir -p /data/dumping-export/sql && \\ nohup /opt/tidb-toolkit-v4.0.5-linux-amd64/bin/dumpling \\ -u 用于导出数据的用户 \\ # 用于导出数据的用户要拥有SELECT、RELOAD、LOCK TABLES、REPLICATION CLIENT服务器权限 -p 用于导出数据的用户密码 \\ -P 3306 \\ -h 192.168.1.4 \\ -B database \\ # 指定要导出的Database --filetype sql \\ # 指定导出文件类型（可为csv/sql） --threads 32 \\ # 指定备份并发线程数 -o /data/dumping-export/sql \\ # 指定导出文件存储路径 -F 256MiB \\ # 指定导出文件最大大小 --logfile /data/dumping-export/export-task.log >/data/dumping-export/dumpling-nohupout.log 2>&1 & 批量修改SQL文件 # 例如源库DB为Test，想把数据导入到目标库Test-2中 old_database_name=test new_database_name=Test-2 for i in $(ls /data/dumping-export/sql/*.sql | grep -v schema-create );do mv /data/dumping-export/sql/$i /data/dumping-export/sql/$new_database_name.${i#*.}; done mv /data/dumping-export/sql/${old_database_name}-schema-create.sql /data/dumping-export/sql/${new_database_name}-schema-create.sql echo \"\" > ${new_database_name}-schema-create.sql TiDB Lighting nohup /opt/tidb-toolkit-v4.0.5-linux-amd64/bin/tidb-lightning \\ -config /data/dumping-export/tidb-lightning.toml \\ --log-file /data/dumping-export/import-task.log > /data/dumping-export/lightning-nohupout.log 2>&1 & nohup tidb-lightning \\ -L info \\ --log-file /data/dumping-export/import-task.log \\ --backend local \\ --status-addr 10080 \\ --enable-checkpoint true \\ -d /data/dumping-export/sql \\ --tidb-host 192.168.1.4 \\ --tidb-port 4000 \\ --tidb-user root \\ --tidb-password ***** > /data/dumping-export/lightning-nohupout.log 2>&1 & 三、结论 大约三千万条记录的表，Navicat数据传输工具同步完耗时约3个小时，而使用TiDB生态圈的工具耗时26分钟 四、其他 1、查询DB下所有表的行数 由于从INFORMATION_SCHEMA.TABLES中显示的表的行数不准确，需要使用count函数来统计表的行数 SELECT CONCAT( 'SELECT \"', TABLE_NAME, '\", COUNT(*) FROM ', TABLE_SCHEMA, '.', TABLE_NAME, ' UNION ALL' ) EXEC_SQL FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'DB名字'; 上述SQL会输出用于统计表的SQL语句，复制以后，删除最后一行末尾的UNION ALL，然后执行 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-27 18:37:59 "},"origin/pulsar-basic.html":{"url":"origin/pulsar-basic.html","title":"基础概念","keywords":"","body":"Pulsar 一、简介 1、功能特性 2、Pulsar架构 Broker 一个或多个Broker 处理和负载接受到的生产者发送的消息数据 调度消息发送给消费者 与Zookeeper进行通信以处理各种协调任务， 将消息存储在BookKeeper实例（又称为bookies）中 依赖于ZooKeeper集群来执行某些任务， 等等 Apache Zookeeper（Standby/Cluster） Pulsar使用ZK存储元数据、集群配置，还有协调各Broker 协调由那个Broker响应数据处理 存储Topic主题的元数据 Apache BookKeeper（又称为bookies） 由一个或多个bookies组成的BookKeeper集群来存储需要持久化的消息数据，和消费者消费消息的游标offset Apache BookKeeper是一个分布式的WAL(write-ahead log)系统 二、基础概念 {persistent|non-persistent}://tenant/namespace/topic {持久化|非持久化}://租户/命名空间/主题 topic主题类型 持久化的 非持久化的 支持的消息压缩格式： LZ4 ZLIB ZSTD SNAPPY 生产者发送消息模式 同步：发送每条消息后，生产者将等待Broker的确认。如果未收到确认，则生产者将发送操作视为失败 异步：将把消息放于阻塞队列中，并立即返回。然后，客户端将在后台将消息发送给 broker 如果队列已满(最大大小可配置)，则调用 API 时，producer 可能会立即被阻止或失败，具体取决于传递给 producer 的参数。 消费者接受消息模式 同步 异步 订阅模式 exclusive(独家)：只允许有一个消费者 shared（共享）：允许多个消费者，消费者间机会均等。消息通过轮询机制分发给不同的消费者，并且每个消息仅会被分发给一个消费者。当消费者断开连接，所有发送给他，但没有被确认的消息将被重新安排，分发给其它存活的消费者。 failover(灾备)：允许多个消费者，消费者有主从之分，主消费者负责接受数据，主消费者挂掉以后，从消费者代替主消费者接着接受数据 key_shared：允许多个消费者，具有相同key或相同订阅key的消息仅传递给一个使用者。 不管消息被重新发送多少次，它都会被发送到同一使用者。当消费者连接或断开连接时，将导致服务的消费者更改某些消息键。 多主题订阅 当consumer订阅pulsar的主题时，它默认指定订阅了一个主题，例如：persistent://public/default/my-topic。 从Pulsar的1.23.0-incubating的版本开始，Pulsar消费者可以同时订阅多个topic 多主题订阅方式： 正则匹配：persistent://public/default/finance-.* 明确指定的topic列表 非持久化主题内的数据处理速度比持久化主题快 消息的默认保留，过期处理方式 立即删除消费者已确认的所有消息 以消息backlog的形式，持久保存所有的未被确认消息 Pulsar 支持保证一条消息只能在broker服务端被持久化一次的特性，即消息去重功能 参考 https://jack-vanlightly.com/blog/2018/10/2/understanding-how-apache-pulsar-works Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-21 11:20:53 "},"origin/pulsar-install.html":{"url":"origin/pulsar-install.html","title":"安装部署","keywords":"","body":"Apache Pulsar的安装部署 一、简介 二、二进制安装 1、prerequisite 三台机器硬件：8核16G内存100G系统磁盘500G数据磁盘的CentOS 7（数据磁盘挂载到/data目录） 三台机器IP地址：192.168.1.121~123 三台都为Zookeeper集群、Apache Bookkeeper集群、Broker集群 三台机器安装Java JDK（过程省略） 2、下载 pulsar_version=2.7.1 curl -s -# https://archive.apache.org/dist/pulsar/pulsar-$pulsar_version/apache-pulsar-$pulsar_version-bin.tar.gz | tar zxvf - -C /opt ln -s /opt/apache-pulsar-$pulsar_version /opt/pulsar echo \"export PULSAR_HOME=/opt/pulsar\\nexport PATH=$PATH:$PULSAR_HOME/bin\" >> /etc/profile mkdir -p /data/pulsar/{data/bookkeeper/journal,logs} /data/zookeeper/{data,logs} source /etc/profile 3、下载connectors pulsar_version=2.7.1 && \\ mkdir /opt/pulsar/connectors && \\ nohup wget https://apachemirror.sg.wuchna.com/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-kafka-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.kddi-research.jp/infosystems/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-redis-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://mirror-hk.koddos.net/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-netty-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.tsukuba.wide.ad.jp/software/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-jdbc-mariadb-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.wayne.edu/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-file-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-elastic-search-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-kafka-connect-adaptor-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.tsukuba.wide.ad.jp/software/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-canal-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.kddi-research.jp/infosystems/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-influxdb-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://apache.website-solution.net/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-rabbitmq-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.yz.yamagata-u.ac.jp/pub/network/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-hdfs2-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-hdfs3-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-jdbc-postgres-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! 之后将整个/opt/apache-pulsar-2.7.1目录拷贝到另外台主机上，设置一下软连，配置一下系统变量 for i in {2..3};do scp -r /opt/apache-pulsar-2.7.1 root@192.168.1.12$i:/opt/ ; ssh root@192.168.1.12$i -c 'ln -s /opt/apache-pulsar-2.7.1 /opt/pulsar && ' done 4、部署Zookeeper集群 sed -i \\ -e 's/dataDir=data\\/zookeeper/dataDir=\\/data\\/zookeeper\\/data/g' \\ -e '$a server.1=192.168.1.121:2888:3888\\nserver.2=192.168.1.122:2888:3888\\nserver.3=192.168.1.123:2888:3888\\n' \\ /opt/pulsar/conf/zookeeper.conf && \\ echo 1 > /data/zookeeper/data/myid && \\ PULSAR_EXTRA_OPTS=\"-Dstats_server_port=8001\" pulsar-daemon start zookeeper && \\ jps -l && \\ netstat -lanp|grep 2181 上述命令三台机器要执行 5、初始化pulsar集群元数据到Zookeeper中 下述命令只用执行一遍即可 pulsar initialize-cluster-metadata \\ --cluster pulsar-cluster-prod \\ --zookeeper 192.168.1.121:2181 \\ --configuration-store 192.168.1.121:2181 \\ --web-service-url http://192.168.1.121:8080,192.168.1.122:8080,192.168.1.123:8080 \\ --broker-service-url pulsar://192.168.1.121:6650,192.168.1.122:6650,192.168.1.123:6650 6、部署Apache Bookkeeper sed -i \\ -e 's/journalDirectory=data\\/bookkeeper\\/journal/journalDirectory=\\/data\\/pulsar\\/data\\/bookkeeper\\/journal/g' \\ -e 's/ledgerDirectories=data\\/bookkeeper\\/ledgers/ledgerDirectories=\\/data\\/pulsar\\/data\\/bookkeeper\\/ledgers/g' \\ -e 's/zkServers=localhost:2181/zkServers=192.168.1.121:2181,192.168.1.122:2181,192.168.1.123:2181/g' \\ /opt/pulsar/conf/bookkeeper.conf && \\ pulsar-daemon start bookie && \\ jps -l 7、部署Broker sed -i \\ -e 's/zookeeperServers=/zookeeperServers=192.168.1.121:2181,192.168.1.122:2181,192.168.1.123:2181/g' \\ -e 's/configurationStoreServers=/configurationStoreServers=192.168.1.121:2181,192.168.1.122:2181,192.168.1.123:2181/g' \\ -e 's/clusterName=/clusterName=pulsar-cluster-prod/g' \\ /opt/pulsar/conf/broker.conf && \\ pulsar-daemon start broker && \\ jps -l 8、测试 ①配置客户端配置 sed -i \\ -e 's/webServiceUrl=http:\\/\\/localhost:8080\\//webServiceUrl=http:\\/\\/192.168.1.121:8080,192.168.1.122:8080,192.168.1.123:8080\\//g' \\ -e 's/brokerServiceUrl=pulsar:\\/\\/localhost:6650\\//brokerServiceUrl=pulsar:\\/\\/192.168.1.121:6650,192.168.1.122:6650,192.168.1.123:6650\\//g' \\ /opt/pulsar/conf/client.conf ②配置命名空间的权限 pulsar-admin namespaces set-persistence -a 3 -e 3 -w 3 -r 3 public/default pulsar-admin namespaces get-persistence public/default ③创建消费者消费消息 pulsar-client consume persistent://public/default/test -n 100 -s \"consumer-test\" -t \"Exclusive\" ④创建生产者产生消息 pulsar-client produce persistent://public/default/test -n 1 -m \"Hello Pulsar\" 三、Docker 四、Kubernetes helm repo add pulsar https://pulsar.apache.org/charts helm update helm search repo pulsar -l latest_version=$(helm search repo pulsar -l | grep -v \"CHART VERSION\" | awk '{print $3}' | sort -n | tail -1) helm show values pulsar/pulsar > pulsar-$latest_version-values.yaml helm upgrade --install pulsar -n pulsar -f pulsar-$latest_version-values.yaml pulsar/pulsar 五、Pulsar Manager Github：https://github.com/apache/pulsar-manager#access-pulsar-manager 以二进制方式安装为例，docker或k8s相关安装配置的参考：https://github.com/apache/pulsar-manager#access-pulsar-manager 和 https://github.com/apache/pulsar-manager/blob/master/src/README.md 1、下载安装 pulsar_manager_version=0.2.0 curl -s -# https://dist.apache.org/repos/dist/release/pulsar/pulsar-manager/pulsar-manager-0.2.0/apache-pulsar-manager-$pulsar_manager_version-bin.tar.gz | tar zxvf - -C /tmp && \\ tar -xvf /tmp/pulsar-manager/pulsar-manager.tar -C /opt && \\ cp -r /tmp/pulsar-manager/dist /opt/pulsar-manager/ui && \\ rm -rf /tmp/pulsar-manager 2、编辑配置文件 只修改/opt/pulsar-manager/application.properties的以下配置项，其他不用 # 开启Swagger swagger.enabled=true # 设置默认集群 default.environment.name=pulsar-cluster-prod default.environment.service_url=http://127.0.0.1:8080 3、启动 nohup /opt/pulsar-manager/bin/pulsar-manager >/dev/null 2>&1 & 4、设置用户名密码 CSRF_TOKEN=$(curl http://localhost:7750/pulsar-manager/csrf-token) && echo $CSRF_TOKEN curl \\ -H 'X-XSRF-TOKEN: $CSRF_TOKEN' \\ -H 'Cookie: XSRF-TOKEN=$CSRF_TOKEN;' \\ -H \"Content-Type: application/json\" \\ -X PUT http://localhost:7750/pulsar-manager/users/superuser \\ -d '{\"name\": \"用户名\", \"password\": \"密码\", \"description\": \"Administrator\", \"email\": \"邮箱地址\"}' 5、访问 http://pulsar-manager服务器地址:7750/ui/index.html http://pulsar-manager服务器地址:7750/swagger-ui.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-11 21:46:46 "},"origin/pulsar-cli.html":{"url":"origin/pulsar-cli.html","title":"Pulsar的CLI命令","keywords":"","body":"Apache Pulsar的命令行 一、pulsar-admin pulsar-admin工具，可以用来管理Pulsar的集群、Brokers、名称空间，租户等。 参考文档：https://pulsar.apache.org/docs/en/pulsar-admin/#list-failure-domains 1、命令格式 pulsar-admin 命令 子命令 参数 子命令 broker-stats：收集Brokers的统计信息 brokers：操作Brokers clusters：操作集群 functions：操作Pulsar函数 functions-worker：收集Pulsar函数Brokers的统计信息 namespaces：操作管理命令空间 ns-isolation-policy：操作管理命令空间的隔离策略 sources： sinks topics：操作管理主题 tenants：操作管理多租户 resource-quotas：操作管理资源配额 schemas：操作管理主题关联的模式 2、常用操作 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-19 22:34:27 "},"origin/pulsar-perf-test.html":{"url":"origin/pulsar-perf-test.html","title":"Pulsar性能测试","keywords":"","body":"一、方案 使用官方压测工具pulsar-perf ，利用多线程模拟生产者和消费者在并发情况下发送或消费数据的情形，以测试pulsar的读写性能 详细文档参考：https://pulsar.apache.org/docs/en/performance-pulsar-perf/ 命令参数：https://pulsar.apache.org/docs/en/reference-cli-tools/#pulsar-perf 二、测试指标 作为生产者并发写入不同类型Topic的TPS 作为消费者并发读取消息的QPS 三、测试环境 基础 三台8核，16G内存，100G SSD系统盘(4800IOPS)、1T 高效数据盘(5000 IOPS)的阿里云ECS服务器 操作系统为Ubuntu 20.04，均安装JDK 11.0.10 数据盘以LVM挂载/data路径下 IP地址：192.168.170.121~123 Pulsar Pulsar版本：2.7.1 使用内置Zookeeper Apache Zookeeper、Apache Bookkeeper 、Pulsar Broker均以集群形式分布在三台集群上。 Apache Zookeeper、Apache Bookkeeper 、Pulsar Broker的JVM -Xms -Xmx均设置为8Gb 测试命令 在192.168.170.121上执行测试命令 四、测试命令及结果 1、并发写入持久化Topic pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 5000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 persistent://public/default/perf-test11 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 4073.5 msg/s --- 127.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 472.911 ms - med: 103.862 - 95pct: 1210.079 - 99pct: 1223.351 - 99.9pct: 1236.391 - 99.99pct: 1244.527 - Max: 1249.999 Throughput produced: 5000.5 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 227.782 ms - med: 7.242 - 95pct: 1557.415 - 99pct: 1883.111 - 99.9pct: 1968.999 - 99.99pct: 1977.207 - Max: 1978.335 Throughput produced: 5000.7 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 363.717 ms - med: 8.834 - 95pct: 1605.079 - 99pct: 1796.095 - 99.9pct: 1856.335 - 99.99pct: 1864.623 - Max: 1865.927 Throughput produced: 4296.8 msg/s --- 134.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 151.693 ms - med: 7.453 - 95pct: 1121.647 - 99pct: 1417.671 - 99.9pct: 1486.287 - 99.99pct: 1492.615 - Max: 1493.135 Throughput produced: 5704.3 msg/s --- 178.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 249.943 ms - med: 8.440 - 95pct: 1692.911 - 99pct: 1980.311 - 99.9pct: 2052.695 - 99.99pct: 2068.639 - Max: 2073.943 pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 8000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 persistent://public/default/perf-test 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 6552.2 msg/s --- 204.8 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 8.609 ms - med: 6.987 - 95pct: 16.778 - 99pct: 37.272 - 99.9pct: 68.692 - 99.99pct: 72.774 - Max: 73.817 Throughput produced: 8772.0 msg/s --- 274.1 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 596.719 ms - med: 273.773 - 95pct: 1966.231 - 99pct: 2160.895 - 99.9pct: 2225.311 - 99.99pct: 2235.519 - Max: 2238.799 Throughput produced: 7993.3 msg/s --- 249.8 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 537.369 ms - med: 168.972 - 95pct: 1815.615 - 99pct: 2078.911 - 99.9pct: 2164.095 - 99.99pct: 2177.231 - Max: 2179.247 Throughput produced: 6447.4 msg/s --- 201.5 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 600.288 ms - med: 336.561 - 95pct: 1775.967 - 99pct: 1957.775 - 99.9pct: 2022.439 - 99.99pct: 2033.391 - Max: 2034.239 Throughput produced: 9575.8 msg/s --- 299.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 728.759 ms - med: 482.111 - 95pct: 2022.175 - 99pct: 2234.175 - 99.9pct: 2332.463 - 99.99pct: 2341.391 - Max: 2345.439 Throughput produced: 6827.5 msg/s --- 213.4 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 756.798 ms - med: 463.187 - 95pct: 2216.655 - 99pct: 2511.359 - 99.9pct: 2572.815 - 99.99pct: 2586.511 - Max: 2590.655 2、并发写入非持久化Topic pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 5000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 non-persistent://public/default/perf-test6 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 4562.4 msg/s --- 142.6 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.718 ms - med: 0.718 - 95pct: 1.162 - 99pct: 1.228 - 99.9pct: 2.022 - 99.99pct: 8.470 - Max: 11.320 Throughput produced: 4999.8 msg/s --- 156.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.709 ms - med: 0.702 - 95pct: 1.149 - 99pct: 1.198 - 99.9pct: 3.810 - 99.99pct: 12.648 - Max: 23.229 Throughput produced: 5000.2 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.701 ms - med: 0.701 - 95pct: 1.145 - 99pct: 1.193 - 99.9pct: 1.802 - 99.99pct: 7.321 - Max: 15.233 Throughput produced: 4999.9 msg/s --- 156.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.706 ms - med: 0.705 - 95pct: 1.151 - 99pct: 1.203 - 99.9pct: 2.960 - 99.99pct: 5.138 - Max: 5.723 Throughput produced: 5000.1 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.703 ms - med: 0.701 - 95pct: 1.148 - 99pct: 1.194 - 99.9pct: 3.385 - 99.99pct: 5.997 - Max: 6.320 Throughput produced: 4999.9 msg/s --- 156.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.697 ms - med: 0.697 - 95pct: 1.147 - 99pct: 1.194 - 99.9pct: 1.298 - 99.99pct: 2.889 - Max: 4.888 pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 8000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 non-persistent://public/default/perf-test8 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 7329.1 msg/s --- 229.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.710 ms - med: 0.703 - 95pct: 1.159 - 99pct: 1.230 - 99.9pct: 3.044 - 99.99pct: 10.701 - Max: 13.688 Throughput produced: 8000.3 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.699 ms - med: 0.694 - 95pct: 1.149 - 99pct: 1.199 - 99.9pct: 2.149 - 99.99pct: 7.919 - Max: 11.846 Throughput produced: 8000.8 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.697 ms - med: 0.693 - 95pct: 1.146 - 99pct: 1.198 - 99.9pct: 2.326 - 99.99pct: 5.694 - Max: 6.710 Throughput produced: 8000.9 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.741 ms - med: 0.700 - 95pct: 1.156 - 99pct: 1.228 - 99.9pct: 12.492 - 99.99pct: 22.429 - Max: 33.309 Throughput produced: 8000.7 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.696 ms - med: 0.693 - 95pct: 1.146 - 99pct: 1.196 - 99.9pct: 1.295 - 99.99pct: 5.035 - Max: 5.478 Throughput produced: 8000.7 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.696 ms - med: 0.693 - 95pct: 1.146 - 99pct: 1.195 - 99.9pct: 1.279 - 99.99pct: 5.308 - Max: 8.603 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 16:33:19 "},"origin/pulsar-kafka-kop.html":{"url":"origin/pulsar-kafka-kop.html","title":"Pulsar的Kafka协议适配器KoP","keywords":"","body":"Pulsar的Kafka协议适配器KoP 一、简介 为了能平滑、不改写代码、无侵入地迁移应用使用pulsar，KoP以插件形式支持Kafka协议。 Github地址：https://github.com/streamnative/kop 二、部署配置 从Pulsar的2.6.2.0开始，KoP x.y.z.m基于Pulsar x.y.z，而m是补丁版本号。 1、下载KoP 下载KoP的nar包到各个Broker节点 mkdir /opt/pulsar/protocol-handler && \\ wget https://github.com/streamnative/kop/releases/download/v2.7.1.5/pulsar-protocol-handler-kafka-2.7.1.5.nar -P /opt/pulsar/protocol-handler 2、配置Broker的配置文件 pulsar_broker_config_file=/opt/pulsar/conf/broker.conf sed -i '/allowAutoTopicCreationType=non-partitioned/d' $pulsar_broker_config_file echo \"### --- KoP Configuration----\" >> $pulsar_broker_config_file echo \"messagingProtocols=kafka\" >> $pulsar_broker_config_file echo \"protocolHandlerDirectory=./protocol-handler\" >> $pulsar_broker_config_file echo \"kafkaListeners=PLAINTEXT://$(ip a |grep eth0|grep inet|awk '{print $2}'|awk -F\"/\" '{print $1}'):9092\" >> $pulsar_broker_config_file echo \"allowAutoTopicCreationType=partitioned\" >> $pulsar_broker_config_file echo \"brokerEntryMetadataInterceptors=org.apache.pulsar.common.intercept.AppendIndexMetadataInterceptor\" >> $pulsar_broker_config_file echo \"advertisedAddress=$(ip a |grep eth0|grep inet|awk '{print $2}'|awk -F\"/\" '{print $1}')\" >> $pulsar_broker_config_file 3、重启Pulsar Broker节点 kill -9 `jps -l |grep \"org.apache.pulsar.PulsarBrokerStarter\" |awk '{print $1}'` ; sleep 3 && \\ pulsar-daemon start broker && \\ tail -f /data/pulsar/logs/pulsar-broker-$(hostname -s).pulsar.prod.log 4、验证 ①验证Broker节点是否开起9092端口 netstat -lanp|grep 9092 && \\ jps -l | grep \"org.apache.pulsar.PulsarBrokerStarter\" ②使用kaf工具 kaf config add-cluster prod-pulsar -b pulsar_broker_ip:9092 kaf config select-cluster -c kaf config select-cluster kaf topic create kop1 -p 10 -r 1 echo \"hello pulsar kop\" | kaf produce kop1 kaf consume kop1 -f ③使用kafka原生客户端 创建Topic kafka-topics.sh --bootstrap-server pulsar_broker_ip:9092 --create --replication 1 --partitions 5 --topic kop 创建生产者 kafka-console-producer.sh --bootstrap-server pulsar_broker_ip:9092 --topic kop 创建消费者 kafka-console-consumer.sh --bootstrap-server pulsar_broker_ip:9092 --topic kop --from-beginning 三、KoP的配置详解 其他配置项参考：https://github.com/streamnative/kop/blob/master/docs/configuration.md 配置项 含义 默认值 messagingProtocols kafka null protocolHandlerDirectory KoP NAR文件相对于安装路径所处的目录路径 ./protocols allowAutoTopicCreationType KoP仅支持分区的Topic。因此，最好设置为partitioned。如果默认情况下将其设置为未分区，则KoP自动创建的主题仍为分区主题。但是，由Pulsar Broker自动创建的主题是未分区的主题。 non-partitioned Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-05-27 13:48:31 "},"origin/golang-basic.html":{"url":"origin/golang-basic.html","title":"基础语法","keywords":"","body":"一、struct与json之间的互转 type Test struct { A string `json:\"a` B int `json:\"b\"` } struct转json var t Test t.A=\"测试\" t.B=2 jsonBytes, err := json.Marshal(t) if err != nil { fmt.Println(err) } fmt.Println(string(jsonBytes)) json转struct jsonStr := `{ \"a\": \"测试\", \"b\": 2 }` var t Test json.Unmarshal([]byte(jsonStr), &t) fmt.Println(t) 二、主函数的初始化 var ( router *gin.Engine ) func init (){ router = gin.Default() } func main(){ router.GET(\"/\",func(context *gin.Context) {}) } 三、字符串的处理 1、分割字符串 str=\"aaaaaaaa\\r\\nBBBBBBBBBBB\\r\\n1111\" split_str := strings.Split(string(res[0:len(res)]), \"\\r\\n\") # split_str类型为字符串数组 2、判断字符串前缀是否包含指定字符 str=\"aaaaaaaa\\r\\nBBBBBBBBBBB\\r\\n1111\" res := strings.HasPrefix(s, \"aaa\") // res为布尔值 四、命令行参数的设置 import (\"flag\" ) var ( omhost string omport string ompasswd string ) func init (){ flag.StringVar(&omhost, \"host\", \"\", \"OpenVPN服务端地址\") flag.StringVar(&omport, \"port\", \"\", \"OpenVPN服务端管理端口，默认为空\") flag.StringVar(&ompasswd, \"passwd\", \"\", \"OpenVPN服务端管理端口密码\") flag.Parse() } func main(){ if omhost == \"\" && omport == \"\" { fmt.Println(\"请在启动命令后添加'-host','-port'参数设置\") os.Exit(0) } else if omhost == \"\" { fmt.Println(\"请在启动命令后添加'-host'参数设置IP地址\") os.Exit(0) } else if omport == \"\" { fmt.Println(\"请在启动命令后添加'-port'参数设置管理端口号\") os.Exit(0) } } 五、数组的遍历 var test_array = [5]float32{1000.0, 2.0, 3.4, 7.0, 50.0} for _,a := range test_array { fmt.Println(a) } var test_array = [3]string{\"test1\",\"test2\"} for _,a := range test_array { fmt.Println(a) } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-18 18:30:07 "},"origin/golang-concurrent-programming.html":{"url":"origin/golang-concurrent-programming.html","title":"go的并发","keywords":"","body":" 在一个函数调用前加上“go”关键词，那么本地调用就会在一个新的goroutine 中并发执行 如果该函数有返回值，则会被抛弃 函数结束，goroutine 也会结束 不要通过共享内存来通信，而应该通过通信来共享内存 go通过消息机制而非共享内存作为并发单元间的通信方式，这个消息机制在go中被称为channel 消息机制认为每一个并发单元都是一个自包含、独立的个体，并且有自己不共享的变量。 每个并发单元的输入输出都只能是消息。 channel是go在语言级别为goroutine 间的通信方式 channel是类型相关的，一个channel只能传递一种类型的值 channel语法 声明 var a chan int：声明一个传递类型为int的channel var b map[string] chan bool ：声明了一个map，元素是bool类型的channel 初始化 a := make(chan int)：声明并初始化一个int类型名为a的channel 读取与写入 a ：将1写入名为a的channel value := ：从名为a的channel中读取数据到value中 设置限制大小带有缓冲的channel 在需要传输大量数据的场景下，传递单个数据的channel就不合适啦 ch := make(chan init ,1024)：声明并创建一个大小1024的int类型的channel 在没有读取方的时候，写入方可以一直写，直到填充完channel前都不会阻塞 单向只读只写channel 默认情况下，通道 channel 是双向的，也就是，既可以往里面发送数据也可以同里面接收数据。但是，我们经常见一个通道作为参数进行传递而只希望对方是单向使用的，要么只让它发送数据，要么只让它接收数据，这时候我们可以指定通道的方向。而所谓的单向channel，可以理解为对channel的限制。例如限制某个函数只能往某个channel中写入数据 如果直接使用make创建单向channel（ch := make()，就毫无意义。通常声明初始化一个正常双向的channel，再使用类型转换创建单向的 ch := make(chan int) // 声明一个只能写入数据的通道类型, 并赋值为ch var chSendOnly chan func producer(out chan 关闭channel 关闭channel直接使用close()即可，但是如何确认channel是否已经关闭?可通过在读取时使用多重返回值的方式进行判断 close(ch) x , ok := 不要从接收端关闭channel，也不要关闭有多个并发发送者的channel。换句话说，如果sender(发送者)只是唯一的sender或者是channel最后一个活跃的sender，那么你应该在sender的goroutine关闭channel，从而通知receiver(s)(接收者们)已经没有值可以读了。维持这条原则将保证永远不会发生向一个已经关闭的channel发送值或者关闭一个已经关闭的channel。 channel的读写堵塞、超时问题的解决 问题：如果往channel中写数据，此时发现channel满了；如果从channel中读取数据，此时发现channel是空的，如果此时没有处理逻辑，会造成个goroutine 堵塞锁死 解决：使用select实现给channel的读写设置超时机制 func main() { ch := make(chan int) quit := make(chan bool) //新开一个协程 go func() { for { select { // select的每个 case 语句里必须是一个 IO 操作 // 如果ch成功读到数据，则进行该case处理语句 case num := Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-15 10:37:47 "},"origin/go-gin.html":{"url":"origin/go-gin.html","title":"Web框架Gin的使用总结","keywords":"","body":"Golang的Web框架Gin使用总结 一、使用BasicAuth中间件限制指定接口 router := gin.Default() authorizedRoute = router.Group(\"/\", gin.BasicAuth(gin.Accounts{ \"admin\": \"123456\", //用户名：密码 \"root\": \"aaaaaaa\", })) // Group函数注册了一个群组路由，gin.BasicAuth是中间件，参数gin.Accounts是一个map[string]string类型的映射，用来记录用户名和密码。 authorizedRoute.StaticFile(\"/favicon.ico\", \"./public/favicon.ico\") authorizedRoute.GET(\"/secrets\", func(c *gin.Context) { // get user, it was set by the BasicAuth middleware user := c.MustGet(gin.AuthUserKey).(string) if secret, ok := secrets[user]; ok { c.JSON(http.StatusOK, gin.H{\"user\": user, \"secret\": secret}) } else { c.JSON(http.StatusOK, gin.H{\"user\": user, \"secret\": \"NO SECRET :(\"}) } }) 二、重定向路由 router := gin.Default() router.GET(\"/\", func(context *gin.Context) { context.Request.URL.Path = \"/public\" router.HandleContext(context) }) // 或者重定向到外部连接 router.GET(\"/test\", func(context *gin.Context) { context.Redirect(http.StatusMovedPermanently, \"http://www.google.com/\") }) // 重定向POST请求 router.POST(\"/test\", func(context *gin.Context) { context.Redirect(http.StatusFound, \"/foo\") }) 三、获取请求中的数据 1、获取URL路径 ①获取单个或多个请求URL路径 router := gin.Default() // 只会匹配“/user/john” 不会匹配 “/user/“ 或者 “/user“ router.GET(\"/user/:name\", func(context *gin.Context) { name := context.Param(\"name\") context.String(http.StatusOK, \"Hello %s\", name) }) // However, this one will match /user/john/ and also /user/john/send // If no other routers match /user/john, it will redirect to /user/john/ router.GET(\"/user/:name/*action\", func(context *gin.Context) { name := context.Param(\"name\") action := context.Param(\"action\") message := name + \" is \" + action context.String(http.StatusOK, message) }) // 对于每个匹配的请求，Context将保留路由定义 router.POST(\"/user/:name/*action\", func(context *gin.Context) { context.FullPath() == \"/user/:name/*action\" // true }) 2、获取URL参数 ①请求中的参数 /welcome?firstname=Jane&lastname=Doe router := gin.Default() router.GET(\"/welcome\", func(context *gin.Context) { firstname := context.DefaultQuery(\"firstname\", \"默认值\") lastname := context.Query(\"lastname\") // 也可以写成 context.Request.URL.Query().Get(\"lastname\") context.String(http.StatusOK, \"Hello %s %s\", firstname, lastname) }) ②请求中的参数数组 /post?ids[a]=1234&ids[b]=hello router := gin.Default() router.GET(\"/welcome\", func(context *gin.Context) { ids := context.QueryMap(\"ids\") context.String(http.StatusOK, \"Hello %s %s\", firstname, lastname) }) 3、获取POST请求表单数据 ①表单的多个数据 POST /post?id=1234&page=1 HTTP/1.1 Content-Type: application/x-www-form-urlencoded name=manu&message=this_is_great router := gin.Default() router.POST(\"/post\", func(context *gin.Context) { name := context.PostForm(\"name\") message := context.PostForm(\"message\") fmt.Printf(\"name: %s; message: %s\", name, message) }) ②表单的数组数据： POST /post?ids[a]=1234&ids[b]=hello HTTP/1.1 Content-Type: application/x-www-form-urlencoded names[first]=thinkerou&names[second]=tianou router := gin.Default() router.POST(\"/post\", func(context *gin.Context) { names := context.PostFormMap(\"names\") fmt.Printf(\"names: %v\", names) }) 四、文件上传 1、单个文件上传 router := gin.Default() // Set a lower memory limit for multipart forms (default is 32 MiB) router.MaxMultipartMemory = 8 curl -X POST http://localhost:8080/upload \\ -F \"file=@/Users/curiouser/test.txt\" \\ -H \"Content-Type: multipart/form-data\" 2、多个文件上传 router := gin.Default() // Set a lower memory limit for multipart forms (default is 32 MiB) router.MaxMultipartMemory = 8 curl -X POST http://localhost:8080/upload \\ -F \"upload[]=@/Users/curiouser/test1.txt\" \\ -F \"upload[]=@/Users/curiouser/test2.txt\" \\ -H \"Content-Type: multipart/form-data\" 五、绑定请求数据到结构体中 1、绑定请求路径到结构体中 GET /thinkerou/987fbc97-4bed-5078-9f07-9141ba07c9f3 GET /thinkerou/not-uuid type Person struct { ID string `uri:\"id\" binding:\"required,uuid\"` Name string `uri:\"name\" binding:\"required\"` } route := gin.Default() route.GET(\"/:name/:id\", func(context *gin.Context) { var person Person if err := context.ShouldBindUri(&person); err != nil { context.JSON(400, gin.H{\"msg\": err}) return } }) 2、绑定请求参数到结构体中 GET /testing?name=curiouser&address=xyz&birthday=1993&createTime=123&unixTime=15622 type Person struct { Name string `form:\"name\"` Address string `form:\"address\"` Birthday time.Time `form:\"birthday\" time_format:\"2006-01-02\" time_utc:\"1\"` CreateTime time.Time `form:\"createTime\" time_format:\"unixNano\"` UnixTime time.Time `form:\"unixTime\" time_format:\"unix\"` } route := gin.Default() route.GET(\"/testing\",func(context *gin.Context) { var person Person if context.ShouldBind(&person) == nil { log.Println(person.Name) log.Println(person.Address) log.Println(person.Birthday) log.Println(person.CreateTime) log.Println(person.UnixTime) } }) 3、绑定请求Header到结构体中 curl -H \"rate:300\" -H \"domain:music\" 127.0.0.1:8080/test type testHeader struct { Rate int `header:\"Rate\"` Domain string `header:\"Domain\"` } route := gin.Default() route.GET(\"/test\", func(context *gin.Context) { h := testHeader{} if err := context.ShouldBindHeader(&h); err != nil { context.JSON(200, err) } }) 4、绑定HTML复选框中的数据到结构体 Check some colors Red Green Blue type colorsForm struct { Colors []string `form:\"colors[]\"` } route := gin.Default() route.GET(\"/test\", func(context *gin.Context) { var foo colorsForm context.ShouldBind(&foo) context.JSON(200, gin.H{\"color\": foo.Colors}) }) 5、绑定表单数据到结构体 curl -X POST -v --form name=user --form \"avatar=@./avatar.png\" http://localhost:8080/profile type ProfileForm struct { Name string `form:\"name\" binding:\"required\"` Avatar *multipart.FileHeader `form:\"avatar\" binding:\"required\"` // or for multiple files // Avatars []*multipart.FileHeader `form:\"avatar\" binding:\"required\"` } router := gin.Default() router.POST(\"/profile\", func(context *gin.Context) { // you can bind multipart form with explicit binding declaration: // c.ShouldBindWith(&form, binding.Form) // or you can simply use autobinding with ShouldBind method: var form ProfileForm // in this case proper binding will be automatically selected if err := context.ShouldBind(&form); err != nil { context.String(http.StatusBadRequest, \"bad request\") return } err := context.SaveUploadedFile(form.Avatar, form.Avatar.Filename) if err != nil { context.String(http.StatusInternalServerError, \"unknown error\") return } // db.Save(&form) context.String(http.StatusOK, \"ok\") }) 六、使用中间件 1、全局设置中间件 router := gin.Default() router.Use(gin.Logger()) router.Use(gin.Recovery()) 2、单个路由设置中间件 router := gin.Defalut() authorized := router.Group(\"/login\") authorized.Use(AuthRequired()) // 简短写法 // authorized = router.Group(\"/login\",gin.BasicAuth(gin.Accounts{ // \"admin\": \"111\", // )) // 或者 router.GET(\"/benchmark\", MyBenchLogger(), benchEndpoint) 七、设置接口支持跨域 router.GET(\"/login\",func(context *gin.Context) { context.Writer.Header().Set(\"Access-Control-Allow-Origin\", \"*\") context.Header(\"Access-Control-Allow-Origin\", \"*\") // 设置允许访问所有域 context.Header(\"Access-Control-Allow-Methods\", \"POST, GET, OPTIONS, PUT, DELETE,UPDATE\") context.Header(\"Access-Control-Allow-Headers\", \"Authorization, Content-Length, X-CSRF-Token, Token,session,X_Requested_With,Accept, Origin, Host, Connection, Accept-Encoding, Accept-Language,DNT, X-CustomHeader, Keep-Alive, User-Agent, X-Requested-With, If-Modified-Since, Cache-Control, Content-Type, Pragma,token,openid,opentoken\") context.Header(\"Access-Control-Expose-Headers\", \"Content-Length, Access-Control-Allow-Origin, Access-Control-Allow-Headers,Cache-Control,Content-Language,Content-Type,Expires,Last-Modified,Pragma,FooBar\") context.Header(\"Access-Control-Max-Age\", \"172800\") context.Header(\"Access-Control-Allow-Credentials\", \"false\") context.Set(\"content-type\", \"application/json\") //设置返回格式是json } 参考：https://www.cnblogs.com/you-men/p/14054348.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-22 13:31:33 "},"origin/golang-statik.html":{"url":"origin/golang-statik.html","title":"statik-将静态资源文件打包到二进制文件中","keywords":"","body":"statik 一、简介 对于Web项目，经常需要将少量的静态资源文件（HTML、JavaScript、CSS）编译到项目二进制文件，而go build是无法直接将静态资源文件编译成二进制文件的。所以此时需要第三方工具 GitHub：https://github.com/rakyll/statik 二、使用 1、安装 go get github.com/rakyll/statik go install github.com/rakyll/statik 2、编译指定目录下的静态资源文件为二进制 statik -src=/path/to/your/project/public 会在项目根目录下生成statik/static.go // Code generated by statik. DO NOT EDIT. package statik import ( \"github.com/rakyll/statik/fs\" ) func init() { data := \"PK\\\\x03\\x04\\x14......省略\" fs.Register(data) } 3、main中引用 package main import ( _ \"github.com/项目名/statik\" \"github.com/rakyll/statik/fs\" ) func main (){ //statikFS为http.FileSystem类型 statikFS, err := fs.New() if err != nil { log.Fatal(err) } http.Handle(\"/public/\", http.StripPrefix(\"/public/\", http.FileServer(statikFS))) // 或者在gin框架下router.StaticFS(\"/public\", statikFS) } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-14 09:35:18 "},"origin/js-kits.html":{"url":"origin/js-kits.html","title":"JavaScript常用工具函数","keywords":"","body":"JavaScript常用工具函数 1、把字节数字转换成易于阅读格式 function getBytesSize(size) { if (!size) return \"\"; var num = 1024.00; //byte if (size 2、将秒转换为时分秒 function formatDuring(s){ var days = parseInt(s / ( 60 * 60 * 24)); var hours = parseInt((s % ( 60 * 60 * 24)) / ( 60 * 60)); var minutes = parseInt((s % ( 60 * 60)) / 60); var seconds = (s % 60) ; return days + \" 天 \" + hours + \" 小时 \" + minutes + \" 分钟 \"; } · Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-14 09:35:15 "}}